title,comment,vote,views,bookmarks,created,modified,languages
"How do I test a class that has private methods, fields or inner classes?","
                
How do I use JUnit to test a class that has internal private methods, fields or nested classes?
It seems bad to change the access modifier for a method just to be able to run a test.
    If you have somewhat of a legacy Java application, and you're not allowed to change the visibility of your methods, the best way to test private methods is to use reflection.
Internally we're using helpers to get/set private and private static variables as well as invoke private and private static methods. The following patterns will let you do pretty much anything related to the private methods and fields. Of course, you can't change private static final variables through reflection.
Method method = TargetClass.getDeclaredMethod(methodName, argClasses);
method.setAccessible(true);
return method.invoke(targetObject, argObjects);

And for fields:
Field field = TargetClass.getDeclaredField(fieldName);
field.setAccessible(true);
field.set(object, value);



Notes:

TargetClass.getDeclaredMethod(methodName, argClasses) lets you look into private methods. The same thing applies for
getDeclaredField.
The setAccessible(true) is required to play around with privates.


    The best way to test a private method is via another public method. If this cannot be done, then one of the following conditions is true:


The private method is dead code
There is a design smell near the class that you are testing
The method that you are trying to test should not be private

    When I have private methods in a class that are sufficiently complicated that I feel the need to test the private methods directly, that is a code smell: my class is too complicated.  

My usual approach to addressing such issues is to tease out a new class that contains the interesting bits.  Often, this method and the fields it interacts with, and maybe another method or two can be extracted in to a new class.  

The new class exposes these methods as 'public', so they're accessible for unit testing.  The new and old classes are now both simpler than the original class, which is great for me (I need to keep things simple, or I get lost!).

Note that I'm not suggesting that people create classes without using their brain! The point here is to use the forces of unit testing to help you find good new classes.  
    I have used reflection to do this for Java in the past, and in my opinion it was a big mistake.

Strictly speaking, you should not be writing unit tests that directly test private methods. What you should be testing is the public contract that the class has with other objects; you should never directly test an object's internals. If another developer wants to make a small internal change to the class, which doesn't affect the classes public contract, he/she then has to modify your reflection based test to ensure that it works. If you do this repeatedly throughout a project, unit tests then stop being a useful measurement of code health, and start to become a hindrance to development, and an annoyance to the development team.

What I recommend doing instead is using a code coverage tool such as Cobertura, to ensure that the unit tests you write provide decent coverage of the code in private methods. That way, you indirectly test what the private methods are doing, and maintain a higher level of agility.
    From this article: Testing Private Methods with JUnit and SuiteRunner (Bill Venners), you basically have 4 options:


Don't test private methods.
Give the methods package access.
Use a nested test class.
Use reflection.


    Generally a unit test is intended to exercise the public interface of a class or unit. Therefore, private methods are implementation detail that you would not expect to test explicitly.
    In the Spring Framework you can test private methods using this method:

ReflectionTestUtils.invokeMethod()


For example:

ReflectionTestUtils.invokeMethod(TestClazz, ""createTest"", ""input data"");

    I want to share a rule I have about testing which particularly is related to this topic:

I think that you should never adapt production code in order to
indulge easer writing of tests.

There are a few suggestions in other posts to adapt the original class in order to test a private method. If you change accessibility of a method/field to package private or protected, just in order to have it accessible to tests, then you defeat the purpose of existence of private access directive.
Why should we have private fields/methods/classes at all, if we want to have test driven development? Should we declare everything as package private, or even public then, so we can test without any effort? - I don't think so.
From another point view. Tests should not burden performance and execution of the production application. If you change production code just for the sake of easier testing then you can burden performance and the execution of the application in some way. And if you start to change private access to package private, you may eventually come up to ""ingenious ideas"" about adding some more code to the original class, which would make additional noise to readability and can burden performance of the application.
On the other hand, with changing private access to some less restrictive, you are opening possibility to a developer for misuse new situation in the future development of the application. Instead of enforcing him to develop in the proper way, you are opening him new possibilities and giving him ability to make wrong choices in the future.
Of course there might be a few exceptions to this rule, but with clear understanding what is the rule and what is the exception and why it is done.
    PowerMockito is made for this.
Use maven dependency
    <dependency>
        <groupId>org.powermock</groupId>
        <artifactId>powermock-core</artifactId>
        <version>2.0.7</version>
        <scope>test</scope>
    </dependency>

Then you can do
import org.powermock.reflect.Whitebox;
...
MyClass sut = new MyClass();
SomeType rval = Whitebox.invokeMethod(sut, ""myPrivateMethod"", params, moreParams);

    Just two examples of where I would want to test a private method:


Decryption routines - I would not
want to make them visible to anyone to see just for
the sake of testing, else anyone can
use them to decrypt. But they are
intrinsic to the code, complicated,
and need to always work (the obvious exception is reflection which can be used to view even private methods in most cases, when SecurityManager is not configured to prevent this).
Creating an SDK for community
consumption. Here public takes on a
wholly different meaning, since this
is code that the whole world may see
(not just internal to my application). I put
code into private methods if I don't
want the SDK users to see it - I
don't see this as code smell, merely
as how SDK programming works. But of
course I still need to test my
private methods, and they are where
the functionality of my SDK actually
lives.


I understand the idea of only testing the ""contract"". But I don't see one can advocate actually not testing code - your mileage may vary.

So my tradeoff involves complicating the JUnits with reflection, rather than compromising my security & SDK.
    Another approach I have used is to change a private method to package private or protected then complement it with the @VisibleForTesting annotation of the Google Guava library.

This will tell anybody using this method to take caution and not access it directly even in a package. Also a test class need not be in same package physically, but in the same package under the test folder.

For example, if a method to be tested is in src/main/java/mypackage/MyClass.java then your test call should be placed in src/test/java/mypackage/MyClassTest.java. That way, you got access to the test method in your test class.
    The private methods are called by a public method, so the inputs to your public methods should also test private methods that are called by those public methods. When a public method fails, then that could be a failure in the private method.
    To test legacy code with large and quirky classes, it is often very helpful to be able to test the one private (or public) method I'm writing right now.

I use the junitx.util.PrivateAccessor-package for Java . Lots of helpful one-liners for accessing private methods and private fields.

import junitx.util.PrivateAccessor;

PrivateAccessor.setField(myObjectReference, ""myCrucialButHardToReachPrivateField"", myNewValue);
PrivateAccessor.invoke(myObjectReference, ""privateMethodName"", java.lang.Class[] parameterTypes, java.lang.Object[] args);

    Having tried Cem Catikkas' solution using reflection for Java, I'd have to say his was a more elegant solution than I have described here. However, if you're looking for an alternative to using reflection, and have access to the source you're testing, this will still be an option.

There is possible merit in testing private methods of a class, particularly with test-driven development, where you would like to design small tests before you write any code.

Creating a test with access to private members and methods can test areas of code which are difficult to target specifically with access only to public methods. If a public method has several steps involved, it can consist of several private methods, which can then be tested individually.

Advantages:


Can test to a finer granularity


Disadvantages:


Test code must reside in the same
file as source code, which can be
more difficult to maintain
Similarly with .class output files, they must remain within the same package as declared in source code


However, if continuous testing requires this method, it may be a signal that the private methods should be extracted, which could be tested in the traditional, public way.

Here is a convoluted example of how this would work:

// Import statements and package declarations

public class ClassToTest
{
    private int decrement(int toDecrement) {
        toDecrement--;
        return toDecrement;
    }

    // Constructor and the rest of the class

    public static class StaticInnerTest extends TestCase
    {
        public StaticInnerTest(){
            super();
        }

        public void testDecrement(){
            int number = 10;
            ClassToTest toTest= new ClassToTest();
            int decremented = toTest.decrement(number);
            assertEquals(9, decremented);
        }

        public static void main(String[] args) {
            junit.textui.TestRunner.run(StaticInnerTest.class);
        }
    }
}


The inner class would be compiled to ClassToTest$StaticInnerTest.

See also: Java Tip 106: Static inner classes for fun and profit
    If using Spring, ReflectionTestUtils provides some handy tools that help out here with minimal effort. For example, to set up a mock on a private member without being forced to add an undesirable public setter:

ReflectionTestUtils.setField(theClass, ""theUnsettableField"", theMockObject);

    Please see below for an example;

The following import statement should be added:

import org.powermock.reflect.Whitebox;


Now you can directly pass the object which has the private method, method name to be called, and additional parameters as below.

Whitebox.invokeMethod(obj, ""privateMethod"", ""param1"");

    Private methods are consumed by public ones. Otherwise, they're dead code. That's why you test the public method, asserting the expected results of the public method and thereby, the private methods it consumes.

Testing private methods should be tested by debugging before running your unit tests on public methods.

They may also be debugged using test-driven development, debugging your unit tests until all your assertions are met.

I personally believe it is better to create classes using TDD; creating the public method stubs, then generating unit tests with all the assertions defined in advance, so the expected outcome of the method is determined before you code it. This way, you don't go down the wrong path of making the unit test assertions fit the results. Your class is then robust and meets requirements when all your unit tests pass.
    As others have said... don't test private methods directly. Here are a few thoughts:


Keep all methods small and focused (easy to test, easy to find what is wrong)
Use code coverage tools. I like Cobertura (oh happy day, looks like a new version is out!)


Run the code coverage on the unit tests. If you see that methods are not fully tested add to the tests to get the coverage up. Aim for 100% code coverage, but realize that you probably won't get it.
    If you're trying to test existing code that you're reluctant or unable to change, reflection is a good choice.

If the class's design is still flexible, and you've got a complicated private method that you'd like to test separately, I suggest you pull it out into a separate class and test that class separately. This doesn't have to change the public interface of the original class; it can internally create an instance of the helper class and call the helper method.

If you want to test difficult error conditions coming from the helper method, you can go a step further. Extract an interface from the helper class, add a public getter and setter to the original class to inject the helper class (used through its interface), and then inject a mock version of the helper class into the original class to test how the original class responds to exceptions from the helper. This approach is also helpful if you want to test the original class without also testing the helper class.
    I tend not to test private methods.  There lies madness.  Personally, I believe you should only test your publicly exposed interfaces (and that includes protected and internal methods).  
    If you're using JUnit, have a look at junit-addons. It has the ability to ignore the Java security model and access private methods and attributes.
    I would suggest you refactoring your code a little bit. When you have to start thinking about using reflection or other kind of stuff, for just testing your code, something is going wrong with your code.

You mentioned different types of problems. Let's start with private fields. In case of private fields I would have added a new constructor and injected fields into that. Instead of this:

public class ClassToTest {

    private final String first = ""first"";
    private final List<String> second = new ArrayList<>();
    ...
}


I'd have used this:

public class ClassToTest {

    private final String first;
    private final List<String> second;

    public ClassToTest() {
        this(""first"", new ArrayList<>());
    }

    public ClassToTest(final String first, final List<String> second) {
        this.first = first;
        this.second = second;
    }
    ...
}


This won't be a problem even with some legacy code. Old code will be using an empty constructor, and if you ask me, refactored code will look cleaner, and you'll be able to inject necessary values in test without reflection.

Now about private methods. In my personal experience when you have to stub a private method for testing, then that method has nothing to do in that class. A common pattern, in that case, would be to wrap it within an interface, like Callable and then you pass in that interface also in the constructor (with that multiple constructor trick):

public ClassToTest() {
    this(...);
}

public ClassToTest(final Callable<T> privateMethodLogic) {
    this.privateMethodLogic = privateMethodLogic;
}


Mostly all that I wrote looks like it's a dependency injection pattern. In my personal experience it's really useful while testing, and I think that this kind of code is cleaner and will be easier to maintain. I'd say the same about nested classes. If a nested class contains heavy logic it would be better if you'd moved it as a package private class and have injected it into a class needing it.

There are also several other design patterns which I have used while refactoring and maintaining legacy code, but it all depends on cases of your code to test. Using reflection mostly is not a problem, but when you have an enterprise application which is heavily tested and tests are run before every deployment everything gets really slow (it's just annoying and I don't like that kind of stuff).

There is also setter injection, but I wouldn't recommended using it. I'd better stick with a constructor and initialize everything when it's really necessary, leaving the possibility for injecting necessary dependencies.
    Hey use this utility class if you are on spring.

ReflectionTestUtils.invokeMethod(new ClassName(), ""privateMethodName"");

    The answer from JUnit.org FAQ page:


  But if you must...
  
  If you are using JDK 1.3 or higher, you can use reflection to subvert
  the access control mechanism with the aid of the PrivilegedAccessor.
  For details on how to use it, read this article.
  
  If you are using JDK 1.6 or higher and you annotate your tests with
  @Test, you can use Dp4j to inject reflection in your test methods. For
  details on how to use it, see this test script.


P.S. I'm the main contributor to Dp4j, ask me if you need help. :)
    If you want to test private methods of a legacy application where you can't change the code, one option for Java is jMockit, which will allow you to create mocks to an object even when they're private to the class.
    Here is my generic function to test private fields:

protected <F> F getPrivateField(String fieldName, Object obj)
    throws NoSuchFieldException, IllegalAccessException {
    Field field =
        obj.getClass().getDeclaredField(fieldName);

    field.setAccessible(true);
    return (F)field.get(obj);
}

    Testing private methods breaks the encapsulation of your class because every time you change the internal implementation you break client code (in this case, the tests).

So don't test private methods.
    Today, I pushed a Java library to help testing private methods and fields. It has been designed with Android in mind, but it can really be used for any Java project.

If you got some code with private methods or fields or constructors, you can use BoundBox. It does exactly what you are looking for.
Here below is an example of a test that accesses two private fields of an Android activity to test it:

@UiThreadTest
public void testCompute() {

    // Given
    boundBoxOfMainActivity = new BoundBoxOfMainActivity(getActivity());

    // When
    boundBoxOfMainActivity.boundBox_getButtonMain().performClick();

    // Then
    assertEquals(""42"", boundBoxOfMainActivity.boundBox_getTextViewMain().getText());
}


BoundBox makes it easy to test private/protected fields, methods and constructors. You can even access stuff that is hidden by inheritance. Indeed, BoundBox breaks encapsulation. It will give you access to all that through reflection, BUT everything is checked at compile time.

It is ideal for testing some legacy code. Use it carefully. ;)

https://github.com/stephanenicolas/boundbox
    Detail my sample with lombok as below. private field, private method:
public static void main(String[] args) throws NoSuchFieldException, SecurityException, IllegalArgumentException, IllegalAccessException, NoSuchMethodException, InvocationTargetException {
    Student student = new Student();

    Field privateFieldName = Student.class.getDeclaredField(""name"");
    privateFieldName.setAccessible(true);
    privateFieldName.set(student, ""Naruto"");

    Field privateFieldAge = Student.class.getDeclaredField(""age"");
    privateFieldAge.setAccessible(true);
    privateFieldAge.set(student, ""28"");

    System.out.println(student.toString());

    Method privateMethodGetInfo = Student.class.getDeclaredMethod(""getInfo"", String.class, String.class);
    privateMethodGetInfo.setAccessible(true);
    System.out.println(privateMethodGetInfo.invoke(student, ""Sasuke"", ""29""));
}


@Setter
@Getter
@ToString
class Student {
  private String name;
  private String age;

  private String getInfo(String name, String age) {
    return name + ""-"" + age;
  }
}

    I recently had this problem and wrote a little tool, called Picklock, that avoids the problems of explicitly using the Java reflection API, two examples:

Calling methods, e.g. private void method(String s) - by Java reflection

Method method = targetClass.getDeclaredMethod(""method"", String.class);
method.setAccessible(true);
return method.invoke(targetObject, ""mystring"");


Calling methods, e.g. private void method(String s) - by Picklock

interface Accessible {
  void method(String s);
}

...
Accessible a = ObjectAccess.unlock(targetObject).features(Accessible.class);
a.method(""mystring"");


Setting fields, e.g. private BigInteger amount; - by Java reflection

Field field = targetClass.getDeclaredField(""amount"");
field.setAccessible(true);
field.set(object, BigInteger.valueOf(42));


Setting fields, e.g. private BigInteger amount; - by Picklock

interface Accessible {
  void setAmount(BigInteger amount);
}

...
Accessible a = ObjectAccess.unlock(targetObject).features(Accessible.class);
a.setAmount(BigInteger.valueOf(42));

    ","[3054, 1781, 763, 363, 316, 233, 144, 45, 9, 16, 74, 44, 64, 39, 33, 27, 14, 28, 28, 26, 17, 17, 15, 4, 18, 18, 13, 20, 12, 3, 9]",1100342,899,2008-08-29T16:11:09,2022-04-23 18:05:12Z,java 
Is it possible to apply CSS to half of a character?,"
                
What I am looking for:

A way to style one HALF of a character. (In this case, half the letter being transparent)

What I have currently searched for and tried (With no luck):


Methods for styling half of a character/letter
Styling part of a character with CSS or JavaScript
Apply CSS to 50% of a character


Below is an example of what I am trying to obtain.



Does a CSS or JavaScript solution exist for this, or am I going to have to resort to images? I would prefer not to go the image route as this text will end up being generated dynamically.



UPDATE:

Since many have asked why I would ever want to style half of a character, this is why. My city had recently spent $250,000 to define a new ""brand"" for itself. This logo is what they came up with. Many people have complained about the simplicity and lack of creativity and continue to do so. My goal was to come up with this website as a joke. Type in 'Halifax' and you will see what I mean.
    Now on GitHub as a Plugin!

 Feel free to fork and improve.

Demo | Download Zip | Half-Style.com (Redirects to GitHub)




Pure CSS for a Single Character
JavaScript used for automation across text or multiple characters
Preserves Text Accessibility for screen readers for the blind or visually
impaired


Part 1: Basic Solution



Demo: http://jsfiddle.net/arbel/pd9yB/1694/



This works on any dynamic text, or a single character, and is all automated. All you need to do is add a class on the target text and the rest is taken care of.

Also, the accessibility of the original text is preserved for screen readers for the blind or visually impaired.

Explanation for a single character:

Pure CSS. All you need to do is to apply .halfStyle class to each element that contains the character you want to be half-styled.

For each span element containing the character, you can create a data attribute, for example here data-content=""X"", and on the pseudo element use content: attr(data-content); so the .halfStyle:before class will be dynamic and you won't need to hard code it for every instance.

Explanation for any text:

Simply add textToHalfStyle class to the element containing the text.



// jQuery for automated mode
jQuery(function($) {
    var text, chars, $el, i, output;

    // Iterate over all class occurences
    $('.textToHalfStyle').each(function(idx, el) {
    $el = $(el);
    text = $el.text();
    chars = text.split('');

    // Set the screen-reader text
    $el.html('<span style=""position: absolute !important;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);"">' + text + '</span>');

    // Reset output for appending
    output = '';

    // Iterate over all chars in the text
    for (i = 0; i < chars.length; i++) {
        // Create a styled element for each character and append to container
        output += '<span aria-hidden=""true"" class=""halfStyle"" data-content=""' + chars[i] + '"">' + chars[i] + '</span>';
    }

    // Write to DOM only once
    $el.append(output);
  });
});.halfStyle {
    position: relative;
    display: inline-block;
    font-size: 80px; /* or any font size will work */
    color: black; /* or transparent, any color */
    overflow: hidden;
    white-space: pre; /* to preserve the spaces from collapsing */
}

.halfStyle:before {
    display: block;
    z-index: 1;
    position: absolute;
    top: 0;
    left: 0;
    width: 50%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow: hidden;
    color: #f00;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>

<p>Single Characters:</p>
<span class=""halfStyle"" data-content=""X"">X</span>
<span class=""halfStyle"" data-content=""Y"">Y</span>
<span class=""halfStyle"" data-content=""Z"">Z</span>
<span class=""halfStyle"" data-content=""A"">A</span>

<hr/>
<p>Automated:</p>

<span class=""textToHalfStyle"">Half-style, please.</span>


(JSFiddle demo)



Part 2: Advanced solution - Independent left and right parts



With this solution you can style left and right parts, individually and independently.

Everything is the same, only more advanced CSS does the magic.

jQuery(function($) {
    var text, chars, $el, i, output;

    // Iterate over all class occurences
    $('.textToHalfStyle').each(function(idx, el) {
        $el = $(el);
        text = $el.text();
        chars = text.split('');

        // Set the screen-reader text
        $el.html('<span style=""position: absolute !important;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);"">' + text + '</span>');

        // Reset output for appending
        output = '';

        // Iterate over all chars in the text
        for (i = 0; i < chars.length; i++) {
            // Create a styled element for each character and append to container
            output += '<span aria-hidden=""true"" class=""halfStyle"" data-content=""' + chars[i] + '"">' + chars[i] + '</span>';
        }

        // Write to DOM only once
        $el.append(output);
    });
});.halfStyle {
    position: relative;
    display: inline-block;
    font-size: 80px; /* or any font size will work */
    color: transparent; /* hide the base character */
    overflow: hidden;
    white-space: pre; /* to preserve the spaces from collapsing */
}

.halfStyle:before { /* creates the left part */
    display: block;
    z-index: 1;
    position: absolute;
    top: 0;
    width: 50%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow: hidden;
    pointer-events: none; /* so the base char is selectable by mouse */
    color: #f00; /* for demo purposes */
    text-shadow: 2px -2px 0px #af0; /* for demo purposes */
}

.halfStyle:after { /* creates the right part */
    display: block;
    direction: rtl; /* very important, will make the width to start from right */
    position: absolute;
    z-index: 2;
    top: 0;
    left: 50%;
    width: 50%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow: hidden;
    pointer-events: none; /* so the base char is selectable by mouse */
    color: #000; /* for demo purposes */
    text-shadow: 2px 2px 0px #0af; /* for demo purposes */
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>
<p>Single Characters:</p>
<span class=""halfStyle"" data-content=""X"">X</span>
<span class=""halfStyle"" data-content=""Y"">Y</span>
<span class=""halfStyle"" data-content=""Z"">Z</span>
<span class=""halfStyle"" data-content=""A"">A</span>

<hr/>
<p>Automated:</p>

<span class=""textToHalfStyle"">Half-style, please.</span>


(JSFiddle demo)





Part 3: Mix-Match and Improve

Now that we know what is possible, let's create some variations.



-Horizontal Half Parts


Without Text Shadow:


Possibility of Text Shadow for each half part independently:




// jQuery for automated mode
jQuery(function($) {
    var text, chars, $el, i, output;

    // Iterate over all class occurences
    $('.textToHalfStyle').each(function(idx, el) {
        $el = $(el);
        text = $el.text();
        chars = text.split('');

        // Set the screen-reader text
        $el.html('<span style=""position: absolute !important;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);"">' + text + '</span>');

        // Reset output for appending
        output = '';

        // Iterate over all chars in the text
        for (i = 0; i < chars.length; i++) {
            // Create a styled element for each character and append to container
            output += '<span aria-hidden=""true"" class=""halfStyle"" data-content=""' + chars[i] + '"">' + chars[i] + '</span>';
        }

        // Write to DOM only once
        $el.append(output);
    });
});.halfStyle {
  position: relative;
  display: inline-block;
  font-size: 80px; /* or any font size will work */
  color: transparent; /* hide the base character */
  overflow: hidden;
  white-space: pre; /* to preserve the spaces from collapsing */
}

.halfStyle:before { /* creates the top part */
  display: block;
  z-index: 2;
  position: absolute;
  top: 0;
  height: 50%;
  content: attr(data-content); /* dynamic content for the pseudo element */
  overflow: hidden;
  pointer-events: none; /* so the base char is selectable by mouse */
  color: #f00; /* for demo purposes */
  text-shadow: 2px -2px 0px #af0; /* for demo purposes */
}

.halfStyle:after { /* creates the bottom part */
  display: block;
  position: absolute;
  z-index: 1;
  top: 0;
  height: 100%;
  content: attr(data-content); /* dynamic content for the pseudo element */
  overflow: hidden;
  pointer-events: none; /* so the base char is selectable by mouse */
  color: #000; /* for demo purposes */
  text-shadow: 2px 2px 0px #0af; /* for demo purposes */
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>
<p>Single Characters:</p>
<span class=""halfStyle"" data-content=""X"">X</span>
<span class=""halfStyle"" data-content=""Y"">Y</span>
<span class=""halfStyle"" data-content=""Z"">Z</span>
<span class=""halfStyle"" data-content=""A"">A</span>

<hr/>
<p>Automated:</p>

<span class=""textToHalfStyle"">Half-style, please.</span>


(JSFiddle demo)





-Vertical 1/3 Parts


Without Text Shadow:


Possibility of Text Shadow for each 1/3 part independently:




// jQuery for automated mode
jQuery(function($) {
    var text, chars, $el, i, output;

    // Iterate over all class occurences
    $('.textToHalfStyle').each(function(idx, el) {
    $el = $(el);
    text = $el.text();
    chars = text.split('');

    // Set the screen-reader text
    $el.html('<span style=""position: absolute !important;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);"">' + text + '</span>');

    // Reset output for appending
    output = '';

    // Iterate over all chars in the text
    for (i = 0; i < chars.length; i++) {
        // Create a styled element for each character and append to container
        output += '<span aria-hidden=""true"" class=""halfStyle"" data-content=""' + chars[i] + '"">' + chars[i] + '</span>';
    }

    // Write to DOM only once
    $el.append(output);
  });
});.halfStyle { /* base char and also the right 1/3 */
    position: relative;
    display: inline-block;
    font-size: 80px; /* or any font size will work */
    color: transparent; /* hide the base character */
    overflow: hidden;
    white-space: pre; /* to preserve the spaces from collapsing */
    color: #f0f; /* for demo purposes */
    text-shadow: 2px 2px 0px #0af; /* for demo purposes */
}

.halfStyle:before { /* creates the left 1/3 */
    display: block;
    z-index: 2;
    position: absolute;
    top: 0;
    width: 33.33%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow: hidden;
    pointer-events: none; /* so the base char is selectable by mouse */
    color: #f00; /* for demo purposes */
    text-shadow: 2px -2px 0px #af0; /* for demo purposes */
}

.halfStyle:after { /* creates the middle 1/3 */
    display: block;
    z-index: 1;
    position: absolute;
    top: 0;
    width: 66.66%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow: hidden;
    pointer-events: none; /* so the base char is selectable by mouse */
    color: #000; /* for demo purposes */
    text-shadow: 2px 2px 0px #af0; /* for demo purposes */
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>

<p>Single Characters:</p>
<span class=""halfStyle"" data-content=""X"">X</span>
<span class=""halfStyle"" data-content=""Y"">Y</span>
<span class=""halfStyle"" data-content=""Z"">Z</span>
<span class=""halfStyle"" data-content=""A"">A</span>

<hr/>
<p>Automated:</p>

<span class=""textToHalfStyle"">Half-style, please.</span>


(JSFiddle demo)





-Horizontal 1/3 Parts


Without Text Shadow:


Possibility of Text Shadow for each 1/3 part independently:




// jQuery for automated mode
jQuery(function($) {
    var text, chars, $el, i, output;

    // Iterate over all class occurences
    $('.textToHalfStyle').each(function(idx, el) {
    $el = $(el);
    text = $el.text();
    chars = text.split('');

    // Set the screen-reader text
    $el.html('<span style=""position: absolute !important;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);"">' + text + '</span>');

    // Reset output for appending
    output = '';

    // Iterate over all chars in the text
    for (i = 0; i < chars.length; i++) {
        // Create a styled element for each character and append to container
        output += '<span aria-hidden=""true"" class=""halfStyle"" data-content=""' + chars[i] + '"">' + chars[i] + '</span>';
    }

    // Write to DOM only once
    $el.append(output);
  });
});.halfStyle { /* base char and also the bottom 1/3 */
  position: relative;
  display: inline-block;
  font-size: 80px; /* or any font size will work */
  color: transparent;
  overflow: hidden;
  white-space: pre; /* to preserve the spaces from collapsing */
  color: #f0f;
  text-shadow: 2px 2px 0px #0af; /* for demo purposes */
}

.halfStyle:before { /* creates the top 1/3 */
  display: block;
  z-index: 2;
  position: absolute;
  top: 0;
  height: 33.33%;
  content: attr(data-content); /* dynamic content for the pseudo element */
  overflow: hidden;
  pointer-events: none; /* so the base char is selectable by mouse */
  color: #f00; /* for demo purposes */
  text-shadow: 2px -2px 0px #fa0; /* for demo purposes */
}

.halfStyle:after { /* creates the middle 1/3 */
  display: block;
  position: absolute;
  z-index: 1;
  top: 0;
  height: 66.66%;
  content: attr(data-content); /* dynamic content for the pseudo element */
  overflow: hidden;
  pointer-events: none; /* so the base char is selectable by mouse */
  color: #000; /* for demo purposes */
  text-shadow: 2px 2px 0px #af0; /* for demo purposes */
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>
<p>Single Characters:</p>
<span class=""halfStyle"" data-content=""X"">X</span>
<span class=""halfStyle"" data-content=""Y"">Y</span>
<span class=""halfStyle"" data-content=""Z"">Z</span>
<span class=""halfStyle"" data-content=""A"">A</span>

<hr/>
<p>Automated:</p>

<span class=""textToHalfStyle"">Half-style, please.</span>


(JSFiddle demo)





-HalfStyle Improvement By @KevinGranger



// jQuery for automated mode
jQuery(function($) {
    var text, chars, $el, i, output;

    // Iterate over all class occurences
    $('.textToHalfStyle').each(function(idx, el) {
    $el = $(el);
    text = $el.text();
    chars = text.split('');

    // Set the screen-reader text
    $el.html('<span style=""position: absolute !important;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);"">' + text + '</span>');

    // Reset output for appending
    output = '';

    // Iterate over all chars in the text
    for (i = 0; i < chars.length; i++) {
        // Create a styled element for each character and append to container
        output += '<span aria-hidden=""true"" class=""halfStyle"" data-content=""' + chars[i] + '"">' + chars[i] + '</span>';
    }

    // Write to DOM only once
    $el.append(output);
  });
});body {
    background-color: black;
}

.textToHalfStyle {
    display: block;
    margin: 200px 0 0 0;
    text-align: center;
}

.halfStyle {
    font-family: 'Libre Baskerville', serif;
    position: relative;
    display: inline-block;
    width: 1;
    font-size: 70px;
    color: black;
    overflow: hidden;
    white-space: pre;
    text-shadow: 1px 2px 0 white;
}

.halfStyle:before {
    display: block;
    z-index: 1;
    position: absolute;
    top: 0;
    width: 50%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow: hidden;
    color: white;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>
<p>Single Characters:</p>
<span class=""halfStyle"" data-content=""X"">X</span>
<span class=""halfStyle"" data-content=""Y"">Y</span>
<span class=""halfStyle"" data-content=""Z"">Z</span>
<span class=""halfStyle"" data-content=""A"">A</span>

<hr/>
<p>Automated:</p>

<span class=""textToHalfStyle"">Half-style, please.</span>


(JSFiddle demo)




-PeelingStyle improvement of HalfStyle by @SamTremaine



// jQuery for automated mode
jQuery(function($) {
    var text, chars, $el, i, output;

    // Iterate over all class occurences
    $('.textToHalfStyle').each(function(idx, el) {
    $el = $(el);
    text = $el.text();
    chars = text.split('');

    // Set the screen-reader text
    $el.html('<span style=""position: absolute !important;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);"">' + text + '</span>');

    // Reset output for appending
    output = '';

    // Iterate over all chars in the text
    for (i = 0; i < chars.length; i++) {
        // Create a styled element for each character and append to container
        output += '<span aria-hidden=""true"" class=""halfStyle"" data-content=""' + chars[i] + '"">' + chars[i] + '</span>';
    }

    // Write to DOM only once
    $el.append(output);
  });
});.halfStyle {
    position: relative;
    display: inline-block;
    font-size: 68px;
    color: rgba(0, 0, 0, 0.8);
    overflow: hidden;
    white-space: pre;
    transform: rotate(4deg);
    text-shadow: 2px 1px 3px rgba(0, 0, 0, 0.3);
}

.halfStyle:before { /* creates the left part */
    display: block;
    z-index: 1;
    position: absolute;
    top: -0.5px;
    left: -3px;
    width: 100%;
    content: attr(data-content);
    overflow: hidden;
    pointer-events: none;
    color: #FFF;
    transform: rotate(-4deg);
    text-shadow: 0px 0px 1px #000;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>
<p>Single Characters:</p>
<span class=""halfStyle"" data-content=""X"">X</span>
<span class=""halfStyle"" data-content=""Y"">Y</span>
<span class=""halfStyle"" data-content=""Z"">Z</span>
<span class=""halfStyle"" data-content=""A"">A</span>

<hr/>
<p>Automated:</p>

<span class=""textToHalfStyle"">Half-style, please.</span>


(JSFiddle demo and on samtremaine.co.uk)




Part 4: Ready for Production

Customized different Half-Style style-sets can be used on desired elements on the same page.
You can define multiple style-sets and tell the plugin which one to use.

The plugin uses data attribute data-halfstyle=""[-CustomClassName-]"" on the target .textToHalfStyle elements and makes all the necessary changes automatically.

So, simply on the element containing the text add textToHalfStyle class and data attribute data-halfstyle=""[-CustomClassName-]"". The plugin will do the rest of the job.



Also the CSS style-sets' class definitions match the [-CustomClassName-] part mentioned above and is chained to .halfStyle, so we will have .halfStyle.[-CustomClassName-]

jQuery(function($) {
    var halfstyle_text, halfstyle_chars, $halfstyle_el, halfstyle_i, halfstyle_output, halfstyle_style;

    // Iterate over all class occurrences
    $('.textToHalfStyle').each(function(idx, halfstyle_el) {
        $halfstyle_el = $(halfstyle_el);
        halfstyle_style = $halfstyle_el.data('halfstyle') || 'hs-base';
        halfstyle_text = $halfstyle_el.text();
        halfstyle_chars = halfstyle_text.split('');

        // Set the screen-reader text
        $halfstyle_el.html('<span style=""position: absolute !important;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);"">' + halfstyle_text + '</span>');

        // Reset output for appending
        halfstyle_output = '';

        // Iterate over all chars in the text
        for (halfstyle_i = 0; halfstyle_i < halfstyle_chars.length; halfstyle_i++) {
            // Create a styled element for each character and append to container
            halfstyle_output += '<span aria-hidden=""true"" class=""halfStyle ' + halfstyle_style + '"" data-content=""' + halfstyle_chars[halfstyle_i] + '"">' + halfstyle_chars[halfstyle_i] + '</span>';
        }

        // Write to DOM only once
        $halfstyle_el.append(halfstyle_output);
    });
});/* start half-style hs-base */

.halfStyle.hs-base {
    position: relative;
    display: inline-block;
    font-size: 80px; /* or any font size will work */
    overflow: hidden;
    white-space: pre; /* to preserve the spaces from collapsing */
    color: #000; /* for demo purposes */
}

.halfStyle.hs-base:before {
    display: block;
    z-index: 1;
    position: absolute;
    top: 0;
    width: 50%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    pointer-events: none; /* so the base char is selectable by mouse */
    overflow: hidden;
    color: #f00; /* for demo purposes */
}

/* end half-style hs-base */


/* start half-style hs-horizontal-third */

.halfStyle.hs-horizontal-third { /* base char and also the bottom 1/3 */
    position: relative;
    display: inline-block;
    font-size: 80px; /* or any font size will work */
    color: transparent;
    overflow: hidden;
    white-space: pre; /* to preserve the spaces from collapsing */
    color: #f0f;
    text-shadow: 2px 2px 0px #0af; /* for demo purposes */
}

.halfStyle.hs-horizontal-third:before { /* creates the top 1/3 */
    display: block;
    z-index: 2;
    position: absolute;
    top: 0;
    height: 33.33%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow: hidden;
    pointer-events: none; /* so the base char is selectable by mouse */
    color: #f00; /* for demo purposes */
    text-shadow: 2px -2px 0px #fa0; /* for demo purposes */
}

.halfStyle.hs-horizontal-third:after { /* creates the middle 1/3 */
    display: block;
    position: absolute;
    z-index: 1;
    top: 0;
    height: 66.66%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow: hidden;
    pointer-events: none; /* so the base char is selectable by mouse */
    color: #000; /* for demo purposes */
    text-shadow: 2px 2px 0px #af0; /* for demo purposes */
}

/* end half-style hs-horizontal-third */


/* start half-style hs-PeelingStyle, by user SamTremaine on Stackoverflow.com */

.halfStyle.hs-PeelingStyle {
  position: relative;
  display: inline-block;
  font-size: 68px;
  color: rgba(0, 0, 0, 0.8);
  overflow: hidden;
  white-space: pre;
  transform: rotate(4deg);
  text-shadow: 2px 1px 3px rgba(0, 0, 0, 0.3);
}

.halfStyle.hs-PeelingStyle:before { /* creates the left part */
  display: block;
  z-index: 1;
  position: absolute;
  top: -0.5px;
  left: -3px;
  width: 100%;
  content: attr(data-content);
  overflow: hidden;
  pointer-events: none;
  color: #FFF;
  transform: rotate(-4deg);
  text-shadow: 0px 0px 1px #000;
}

/* end half-style hs-PeelingStyle */


/* start half-style hs-KevinGranger, by user KevinGranger on StackOverflow.com*/

.textToHalfStyle.hs-KevinGranger {
  display: block;
  margin: 200px 0 0 0;
  text-align: center;
}

.halfStyle.hs-KevinGranger {
  font-family: 'Libre Baskerville', serif;
  position: relative;
  display: inline-block;
  width: 1;
  font-size: 70px;
  color: black;
  overflow: hidden;
  white-space: pre;
  text-shadow: 1px 2px 0 white;
}

.halfStyle.hs-KevinGranger:before {
  display: block;
  z-index: 1;
  position: absolute;
  top: 0;
  width: 50%;
  content: attr(data-content); /* dynamic content for the pseudo element */
  overflow: hidden;
  color: white;
}

/* end half-style hs-KevinGranger<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>
<p>
    <span class=""textToHalfStyle"" data-halfstyle=""hs-base"">Half-style, please.</span>
</p>
<p>
    <span class=""textToHalfStyle"" data-halfstyle=""hs-horizontal-third"">Half-style, please.</span>
</p>
<p>
    <span class=""textToHalfStyle"" data-halfstyle=""hs-PeelingStyle"">Half-style, please.</span>
</p>
<p style=""background-color:#000;"">
    <span class=""textToHalfStyle"" data-halfstyle=""hs-KevinGranger"">Half-style, please.</span>
</p>


(JSFiddle demo)
    

I've just finished developing the plugin and it is available for everyone to use! Hope you will enjoy it. 

View Project on GitHub - View Project Website. (so you can see all the split styles)

Usage

First of all, make sure you have the jQuery library is included. The best way to get the latest jQuery version is to update your head tag with:

<script src=""http://code.jquery.com/jquery-latest.min.js""></script>


After downloading the files, make sure you include them in your project:

<link rel=""stylesheet"" type=""text/css"" href=""css/splitchar.css"">
<script type=""text/javascript"" src=""js/splitchar.js""></script>


Markup

All you have to do is to asign the class splitchar , followed by the desired style to the element wrapping your text. e.g

<h1 class=""splitchar horizontal"">Splitchar</h1>


After all this is done, just make sure you call the jQuery function in your document ready file like this:

$("".splitchar"").splitchar();


Customizing

In order to make the text look exactly as you want it to, all you have to do is apply your design like this:

.horizontal { /* Base CSS - e.g font-size */ }
.horizontal:before { /* CSS for the left half */ }
.horizontal:after { /* CSS for the right half */ }



That's it! Now you have the Splitchar plugin all set. More info about it at http://razvanbalosin.com/Splitchar.js/.
    Yes, you can do this with only one character and only CSS:
http://jsbin.com/rexoyice/1/
h1 {
  display: inline-block;
  margin: 0; /* for demo snippet */
  line-height: 1em; /* for demo snippet */
  font-family: helvetica, arial, sans-serif;
  font-weight: bold;
  font-size: 300px;
  background: linear-gradient(to right, #7db9e8 50%,#1e5799 50%);
  background-clip: text;
  -webkit-text-fill-color: transparent;
}<h1>X</h1>

Visually, all the examples that use two characters (be it via JS, CSS pseudo elements, or just HTML) look fine, but note that that all adds content to the DOM which may cause accessibility--as well as text selection/cut/paste issues.
    

JSFiddle DEMO
We'll do it using just CSS pseudo selectors!
This technique will work with dynamically generated content and different font sizes and widths.
HTML:
<div class='split-color'>Two is better than one.</div>

CSS:
.split-color > span {
    white-space: pre-line;
    position: relative;
    color: #409FBF;
}

.split-color > span:before {
    content: attr(data-content);
    pointer-events: none;  /* Prevents events from targeting pseudo-element */
    position: absolute;
    overflow: hidden;
    color: #264A73;
    width: 50%;
    z-index: 1;
}

To wrap the dynamically generated string, you could use a function like this:
// Wrap each letter in a span tag and return an HTML string
// that can be used to replace the original text
function wrapString(str) {
  var output = [];
  str.split('').forEach(function(letter) {
    var wrapper = document.createElement('span');
    wrapper.dataset.content = wrapper.innerHTML = letter;

    output.push(wrapper.outerHTML);
  });

  return output.join('');
}

// Replace the original text with the split-color text
window.onload = function() {
    var el  = document.querySelector('.split-color'),
        txt = el.innerHTML;
    
    el.innerHTML = wrapString(txt);
}

    If you are interested in this, then Lucas Bebber's Glitch is a very similar and super cool effect:



Created using a simple SASS Mixin such as

.example-one {
  font-size: 100px;
  @include textGlitch(""example-one"", 17, white, black, red, blue, 450, 115);
}


More details at Chris Coyer's CSS Tricks and Lucas Bebber's Codepen page
    Here an ugly implementation in canvas. I tried this solution, but the results are worse than I expected, so here it is anyway.



$(""div"").each(function() {
  var CHARS = $(this).text().split('');
  $(this).html("""");
  $.each(CHARS, function(index, char) {
    var canvas = $(""<canvas />"")
      .css(""width"", ""40px"")
      .css(""height"", ""40px"")
      .get(0);
    $(""div"").append(canvas);
    var ctx = canvas.getContext(""2d"");
    var gradient = ctx.createLinearGradient(0, 0, 130, 0);
    gradient.addColorStop(""0"", ""blue"");
    gradient.addColorStop(""0.5"", ""blue"");
    gradient.addColorStop(""0.51"", ""red"");
    gradient.addColorStop(""1.0"", ""red"");
    ctx.font = '130pt Calibri';
    ctx.fillStyle = gradient;
    ctx.fillText(char, 10, 130);
  });
});<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<div>Example Text</div>

    How about something like this for shorter text? 

It could even work for longer text if you did something with a loop, repeating the characters with JavaScript. Anyway, the result is something like this:



p.char {
  position: relative;
  display: inline-block;
  font-size: 60px;
  color: red;
}

p.char:before {
  position: absolute;
  content: attr(char);
  width: 50%;
  overflow: hidden;
  color: black;
}<p class=""char"" char=""S"">S</p>
<p class=""char"" char=""t"">t</p>
<p class=""char"" char=""a"">a</p>
<p class=""char"" char=""c"">c</p>
<p class=""char"" char=""k"">k</p>
<p class=""char"" char=""o"">o</p>
<p class=""char"" char=""v"">v</p>
<p class=""char"" char=""e"">e</p>
<p class=""char"" char=""r"">r</p>
<p class=""char"" char=""f"">f</p>
<p class=""char"" char=""l"">l</p>
<p class=""char"" char=""o"">o</p>
<p class=""char"" char=""w"">w</p>

    A nice solution that takes advantage of the background-clip: text support: http://jsfiddle.net/sandro_paganotti/wLkVt/
span{
   font-size: 100px;
   background: linear-gradient(to right, black, black 50%, grey 50%, grey);
   background-clip: text;
   -webkit-text-fill-color: transparent;
}

    Closest I can get:

$(function(){
  $('span').width($('span').width()/2);
  $('span:nth-child(2)').css('text-indent', -$('span').width());
});body{
  font-family: arial;
}
span{
  display: inline-block;
  overflow: hidden;
}
span:nth-child(2){
  color: red;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<span>X</span><span>X</span>


Demo: http://jsfiddle.net/9wxfY/2/

Heres a version that just uses one span: http://jsfiddle.net/9wxfY/4/
    

I just played with @Arbel's solution:

var textToHalfStyle = $('.textToHalfStyle').text();
var textToHalfStyleChars = textToHalfStyle.split('');
$('.textToHalfStyle').html('');
$.each(textToHalfStyleChars, function(i,v){
    $('.textToHalfStyle').append('<span class=""halfStyle"" data-content=""' + v + '"">' + v + '</span>');
});body{
    background-color: black;
}
.textToHalfStyle{
    display:block;
    margin: 200px 0 0 0;
    text-align:center;
}
.halfStyle {
    font-family: 'Libre Baskerville', serif;
    position:relative;
    display:inline-block;
    width:1;
    font-size:70px;
    color: black;
    overflow:hidden;
    white-space: pre;
    text-shadow: 1px 2px 0 white;
}
.halfStyle:before {
    display:block;
    z-index:1;
    position:absolute;
    top:0;
    width: 50%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow:hidden;
    color: white;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js""></script>
<span class=""textToHalfStyle"">Dr. Jekyll and M. Hide</span>

    FWIW, here's my take on this doing it only with CSS: http://codepen.io/ricardozea/pen/uFbts/

Several notes:


The main reason I did this was to test myself and see if I was able to accomplish styling half of a character while actually providing a meaningful answer to the OP.
I am aware that this is not an ideal or the most scalable solution and the solutions proposed by the people here are far better for ""real world"" scenarios.
The CSS code I created is based on the first thoughts that came to my mind and my own personal approach to the problem.
My solution only works on symmetrical characters, like X, A, O, M. **It does not work on asymmetric characters like B, C, F, K or lower case letters.
** HOWEVER, this approach creates very interesting 'shapes' with asymmetric characters. Try changing the X to a K or to a lower case letter like an h or a p in the CSS :)


HTML

<span class=""half-letter""></span>


SCSS

.half-character { 
  display: inline-block;
  font: bold 350px/.8 Arial;
  position: relative;

  &:before, &:after {
    content: 'X'; //Change character here
    display: inline-block;
    width: 50%;
    overflow: hidden;
    color: #7db9e8;
  }
  &:after {
    position: absolute;
    top: 0;
    left: 50%;
    color: #1e5799;
    transform: rotateY(-180deg);
  }
}

    This can be achieved with just CSS :before selector and content property value.

.halfed, .halfed1 {
  float: left;
}

.halfed, .halfed1 {
  font-family: arial;
  font-size: 300px;
  font-weight: bolder;
  width: 200px;
  height: 300px;
  position: relative; /* To help hold the content value within */
  overflow: hidden;
  color: #000;
}




.halfed:before, .halfed1:before   {
  width: 50%; /* How much we'd like to show */
  overflow: hidden; /* Hide what goes beyond our dimension */  
  content: 'X'; /* Halfed character */
  height: 100%;
  position: absolute;
  color: #28507D;

}



/* For Horizontal cut off */ 

.halfed1:before   {
  width: 100%;
  height: 55%;
  
}<div class=""halfed""> X </div>

<div class=""halfed1""> X </div>


>> See on jsFiddle
    .halfStyle {
    position:relative;
    display:inline-block;
    font-size:68px; /* or any font size will work */
    color: rgba(0,0,0,0.8); /* or transparent, any color */
    overflow:hidden;
    white-space: pre; /* to preserve the spaces from collapsing */
    transform:rotate(4deg);
    -webkit-transform:rotate(4deg);
    text-shadow:2px 1px 3px rgba(0,0,0,0.3);
}
.halfStyle:before {
    display:block;
    z-index:1;
    position:absolute;
    top:-0.5px;
    left:-3px;
    width: 100%;
    content: attr(data-content); /* dynamic content for the pseudo element */
    overflow:hidden;
    color: white;
    transform:rotate(-4deg);
    -webkit-transform:rotate(-4deg);
    text-shadow:0 0 1px black;

}


http://experimental.samtremaine.co.uk/half-style/

You can crowbar this code into doing all sorts of interesting things - this is just one implementation my associate and I came up with last night.
    Just for the record in history!

I've come up with a solution for my own work from 5-6 years ago, which is Gradext ( pure javascript and pure css, no dependency ) .

The technical explanation is you can create an element like this:

<span>A</span>


now if you want to make a gradient on text, you need to create some multiple layers, each individually specifically colored and the spectrum created will illustrate the gradient effect.

for example look at this is the word lorem inside of a <span> and will cause a horizontal gradient effect ( check the examples ):

 <span data-i=""0"" style=""color: rgb(153, 51, 34);"">L</span>
 <span data-i=""1"" style=""color: rgb(154, 52, 35);"">o</span>
 <span data-i=""2"" style=""color: rgb(155, 53, 36);"">r</span>
 <span data-i=""3"" style=""color: rgb(156, 55, 38);"">e</span>
 <span data-i=""4"" style=""color: rgb(157, 56, 39);"">m</span>


and you can continue doing this pattern for a long time and long paragraph as well.



But!

What if you want to create a vertical gradient effect on texts?

Then there's another solution which could be helpful. I will describe in details.

Assuming our first <span> again. but the content shouldn't be the letters individually; the content should be the whole text, and now we're going to copy the same <span> again and again ( count of spans will define the quality of your gradient, more span, better result, but poor performance ). have a look at this:

<span data-i=""6"" style=""color: rgb(81, 165, 39); overflow: hidden; height: 11.2px;"">Lorem ipsum dolor sit amet, tincidunt ut laoreet dolore magna aliquam erat volutpat.</span>
<span data-i=""7"" style=""color: rgb(89, 174, 48); overflow: hidden; height: 12.8px;"">Lorem ipsum dolor sit amet, tincidunt ut laoreet dolore magna aliquam erat volutpat.</span>
<span data-i=""8"" style=""color: rgb(97, 183, 58); overflow: hidden; height: 14.4px;"">Lorem ipsum dolor sit amet, tincidunt ut laoreet dolore magna aliquam erat volutpat.</span>
<span data-i=""9"" style=""color: rgb(105, 192, 68); overflow: hidden; height: 16px;"">Lorem ipsum dolor sit amet, tincidunt ut laoreet dolore magna aliquam erat volutpat.</span>
<span data-i=""10"" style=""color: rgb(113, 201, 78); overflow: hidden; height: 17.6px;"">Lorem ipsum dolor sit amet, tincidunt ut laoreet dolore magna aliquam erat volutpat.</span>
<span data-i=""11"" style=""color: rgb(121, 210, 88); overflow: hidden; height: 19.2px;"">Lorem ipsum dolor sit amet, tincidunt ut laoreet dolore magna aliquam erat volutpat.</span>




Again, But!

what if you want to make these gradient effects to move and create an animation out of it?

well, there's another solution for it too. You should definitely check animation: true or even .hoverable() method which will lead to a gradient to start based on cursor position! ( sounds cool xD )



this is simply how we're creating gradients ( linear or radial ) on texts. If you liked the idea or want to know more about it, you should check the links provided.



Maybe this is not the best option, maybe not the best performant way to do this, but it will open up some space to create exciting and delightful animations to inspire some other people for a better solution.

It will allow you to use gradient style on texts, which is supported by even IE8!

Here you can find a working live demo and the original repository is here on GitHub as well, open source and ready to get some updates ( :D )

This is my first time ( yeah, after 5 years, you've heard it right ) to mention this repository anywhere on the Internet, and I'm excited about that!



[Update - 2019 August:] Github removed github-pages demo of that repository because I'm from Iran! Only the source code is available here tho...
    Limited CSS and jQuery Solution

I am not sure how elegant this solution is, but it cuts everything exactly in half: http://jsfiddle.net/9wxfY/11/

Otherwise, I have created a nice solution for you... All you need to do is have this for your HTML:

Take a look at this most recent, and accurate, edit as of 6/13/2016 : http://jsfiddle.net/9wxfY/43/

As for the CSS, it is very limited... You only need to apply it to :nth-child(even)

$(function(){
  var $hc = $('.half-color');
  var str = $hc.text();
  $hc.html("""");

  var i = 0;
  var chars;
  var dupText;

  while(i < str.length){
    chars = str[i];
    if(chars == "" "") chars = ""&nbsp;"";
    dupText = ""<span>"" + chars + ""</span>"";

    var firstHalf = $(dupText);
    var secondHalf = $(dupText);

    $hc.append(firstHalf)
    $hc.append(secondHalf)

    var width = firstHalf.width()/2;

    firstHalf.width(width);
    secondHalf.css('text-indent', -width);

    i++;
  }
});.half-color span{
  font-size: 2em;
  display: inline-block;
  overflow: hidden;
}
.half-color span:nth-child(even){
  color: red;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<div class=""half-color"">This is a sentence</div>

    Another CSS-only solution (though data-attribute is needed if you don't want to write letter-specific CSS). This one works more across the board (Tested IE 9/10, Chrome latest & FF latest)

span {
  position: relative;
  color: rgba(50,50,200,0.5);
}

span:before {
  content: attr(data-char);
  position: absolute;
  width: 50%;
  overflow: hidden;
  color: rgb(50,50,200);
}<span data-char=""X"">X</span>

    You can use below code. Here in this example I have used h1 tag and added an attribute data-title-text=""Display Text"" which will appear with different color text on h1 tag text element, which gives effect halfcolored text as shown in below example 



body {
  text-align: center;
  margin: 0;
}

h1 {
  color: #111;
  font-family: arial;
  position: relative;
  font-family: 'Oswald', sans-serif;
  display: inline-block;
  font-size: 2.5em;
}

h1::after {
  content: attr(data-title-text);
  color: #e5554e;
  position: absolute;
  left: 0;
  top: 0;
  clip: rect(0, 1000px, 30px, 0);
}<h1 data-title-text=""Display Text"">Display Text</h1>

    You can also do it using SVG, if you wish:

var title = document.querySelector('h1'),
    text = title.innerHTML,
    svgTemplate = document.querySelector('svg'),
    charStyle = svgTemplate.querySelector('#text');

svgTemplate.style.display = 'block';

var space = 0;

for (var i = 0; i < text.length; i++) {
  var x = charStyle.cloneNode();
  x.textContent = text[i];
  svgTemplate.appendChild(x);
  x.setAttribute('x', space);
  space += x.clientWidth || 15;
}

title.innerHTML = '';
title.appendChild(svgTemplate);<svg style=""display: none; height: 100px; width: 100%"" xmlns=""http://www.w3.org/2000/svg"" xmlns:svg=""http://www.w3.org/2000/svg"" xmlns:xlink=""http://www.w3.org/1999/xlink"" version=""1.1"">
    <defs id=""FooDefs"">
        <linearGradient id=""MyGradient"" x1=""0%"" y1=""0%"" x2=""100%"" y2=""0%"">
            <stop offset=""50%"" stop-color=""blue"" />
            <stop offset=""50%"" stop-color=""red"" />
        </linearGradient>
    </defs>
    <text y=""50%"" id=""text"" style=""font-size: 72px; fill: url(#MyGradient)""></text>
</svg>

<h1>This is not a solution X</h1>


http://codepen.io/nicbell/pen/jGcbq
    
Here is a CSS only solution for a full line of text, not just a character element.
div {
    position: relative;
    top: 2em;
    height: 2em;
    text-transform: full-width;
}

div:before,
div:after {
    content: attr(data-content);
    position: absolute;
    top: 0;
    right: 0;
    bottom: 0;
    left: 0;
}

div:after {
    color: red;
    /* mask for a single character. By repeating this mask, all the string becomes masked */
    -webkit-mask-image: linear-gradient(to right, transparent 0, transparent .5em, white .5em, white 1em);
    -webkit-mask-repeat: repeat-x;  /* repeat the mask towards the right */
    -webkit-mask-size: 1em;         /* relative width of a single character */ 
    
    /* non-vendor mask settings */
    mask-image: linear-gradient(to right, transparent 0, transparent .5em, white .5em, white 1em);
    mask-repeat: repeat-x;
    mask-size: 1em;
}


/* demo purposes */
input[name=""fontSize""]:first-of-type:checked ~ div {
    font-size: 1em;
}

input[name=""fontSize""]:first-of-type + input:checked ~ div {
    font-size: 2em;
}

input[name=""fontSize""]:first-of-type + input + input:checked ~ div {
    font-size: 3em;
}Font-size:
  <input type=""radio"" name=""fontSize"" value=""1em"">
  <input type=""radio"" name=""fontSize"" value=""2em"" checked>
  <input type=""radio"" name=""fontSize"" value=""3em"">

  <div data-content=""A CSS only solution...""></div>
  <div data-content=""Try it on Firefox!""></div>

The idea is to apply an horizontal CSS mask for each character, that hides the first half of it [0 - 0.5em] and shows the second half [0.5em - 1em].
The width of the mask is mask-size: 1em to match the width of the very first character in the string.
By using the mask-repeat: repeat-x, the same mask is applied to the second, third character and so on.
I thought that using the font monospace would solve the problem of using same-width letters, but I was wrong.
Instead, I solved it by using the text-transform: full-width, that unfortunatelly is only supported by Firefox, I believe.
The use of relative unit em allows the design to scale up/down depending on the font-size.
Vanilla JavaScript solution for all browsers
If Firefox is not an option, then use this script for the rescue.
It works by inserting a child span for each character. Inside each span, a non-repeated CSS mask is placed from [0% - 50%] and [50% - 100%] the width of the letter (which is the width of the span element).
This way we don't have anymore the restriction of using same-width characters.
const
    dataElement = document.getElementById(""data""),
    content = dataElement.textContent,
    zoom = function (fontSize) {
        dataElement.style['font-size'] = fontSize + 'em';           
    };

while (dataElement.firstChild) {
    dataElement.firstChild.remove()
}
for(var i = 0; i < content.length; ++i) {
    const
        spanElem = document.createElement('span'),
        ch = content[i];    
    spanElem.setAttribute('data-ch', ch);
    spanElem.appendChild(document.createTextNode(ch === ' ' ? '\u00A0' : ch));
    data.appendChild(spanElem);
}#data {
    position: relative;
    top: 2em;
    height: 2em;
    font-size: 2em;
}

#data span {
    display: inline-block;
    position: relative;
    color: transparent;
}

#data span:before,
#data span:after {
    content: attr(data-ch);
    display: inline-block;
    position: absolute;
    top: 0;
    right: 0;
    bottom: 0;
    left: 0;
    text-align: center;
    color: initial;
}

#data span:after {
    color: red;
    -webkit-mask-image: linear-gradient(to right, transparent 0, transparent 50%, white 50%, white 100%);
    mask-image: linear-gradient(to right, transparent 0, transparent 50%, white 50%, white 100%);
}Font-size:
<input type=""range"" min=1 max=4 step=0.05 value=2 oninput=""zoom(this.value)"" onchange=""zoom(this.value)"">

<div id=""data"">A Fallback Solution...For all browsers</div>

    All solutions work by splitting letters and wrapping them in <span>. We don't have to split letters in two cases:

If font is monospace.
If vertical layout is used.

div {
  font-size: 80px;
  font-weight: bolder;
  color: transparent;
  padding: 0;
  margin: 0;
  background: linear-gradient(90deg, rgb(34, 67, 143) 0% 50%, #409FBF 50%);
  background-clip: text;
  -webkit-background-clip: text;
}

.one {
  font-family: 'Nova Mono';
  background-repeat: repeat-x;
  background-size: 45px;
}

.two {
  font-family: 'Gideon Roman';
  writing-mode: vertical-lr;
  text-orientation: upright;
  letter-spacing: -35px;
  height: 500px;
}<!-- get the fonts -->
<link rel=""preconnect"" href=""https://fonts.googleapis.com"">
<link rel=""preconnect"" href=""https://fonts.gstatic.com"" crossorigin>
<link href=""https://fonts.googleapis.com/css2?family=Nova+Mono&display=swap"" rel=""stylesheet"">
<link href=""https://fonts.googleapis.com/css2?family=Gideon+Roman&display=swap"" rel=""stylesheet"">


<div id='one' class=""one"">X-RAY Winter</div>
<div class=""two"">Minty</div>

Expected output, in case the fonts are not available:

I know use of background-clip and gradient has been already demonstrated in other answers, just putting the cases where you don't have to split the letters.
    ","[3052, 3089, 515, 256, 169, 81, 82, 30, 34, 65, 56, 26, 19, 33, 8, 40, 45, 10, 19, 2, 1]",280933,1162,2014-05-09T16:16:57,2022-01-26 05:15:30Z,javascript html css 
pretty-print JSON using JavaScript,"
                
How can I display JSON in an easy-to-read (for human readers) format? I'm looking primarily for indentation and whitespace, with perhaps even colors / font-styles / etc.
    Pretty-printing is implemented natively in JSON.stringify(). The third argument enables pretty printing and sets the spacing to use:

var str = JSON.stringify(obj, null, 2); // spacing level = 2


If you need syntax highlighting, you might use some regex magic like so:

function syntaxHighlight(json) {
    if (typeof json != 'string') {
         json = JSON.stringify(json, undefined, 2);
    }
    json = json.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');
    return json.replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, function (match) {
        var cls = 'number';
        if (/^""/.test(match)) {
            if (/:$/.test(match)) {
                cls = 'key';
            } else {
                cls = 'string';
            }
        } else if (/true|false/.test(match)) {
            cls = 'boolean';
        } else if (/null/.test(match)) {
            cls = 'null';
        }
        return '<span class=""' + cls + '"">' + match + '</span>';
    });
}


See in action here: jsfiddle

Or a full snippet provided below:

function output(inp) {
    document.body.appendChild(document.createElement('pre')).innerHTML = inp;
}

function syntaxHighlight(json) {
    json = json.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');
    return json.replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, function (match) {
        var cls = 'number';
        if (/^""/.test(match)) {
            if (/:$/.test(match)) {
                cls = 'key';
            } else {
                cls = 'string';
            }
        } else if (/true|false/.test(match)) {
            cls = 'boolean';
        } else if (/null/.test(match)) {
            cls = 'null';
        }
        return '<span class=""' + cls + '"">' + match + '</span>';
    });
}

var obj = {a:1, 'b':'foo', c:[false,'false',null, 'null', {d:{e:1.3e5,f:'1.3e5'}}]};
var str = JSON.stringify(obj, undefined, 4);

output(str);
output(syntaxHighlight(str));pre {outline: 1px solid #ccc; padding: 5px; margin: 5px; }
.string { color: green; }
.number { color: darkorange; }
.boolean { color: blue; }
.null { color: magenta; }
.key { color: red; }

    User Pumbaa80's answer is great if you have an object you want pretty printed. If you're starting from a valid JSON string that you want to pretty printed, you need to convert it to an object first:

var jsonString = '{""some"":""json""}';
var jsonPretty = JSON.stringify(JSON.parse(jsonString),null,2);  


This builds a JSON object from the string, and then converts it back to a string using JSON stringify's pretty print.
    Better way.

Prettify JSON Array in Javascript

JSON.stringify(jsonobj,null,'\t')

    var jsonObj = {""streetLabel"": ""Avenue Anatole France"", ""city"": ""Paris 07"",  ""postalCode"": ""75007"", ""countryCode"": ""FRA"",  ""countryLabel"": ""France"" };

document.getElementById(""result-before"").innerHTML = JSON.stringify(jsonObj);


In case of displaying in HTML, you should to add a balise <pre></pre>

document.getElementById(""result-after"").innerHTML = ""<pre>""+JSON.stringify(jsonObj,undefined, 2) +""</pre>""


Example:

var jsonObj = {""streetLabel"": ""Avenue Anatole France"", ""city"": ""Paris 07"",  ""postalCode"": ""75007"", ""countryCode"": ""FRA"",  ""countryLabel"": ""France"" };

document.getElementById(""result-before"").innerHTML = JSON.stringify(jsonObj);

document.getElementById(""result-after"").innerHTML = ""<pre>""+JSON.stringify(jsonObj,undefined, 2) +""</pre>""div { float:left; clear:both; margin: 1em 0; }<div id=""result-before""></div>
<div id=""result-after""></div>

    If you are using ES5, simply call JSON.stringify with:

2nd arg: replacer; set to null,
3rd arg: space; use tab.

JSON.stringify(anObject, null, '\t');

Source: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify
    You can use JSON.stringify(your object, null, 2)
The second parameter can be used as a replacer function which takes key and Val as parameters.This can be used in case you want to modify something within your JSON object.

more reference : https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify
    Based on Pumbaa80's answer I have modified the code to use the console.log colours (working on Chrome for sure) and not HTML. Output can be seen inside console. You can edit the _variables inside the function adding some more styling.

function JSONstringify(json) {
    if (typeof json != 'string') {
        json = JSON.stringify(json, undefined, '\t');
    }

    var 
        arr = [],
        _string = 'color:green',
        _number = 'color:darkorange',
        _boolean = 'color:blue',
        _null = 'color:magenta',
        _key = 'color:red';

    json = json.replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, function (match) {
        var style = _number;
        if (/^""/.test(match)) {
            if (/:$/.test(match)) {
                style = _key;
            } else {
                style = _string;
            }
        } else if (/true|false/.test(match)) {
            style = _boolean;
        } else if (/null/.test(match)) {
            style = _null;
        }
        arr.push(style);
        arr.push('');
        return '%c' + match + '%c';
    });

    arr.unshift(json);

    console.log.apply(console, arr);
}


Here is a bookmarklet you can use:

javascript:function JSONstringify(json) {if (typeof json != 'string') {json = JSON.stringify(json, undefined, '\t');}var arr = [],_string = 'color:green',_number = 'color:darkorange',_boolean = 'color:blue',_null = 'color:magenta',_key = 'color:red';json = json.replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, function (match) {var style = _number;if (/^""/.test(match)) {if (/:$/.test(match)) {style = _key;} else {style = _string;}} else if (/true|false/.test(match)) {style = _boolean;} else if (/null/.test(match)) {style = _null;}arr.push(style);arr.push('');return '%c' + match + '%c';});arr.unshift(json);console.log.apply(console, arr);};void(0);


Usage:

var obj = {a:1, 'b':'foo', c:[false,null, {d:{e:1.3e5}}]};
JSONstringify(obj);


Edit: I just tried to escape the % symbol with this line, after the variables declaration:

json = json.replace(/%/g, '%%');


But I find out that Chrome is not supporting % escaping in the console. Strange... Maybe this will work in the future.

Cheers!


    You can use console.dir(), which is a shortcut for console.log(util.inspect()).
(The only difference is that it bypasses any custom inspect() function defined on an object.)

It uses syntax-highlighting, smart indentation, removes quotes from keys and just makes the output as pretty as it gets.

const object = JSON.parse(jsonString)

console.dir(object, {depth: null, colors: true})


and for the command line:

cat package.json | node -e ""process.stdin.pipe(new stream.Writable({write: chunk => console.dir(JSON.parse(chunk), {depth: null, colors: true})}))""
    Here's user123444555621's awesome HTML one adapted for terminals. Handy for debugging Node scripts:

function prettyJ(json) {
  if (typeof json !== 'string') {
    json = JSON.stringify(json, undefined, 2);
  }
  return json.replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, 
    function (match) {
      let cls = ""\x1b[36m"";
      if (/^""/.test(match)) {
        if (/:$/.test(match)) {
          cls = ""\x1b[34m"";
        } else {
          cls = ""\x1b[32m"";
        }
      } else if (/true|false/.test(match)) {
        cls = ""\x1b[35m""; 
      } else if (/null/.test(match)) {
        cls = ""\x1b[31m"";
      }
      return cls + match + ""\x1b[0m"";
    }
  );
}


Usage:

// thing = any json OR string of json
prettyJ(thing);

    I use the JSONView Chrome extension (it is as pretty as it gets :):

Edit: added jsonreport.js

I've also released an online stand-alone JSON pretty print viewer, jsonreport.js, that provides a human readable HTML5 report you can use to view any JSON data.

You can read more about the format in New JavaScript HTML5 Report Format.
    I think you're looking for something like this :
JSON.stringify(obj, null, '\t');

This ""pretty-prints"" your JSON string, using a tab for indentation.
If you prefer to use spaces instead of tabs, you could also use a number for the number of spaces you'd like :
JSON.stringify(obj, null, 2);

    I'd like to show my jsonAnalyze method here, it does a pretty print of the JSON structure only, but in some cases can be more usefull that printing the whole JSON.
Say you have a complex JSON like this:
let theJson = {
'username': 'elen',
'email': 'elen@test.com',
'state': 'married',
'profiles': [
    {'name': 'elenLove', 'job': 'actor' },
    {'name': 'elenDoe', 'job': 'spy'}
],
'hobbies': ['run', 'movies'],
'status': {
    'home': { 
        'ownsHome': true,
        'addresses': [
            {'town': 'Mexico', 'address': '123 mexicoStr'},
            {'town': 'Atlanta', 'address': '4B atlanta 45-48'},
        ]
    },
    'car': {
        'ownsCar': true,
        'cars': [
            {'brand': 'Nissan', 'plate': 'TOKY-114', 'prevOwnersIDs': ['4532354531', '3454655344', '5566753422']},
            {'brand': 'Benz', 'plate': 'ELEN-1225', 'prevOwnersIDs': ['4531124531', '97864655344', '887666753422']}
        ]
    }
},
'active': true,
'employed': false,
};

Then the method will return the structure like this:
username
email
state
profiles[]
    profiles[].name
    profiles[].job
hobbies[]
status{}
    status{}.home{}
        status{}.home{}.ownsHome
        status{}.home{}.addresses[]
            status{}.home{}.addresses[].town
            status{}.home{}.addresses[].address
    status{}.car{}
        status{}.car{}.ownsCar
        status{}.car{}.cars[]
            status{}.car{}.cars[].brand
            status{}.car{}.cars[].plate
            status{}.car{}.cars[].prevOwnersIDs[]
active
employed

So this is the jsonAnalyze() code:
function jsonAnalyze(obj) {
        let arr = [];
        analyzeJson(obj, null, arr);
        return logBeautifiedDotNotation(arr);

    function analyzeJson(obj, parentStr, outArr) {
        let opt;
        if (!outArr) {
            return ""no output array given""
        }
        for (let prop in obj) {
            opt = parentStr ? parentStr + '.' + prop : prop;
            if (Array.isArray(obj[prop]) && obj[prop] !== null) {
                    let arr = obj[prop];
                if ((Array.isArray(arr[0]) || typeof arr[0] == ""object"") && arr[0] != null) {                        
                    outArr.push(opt + '[]');
                    analyzeJson(arr[0], opt + '[]', outArr);
                } else {
                    outArr.push(opt + '[]');
                }
            } else if (typeof obj[prop] == ""object"" && obj[prop] !== null) {
                    outArr.push(opt + '{}');
                    analyzeJson(obj[prop], opt + '{}', outArr);
            } else {
                if (obj.hasOwnProperty(prop) && typeof obj[prop] != 'function') {
                    outArr.push(opt);
                }
            }
        }
    }

    function logBeautifiedDotNotation(arr) {
        retStr = '';
        arr.map(function (item) {
            let dotsAmount = item.split(""."").length - 1;
            let dotsString = Array(dotsAmount + 1).join('    ');
            retStr += dotsString + item + '\n';
            console.log(dotsString + item)
        });
        return retStr;
    }
}

jsonAnalyze(theJson);

    Couldn't find any solution that had good syntax highlighting for the console, so here's my 2p

Install & Add cli-highlight dependency

npm install cli-highlight --save


Define logjson globally

const highlight = require('cli-highlight').highlight
console.logjson = (obj) => console.log(
                               highlight( JSON.stringify(obj, null, 4), 
                                          { language: 'json', ignoreIllegals: true } ));


Use

console.logjson({foo: ""bar"", someArray: [""string1"", ""string2""]});



    For debugging purpose I use:


console.debug(""%o"", data);



https://getfirebug.com/wiki/index.php/Console_API
https://developer.mozilla.org/en-US/docs/DOM/console

    Unsatisfied with other pretty printers for Ruby, I wrote my own (NeatJSON) and then ported it to JavaScript including a free online formatter. The code is free under MIT license (quite permissive).

Features (all optional):


Set a line width and wrap in a way that keeps objects and arrays on the same line when they fit, wrapping one value per line when they don't.
Sort object keys if you like.
Align object keys (line up the colons).
Format floating point numbers to specific number of decimals, without messing up the integers.
'Short' wrapping mode puts opening and closing brackets/braces on the same line as values, providing a format that some prefer.
Granular control over spacing for arrays and objects, between brackets, before/after colons and commas.
Function is made available to both web browsers and Node.js.


I'll copy the source code here so that this is not just a link to a library, but I encourage you to go to the GitHub project page, as that will be kept up-to-date and the code below will not.

(function(exports){
exports.neatJSON = neatJSON;

function neatJSON(value,opts){
  opts = opts || {}
  if (!('wrap'          in opts)) opts.wrap = 80;
  if (opts.wrap==true) opts.wrap = -1;
  if (!('indent'        in opts)) opts.indent = '  ';
  if (!('arrayPadding'  in opts)) opts.arrayPadding  = ('padding' in opts) ? opts.padding : 0;
  if (!('objectPadding' in opts)) opts.objectPadding = ('padding' in opts) ? opts.padding : 0;
  if (!('afterComma'    in opts)) opts.afterComma    = ('aroundComma' in opts) ? opts.aroundComma : 0;
  if (!('beforeComma'   in opts)) opts.beforeComma   = ('aroundComma' in opts) ? opts.aroundComma : 0;
  if (!('afterColon'    in opts)) opts.afterColon    = ('aroundColon' in opts) ? opts.aroundColon : 0;
  if (!('beforeColon'   in opts)) opts.beforeColon   = ('aroundColon' in opts) ? opts.aroundColon : 0;

  var apad  = repeat(' ',opts.arrayPadding),
      opad  = repeat(' ',opts.objectPadding),
      comma = repeat(' ',opts.beforeComma)+','+repeat(' ',opts.afterComma),
      colon = repeat(' ',opts.beforeColon)+':'+repeat(' ',opts.afterColon);

  return build(value,'');

  function build(o,indent){
    if (o===null || o===undefined) return indent+'null';
    else{
      switch(o.constructor){
        case Number:
          var isFloat = (o === +o && o !== (o|0));
          return indent + ((isFloat && ('decimals' in opts)) ? o.toFixed(opts.decimals) : (o+''));

        case Array:
          var pieces  = o.map(function(v){ return build(v,'') });
          var oneLine = indent+'['+apad+pieces.join(comma)+apad+']';
          if (opts.wrap===false || oneLine.length<=opts.wrap) return oneLine;
          if (opts.short){
            var indent2 = indent+' '+apad;
            pieces = o.map(function(v){ return build(v,indent2) });
            pieces[0] = pieces[0].replace(indent2,indent+'['+apad);
            pieces[pieces.length-1] = pieces[pieces.length-1]+apad+']';
            return pieces.join(',\n');
          }else{
            var indent2 = indent+opts.indent;
            return indent+'[\n'+o.map(function(v){ return build(v,indent2) }).join(',\n')+'\n'+indent+']';
          }

        case Object:
          var keyvals=[],i=0;
          for (var k in o) keyvals[i++] = [JSON.stringify(k), build(o[k],'')];
          if (opts.sorted) keyvals = keyvals.sort(function(kv1,kv2){ kv1=kv1[0]; kv2=kv2[0]; return kv1<kv2?-1:kv1>kv2?1:0 });
          keyvals = keyvals.map(function(kv){ return kv.join(colon) }).join(comma);
          var oneLine = indent+""{""+opad+keyvals+opad+""}"";
          if (opts.wrap===false || oneLine.length<opts.wrap) return oneLine;
          if (opts.short){
            var keyvals=[],i=0;
            for (var k in o) keyvals[i++] = [indent+' '+opad+JSON.stringify(k),o[k]];
            if (opts.sorted) keyvals = keyvals.sort(function(kv1,kv2){ kv1=kv1[0]; kv2=kv2[0]; return kv1<kv2?-1:kv1>kv2?1:0 });
            keyvals[0][0] = keyvals[0][0].replace(indent+' ',indent+'{');
            if (opts.aligned){
              var longest = 0;
              for (var i=keyvals.length;i--;) if (keyvals[i][0].length>longest) longest = keyvals[i][0].length;
              var padding = repeat(' ',longest);
              for (var i=keyvals.length;i--;) keyvals[i][0] = padRight(padding,keyvals[i][0]);
            }
            for (var i=keyvals.length;i--;){
              var k=keyvals[i][0], v=keyvals[i][1];
              var indent2 = repeat(' ',(k+colon).length);
              var oneLine = k+colon+build(v,'');
              keyvals[i] = (opts.wrap===false || oneLine.length<=opts.wrap || !v || typeof v!=""object"") ? oneLine : (k+colon+build(v,indent2).replace(/^\s+/,''));
            }
            return keyvals.join(',\n') + opad + '}';
          }else{
            var keyvals=[],i=0;
            for (var k in o) keyvals[i++] = [indent+opts.indent+JSON.stringify(k),o[k]];
            if (opts.sorted) keyvals = keyvals.sort(function(kv1,kv2){ kv1=kv1[0]; kv2=kv2[0]; return kv1<kv2?-1:kv1>kv2?1:0 });
            if (opts.aligned){
              var longest = 0;
              for (var i=keyvals.length;i--;) if (keyvals[i][0].length>longest) longest = keyvals[i][0].length;
              var padding = repeat(' ',longest);
              for (var i=keyvals.length;i--;) keyvals[i][0] = padRight(padding,keyvals[i][0]);
            }
            var indent2 = indent+opts.indent;
            for (var i=keyvals.length;i--;){
              var k=keyvals[i][0], v=keyvals[i][1];
              var oneLine = k+colon+build(v,'');
              keyvals[i] = (opts.wrap===false || oneLine.length<=opts.wrap || !v || typeof v!=""object"") ? oneLine : (k+colon+build(v,indent2).replace(/^\s+/,''));
            }
            return indent+'{\n'+keyvals.join(',\n')+'\n'+indent+'}'
          }

        default:
          return indent+JSON.stringify(o);
      }
    }
  }

  function repeat(str,times){ // http://stackoverflow.com/a/17800645/405017
    var result = '';
    while(true){
      if (times & 1) result += str;
      times >>= 1;
      if (times) str += str;
      else break;
    }
    return result;
  }
  function padRight(pad, str){
    return (str + pad).substring(0, pad.length);
  }
}
neatJSON.version = ""0.5"";

})(typeof exports === 'undefined' ? this : exports);

    Here is a simple JSON format/color component written in React:

const HighlightedJSON = ({ json }: Object) => {
  const highlightedJSON = jsonObj =>
    Object.keys(jsonObj).map(key => {
      const value = jsonObj[key];
      let valueType = typeof value;
      const isSimpleValue =
        [""string"", ""number"", ""boolean""].includes(valueType) || !value;
      if (isSimpleValue && valueType === ""object"") {
        valueType = ""null"";
      }
      return (
        <div key={key} className=""line"">
          <span className=""key"">{key}:</span>
          {isSimpleValue ? (
            <span className={valueType}>{`${value}`}</span>
          ) : (
            highlightedJSON(value)
          )}
        </div>
      );
    });
  return <div className=""json"">{highlightedJSON(json)}</div>;
};


See it working in this CodePen:
https://codepen.io/benshope/pen/BxVpjo

Hope that helps!
    It works well: 

console.table()


Read more here: https://developer.mozilla.org/pt-BR/docs/Web/API/Console/table
    Thanks a lot @all!
Based on the previous answers, here is another variant method providing custom replacement rules as parameter:

 renderJSON : function(json, rr, code, pre){
   if (typeof json !== 'string') {
      json = JSON.stringify(json, undefined, '\t');
   }
  var rules = {
   def : 'color:black;',    
   defKey : function(match){
             return '<strong>' + match + '</strong>';
          },
   types : [
       {
          name : 'True',
          regex : /true/,
          type : 'boolean',
          style : 'color:lightgreen;'
       },

       {
          name : 'False',
          regex : /false/,
          type : 'boolean',
          style : 'color:lightred;'
       },

       {
          name : 'Unicode',
          regex : /""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?/,
          type : 'string',
          style : 'color:green;'
       },

       {
          name : 'Null',
          regex : /null/,
          type : 'nil',
          style : 'color:magenta;'
       },

       {
          name : 'Number',
          regex : /-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?/,
          type : 'number',
          style : 'color:darkorange;'
       },

       {
          name : 'Whitespace',
          regex : /\s+/,
          type : 'whitespace',
          style : function(match){
             return '&nbsp';
          }
       } 

    ],

    keys : [
       {
           name : 'Testkey',
           regex : /(""testkey"")/,
           type : 'key',
           style : function(match){
             return '<h1>' + match + '</h1>';
          }
       }
    ],

    punctuation : {
          name : 'Punctuation',
          regex : /([\,\.\}\{\[\]])/,
          type : 'punctuation',
          style : function(match){
             return '<p>________</p>';
          }
       }

  };

  if('undefined' !== typeof jQuery){
     rules = $.extend(rules, ('object' === typeof rr) ? rr : {});  
  }else{
     for(var k in rr ){
        rules[k] = rr[k];
     }
  }
    var str = json.replace(/([\,\.\}\{\[\]]|""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, function (match) {
    var i = 0, p;
    if (rules.punctuation.regex.test(match)) {
               if('string' === typeof rules.punctuation.style){
                   return '<span style=""'+ rules.punctuation.style + '"">' + match + '</span>';
               }else if('function' === typeof rules.punctuation.style){
                   return rules.punctuation.style(match);
               } else{
                  return match;
               }            
    }

    if (/^""/.test(match)) {
        if (/:$/.test(match)) {
            for(i=0;i<rules.keys.length;i++){
            p = rules.keys[i];
            if (p.regex.test(match)) {
               if('string' === typeof p.style){
                   return '<span style=""'+ p.style + '"">' + match + '</span>';
               }else if('function' === typeof p.style){
                   return p.style(match);
               } else{
                  return match;
               }                
             }              
           }   
            return ('function'===typeof rules.defKey) ? rules.defKey(match) : '<span style=""'+ rules.defKey + '"">' + match + '</span>';            
        } else {
            return ('function'===typeof rules.def) ? rules.def(match) : '<span style=""'+ rules.def + '"">' + match + '</span>';
        }
    } else {
        for(i=0;i<rules.types.length;i++){
         p = rules.types[i];
         if (p.regex.test(match)) {
               if('string' === typeof p.style){
                   return '<span style=""'+ p.style + '"">' + match + '</span>';
               }else if('function' === typeof p.style){
                   return p.style(match);
               } else{
                  return match;
               }                
          }             
        }

     }

    });

  if(true === pre)str = '<pre>' + str + '</pre>';
  if(true === code)str = '<code>' + str + '</code>';
  return str;
 }

    based on @user123444555621, just slightly more modern.
const clsMap = [
    [/^"".*:$/, ""key""],
    [/^""/, ""string""],
    [/true|false/, ""boolean""],
    [/null/, ""key""],
    [/.*/, ""number""],
]

const syntaxHighlight = obj => JSON.stringify(obj, null, 4)
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, match => `<span class=""${clsMap.find(([regex]) => regex.test(match))[1]}"">${match}</span>`);

you can also specify the colors inside js (no CSS needed)
const clsMap = [
    [/^"".*:$/, ""red""],
    [/^""/, ""green""],
    [/true|false/, ""blue""],
    [/null/, ""magenta""],
    [/.*/, ""darkorange""],
]

const syntaxHighlight = obj => JSON.stringify(obj, null, 4)
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, match => `<span style=""color:${clsMap.find(([regex]) => regex.test(match))[1]}"">${match}</span>`);

and a version with less regex
const clsMap = [
    [match => match.startsWith('""') && match.endsWith(':'), ""red""],
    [match => match.startsWith('""'), ""green""],
    [match => match === ""true"" || match === ""false"" , ""blue""],
    [match => match === ""null"", ""magenta""],
    [() => true, ""darkorange""],
];
    
const syntaxHighlight = obj => JSON.stringify(obj, null, 4)
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g, match => `<span style=""color:${clsMap.find(([fn]) => fn(match))[1]}"">${match}</span>`);

    This is nice:

https://github.com/mafintosh/json-markup from mafintosh

const jsonMarkup = require('json-markup')
const html = jsonMarkup({hello:'world'})
document.querySelector('#myElem').innerHTML = html


HTML

<link ref=""stylesheet"" href=""style.css"">
<div id=""myElem></div>


Example stylesheet can be found here

https://raw.githubusercontent.com/mafintosh/json-markup/master/style.css

    it's for Laravel, Codeigniter
Html:
<pre class=""jsonPre""> </pre>
Controller: Return the JSON value from the controller as like as
return json_encode($data, JSON_PRETTY_PRINT);
In script:
<script> $('.jsonPre').html(result); </script>
result will be

    If you're looking for a nice library to prettify json on a web page...

Prism.js is pretty good.

http://prismjs.com/

I found using JSON.stringify(obj, undefined, 2) to get the indentation, and then using prism to add a theme was a good approach.

If you're loading in JSON via an ajax call, then you can run one of Prism's utility methods to prettify

For example:

Prism.highlightAll()

    To highlight and beautify it in HTML using Bootstrap:

function prettifyJson(json, prettify) {
    if (typeof json !== 'string') {
        if (prettify) {
            json = JSON.stringify(json, undefined, 4);
        } else {
            json = JSON.stringify(json);
        }
    }
    return json.replace(/(""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?|\b(true|false|null)\b|-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?)/g,
        function(match) {
            let cls = ""<span>"";
            if (/^""/.test(match)) {
                if (/:$/.test(match)) {
                    cls = ""<span class='text-danger'>"";
                } else {
                    cls = ""<span>"";
                }
            } else if (/true|false/.test(match)) {
                cls = ""<span class='text-primary'>"";
            } else if (/null/.test(match)) {
                cls = ""<span class='text-info'>"";
            }
            return cls + match + ""</span>"";
        }
    );
}

    If you need this to work in a textarea the accepted solution will not work.

<textarea id='textarea'></textarea>

$(""#textarea"").append(formatJSON(JSON.stringify(jsonobject),true));   

function formatJSON(json,textarea) {
    var nl;
    if(textarea) {
        nl = ""&#13;&#10;"";
    } else {
        nl = ""<br>"";
    }
    var tab = ""&#160;&#160;&#160;&#160;"";
    var ret = """";
    var numquotes = 0;
    var betweenquotes = false;
    var firstquote = false;
    for (var i = 0; i < json.length; i++) {
        var c = json[i];
        if(c == '""') {
            numquotes ++;
            if((numquotes + 2) % 2 == 1) {
                betweenquotes = true;
            } else {
                betweenquotes = false;
            }
            if((numquotes + 3) % 4 == 0) {
                firstquote = true;
            } else {
                firstquote = false;
            }
        }

        if(c == '[' && !betweenquotes) {
            ret += c;
            ret += nl;
            continue;
        }
        if(c == '{' && !betweenquotes) {
            ret += tab;
            ret += c;
            ret += nl;
            continue;
        }
        if(c == '""' && firstquote) {
            ret += tab + tab;
            ret += c;
            continue;
        } else if (c == '""' && !firstquote) {
            ret += c;
            continue;
        }
        if(c == ',' && !betweenquotes) {
            ret += c;
            ret += nl;
            continue;
        }
        if(c == '}' && !betweenquotes) {
            ret += nl;
            ret += tab;
            ret += c;
            continue;
        }
        if(c == ']' && !betweenquotes) {
            ret += nl;
            ret += c;
            continue;
        }
        ret += c;
    } // i loop
    return ret;
}

    Douglas Crockford's JSON in JavaScript library will pretty print JSON via the stringify method.

You may also find the answers to this older question useful: How can I pretty-print JSON in (unix) shell script?
    I ran into an issue today with @Pumbaa80's code. I'm trying to apply JSON syntax highlighting to data that I'm rendering in a Mithril view, so I need to create DOM nodes for everything in the JSON.stringify output.

I split the really long regex into its component parts as well.

render_json = (data) ->
  # wraps JSON data in span elements so that syntax highlighting may be
  # applied. Should be placed in a `whitespace: pre` context
  if typeof(data) isnt 'string'
    data = JSON.stringify(data, undefined, 2)
  unicode =     /""(\\u[a-zA-Z0-9]{4}|\\[^u]|[^\\""])*""(\s*:)?/
  keyword =     /\b(true|false|null)\b/
  whitespace =  /\s+/
  punctuation = /[,.}{\[\]]/
  number =      /-?\d+(?:\.\d*)?(?:[eE][+\-]?\d+)?/

  syntax = '(' + [unicode, keyword, whitespace,
            punctuation, number].map((r) -> r.source).join('|') + ')'
  parser = new RegExp(syntax, 'g')

  nodes = data.match(parser) ? []
  select_class = (node) ->
    if punctuation.test(node)
      return 'punctuation'
    if /^\s+$/.test(node)
      return 'whitespace'
    if /^\""/.test(node)
      if /:$/.test(node)
        return 'key'
      return 'string'

    if /true|false/.test(node)
      return 'boolean'

     if /null/.test(node)
       return 'null'
     return 'number'
  return nodes.map (node) ->
    cls = select_class(node)
    return Mithril('span', {class: cls}, node)


Code in context on Github here
    Here is how you can print without using native function.

function pretty(ob, lvl = 0) {

  let temp = [];

  if(typeof ob === ""object""){
    for(let x in ob) {
      if(ob.hasOwnProperty(x)) {
        temp.push( getTabs(lvl+1) + x + "":"" + pretty(ob[x], lvl+1) );
      }
    }
    return ""{\n""+ temp.join("",\n"") +""\n"" + getTabs(lvl) + ""}"";
  }
  else {
    return ob;
  }

}

function getTabs(n) {
  let c = 0, res = """";
  while(c++ < n)
    res+=""\t"";
  return res;
}

let obj = {a: {b: 2}, x: {y: 3}};
console.log(pretty(obj));

/*
  {
    a: {
      b: 2
    },
    x: {
      y: 3
    }
  }
*/

    The simplest way to display an object for debugging purposes:

console.log(""data"",data) // lets you unfold the object manually


If you want to display the object in the DOM, you should consider that it could contain strings that would be interpreted as HTML.  Therefore, you need to do some escaping...

var s = JSON.stringify(data,null,2) // format
var e = new Option(s).innerHTML // escape
document.body.insertAdjacentHTML('beforeend','<pre>'+e+'</pre>') // display

    <!-- here is a complete example pretty print with more space between lines-->
<!-- be sure to pass a json string not a json object -->
<!-- use line-height to increase or decrease spacing between json lines -->

<style  type=""text/css"">
.preJsonTxt{
  font-size: 18px;
  text-overflow: ellipsis;
  overflow: hidden;
  line-height: 200%;
}
.boxedIn{
  border: 1px solid black;
  margin: 20px;
  padding: 20px;
}
</style>

<div class=""boxedIn"">
    <h3>Configuration Parameters</h3>
    <pre id=""jsonCfgParams"" class=""preJsonTxt"">{{ cfgParams }}</pre>
</div>

<script language=""JavaScript"">
$( document ).ready(function()
{
     $(formatJson);

     <!-- this will do a pretty print on the json cfg params      -->
     function formatJson() {
         var element = $(""#jsonCfgParams"");
         var obj = JSON.parse(element.text());
        element.html(JSON.stringify(obj, undefined, 2));
     }
});
</script>

    Quick pretty human-readable JSON output in 1 line code (without colors):
document.documentElement.innerHTML='<pre>'+JSON.stringify(obj, null, 2)+'</pre>';

    ","[3039, 6175, 360, 84, 56, 13, 14, 39, 28, 16, 29, 4, 5, 6, 12, 11, 7, 8, 8, 2, 3, 2, 4, 2, 4, 5, 5, 1, 1, 1, 0]",1461334,710,2011-01-26T22:33:53,2022-04-12 00:44:50Z,javascript 
How do I make git use the editor of my choice for commits?,"
                
I would prefer to write my commit messages in Vim, but git is opening them in Emacs.
How do I configure git to always use Vim?  Note that I want to do this globally, not just for a single project.
    If you want to set the editor only for Git, do either (you dont need both):

Set core.editor in your Git config: git config --global core.editor ""vim""

OR

Set the GIT_EDITOR environment variable: export GIT_EDITOR=vim


If you want to set the editor for Git and also other programs, set the standardized VISUAL and EDITOR environment variables*:
export VISUAL=vim
export EDITOR=""$VISUAL""

NOTE: Setting both is not necessarily needed, but some programs may not use the more-correct VISUAL. See VISUAL vs. EDITOR.

Some editors require a --wait flag, or they will open a blank page. For example:

Sublime Text (if correctly set up; or use the full path to the executable in place of subl):
export VISUAL=""subl --wait""

VS Code (after adding the shell command):
export VISUAL=""code --wait""


    Copy paste this:
git config --global core.editor ""vim""

In case you'd like to know what you're doing. From man git-commit:

ENVIRONMENT AND CONFIGURATION VARIABLES
The editor used to edit the commit log message will be chosen from the GIT_EDITOR environment variable, the core.editor configuration variable, the VISUAL environment variable, or the EDITOR environment variable (in that order).

    On Ubuntu and also Debian (thanks @MichielB) changing the default editor is also possible by running: 

sudo update-alternatives --config editor


Which will prompt the following:

There are 4 choices for the alternative editor (providing /usr/bin/editor).

  Selection    Path                Priority   Status
------------------------------------------------------------
  0            /bin/nano            40        auto mode
  1            /bin/ed             -100       manual mode
  2            /bin/nano            40        manual mode
* 3            /usr/bin/vim.basic   30        manual mode
  4            /usr/bin/vim.tiny    10        manual mode

Press enter to keep the current choice[*], or type selection number: 

    To Make Visual Studio Code (vscode) your default git editor

git config --global core.editor ""code --wait""

    In Windows 7, setting editor to Notepad++

Open any text editor.
Open this file: C:\Users\YOUR_USERNAME\.gitconfig
Add this section to the bottom:


For 64 bit Notepad++ use:
[core]
    editor = 'C:/Program Files/Notepad++/notepad++.exe' -multiInst -notabbar

For 32 bit Notepad++ use:
[core]
    editor = 'C:/Program Files (x86)/Notepad++/notepad++.exe' -multiInst -notabbar


Save and close the file.
When you're committing with git, just write git commit and press Enter. It will pop open Notepad++.
Write your commit message at the top of the file, and save and close the file. Done!

    To make vim the default editor for git on ubuntu 20:04 run the following command:

git config --global core.editor vim

    In windows 7, while adding the ""Sublime"" editor it was still giving me an error:

Aborting commit due to empty commit message.

Sublime was not able to keep the focus.
To fix this I opened the .gitconfig file in c:/users/username/ folder and added the following line with --wait option outside the single quotes.
[core]
      editor = 'F:/Program Files/Sublime Text 2/sublime_text.exe' --wait

Hope its helpful to somebody facing similar issue with Sublime.
    Atom as your git editor
git config --global core.editor ""atom --wait""


Atom needs to be configured to run from the command line for the above to work:
OS X: install shell commands from Atom: menu bar > Atom >
Install Shell Commands
Windows: no action required - atom is configured to run from the command line by default

    This provides an answer for people who arrive at this Question that may want to link an editor other than vim.

The linked resource, by Github,is likely to be kept up to date, when editors are updated, even if answers on SO (including this one) are not.

Associating Text Editors with git  

Github's post shows exactly what to type in to your command line for various editors, including the options/flags specific to each editor for it to work best with git.  

Notepad++:
git config --global core.editor ""'C:/Program Files (x86)/Notepad++/notepad++.exe' -multiInst -notabbar -nosession -noPlugin""

Sublime Text:
git config --global core.editor ""'c:/Program Files/sublime text 3/subl.exe' -w""

Atom:
git config --global core.editor ""atom --wait""

The commands above assume your editor has been installed in the default directory for a windows machine.

The commands basically add the text between double-quotes to .gitconfig in your home directory.
On a windows machine home is likely to be C:\Users\your-user-name, where your-user-name is your login name.
From the command line, you can reach this directory by typing in cd ~.

for example, a command above would be add the following line under the [core] section like so:
[core] 
       editor = 'C:/Program Files/sublime text 3/subl.exe' -w


If you have a different editor, just replace with the path to your editor, using either method above. (and hope no flags are needed for optimal usage.)
    there is a list of commad that you can use but for vs code use this
 git config --global core.editor ""code --wait""

this is the link for all editor :https://git-scm.com/book/en/v2/Appendix-C%3A-Git-Commands-Setup-and-Config
    Windows: setting notepad as the default commit message editor

git config --global core.editor notepad.exe


Hit Ctrl+S to save your commit message. To discard, just close the notepad window without saving.

In case you hit the shortcut for save, then decide to abort, go to File->Save as, and in the dialog that opens, change ""Save as type"" to ""All files (*.*)"". You will see a file named ""COMMIT_EDITMSG"". Delete it, and close notepad window. 

Edit: Alternatively, and more easily, delete all the contents from the open notepad window and hit save. (thanks mwfearnley for the comment!)

I think for small write-ups such as commit messages notepad serves best, because it is simple, is there with windows, opens up in no time. Even your sublime may take a second or two to get fired up when you have a load of plugins and stuff.
    Best settings for Sublime Text 3 or 4 as your Git editor (Windows & Linux instructions):
To follow these instructions in Windows make sure you have installed Git for Windows. In Windows, I like to use Git Bash so that it feels more like Linux.
First, we want to create a special Sublime Text project so that we can specify special project settings we want set whenever Git calls the editor, to make things easier when editing in Git. For example, I normally set my ruler to 120 chars in most projects, but for Git commit messages I want it to be 72 characters so that it fits nicely in a terminal when you call git log or git lg.

1. Create a Sublime Text project with settings we want to use to edit Git commit messages
Open Sublime Text and go to menu ""File""  ""New Window"" to create a new anonymous project. Go to menu ""Project""  ""Save Project As..."" and choose a place to save it. In Linux I saved it in my Linux home directory with the file name .gitconfig.sublime-project. Its path is therefore: ~/.gitconfig.sublime-project. In Windows also save it in your home directory, for example: C:\Users\MY_USER_NAME\.gitconfig.sublime-project  Now go to menu ""Project""  ""Edit Project"" to edit the project settings. Paste the following and save the settings. Make any further edits for your project settings if desired.
{
    // For folder settings help see here: https://www.sublimetext.com/docs/3/projects.html

    ""folders"":
    [

    ],

    ""settings"":
    {

        // Disables horizontal scrolling if enabled.
        // May be set to true, false, or ""auto"", where it will be disabled for
        // source code, and otherwise enabled.
        ""word_wrap"": false,

        // Set to a value other than 0 to force wrapping at that column rather than the
        // window width
        ""wrap_width"": 0,

        // Columns in which to display vertical rulers
        ""rulers"": [72, 50], //72 is recommended by git for commit message content, and 50 for commit titles

        // The number of spaces a tab is considered equal to
        ""tab_size"": 4,

        // Set to true to insert spaces when tab is pressed
        ""translate_tabs_to_spaces"": true,
    },

    ""build_systems"":
    [

    ]

}


2. Set the editor to be used by Git
Now we need to set the editor to be used by Git, by editing the .gitconfig file.
For Linux:
Your user copy of this will be located in ~/.gitconfig. Open this file and add the following lines. Be sure to use the proper path name to the Git project you just created above! I'm using ~/.gitconfig.sublime-project.
[core]
    editor = subl --project ~/.gitconfig.sublime-project --wait

The --wait is important, as it forces Git to wait until you close the file before it continues on. The --project line is important to tell Sublime Text which project you want opened whenever Git opens Sublime Text.
Per @digitaldreamer's answer above (https://stackoverflow.com/a/2596835/4561887), ""subl can be replaced by the full path of the executable but [the alias subl] is usually available when [Sublime is] correctly installed.""
For Windows:
For Windows, first read the Linux instructions for background information. Now we will do something almost identical.
(OPTIONAL: create a subl alias for use in Git Bash):
Open up a text editor (for example, Notepad, Notepad++, Sublime Text, Geany, etc.), and create a file called "".bash_profile"" in your home directory. Its path will therefore be: C:\Users\MY_USER_NAME\.bash_profile. Save the following into it:
alias subl=""/c/Program\ Files/Sublime\ Text\ 3/subl.exe""

This creates a Git Bash alias called subl that we can now use in Git Bash for Windows, to easily open Sublime Text. This step isn't required, but it's useful for general Git Bash use. Now you can call subl ., for instance, in Git Bash to open up a new Sublime Text project in your current directory.
(MANDATORY):
Edit the .gitconfig file found in your home directory: C:\Users\MY_USER_NAME\.gitconfig, by adding the following to it. Notice the subtle changes from the Linux instructions above:
[core]
  editor = 'C:/Program Files/Sublime Text 3/subl.exe' --project ~/.gitconfig.sublime-project --wait


Notice that you must specify the full path to the Sublime Text executable. Note the direction of the slashes! Use / NOT \ to separate folders in the path name! (Thanks VonC for helping me see this).
Our subl alias we made for Git Bash above doesn't work here, so you can't use it like we did in the Linux example, instead you must specify the whole path as shown above.
The ~ symbol, however, does still work here to get to your Windows home directory.


2.5. (Optional) Install the ""Git"" package into Sublime Text 3.
This gives you syntax highlighting for git commit messages, as well as access to other Git commands such as git blame (which I use frequently in Sublime Text) or git commit (which I don't use in Sublime Text since I'd prefer the command-line for general Git flow, as I've mentioned in my comments below this answer).
To install a package: First, ensure Package Control is installed. Next, press Ctrl + Shift + P (same as Tools  Command Palette) and type all or part of Package Control: Install Package, then press Enter. In the search box that comes up, search for the package ""Git"" and hit Enter on it, or click on it, to automatically install it.
Once installed, Ctrl + Shift + P then searching for ""git"" will bring up Git commands you can use internally inside Sublime Text now, such as git blame.

3. Use it
Now when you call git commit, for instance, as normal from the command-line, Sublime Text will open up into the .gitconfig.sublime-project we created above, with that project's settings! As you type a paragraph you'll notice it extends past the ruler we set since soft word-wrap is off. To force hard wrap via auto-inserted hard-returns at the end of each line, put your cursor on the long line you want auto-wrapped and press Alt + Q. It will now hard-wrap/hard-fold at 72 characters, which is what we set in the project settings' ""rulers"" parameter above.
Now, save your commit message with Ctrl + S, and exit (to complete your git commit) with Ctrl + Shift + W.
Done!
Related:

Git mergetool with Meld on Windows
https://github.com/ElectricRCAircraftGuy/eRCaGuy_dotfiles

    Setting Sublime Text 2 as Git commit editor in Mac OSX 10

Run this command:

$ git config --global core.editor ""/Applications/Sublime\ Text\ 2.app/Contents/SharedSupport/bin/subl""


Or just:

$ git config --global core.editor ""subl -w""

    For emacs users

.emacs:

(server-start)


shellrc:

export EDITOR=emacsclient

    And if you are working with designers using the command line then Pico, and dont know short cuts ;)

git config --global core.editor ""pico""


Or

export VISUAL=pico
export EDITOR=pico

    For MacOSX, using TextEdit or the natural environmental editor for text:

git config --global core.editor ""open -W -n""

    Just because I came here looking for a one-time solution (in my case, I usually use vim but this one time I wanted to use VS Code) for a single command and others might want to know as well:

GIT_EDITOR='code -w' git rebase -i 


Here's my git/hub version just for context:

git version 2.24.2 (Apple Git-127)
hub version 2.14.1

    For Windows users who want to use neovim with the Windows Subsystem for Linux:

git config core.editor ""C:/Windows/system32/bash.exe --login -c 'nvim .git/COMMIT_EDITMSG'""


This is not a fool-proof solution as it doesn't handle interactive rebasing (for example). Improvements very welcome!
    Mvim as your git editor

Like all the other GUI applications, you have to launch mvim with the wait flag.

git config --global core.editor ""mvim --remote-wait""

    For Windows, Neovim:
# .gitconfig

[core]
    editor='C:/tools/neovim/Neovim/bin/nvim-qt.exe'

    On macOS Big Sur (11.0) beta for TextMate: none of the environment variable options worked. (Set all three: GIT_EDITOR, VISUAL, and EDITOR.)
Finally set the global core.editor in git, and that worked:
git config --global core.editor ""~/bin/mate -w""
    When using git-review I had to modify sequence.editor value to be able to do interactive rebase (git rebase -i -p):

git config --global sequence.editor ""gvim""  # or whatever your prefer


gvim require: apt install vim-gtk

References


#2944 Interactive Rebase failed with warning: ""Vim: Warning: Output is not to a terminal"" 
Install section of sjurba/rebase-editor 

    Just try EDITOR=vim git commit. 

Or you can set your EDITOR to vim by export EDITOR=vim in your bashrc. 
    For users of TextWrangler from the Mac app store:

git config --global core.editor ""open -n -W -a TextWrangler""


Also, make sure your ""TextWrangler > Preferences > Application > When TextWrangler becomes active:"" setting is set to ""Do nothing""

This works for me on OS X 10.11.4 with TextWrangler 5.0.2 from the Mac app store.

Explanation:

The -n means open in a new instance.

The -W means to wait until the application exits before using the contents of the edited file as the commit message.

The -a TextWrangler means use the TextWrangler application to open the file.

See man open in your Mac Terminal app for more details.
    For IntelliJ users 

When i was trying to git rebase i was getting the following error: 
'hint: Waiting for your editor to close the file... code -n -w: code: command not found error: There was a problem with the editor 'code -n -w'.' 

The same error showed up when i was trying to associate IntelliJ with Git:


The problem was that I did not have the command code added in my environment PATH variable. And i didn't want to use Visual Studio Code from my terminal. So that is why it prompted ""command not found"". I solved this by deleting 


  editor = code -n -w


from the core section in my .gitconfig file. Git worked properly again. 
    For EmEditor users
To set EmEditor as the default text editor for Git, open Git Bash, and type:
git config --global core.editor ""emeditor.exe -sp""
EmEditor v19.9.2 or later required.
    For MAC, BBEdit:
First, open BBEdit, click on the BBEdit logo, and choose Install Command-Line Tools.
Then from the command line,
git config --global core.editor ""BBEdit -w""

    For Textmate Users

This opens Textmate editor in when you want to edit your commits.
Requires textmate command line tools to be installed.

git config --global core.editor ""mate -w""
    For Windows users who want to use Kinesics Text Editor

Create a file called 'k.sh', add the following text and place in your home directory (~):

winpty ""C:\Program Files (x86)\Kinesics Text Editor\x64\k.exe"" $1


At the git prompt type:

git config --global core.editor ~/k.sh

    ","[3025, 4116, 755, 213, 42, 63, 19, 66, 22, 15, 4, 15, 10, 20, 12, 25, 3, 2, 3, 4, 1, 1, 1, 1, 1, 0, 0, 0, -1, -1]",1171670,560,2010-04-08T00:28:46,2021-11-18 18:44:34Z,
"Is there an ""exists"" function for jQuery?","
                
How can I check the existence of an element in jQuery?

The current code that I have is this:

if ($(selector).length > 0) {
    // Do something
}


Is there a more elegant way to approach this? Perhaps a plugin or a function?
    In JavaScript, everything is 'truthy' or 'falsy', and for numbers 0 means false, everything else true. So you could write:
if ($(selector).length)

You don't need that >0 part.
    Yes!

jQuery.fn.exists = function(){ return this.length > 0; }

if ($(selector).exists()) {
    // Do something
}


This is in response to: Herding Code podcast with Jeff Atwood
    If you used

jQuery.fn.exists = function(){return ($(this).length > 0);}
if ($(selector).exists()) { }


you would imply that chaining was possible when it is not.

This would be better:

jQuery.exists = function(selector) {return ($(selector).length > 0);}
if ($.exists(selector)) { }


Alternatively, from the FAQ:

if ( $('#myDiv').length ) { /* Do something */ }


You could also use the following. If there are no values in the jQuery object array then getting the first item in the array would return undefined.

if ( $('#myDiv')[0] ) { /* Do something */ }

    You can use this:

// if element exists
if($('selector').length){ /* do something */ }




// if element does not exist
if(!$('selector').length){ /* do something */ }

    The fastest and most semantically self explaining way to check for existence is actually by using plain JavaScript:

if (document.getElementById('element_id')) {
    // Do something
}


It is a bit longer to write than the jQuery length alternative, but executes faster since it is a native JS method.

And it is better than the alternative of writing your own jQuery function. That alternative is slower, for the reasons @snover stated. But it would also give other programmers the impression that the exists() function is something inherent to jQuery. JavaScript would/should be understood by others editing your code, without increased knowledge debt.

NB: Notice the lack of an '#' before the element_id (since this is plain JS, not jQuery).
    You can save a few bytes by writing:

if ($(selector)[0]) { ... }


This works because each jQuery object also masquerades as an array, so we can use the array dereferencing operator to get the first item from the array. It returns undefined if there is no item at the specified index.
    I see most of the answers here are not accurate as they should be, they check element length, it can be OK in many cases, but not 100%, imagine if number pass to the function instead, so I prototype a function which check all conditions and return the answer as it should be:

$.fn.exists = $.fn.exists || function() { 
  return !!(this.length && (this[0] instanceof HTMLDocument || this[0] instanceof HTMLElement)); 
}


This will check both length and type, Now you can check it this way:

$(1980).exists(); //return false
$([1,2,3]).exists(); //return false
$({name: 'stackoverflow', url: 'http://www.stackoverflow.com'}).exists(); //return false
$([{nodeName: 'foo'}]).exists() // returns false
$('div').exists(); //return true
$('.header').exists(); //return true
$(document).exists(); //return true
$('body').exists(); //return true

    You can use:

if ($(selector).is('*')) {
  // Do something
}


A little more elegant, perhaps.
    This plugin can be used in an if statement like if ($(ele).exist()) { /* DO WORK */ } or using a callback.

Plugin

;;(function($) {
    if (!$.exist) {
        $.extend({
            exist: function() {
                var ele, cbmExist, cbmNotExist;
                if (arguments.length) {
                    for (x in arguments) {
                        switch (typeof arguments[x]) {
                            case 'function':
                                if (typeof cbmExist == ""undefined"") cbmExist = arguments[x];
                                else cbmNotExist = arguments[x];
                                break;
                            case 'object':
                                if (arguments[x] instanceof jQuery) ele = arguments[x];
                                else {
                                    var obj = arguments[x];
                                    for (y in obj) {
                                        if (typeof obj[y] == 'function') {
                                            if (typeof cbmExist == ""undefined"") cbmExist = obj[y];
                                            else cbmNotExist = obj[y];
                                        }
                                        if (typeof obj[y] == 'object' && obj[y] instanceof jQuery) ele = obj[y];
                                        if (typeof obj[y] == 'string') ele = $(obj[y]);
                                    }
                                }
                                break;
                            case 'string':
                                ele = $(arguments[x]);
                                break;
                        }
                    }
                }

                if (typeof cbmExist == 'function') {
                    var exist =  ele.length > 0 ? true : false;
                    if (exist) {
                        return ele.each(function(i) { cbmExist.apply(this, [exist, ele, i]); });
                    }
                    else if (typeof cbmNotExist == 'function') {
                        cbmNotExist.apply(ele, [exist, ele]);
                        return ele;
                    }
                    else {
                        if (ele.length <= 1) return ele.length > 0 ? true : false;
                        else return ele.length;
                    }
                }
                else {
                    if (ele.length <= 1) return ele.length > 0 ? true : false;
                    else return ele.length;
                }

                return false;
            }
        });
        $.fn.extend({
            exist: function() {
                var args = [$(this)];
                if (arguments.length) for (x in arguments) args.push(arguments[x]);
                return $.exist.apply($, args);
            }
        });
    }
})(jQuery);


jsFiddle

You may specify one or two callbacks. The first one will fire if the element exists, the second one will fire if the element does not exist. However, if you choose to pass only one function, it will only fire when the element exists. Thus, the chain will die if the selected element does not exist. Of course, if it does exist, the first function will fire and the chain will continue.

Keep in mind that using the callback variant helps maintain chainability  the element is returned and you can continue chaining commands as with any other jQuery method!

Example Uses

if ($.exist('#eleID')) {    /*    DO WORK    */ }        //    param as STRING
if ($.exist($('#eleID'))) { /*    DO WORK    */ }        //    param as jQuery OBJECT
if ($('#eleID').exist()) {  /*    DO WORK    */ }        //    enduced on jQuery OBJECT

$.exist('#eleID', function() {            //    param is STRING && CALLBACK METHOD
    /*    DO WORK    */
    /*    This will ONLY fire if the element EXIST    */
}, function() {            //    param is STRING && CALLBACK METHOD
    /*    DO WORK    */
    /*    This will ONLY fire if the element DOES NOT EXIST    */
})

$('#eleID').exist(function() {            //    enduced on jQuery OBJECT with CALLBACK METHOD
    /*    DO WORK    */
    /*    This will ONLY fire if the element EXIST    */
})

$.exist({                        //    param is OBJECT containing 2 key|value pairs: element = STRING, callback = METHOD
    element: '#eleID',
    callback: function() {
        /*    DO WORK    */
        /*    This will ONLY fire if the element EXIST    */
    }
})

    Checking for existence of an element is documented neatly in the official jQuery website itself!


  Use the .length property of the jQuery collection returned by your
  selector:

if ($(""#myDiv"").length) {
    $(""#myDiv"").show();
}

  
  Note that it isn't always necessary to test whether an element exists.
  The following code will show the element if it exists, and do nothing
  (with no errors) if it does not:

$(""#myDiv"").show();


    You can check element is present or not using length in java script.
   If length is greater than zero then element is present if length is zero then
   element is not present   

// These by Id
if ($(""#elementid"").length > 0) {
  // Element is Present
} else {
  // Element is not Present
}

// These by Class
if ($("".elementClass"").length > 0) {
  // Element is Present
} else {
  // Element is not Present
}

    You could use this:

jQuery.fn.extend({
    exists: function() { return this.length }
});

if($(selector).exists()){/*do something*/}

    There's no need for jQuery really. With plain JavaScript it's easier and semantically correct to check for:

if(document.getElementById(""myElement"")) {
    //Do something...
}


If for any reason you don't want to put an id to the element, you can still use any other JavaScript method designed to access the DOM.

jQuery is really cool, but don't let pure JavaScript fall into oblivion...
    Is $.contains() what you want?

jQuery.contains( container, contained )
The $.contains() method returns true if the DOM element provided by the second argument is a descendant of the DOM element provided by the first argument, whether it is a direct child or nested more deeply. Otherwise, it returns false. Only element nodes are supported; if the second argument is a text or comment node, $.contains() will return false.
Note: The first argument must be a DOM element, not a jQuery object or plain JavaScript object.

    No need for jQuery (basic solution)

if(document.querySelector('.a-class')) {
  // do something
}


Much more performant option below(notice lack of a dot before a-class).

if(document.getElementsByClassName('a-class')[0]) {
  // do something
}


querySelector uses a proper matching engine like $() (sizzle) in jQuery and uses more computing power but in 99% cases will do just fine. The second option is more explicit and tells the code exactly what to do. It's much faster according to jsperf https://jsperf.com/getelementsbyclassname-vs-queryselectorall/25
    The reason all of the previous answers require the .length parameter is that they are mostly using jquery's $() selector which has querySelectorAll behind the curtains (or they are using it directly).  This method is rather slow because it needs to parse the entire DOM tree looking for all matches to that selector and populating an array with them.

The ['length'] parameter is not needed or useful and the code will be a lot faster if you directly use document.querySelector(selector) instead, because it returns the first element it matches or null if not found.

function elementIfExists(selector){  //named this way on purpose, see below
    return document.querySelector(selector);
}
/* usage: */
var myelement = elementIfExists(""#myid"") || myfallbackelement;


However this method leaves us with the actual object being returned; which is fine if it isn't going to be saved as variable and used repeatedly (thus keeping the reference around if we forget).

var myel=elementIfExists(""#myid"");
// now we are using a reference to the element which will linger after removal
myel.getParentNode.removeChild(myel);
console.log(elementIfExists(""#myid"")); /* null */
console.log(myel); /* giant table lingering around detached from document */
myel=null; /* now it can be garbage collected */


In some cases this may be desired.  It can be used in a for loop like this:

/* locally scoped myel gets garbage collected even with the break; */
for (var myel; myel = elementIfExist(sel); myel.getParentNode.removeChild(myel))
    if (myel == myblacklistedel) break;


If you don't actually need the element and want to get/store just a true/false, just double not it !!  It works for shoes that come untied, so why knot here?

function elementExists(selector){
    return !!document.querySelector(selector);
}
/* usage: */
var hastables = elementExists(""table"");  /* will be true or false */
if (hastables){
    /* insert css style sheet for our pretty tables */
}
setTimeOut(function (){if (hastables && !elementExists(""#mytablecss""))
                           alert(""bad table layouts"");},3000);

    In Javascript
if (typeof selector != ""undefined"") {
   console.log(""selector exists"");
} else {
   console.log(""selector does not exists"");
}

In jQuery
if($('selector').length){
    alert(""selector exists"");
} else{
    alert(""selector does not exists"");
}

    I have found if ($(selector).length) {} to be insufficient. It will silently break your app when selector is an empty object {}.

var $target = $({});        
console.log($target, $target.length);

// Console output:
// -------------------------------------
// [ Object              ] 1
//     __proto__: Object


My only suggestion is to perform an additional check for {}.

if ($.isEmptyObject(selector) || !$(selector).length) {
    throw new Error('Unable to work with the given selector.');
}


I'm still looking for a better solution though as this one is a bit heavy.

Edit: WARNING! This doesn't work in IE when selector is a string.

$.isEmptyObject('hello') // FALSE in Chrome and TRUE in IE

    Try this.

simple and short and usable in the whole project:

jQuery.fn.exists=function(){return !!this[0];}; //jQuery Plugin


Usage:

console.log($(""element-selector"").exists());


_________________________________

OR EVEN SHORTER:
(for when you don't want to define a jQuery plugin):

if(!!$(""elem-selector"")[0]) ...;


or even

if($(""elem-selector"")[0]) ...;

    You don't have to check if it's greater than 0 like $(selector).length > 0, $(selector).length it's enough and an elegant way to check the existence of elements. I don't think that it is worth to write a function only for this, if you want to do more extra things, then yes.
if($(selector).length){
  // true if length is not 0
} else {
  // false if length is 0
}

    I found that this is the most jQuery way, IMHO.
Extending the default function is easy and can be done in a global extension file.
$.fn.exist = function(){
  return !!this.length;
};

console.log($(""#yes"").exist())

console.log($(""#no"").exist())<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>

<div id=""yes"">id = yes</div>

    this is very similar to all of the answers, but why not use the ! operator twice so you can get a boolean:

jQuery.fn.exists = function(){return !!this.length};

if ($(selector).exists()) {
    // the element exists, now what?...
}

    Try testing for DOM element

if (!!$(selector)[0]) // do stuff

    By default - No.
There's the length property that is commonly used for the same result in the following way:
if ($(selector).length)

Here, 'selector' is to be replaced by the actual selector you are interested to find if it exists or not. If it does exist, the length  property will output an integer more than 0 and hence the if statement will become true and hence execute the if block. If it doesn't, it will output the integer '0' and hence the if block won't get executed.
    Thanks for sharing this question. First of all, there are multiple ways to check it.
If you want to check whether an HTML element exists in DOM. To do so you can try the following ways.

Using Id selector: In DOM to select element by id, you should provide the name of the id starting with the prefix (#). You have to make sure that each Html element in the DOM must have unique id.
Using class selector: You can select all elements with belonging to a specific class by using the prefix (.).

Now if you want to check if the element exist or not in DOM you can check it using the following code.
if($(""#myId"").length){
   //id selector
} 

if($("".myClass"").length){
   //class selector
} 

If you want to check any variable is undefined or not. You can check it using the following code.
let x
if(x)
  console.log(""X"");
else
  console.log(""X is not defined"");

    I just like to use plain vanilla javascript to do this.

function isExists(selector){
  return document.querySelectorAll(selector).length>0;
}

    Inspired by hiway's answer I came up with the following:

$.fn.exists = function() {
    return $.contains( document.documentElement, this[0] );
}


jQuery.contains takes two DOM elements and checks whether the first one contains the second one.

Using document.documentElement as the first argument fulfills the semantics of the exists method when we want to apply it solely to check the existence of an element in the current document.

Below, I've put together a snippet that compares jQuery.exists() against the $(sel)[0] and $(sel).length approaches which both return truthy values for $(4) while $(4).exists() returns false. In the context of checking for existence of an element in the DOM this seems to be the desired result.

$.fn.exists = function() {
    return $.contains(document.documentElement, this[0]); 
  }
  
  var testFuncs = [
    function(jq) { return !!jq[0]; },
    function(jq) { return !!jq.length; },
    function(jq) { return jq.exists(); },
  ];
    
  var inputs = [
    [""$()"",$()],
    [""$(4)"",$(4)],
    [""$('#idoexist')"",$('#idoexist')],
    [""$('#idontexist')"",$('#idontexist')]
  ];
  
  for( var i = 0, l = inputs.length, tr, input; i < l; i++ ) {
    input = inputs[i][1];
    tr = ""<tr><td>"" + inputs[i][0] + ""</td><td>""
          + testFuncs[0](input) + ""</td><td>""
          + testFuncs[1](input) + ""</td><td>""
          + testFuncs[2](input) + ""</td></tr>"";
    $(""table"").append(tr);
  }td { border: 1px solid black }<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<div id=""idoexist"">#idoexist</div>
<table style>
<tr>
  <td>Input</td><td>!!$(sel)[0]</td><td>!!$(sel).length</td><td>$(sel).exists()</td>
</tr>
</table>
<script>
  
  $.fn.exists = function() {
    return $.contains(document.documentElement, this[0]); 
  }
  
</script>

    Here is the complete example of different situations and way to check if element exists using direct if on jQuery selector may or may not work because it returns array or elements.
var a = null;

var b = []

var c = undefined ;

if(a) { console.log("" a exist"")} else { console.log(""a doesn't exit"")}
// output: a doesn't exit

if(b) { console.log("" b exist"")} else { console.log(""b doesn't exit"")}
// output: b exist

if(c) { console.log("" c exist"")} else { console.log(""c doesn't exit"")}
// output: c doesn't exit

FINAL SOLUTION
if($(""#xysyxxs"").length){ console.log(""xusyxxs exist"")} else { console.log(""xusyxxs doesnn't exist"") }
//output : xusyxxs doesnn't exist

if($("".xysyxxs"").length){ console.log(""xusyxxs exist"")} else { console.log(""xusyxxs doesnn't exist"") }
    //output : xusyxxs doesnn't exist

Demo
console.log(""existing id"", $('#id-1').length)
console.log(""non existing id"", $('#id-2').length)

console.log(""existing class single instance"", $('.cls-1').length)
console.log(""existing class multiple instance"", $('.cls-2').length)
console.log(""non existing class"", $('.cls-3').length)<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<div id=""id-1"">
  <div class=""cls-1 cls-2""></div>
  <div class=""cls-2""></div>
</div>

    I stumbled upon this question and i'd like to share a snippet of code i currently use:

$.fn.exists = function(callback) {
    var self = this;
    var wrapper = (function(){
            function notExists () {}

            notExists.prototype.otherwise = function(fallback){
                if (!self.length) {                    
                    fallback.call();
                }
            };

            return new notExists;
        })();

    if(self.length) {
        callback.call();    
    }

    return wrapper;
}


And now i can write code like this - 

$(""#elem"").exists(function(){
    alert (""it exists"");
}).otherwise(function(){
    alert (""it doesn't exist"");
});


It might seem a lot of code, but when written in CoffeeScript it is quite small:

$.fn.exists = (callback) ->
    exists = @length
    callback.call() if exists        
    new class
       otherwise: (fallback) ->            
            fallback.call() if not exists

    $(""selector"") returns an object which has the length property. If the selector finds any elements, they will be included in the object. So if you check its length you can see if any elements exist. In JavaScript 0 == false, so if you don't get 0 your code will run.

if($(""selector"").length){
   //code in the case
} 

    ","[3021, 2688, 1410, 387, 144, 111, 82, 67, 72, 70, 38, 44, 58, 58, 44, 29, 47, 3, 42, 12, 15, 3, 33, 29, 3, 2, 26, 28, 13, 24, 16]",832111,421,2008-08-27T19:49:41,2021-12-06 12:57:19Z,javascript 
How to make function decorators and chain them together?,"
                
How can I make two decorators in Python that would do the following?
@makebold
@makeitalic
def say():
   return ""Hello""

...which should return:
""<b><i>Hello</i></b>""

    If you are not into long explanations, see Paolo Bergantinos answer.
Decorator Basics
Pythons functions are objects
To understand decorators, you must first understand that functions are objects in Python. This has important consequences. Lets see why with a simple example :
def shout(word=""yes""):
    return word.capitalize()+""!""

print(shout())
# outputs : 'Yes!'

# As an object, you can assign the function to a variable like any other object 
scream = shout

# Notice we don't use parentheses: we are not calling the function,
# we are putting the function ""shout"" into the variable ""scream"".
# It means you can then call ""shout"" from ""scream"":

print(scream())
# outputs : 'Yes!'

# More than that, it means you can remove the old name 'shout',
#and the function will still be accessible from 'scream'

del shout
try:
    print(shout())
except NameError as e:
    print(e)
    #outputs: ""name 'shout' is not defined""

print(scream())
# outputs: 'Yes!'

Keep this in mind. Well circle back to it shortly.
Another interesting property of Python functions is they can be defined inside another function!
def talk():

    # You can define a function on the fly in ""talk"" ...
    def whisper(word=""yes""):
        return word.lower()+""...""

    # ... and use it right away!
    print(whisper())

# You call ""talk"", that defines ""whisper"" EVERY TIME you call it, then
# ""whisper"" is called in ""talk"". 
talk()
# outputs: 
# ""yes...""

# But ""whisper"" DOES NOT EXIST outside ""talk"":

try:
    print(whisper())
except NameError as e:
    print(e)
    #outputs : ""name 'whisper' is not defined""*
    #Python's functions are objects

Functions references
Okay, still here? Now the fun part...
Youve seen that functions are objects. Therefore, functions:

can be assigned to a variable
can be defined in another function

That means that a function can return another function.
def getTalk(kind=""shout""):

    # We define functions on the fly
    def shout(word=""yes""):
        return word.capitalize()+""!""

    def whisper(word=""yes"") :
        return word.lower()+""...""

    # Then we return one of them
    if kind == ""shout"":
        # We don't use ""()"", we are not calling the function,
        # we are returning the function object
        return shout  
    else:
        return whisper

# How do you use this strange beast?

# Get the function and assign it to a variable
talk = getTalk()      

# You can see that ""talk"" is here a function object:
print(talk)
#outputs : <function shout at 0xb7ea817c>

# The object is the one returned by the function:
print(talk())
#outputs : Yes!

# And you can even use it directly if you feel wild:
print(getTalk(""whisper"")())
#outputs : yes...

Theres more!
If you can return a function, you can pass one as a parameter:
def doSomethingBefore(func): 
    print(""I do something before then I call the function you gave me"")
    print(func())

doSomethingBefore(scream)
#outputs: 
#I do something before then I call the function you gave me
#Yes!

Well, you just have everything needed to understand decorators. You see, decorators are wrappers, which means that they let you execute code before and after the function they decorate without modifying the function itself.
Handcrafted decorators
How youd do it manually:
# A decorator is a function that expects ANOTHER function as parameter
def my_shiny_new_decorator(a_function_to_decorate):

    # Inside, the decorator defines a function on the fly: the wrapper.
    # This function is going to be wrapped around the original function
    # so it can execute code before and after it.
    def the_wrapper_around_the_original_function():

        # Put here the code you want to be executed BEFORE the original function is called
        print(""Before the function runs"")

        # Call the function here (using parentheses)
        a_function_to_decorate()

        # Put here the code you want to be executed AFTER the original function is called
        print(""After the function runs"")

    # At this point, ""a_function_to_decorate"" HAS NEVER BEEN EXECUTED.
    # We return the wrapper function we have just created.
    # The wrapper contains the function and the code to execute before and after. Its ready to use!
    return the_wrapper_around_the_original_function

# Now imagine you create a function you don't want to ever touch again.
def a_stand_alone_function():
    print(""I am a stand alone function, don't you dare modify me"")

a_stand_alone_function() 
#outputs: I am a stand alone function, don't you dare modify me

# Well, you can decorate it to extend its behavior.
# Just pass it to the decorator, it will wrap it dynamically in 
# any code you want and return you a new function ready to be used:

a_stand_alone_function_decorated = my_shiny_new_decorator(a_stand_alone_function)
a_stand_alone_function_decorated()
#outputs:
#Before the function runs
#I am a stand alone function, don't you dare modify me
#After the function runs

Now, you probably want that every time you call a_stand_alone_function, a_stand_alone_function_decorated is called instead. Thats easy, just overwrite a_stand_alone_function with the function returned by my_shiny_new_decorator:
a_stand_alone_function = my_shiny_new_decorator(a_stand_alone_function)
a_stand_alone_function()
#outputs:
#Before the function runs
#I am a stand alone function, don't you dare modify me
#After the function runs

# Thats EXACTLY what decorators do!

Decorators demystified
The previous example, using the decorator syntax:
@my_shiny_new_decorator
def another_stand_alone_function():
    print(""Leave me alone"")

another_stand_alone_function()  
#outputs:  
#Before the function runs
#Leave me alone
#After the function runs

Yes, thats all, its that simple. @decorator is just a shortcut to:
another_stand_alone_function = my_shiny_new_decorator(another_stand_alone_function)

Decorators are just a pythonic variant of the decorator design pattern. There are several classic design patterns embedded in Python to ease development (like iterators).
Of course, you can accumulate decorators:
def bread(func):
    def wrapper():
        print(""</''''''\>"")
        func()
        print(""<\______/>"")
    return wrapper

def ingredients(func):
    def wrapper():
        print(""#tomatoes#"")
        func()
        print(""~salad~"")
    return wrapper

def sandwich(food=""--ham--""):
    print(food)

sandwich()
#outputs: --ham--
sandwich = bread(ingredients(sandwich))
sandwich()
#outputs:
#</''''''\>
# #tomatoes#
# --ham--
# ~salad~
#<\______/>

Using the Python decorator syntax:
@bread
@ingredients
def sandwich(food=""--ham--""):
    print(food)

sandwich()
#outputs:
#</''''''\>
# #tomatoes#
# --ham--
# ~salad~
#<\______/>

The order you set the decorators MATTERS:
@ingredients
@bread
def strange_sandwich(food=""--ham--""):
    print(food)

strange_sandwich()
#outputs:
##tomatoes#
#</''''''\>
# --ham--
#<\______/>
# ~salad~


Now: to answer the question...
As a conclusion, you can easily see how to answer the question:
# The decorator to make it bold
def makebold(fn):
    # The new function the decorator returns
    def wrapper():
        # Insertion of some code before and after
        return ""<b>"" + fn() + ""</b>""
    return wrapper

# The decorator to make it italic
def makeitalic(fn):
    # The new function the decorator returns
    def wrapper():
        # Insertion of some code before and after
        return ""<i>"" + fn() + ""</i>""
    return wrapper

@makebold
@makeitalic
def say():
    return ""hello""

print(say())
#outputs: <b><i>hello</i></b>

# This is the exact equivalent to 
def say():
    return ""hello""
say = makebold(makeitalic(say))

print(say())
#outputs: <b><i>hello</i></b>

You can now just leave happy, or burn your brain a little bit more and see advanced uses of decorators.

Taking decorators to the next level
Passing arguments to the decorated function
# Its not black magic, you just have to let the wrapper 
# pass the argument:

def a_decorator_passing_arguments(function_to_decorate):
    def a_wrapper_accepting_arguments(arg1, arg2):
        print(""I got args! Look: {0}, {1}"".format(arg1, arg2))
        function_to_decorate(arg1, arg2)
    return a_wrapper_accepting_arguments

# Since when you are calling the function returned by the decorator, you are
# calling the wrapper, passing arguments to the wrapper will let it pass them to 
# the decorated function

@a_decorator_passing_arguments
def print_full_name(first_name, last_name):
    print(""My name is {0} {1}"".format(first_name, last_name))
    
print_full_name(""Peter"", ""Venkman"")
# outputs:
#I got args! Look: Peter Venkman
#My name is Peter Venkman

Decorating methods
One nifty thing about Python is that methods and functions are really the same.  The only difference is that methods expect that their first argument is a reference to the current object (self).
That means you can build a decorator for methods the same way! Just remember to take self into consideration:
def method_friendly_decorator(method_to_decorate):
    def wrapper(self, lie):
        lie = lie - 3 # very friendly, decrease age even more :-)
        return method_to_decorate(self, lie)
    return wrapper
    
    
class Lucy(object):
    
    def __init__(self):
        self.age = 32
    
    @method_friendly_decorator
    def sayYourAge(self, lie):
        print(""I am {0}, what did you think?"".format(self.age + lie))
        
l = Lucy()
l.sayYourAge(-3)
#outputs: I am 26, what did you think?

If youre making general-purpose decorator--one youll apply to any function or method, no matter its arguments--then just use *args, **kwargs:
def a_decorator_passing_arbitrary_arguments(function_to_decorate):
    # The wrapper accepts any arguments
    def a_wrapper_accepting_arbitrary_arguments(*args, **kwargs):
        print(""Do I have args?:"")
        print(args)
        print(kwargs)
        # Then you unpack the arguments, here *args, **kwargs
        # If you are not familiar with unpacking, check:
        # http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/
        function_to_decorate(*args, **kwargs)
    return a_wrapper_accepting_arbitrary_arguments

@a_decorator_passing_arbitrary_arguments
def function_with_no_argument():
    print(""Python is cool, no argument here."")

function_with_no_argument()
#outputs
#Do I have args?:
#()
#{}
#Python is cool, no argument here.

@a_decorator_passing_arbitrary_arguments
def function_with_arguments(a, b, c):
    print(a, b, c)
    
function_with_arguments(1,2,3)
#outputs
#Do I have args?:
#(1, 2, 3)
#{}
#1 2 3 
 
@a_decorator_passing_arbitrary_arguments
def function_with_named_arguments(a, b, c, platypus=""Why not ?""):
    print(""Do {0}, {1} and {2} like platypus? {3}"".format(a, b, c, platypus))

function_with_named_arguments(""Bill"", ""Linus"", ""Steve"", platypus=""Indeed!"")
#outputs
#Do I have args ? :
#('Bill', 'Linus', 'Steve')
#{'platypus': 'Indeed!'}
#Do Bill, Linus and Steve like platypus? Indeed!

class Mary(object):
    
    def __init__(self):
        self.age = 31
    
    @a_decorator_passing_arbitrary_arguments
    def sayYourAge(self, lie=-3): # You can now add a default value
        print(""I am {0}, what did you think?"".format(self.age + lie))

m = Mary()
m.sayYourAge()
#outputs
# Do I have args?:
#(<__main__.Mary object at 0xb7d303ac>,)
#{}
#I am 28, what did you think?

Passing arguments to the decorator
Great, now what would you say about passing arguments to the decorator itself?
This can get somewhat twisted, since a decorator must accept a function as an argument. Therefore, you cannot pass the decorated functions arguments directly to the decorator.
Before rushing to the solution, lets write a little reminder:
# Decorators are ORDINARY functions
def my_decorator(func):
    print(""I am an ordinary function"")
    def wrapper():
        print(""I am function returned by the decorator"")
        func()
    return wrapper

# Therefore, you can call it without any ""@""

def lazy_function():
    print(""zzzzzzzz"")

decorated_function = my_decorator(lazy_function)
#outputs: I am an ordinary function
            
# It outputs ""I am an ordinary function"", because thats just what you do:
# calling a function. Nothing magic.

@my_decorator
def lazy_function():
    print(""zzzzzzzz"")
    
#outputs: I am an ordinary function

Its exactly the same. ""my_decorator"" is called. So when you @my_decorator, you are telling Python to call the function 'labelled by the variable ""my_decorator""'.
This is important! The label you give can point directly to the decoratoror not.
Lets get evil. 
def decorator_maker():
    
    print(""I make decorators! I am executed only once: ""
          ""when you make me create a decorator."")
            
    def my_decorator(func):
        
        print(""I am a decorator! I am executed only when you decorate a function."")
               
        def wrapped():
            print(""I am the wrapper around the decorated function. ""
                  ""I am called when you call the decorated function. ""
                  ""As the wrapper, I return the RESULT of the decorated function."")
            return func()
        
        print(""As the decorator, I return the wrapped function."")
        
        return wrapped
    
    print(""As a decorator maker, I return a decorator"")
    return my_decorator
            
# Lets create a decorator. Its just a new function after all.
new_decorator = decorator_maker()       
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator

# Then we decorate the function
            
def decorated_function():
    print(""I am the decorated function."")
   
decorated_function = new_decorator(decorated_function)
#outputs:
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function
     
# Lets call the function:
decorated_function()
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.

No surprise here.
Lets do EXACTLY the same thing, but skip all the pesky intermediate variables:
def decorated_function():
    print(""I am the decorated function."")
decorated_function = decorator_maker()(decorated_function)
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function.

# Finally:
decorated_function()    
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.

Lets make it even shorter:
@decorator_maker()
def decorated_function():
    print(""I am the decorated function."")
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function.

#Eventually: 
decorated_function()    
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.

Hey, did you see that? We used a function call with the ""@"" syntax! :-)
So, back to decorators with arguments. If we can use functions to generate the decorator on the fly, we can pass arguments to that function, right?
def decorator_maker_with_arguments(decorator_arg1, decorator_arg2):
    
    print(""I make decorators! And I accept arguments: {0}, {1}"".format(decorator_arg1, decorator_arg2))
            
    def my_decorator(func):
        # The ability to pass arguments here is a gift from closures.
        # If you are not comfortable with closures, you can assume its ok,
        # or read: https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python
        print(""I am the decorator. Somehow you passed me arguments: {0}, {1}"".format(decorator_arg1, decorator_arg2))
               
        # Don't confuse decorator arguments and function arguments!
        def wrapped(function_arg1, function_arg2) :
            print(""I am the wrapper around the decorated function.\n""
                  ""I can access all the variables\n""
                  ""\t- from the decorator: {0} {1}\n""
                  ""\t- from the function call: {2} {3}\n""
                  ""Then I can pass them to the decorated function""
                  .format(decorator_arg1, decorator_arg2,
                          function_arg1, function_arg2))
            return func(function_arg1, function_arg2)
        
        return wrapped
    
    return my_decorator

@decorator_maker_with_arguments(""Leonard"", ""Sheldon"")
def decorated_function_with_arguments(function_arg1, function_arg2):
    print(""I am the decorated function and only knows about my arguments: {0}""
           "" {1}"".format(function_arg1, function_arg2))
          
decorated_function_with_arguments(""Rajesh"", ""Howard"")
#outputs:
#I make decorators! And I accept arguments: Leonard Sheldon
#I am the decorator. Somehow you passed me arguments: Leonard Sheldon
#I am the wrapper around the decorated function. 
#I can access all the variables 
#   - from the decorator: Leonard Sheldon 
#   - from the function call: Rajesh Howard 
#Then I can pass them to the decorated function
#I am the decorated function and only knows about my arguments: Rajesh Howard

Here it is: a decorator with arguments. Arguments can be set as variable:
c1 = ""Penny""
c2 = ""Leslie""

@decorator_maker_with_arguments(""Leonard"", c1)
def decorated_function_with_arguments(function_arg1, function_arg2):
    print(""I am the decorated function and only knows about my arguments:""
           "" {0} {1}"".format(function_arg1, function_arg2))

decorated_function_with_arguments(c2, ""Howard"")
#outputs:
#I make decorators! And I accept arguments: Leonard Penny
#I am the decorator. Somehow you passed me arguments: Leonard Penny
#I am the wrapper around the decorated function. 
#I can access all the variables 
#   - from the decorator: Leonard Penny 
#   - from the function call: Leslie Howard 
#Then I can pass them to the decorated function
#I am the decorated function and only know about my arguments: Leslie Howard

As you can see, you can pass arguments to the decorator like any function using this trick. You can even use *args, **kwargs if you wish. But remember decorators are called only once. Just when Python imports the script. You can't dynamically set the arguments afterwards. When you do ""import x"", the function is already decorated, so you can't
change anything.

Lets practice: decorating a decorator
Okay, as a bonus, I'll give you a snippet to make any decorator accept generically any argument. After all, in order to accept arguments, we created our decorator using another function.
We wrapped the decorator.
Anything else we saw recently that wrapped function?
Oh yes, decorators!
Lets have some fun and write a decorator for the decorators:
def decorator_with_args(decorator_to_enhance):
    """""" 
    This function is supposed to be used as a decorator.
    It must decorate an other function, that is intended to be used as a decorator.
    Take a cup of coffee.
    It will allow any decorator to accept an arbitrary number of arguments,
    saving you the headache to remember how to do that every time.
    """"""
    
    # We use the same trick we did to pass arguments
    def decorator_maker(*args, **kwargs):
       
        # We create on the fly a decorator that accepts only a function
        # but keeps the passed arguments from the maker.
        def decorator_wrapper(func):
       
            # We return the result of the original decorator, which, after all, 
            # IS JUST AN ORDINARY FUNCTION (which returns a function).
            # Only pitfall: the decorator must have this specific signature or it won't work:
            return decorator_to_enhance(func, *args, **kwargs)
        
        return decorator_wrapper
    
    return decorator_maker
       

It can be used as follows:
# You create the function you will use as a decorator. And stick a decorator on it :-)
# Don't forget, the signature is ""decorator(func, *args, **kwargs)""
@decorator_with_args 
def decorated_decorator(func, *args, **kwargs): 
    def wrapper(function_arg1, function_arg2):
        print(""Decorated with {0} {1}"".format(args, kwargs))
        return func(function_arg1, function_arg2)
    return wrapper
    
# Then you decorate the functions you wish with your brand new decorated decorator.

@decorated_decorator(42, 404, 1024)
def decorated_function(function_arg1, function_arg2):
    print(""Hello {0} {1}"".format(function_arg1, function_arg2))

decorated_function(""Universe and"", ""everything"")
#outputs:
#Decorated with (42, 404, 1024) {}
#Hello Universe and everything

# Whoooot!

I know, the last time you had this feeling, it was after listening a guy saying: ""before understanding recursion, you must first understand recursion"". But now, don't you feel good about mastering this?

Best practices: decorators

Decorators were introduced in Python 2.4, so be sure your code will be run on >= 2.4.
Decorators slow down the function call. Keep that in mind.
You cannot un-decorate a function. (There are hacks to create decorators that can be removed, but nobody uses them.) So once a function is decorated, its decorated for all the code.
Decorators wrap functions, which can make them hard to debug.  (This gets better from Python >= 2.5; see below.)

The functools module was introduced in Python 2.5. It includes the function functools.wraps(), which copies the name, module, and docstring of the decorated function to its wrapper.
(Fun fact: functools.wraps() is a decorator! )
# For debugging, the stacktrace prints you the function __name__
def foo():
    print(""foo"")
    
print(foo.__name__)
#outputs: foo
    
# With a decorator, it gets messy    
def bar(func):
    def wrapper():
        print(""bar"")
        return func()
    return wrapper

@bar
def foo():
    print(""foo"")

print(foo.__name__)
#outputs: wrapper

# ""functools"" can help for that

import functools

def bar(func):
    # We say that ""wrapper"", is wrapping ""func""
    # and the magic begins
    @functools.wraps(func)
    def wrapper():
        print(""bar"")
        return func()
    return wrapper

@bar
def foo():
    print(""foo"")

print(foo.__name__)
#outputs: foo


How can the decorators be useful?
Now the big question: What can I use decorators for?
Seem cool and powerful, but a practical example would be great. Well, there are 1000 possibilities. Classic uses are extending a function behavior from an external lib (you can't modify it), or for debugging (you don't want to modify it because its temporary).
You can use them to extend several functions in a DRYs way, like so:
def benchmark(func):
    """"""
    A decorator that prints the time a function takes
    to execute.
    """"""
    import time
    def wrapper(*args, **kwargs):
        t = time.clock()
        res = func(*args, **kwargs)
        print(""{0} {1}"".format(func.__name__, time.clock()-t))
        return res
    return wrapper


def logging(func):
    """"""
    A decorator that logs the activity of the script.
    (it actually just prints it, but it could be logging!)
    """"""
    def wrapper(*args, **kwargs):
        res = func(*args, **kwargs)
        print(""{0} {1} {2}"".format(func.__name__, args, kwargs))
        return res
    return wrapper


def counter(func):
    """"""
    A decorator that counts and prints the number of times a function has been executed
    """"""
    def wrapper(*args, **kwargs):
        wrapper.count = wrapper.count + 1
        res = func(*args, **kwargs)
        print(""{0} has been used: {1}x"".format(func.__name__, wrapper.count))
        return res
    wrapper.count = 0
    return wrapper

@counter
@benchmark
@logging
def reverse_string(string):
    return str(reversed(string))

print(reverse_string(""Able was I ere I saw Elba""))
print(reverse_string(""A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!""))

#outputs:
#reverse_string ('Able was I ere I saw Elba',) {}
#wrapper 0.0
#wrapper has been used: 1x 
#ablE was I ere I saw elbA
#reverse_string ('A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!',) {}
#wrapper 0.0
#wrapper has been used: 2x
#!amanaP :lanac a ,noep a ,stah eros ,raj a ,hsac ,oloR a ,tur a ,mapS ,snip ,eperc a ,)lemac a ro( niaga gab ananab a ,gat a ,nat a ,gab ananab a ,gag a ,inoracam ,elacrep ,epins ,spam ,arutaroloc a ,shajar ,soreh ,atsap ,eonac a ,nalp a ,nam A

Of course the good thing with decorators is that you can use them right away on almost anything without rewriting. DRY, I said:
@counter
@benchmark
@logging
def get_random_futurama_quote():
    from urllib import urlopen
    result = urlopen(""http://subfusion.net/cgi-bin/quote.pl?quote=futurama"").read()
    try:
        value = result.split(""<br><b><hr><br>"")[1].split(""<br><br><hr>"")[0]
        return value.strip()
    except:
        return ""No, I'm ... doesn't!""

    
print(get_random_futurama_quote())
print(get_random_futurama_quote())

#outputs:
#get_random_futurama_quote () {}
#wrapper 0.02
#wrapper has been used: 1x
#The laws of science be a harsh mistress.
#get_random_futurama_quote () {}
#wrapper 0.01
#wrapper has been used: 2x
#Curse you, merciful Poseidon!

Python itself provides several decorators: property, staticmethod, etc.

Django uses decorators to manage caching and view permissions.
Twisted to fake inlining asynchronous functions calls.

This really is a large playground.
    Check out the documentation to see how decorators work. Here is what you asked for:
from functools import wraps

def makebold(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        return ""<b>"" + fn(*args, **kwargs) + ""</b>""
    return wrapper

def makeitalic(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        return ""<i>"" + fn(*args, **kwargs) + ""</i>""
    return wrapper

@makebold
@makeitalic
def hello():
    return ""hello world""

@makebold
@makeitalic
def log(s):
    return s

print hello()        # returns ""<b><i>hello world</i></b>""
print hello.__name__ # with functools.wraps() this returns ""hello""
print log('hello')   # returns ""<b><i>hello</i></b>""

    It looks like the other people have already told you how to solve the problem. I hope this will help you understand what decorators are.

Decorators are just syntactical sugar.

This

@decorator
def func():
    ...


expands to    

def func():
    ...
func = decorator(func)

    Alternatively, you could write a factory function which return a decorator which wraps the return value of the decorated function in a tag passed to the factory function. For example:

from functools import wraps

def wrap_in_tag(tag):
    def factory(func):
        @wraps(func)
        def decorator():
            return '<%(tag)s>%(rv)s</%(tag)s>' % (
                {'tag': tag, 'rv': func()})
        return decorator
    return factory


This enables you to write:

@wrap_in_tag('b')
@wrap_in_tag('i')
def say():
    return 'hello'


or

makebold = wrap_in_tag('b')
makeitalic = wrap_in_tag('i')

@makebold
@makeitalic
def say():
    return 'hello'


Personally I would have written the decorator somewhat differently:

from functools import wraps

def wrap_in_tag(tag):
    def factory(func):
        @wraps(func)
        def decorator(val):
            return func('<%(tag)s>%(val)s</%(tag)s>' %
                        {'tag': tag, 'val': val})
        return decorator
    return factory


which would yield:

@wrap_in_tag('b')
@wrap_in_tag('i')
def say(val):
    return val
say('hello')


Don't forget the construction for which decorator syntax is a shorthand:

say = wrap_in_tag('b')(wrap_in_tag('i')(say)))

    And of course you can return lambdas as well from a decorator function:

def makebold(f): 
    return lambda: ""<b>"" + f() + ""</b>""
def makeitalic(f): 
    return lambda: ""<i>"" + f() + ""</i>""

@makebold
@makeitalic
def say():
    return ""Hello""

print say()

    You could make two separate decorators that do what you want as illustrated directly below. Note the use of *args, **kwargs in the declaration of the wrapped() function which supports the decorated function having multiple arguments (which isn't really necessary for the example say() function, but is included for generality).

For similar reasons, the functools.wraps decorator is used to change the meta attributes of the wrapped function to be those of the one being decorated. This makes error messages and embedded function documentation (func.__doc__) be those of the decorated function instead of wrapped()'s.

from functools import wraps

def makebold(fn):
    @wraps(fn)
    def wrapped(*args, **kwargs):
        return ""<b>"" + fn(*args, **kwargs) + ""</b>""
    return wrapped

def makeitalic(fn):
    @wraps(fn)
    def wrapped(*args, **kwargs):
        return ""<i>"" + fn(*args, **kwargs) + ""</i>""
    return wrapped

@makebold
@makeitalic
def say():
    return 'Hello'

print(say())  # -> <b><i>Hello</i></b>


Refinements

As you can see there's a lot of duplicate code in these two decorators. Given this similarity it would be better for you to instead make a generic one that was actually a decorator factoryin other words, a decorator function that makes other decorators. That way there would be less code repetitionand allow the DRY principle to be followed.

def html_deco(tag):
    def decorator(fn):
        @wraps(fn)
        def wrapped(*args, **kwargs):
            return '<%s>' % tag + fn(*args, **kwargs) + '</%s>' % tag
        return wrapped
    return decorator

@html_deco('b')
@html_deco('i')
def greet(whom=''):
    return 'Hello' + (' ' + whom) if whom else ''

print(greet('world'))  # -> <b><i>Hello world</i></b>


To make the code more readable, you can assign a more descriptive name to the factory-generated decorators:

makebold = html_deco('b')
makeitalic = html_deco('i')

@makebold
@makeitalic
def greet(whom=''):
    return 'Hello' + (' ' + whom) if whom else ''

print(greet('world'))  # -> <b><i>Hello world</i></b>


or even combine them like this:

makebolditalic = lambda fn: makebold(makeitalic(fn))

@makebolditalic
def greet(whom=''):
    return 'Hello' + (' ' + whom) if whom else ''

print(greet('world'))  # -> <b><i>Hello world</i></b>


Efficiency

While the above examples do all work, the code generated involves a fair amount of overhead in the form of extraneous function calls when multiple decorators are applied at once. This may not matter, depending the exact usage (which might be I/O-bound, for instance).

If speed of the decorated function is important, the overhead can be kept to a single extra function call by writing a slightly different decorator factory-function which implements adding all the tags at once, so it can generate code that avoids the addtional function calls incurred by using separate decorators for each tag.

This requires more code in the decorator itself, but this only runs when it's being applied to function definitions, not later when they themselves are called. This also applies when creating more readable names by using lambda functions as previously illustrated. Sample:

def multi_html_deco(*tags):
    start_tags, end_tags = [], []
    for tag in tags:
        start_tags.append('<%s>' % tag)
        end_tags.append('</%s>' % tag)
    start_tags = ''.join(start_tags)
    end_tags = ''.join(reversed(end_tags))

    def decorator(fn):
        @wraps(fn)
        def wrapped(*args, **kwargs):
            return start_tags + fn(*args, **kwargs) + end_tags
        return wrapped
    return decorator

makebolditalic = multi_html_deco('b', 'i')

@makebolditalic
def greet(whom=''):
    return 'Hello' + (' ' + whom) if whom else ''

print(greet('world'))  # -> <b><i>Hello world</i></b>

    Python decorators add extra functionality to another function

An italics decorator could be like

def makeitalic(fn):
    def newFunc():
        return ""<i>"" + fn() + ""</i>""
    return newFunc


Note that a function is defined inside a function.
What it basically does is replace a function with the newly defined one. For example, I have this class

class foo:
    def bar(self):
        print ""hi""
    def foobar(self):
        print ""hi again""


Now say, I want both functions to print ""---"" after and before they are done.
I could add a print ""---"" before and after each print statement.
But because I don't like repeating myself, I will make a decorator

def addDashes(fn): # notice it takes a function as an argument
    def newFunction(self): # define a new function
        print ""---""
        fn(self) # call the original function
        print ""---""
    return newFunction
    # Return the newly defined function - it will ""replace"" the original


So now I can change my class to 

class foo:
    @addDashes
    def bar(self):
        print ""hi""

    @addDashes
    def foobar(self):
        print ""hi again""


For more on decorators, check
http://www.ibm.com/developerworks/linux/library/l-cpdecor.html
    This answer has long been answered, but I thought I would share my Decorator class which makes writing new decorators easy and compact.

from abc import ABCMeta, abstractclassmethod

class Decorator(metaclass=ABCMeta):
    """""" Acts as a base class for all decorators """"""

    def __init__(self):
        self.method = None

    def __call__(self, method):
        self.method = method
        return self.call

    @abstractclassmethod
    def call(self, *args, **kwargs):
        return self.method(*args, **kwargs)


For one I think this makes the behavior of decorators very clear, but it also makes it easy to define new decorators very concisely. For the example listed above, you could then solve it as:

class MakeBold(Decorator):
    def call():
        return ""<b>"" + self.method() + ""</b>""

class MakeItalic(Decorator):
    def call():
        return ""<i>"" + self.method() + ""</i>""

@MakeBold()
@MakeItalic()
def say():
   return ""Hello""


You could also use it to do more complex tasks, like for instance a decorator which automatically makes the function get applied recursively to all arguments in an iterator:

class ApplyRecursive(Decorator):
    def __init__(self, *types):
        super().__init__()
        if not len(types):
            types = (dict, list, tuple, set)
        self._types = types

    def call(self, arg):
        if dict in self._types and isinstance(arg, dict):
            return {key: self.call(value) for key, value in arg.items()}

        if set in self._types and isinstance(arg, set):
            return set(self.call(value) for value in arg)

        if tuple in self._types and isinstance(arg, tuple):
            return tuple(self.call(value) for value in arg)

        if list in self._types and isinstance(arg, list):
            return list(self.call(value) for value in arg)

        return self.method(arg)


@ApplyRecursive(tuple, set, dict)
def double(arg):
    return 2*arg

print(double(1))
print(double({'a': 1, 'b': 2}))
print(double({1, 2, 3}))
print(double((1, 2, 3, 4)))
print(double([1, 2, 3, 4, 5]))


Which prints:

2
{'a': 2, 'b': 4}
{2, 4, 6}
(2, 4, 6, 8)
[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]


Notice that this example didn't include the list type in the instantiation of the decorator, so in the final print statement the method gets applied to the list itself, not the elements of the list.
    Another way of doing the same thing:

class bol(object):
  def __init__(self, f):
    self.f = f
  def __call__(self):
    return ""<b>{}</b>"".format(self.f())

class ita(object):
  def __init__(self, f):
    self.f = f
  def __call__(self):
    return ""<i>{}</i>"".format(self.f())

@bol
@ita
def sayhi():
  return 'hi'


Or, more flexibly:

class sty(object):
  def __init__(self, tag):
    self.tag = tag
  def __call__(self, f):
    def newf():
      return ""<{tag}>{res}</{tag}>"".format(res=f(), tag=self.tag)
    return newf

@sty('b')
@sty('i')
def sayhi():
  return 'hi'

    
  How can I make two decorators in Python that would do the following?


You want the following function, when called:


@makebold
@makeitalic
def say():
    return ""Hello""



To return:


<b><i>Hello</i></b>



Simple solution

To most simply do this, make decorators that return lambdas (anonymous functions) that close over the function (closures) and call it:

def makeitalic(fn):
    return lambda: '<i>' + fn() + '</i>'

def makebold(fn):
    return lambda: '<b>' + fn() + '</b>'


Now use them as desired:

@makebold
@makeitalic
def say():
    return 'Hello'


and now:

>>> say()
'<b><i>Hello</i></b>'


Problems with the simple solution

But we seem to have nearly lost the original function. 

>>> say
<function <lambda> at 0x4ACFA070>


To find it, we'd need to dig into the closure of each lambda, one of which is buried in the other:

>>> say.__closure__[0].cell_contents
<function <lambda> at 0x4ACFA030>
>>> say.__closure__[0].cell_contents.__closure__[0].cell_contents
<function say at 0x4ACFA730>


So if we put documentation on this function, or wanted to be able to decorate functions that take more than one argument, or we just wanted to know what function we were looking at in a debugging session, we need to do a bit more with our wrapper.

Full featured solution - overcoming most of these problems

We have the decorator wraps from the functools module in the standard library! 

from functools import wraps

def makeitalic(fn):
    # must assign/update attributes from wrapped function to wrapper
    # __module__, __name__, __doc__, and __dict__ by default
    @wraps(fn) # explicitly give function whose attributes it is applying
    def wrapped(*args, **kwargs):
        return '<i>' + fn(*args, **kwargs) + '</i>'
    return wrapped

def makebold(fn):
    @wraps(fn)
    def wrapped(*args, **kwargs):
        return '<b>' + fn(*args, **kwargs) + '</b>'
    return wrapped


It is unfortunate that there's still some boilerplate, but this is about as simple as we can make it. 

In Python 3, you also get __qualname__ and __annotations__ assigned by default.

So now:

@makebold
@makeitalic
def say():
    """"""This function returns a bolded, italicized 'hello'""""""
    return 'Hello'


And now:

>>> say
<function say at 0x14BB8F70>
>>> help(say)
Help on function say in module __main__:

say(*args, **kwargs)
    This function returns a bolded, italicized 'hello'


Conclusion

So we see that wraps makes the wrapping function do almost everything except tell us exactly what the function takes as arguments. 

There are other modules that may attempt to tackle the problem, but the solution is not yet in the standard library.
    Paolo Bergantino's answer has the great advantage of only using the stdlib, and works for this simple example where there are no decorator arguments nor decorated function arguments. 

However it has 3 major limitations if you want to tackle more general cases:


as already noted in several answers, you can not easily modify the code to add optional decorator arguments. For example creating a makestyle(style='bold') decorator is non-trivial.
besides, wrappers created with @functools.wraps do not preserve the signature, so if bad arguments are provided they will start executing, and might raise a different kind of error than the usual TypeError.
finally, it is quite difficult in wrappers created with @functools.wraps to access an argument based on its name. Indeed the argument can appear in *args, in **kwargs, or may not appear at all (if it is optional).


I wrote decopatch to solve the first issue, and wrote makefun.wraps to solve the other two. Note that makefun leverages the same trick than the famous decorator lib.

This is how you would create a decorator with arguments, returning truly signature-preserving wrappers:

from decopatch import function_decorator, DECORATED
from makefun import wraps

@function_decorator
def makestyle(st='b', fn=DECORATED):
    open_tag = ""<%s>"" % st
    close_tag = ""</%s>"" % st

    @wraps(fn)
    def wrapped(*args, **kwargs):
        return open_tag + fn(*args, **kwargs) + close_tag

    return wrapped


decopatch provides you with two other development styles that hide or show the various python concepts, depending on your preferences. The most compact style is the following:

from decopatch import function_decorator, WRAPPED, F_ARGS, F_KWARGS

@function_decorator
def makestyle(st='b', fn=WRAPPED, f_args=F_ARGS, f_kwargs=F_KWARGS):
    open_tag = ""<%s>"" % st
    close_tag = ""</%s>"" % st
    return open_tag + fn(*f_args, **f_kwargs) + close_tag


In both cases you can check that the decorator works as expected:

@makestyle
@makestyle('i')
def hello(who):
    return ""hello %s"" % who

assert hello('world') == '<b><i>hello world</i></b>'    


Please refer to the documentation for details.
    #decorator.py
def makeHtmlTag(tag, *args, **kwds):
    def real_decorator(fn):
        css_class = "" class='{0}'"".format(kwds[""css_class""]) \
                                 if ""css_class"" in kwds else """"
        def wrapped(*args, **kwds):
            return ""<""+tag+css_class+"">"" + fn(*args, **kwds) + ""</""+tag+"">""
        return wrapped
    # return decorator dont call it
    return real_decorator

@makeHtmlTag(tag=""b"", css_class=""bold_css"")
@makeHtmlTag(tag=""i"", css_class=""italic_css"")
def hello():
    return ""hello world""

print hello()


You can also write decorator in Class

#class.py
class makeHtmlTagClass(object):
    def __init__(self, tag, css_class=""""):
        self._tag = tag
        self._css_class = "" class='{0}'"".format(css_class) \
                                       if css_class != """" else """"

    def __call__(self, fn):
        def wrapped(*args, **kwargs):
            return ""<"" + self._tag + self._css_class+"">""  \
                       + fn(*args, **kwargs) + ""</"" + self._tag + "">""
        return wrapped

@makeHtmlTagClass(tag=""b"", css_class=""bold_css"")
@makeHtmlTagClass(tag=""i"", css_class=""italic_css"")
def hello(name):
    return ""Hello, {}"".format(name)

print hello(""Your name"")

    Decorate functions with different number of arguments:
def frame_tests(fn):
    def wrapper(*args):
        print ""\nStart: %s"" %(fn.__name__)
        fn(*args)
        print ""End: %s\n"" %(fn.__name__)
    return wrapper

@frame_tests
def test_fn1():
    print ""This is only a test!""

@frame_tests
def test_fn2(s1):
    print ""This is only a test! %s"" %(s1)

@frame_tests
def test_fn3(s1, s2):
    print ""This is only a test! %s %s"" %(s1, s2)

if __name__ == ""__main__"":
    test_fn1()
    test_fn2('OK!')
    test_fn3('OK!', 'Just a test!')

Result:
Start: test_fn1  
This is only a test!  
End: test_fn1  
  
  
Start: test_fn2  
This is only a test! OK!  
End: test_fn2  
  
  
Start: test_fn3  
This is only a test! OK! Just a test!  
End: test_fn3  

    Here is a simple example of chaining decorators.  Note the last line - it shows what is going on under the covers.

############################################################
#
#    decorators
#
############################################################

def bold(fn):
    def decorate():
        # surround with bold tags before calling original function
        return ""<b>"" + fn() + ""</b>""
    return decorate


def uk(fn):
    def decorate():
        # swap month and day
        fields = fn().split('/')
        date = fields[1] + ""/"" + fields[0] + ""/"" + fields[2]
        return date
    return decorate

import datetime
def getDate():
    now = datetime.datetime.now()
    return ""%d/%d/%d"" % (now.day, now.month, now.year)

@bold
def getBoldDate(): 
    return getDate()

@uk
def getUkDate():
    return getDate()

@bold
@uk
def getBoldUkDate():
    return getDate()


print getDate()
print getBoldDate()
print getUkDate()
print getBoldUkDate()
# what is happening under the covers
print bold(uk(getDate))()


The output looks like:

17/6/2013
<b>17/6/2013</b>
6/17/2013
<b>6/17/2013</b>
<b>6/17/2013</b>

    Speaking of the counter example - as given above, the counter will be shared between all functions that use the decorator:

def counter(func):
    def wrapped(*args, **kws):
        print 'Called #%i' % wrapped.count
        wrapped.count += 1
        return func(*args, **kws)
    wrapped.count = 0
    return wrapped


That way, your decorator can be reused for different functions (or used to decorate the same function multiple times: func_counter1 = counter(func); func_counter2 = counter(func)), and the counter variable will remain private to each. 
    A decorator takes the function definition and creates a new function that executes this function and transforms the result.

@deco
def do():
    ...


is equivalent to:

do = deco(do)


Example:

def deco(func):
    def inner(letter):
        return func(letter).upper()  #upper
    return inner


This

@deco
def do(number):
    return chr(number)  # number to letter


is equivalent to this

def do2(number):
    return chr(number)

do2 = deco(do2)


65 <=> 'a'

print(do(65))
print(do2(65))
>>> B
>>> B


To understand the decorator, it is important to notice, that decorator created a new function do which is inner that executes function and transforms the result.
    I add a case when you need to add custom parameters in decorator, pass it to final function and then work it with.
the very decorators:
def jwt_or_redirect(fn):
  @wraps(fn)
  def decorator(*args, **kwargs):
    ...
    return fn(*args, **kwargs)
  return decorator

def jwt_refresh(fn):
  @wraps(fn)
  def decorator(*args, **kwargs):
    ...
    new_kwargs = {'refreshed_jwt': 'xxxxx-xxxxxx'}
    new_kwargs.update(kwargs)
    return fn(*args, **new_kwargs)
  return decorator

and the final function:
@app.route('/')
@jwt_or_redirect
@jwt_refresh
def home_page(*args, **kwargs):
  return kwargs['refreched_jwt']

    Yet another example of nested decorators for plotting an image:
import matplotlib.pylab as plt

def remove_axis(func):
    def inner(img, alpha):
        plt.axis('off')
        func(img, alpha)
    return inner

def plot_gray(func):
    def inner(img, alpha):
        plt.gray()
        func(img, alpha)
    return inner

@remove_axis
@plot_gray
def plot_image(img, alpha):
    plt.imshow(img, alpha=alpha)
    plt.show()

Now, let's show a color image first without axis labels using the nested decorators:
plot_image(plt.imread('lena_color.jpg'), 0.4)


Next, let's show a gray scale image without axis labels using the nested decorators remove_axis and plot_gray (we need to cmap='gray', otherwise the default colormap is viridis, so a grayscale image is by default not displayed in black and white shades, unless explicitly specified)
plot_image(plt.imread('lena_bw.jpg'), 0.8)


The above function call reduces down to the following nested call
remove_axis(plot_gray(plot_image))(img, alpha)

    ","[3018, 4576, 3047, 137, 153, 73, 44, 66, 11, 23, 21, 7, 10, 7, 8, 7, 14, 0, 0]",601827,2450,2009-04-11T07:05:31,2022-04-01 01:36:50Z,python 
"Why does my JavaScript code receive a ""No 'Access-Control-Allow-Origin' header is present on the requested resource"" error, while Postman does not?","
                

Mod note: This question is about why XMLHttpRequest/fetch/etc. on the browser are subject to the Same Access Policy restrictions (you get errors mentioning CORB or CORS) while Postman is not. This question is not about how to fix a ""No 'Access-Control-Allow-Origin'..."" error. It's about why they happen.


Please stop posting:

CORS configurations for every language/framework under the sun. Instead find your relevant language/framework's question.
3rd party services that allow a request to circumvent CORS
Command line options for turning off CORS for various browsers



I am trying to do authorization using JavaScript by connecting to the RESTful API built-in Flask. However, when I make the request, I get the following error:

XMLHttpRequest cannot load http://myApiUrl/login. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access.

I know that the API or remote resource must set the header, but why did it work when I made the request via the Chrome extension Postman?
This is the request code:
$.ajax({
  type: 'POST',
  dataType: 'text',
  url: api,
  username: 'user',
  password: 'pass',
  crossDomain: true,
  xhrFields: {
    withCredentials: true,
  },
})
  .done(function (data) {
    console.log('done');
  })
  .fail(function (xhr, textStatus, errorThrown) {
    alert(xhr.responseText);
    alert(textStatus);
  });

    If I understood it right you are doing an XMLHttpRequest to a different domain than your page is on. So the browser is blocking it as it usually allows a request in the same origin for security reasons. You need to do something different when you want to do a cross-domain request. A tutorial about how to achieve that is Using CORS.
When you are using Postman they are not restricted by this policy. Quoted from Cross-Origin XMLHttpRequest:

Regular web pages can use the XMLHttpRequest object to send and receive data from remote servers, but they're limited by the same origin policy. Extensions aren't so limited. An extension can talk to remote servers outside of its origin, as long as it first requests cross-origin permissions.

    
  WARNING: Using Access-Control-Allow-Origin: * can make your API/website vulnerable to cross-site request forgery (CSRF) attacks. Make certain you understand the risks before using this code.


It's very simple to solve if you are using PHP. Just add the following script in the beginning of your PHP page which handles the request:

<?php header('Access-Control-Allow-Origin: *'); ?>


If you are using Node-red you have to allow CORS in the node-red/settings.js file by un-commenting the following lines:

// The following property can be used to configure cross-origin resource sharing
// in the HTTP nodes.
// See https://github.com/troygoode/node-cors#configuration-options for
// details on its contents. The following is a basic permissive set of options:
httpNodeCors: {
 origin: ""*"",
 methods: ""GET,PUT,POST,DELETE""
},


If you are using Flask same as the question; you have first to install flask-cors

$ pip install -U flask-cors


Then include the Flask cors in your application.

from flask_cors import CORS


A simple application will look like:

from flask import Flask
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

@app.route(""/"")
def helloWorld():
  return ""Hello, cross-origin-world!""


For more details, you can check the Flask documentation.
    Because 
$.ajax({type: ""POST"" - calls OPTIONS 
$.post( - Calls POST 
Both are different. Postman calls ""POST"" properly, but when we call it, it will be ""OPTIONS"".
For C# web services - Web API
Please add the following code in your web.config file under <system.webServer> tag. This will work:
<httpProtocol>
    <customHeaders>
        <add name=""Access-Control-Allow-Origin"" value=""*"" />
    </customHeaders>
</httpProtocol>

Please make sure you are not doing any mistake in the Ajax call
jQuery
$.ajax({
    url: 'http://mysite.microsoft.sample.xyz.com/api/mycall',
    headers: {
        'Content-Type': 'application/x-www-form-urlencoded'
    },
    type: ""POST"", /* or type:""GET"" or type:""PUT"" */
    dataType: ""json"",
    data: {
    },
    success: function (result) {
        console.log(result);
    },
    error: function () {
        console.log(""error"");
    }
});

Note: If you are looking for downloading content from a third-party website then this will not help you.  You can try the following code, but not JavaScript.
System.Net.WebClient wc = new System.Net.WebClient();
string str = wc.DownloadString(""http://mysite.microsoft.sample.xyz.com/api/mycall"");

    Deep
In the below investigation as API, I use http://example.com instead of http://myApiUrl/login from your question, because this first one working. I assume that your page is on http://my-site.local:8088.
NOTE: The API and your page have different domains!
The reason why you see different results is that Postman:

set header Host=example.com (your API)
NOT set header Origin
Postman actually not use your website url at all (you only type your API address into Postman) - he only send request to API, so he assume that website has same address as API (browser not assume this)

This is similar to browsers' way of sending requests when the site and API has the same domain (browsers also set the header item Referer=http://my-site.local:8088, however I don't see it in Postman). When Origin header is not set, usually servers allow such requests by default.

This is the standard way how Postman sends requests. But a browser sends requests differently when your site and API have different domains, and then CORS occurs and the browser automatically:

sets header Host=example.com (yours as API)
sets header Origin=http://my-site.local:8088 (your site)

(The header Referer has the same value as Origin). And now in Chrome's Console & Networks tab you will see:


When you have Host != Origin this is CORS, and when the server detects such a request, it usually blocks it by default.
Origin=null is set when you open HTML content from a local directory, and it sends a request. The same situation is when you send a request inside an <iframe>, like in the below snippet (but here the Host header is not set at all) - in general, everywhere the HTML specification says opaque origin, you can translate that to Origin=null. More information about this you can find here.
fetch('http://example.com/api', {method: 'POST'});Look on chrome-console > network tab

If you do not use a simple CORS request, usually the browser automatically also sends an OPTIONS request before sending the main request - more information is here. The snippet below shows it:
fetch('http://example.com/api', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json'}
});Look in chrome-console -> network tab to 'api' request.
This is the OPTIONS request (the server does not allow sending a POST request)

You can change the configuration of your server to allow CORS requests.
Here is an example configuration which turns on CORS on nginx (nginx.conf file) - be very careful with setting always/""$http_origin"" for nginx and ""*"" for Apache - this will unblock CORS from any domain (in production instead of stars use your concrete page adres which consume your api)
location ~ ^/index\.php(/|$) {
   ...
    add_header 'Access-Control-Allow-Origin' ""$http_origin"" always;
    add_header 'Access-Control-Allow-Credentials' 'true' always;
    if ($request_method = OPTIONS) {
        add_header 'Access-Control-Allow-Origin' ""$http_origin""; # DO NOT remove THIS LINES (doubled with outside 'if' above)
        add_header 'Access-Control-Allow-Credentials' 'true';
        add_header 'Access-Control-Max-Age' 1728000; # cache preflight value for 20 days
        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
        add_header 'Access-Control-Allow-Headers' 'My-First-Header,My-Second-Header,Authorization,Content-Type,Accept,Origin';
        add_header 'Content-Length' 0;
        add_header 'Content-Type' 'text/plain charset=UTF-8';
        return 204;
    }
}

Here is an example configuration which turns on CORS on Apache (.htaccess file)
# ------------------------------------------------------------------------------
# | Cross-domain Ajax requests                                                 |
# ------------------------------------------------------------------------------

# Enable cross-origin Ajax requests.
# http://code.google.com/p/html5security/wiki/CrossOriginRequestSecurity
# http://enable-cors.org/

# <IfModule mod_headers.c>
#    Header set Access-Control-Allow-Origin ""*""
# </IfModule>

# Header set Header set Access-Control-Allow-Origin ""*""
# Header always set Access-Control-Allow-Credentials ""true""

Access-Control-Allow-Origin ""http://your-page.com:80""
Header always set Access-Control-Allow-Methods ""POST, GET, OPTIONS, DELETE, PUT""
Header always set Access-Control-Allow-Headers ""My-First-Header,My-Second-Header,Authorization, content-type, csrf-token""

    The error you get is due to the CORS standard, which sets some restrictions on how JavaScript can perform ajax requests.
The CORS standard is a client-side standard, implemented in the browser. So it is the browser which prevent the call from completing and generates the error message - not the server.
Postman does not implement the CORS restrictions, which is why you don't see the same error when making the same call from Postman.
Why doesn't Postman implement CORS? CORS defines the restrictions relative to the origin (URL domain) of the page which initiates the request. But in Postman the requests doesn't originate from a page with an URL so CORS does not apply.
    Applying a CORS restriction is a security feature defined by a server and implemented by a browser. 


  The browser looks at the CORS policy of the server and respects it.


However, the Postman tool does not bother about the CORS policy of the server.

That is why the CORS error appears in the browser, but not in Postman.
    Solution & Issue Origins
You are making a XMLHttpRequest to different domains, example:

Domain one: some-domain.com
Domain Two: some-different-domain.com

This difference in domain names triggers CORS (Cross-Origin Resource Sharing) policy called SOP (Same-Origin Policy) that enforces the use of same domains (hence Origin) in Ajax, XMLHttpRequest and other HTTP requests.

Why did it work when I made the request via the Chrome extension
Postman?

A client (most Browsers and Development Tools) has a choice to enforce the Same-Origin Policy.
Most browsers enforce the policy of Same-Origin Policy to prevent issues related to CSRF (Cross-Site Request Forgery) attack.
Postman as a development tool chooses not to enforce SOP while some browsers enforce, this is why you can send requests via Postman that you cannot send with XMLHttpRequest via JS using the browser.
    For browser testing purposes:
Windows - Run:
chrome.exe --user-data-dir=""C://Chrome dev session"" --disable-web-security

    You might also get this error if your gateway timeout is too short and the resource you are accessing takes longer to process than the timeout. This may be the case for complex database queries etc. Thus, the above error code can be disguishing this problem. Just check if the error code is 504 instead of 404 as in Kamils answer above or something else. If it is 504, then increasing the gateway timeout might fix the problem.
In my case the CORS error could be removed by disabling the same origin policy (CORS) in the IE browser, see How to disable same origin policy Internet Explorer. After doing this it was a pure 504 error in the log.
    Your IP is not whitelisted so you are getting this error.
Ask the backend staff to whitelist your ip for the service you are accessing
https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Access-Control-Allow-Headers
    To resolve this issue write this line of code in your doGet() or doPost() function whichever you are using in backend.
response.setHeader(""Access-Control-Allow-Origin"", ""*"");
    ","[3016, 1565, 326, 94, 43, 27, 30, 8, 2, 2, 0, 0]",5426727,782,2013-11-17T19:29:06,2022-04-13 19:35:01Z,javascript 
How to iterate over a dictionary?,"
                
I've seen a few different ways to iterate over a dictionary in C#.  Is there a standard way?
    foreach(KeyValuePair<string, string> entry in myDictionary)
{
    // do something with entry.Value or entry.Key
}

    If you are trying to use a generic Dictionary in C# like you would use an associative array in another language:

foreach(var item in myDictionary)
{
  foo(item.Key);
  bar(item.Value);
}


Or, if you only need to iterate over the collection of keys, use

foreach(var item in myDictionary.Keys)
{
  foo(item);
}


And lastly, if you're only interested in the values:

foreach(var item in myDictionary.Values)
{
  foo(item);
}


(Take note that the var keyword is an optional C# 3.0 and above feature, you could also use the exact type of your keys/values here)
    In some cases you may need a counter that may be provided by for-loop implementation. For that, LINQ provides ElementAt which enables the following:

for (int index = 0; index < dictionary.Count; index++) {
  var item = dictionary.ElementAt(index);
  var itemKey = item.Key;
  var itemValue = item.Value;
}

    C# 7.0 introduced Deconstructors and if you are using .NET Core 2.0+ Application, the struct KeyValuePair<> already include a Deconstruct() for you. So you can do:

var dic = new Dictionary<int, string>() { { 1, ""One"" }, { 2, ""Two"" }, { 3, ""Three"" } };
foreach (var (key, value) in dic) {
    Console.WriteLine($""Item [{key}] = {value}"");
}
//Or
foreach (var (_, value) in dic) {
    Console.WriteLine($""Item [NO_ID] = {value}"");
}
//Or
foreach ((int key, string value) in dic) {
    Console.WriteLine($""Item [{key}] = {value}"");
}



    As already pointed out on this answer, KeyValuePair<TKey, TValue> implements a Deconstruct method starting on .NET Core 2.0, .NET Standard 2.1 and .NET Framework 5.0 (preview).

With this, it's possible to iterate through a dictionary in a KeyValuePair agnostic way:

var dictionary = new Dictionary<int, string>();

// ...

foreach (var (key, value) in dictionary)
{
    // ...
}

    Depends on whether you're after the keys or the values...

From the MSDN Dictionary(TKey, TValue) Class description:

// When you use foreach to enumerate dictionary elements,
// the elements are retrieved as KeyValuePair objects.
Console.WriteLine();
foreach( KeyValuePair<string, string> kvp in openWith )
{
    Console.WriteLine(""Key = {0}, Value = {1}"", 
        kvp.Key, kvp.Value);
}

// To get the values alone, use the Values property.
Dictionary<string, string>.ValueCollection valueColl =
    openWith.Values;

// The elements of the ValueCollection are strongly typed
// with the type that was specified for dictionary values.
Console.WriteLine();
foreach( string s in valueColl )
{
    Console.WriteLine(""Value = {0}"", s);
}

// To get the keys alone, use the Keys property.
Dictionary<string, string>.KeyCollection keyColl =
    openWith.Keys;

// The elements of the KeyCollection are strongly typed
// with the type that was specified for dictionary keys.
Console.WriteLine();
foreach( string s in keyColl )
{
    Console.WriteLine(""Key = {0}"", s);
}

    Generally, asking for ""the best way"" without a specific context is like asking 
what is the best color?

One the one hand, there are many colors and there's no best color.  It depends on the need and often on taste, too.

On the other hand, there are many ways to iterate over a Dictionary in C# and there's no best way. It depends on the need and often on taste, too.

Most straightforward way

foreach (var kvp in items)
{
    // key is kvp.Key
    doStuff(kvp.Value)
}


If you need only the value (allows to call it item, more readable than kvp.Value).

foreach (var item in items.Values)
{
    doStuff(item)
}


If you need a specific sort order

Generally, beginners are surprised about order of enumeration of a Dictionary.

LINQ provides a concise syntax that allows to specify order (and many other things), e.g.:

foreach (var kvp in items.OrderBy(kvp => kvp.Key))
{
    // key is kvp.Key
    doStuff(kvp.Value)
}


Again you might only need the value. LINQ also provides a concise solution to: 


iterate directly on the value (allows to call it item, more readable than kvp.Value)
but sorted by the keys


Here it is:

foreach (var item in items.OrderBy(kvp => kvp.Key).Select(kvp => kvp.Value))
{
    doStuff(item)
}


There are many more real-world use case you can do from these examples.
If you don't need a specific order, just stick to the ""most straightforward way"" (see above)!
    I would say foreach is the standard way, though it obviously depends on what you're looking for
foreach(var kvp in my_dictionary) {
  ...
}

Is that what you're looking for?
    foreach is fastest and if you only iterate over ___.Values, it is also faster


    You can also try this on big dictionaries for multithreaded processing.

dictionary
.AsParallel()
.ForAll(pair => 
{ 
    // Process pair.Key and pair.Value here
});

    With .NET Framework 4.7 one can use decomposition

var fruits = new Dictionary<string, int>();
...
foreach (var (fruit, number) in fruits)
{
    Console.WriteLine(fruit + "": "" + number);
}


To make this code work on lower C# versions, add System.ValueTuple NuGet package and write somewhere

public static class MyExtensions
{
    public static void Deconstruct<T1, T2>(this KeyValuePair<T1, T2> tuple,
        out T1 key, out T2 value)
    {
        key = tuple.Key;
        value = tuple.Value;
    }
}

    If you want to use for loop, you can do this:

var keyList=new List<string>(dictionary.Keys);
for (int i = 0; i < keyList.Count; i++)
{
   var key= keyList[i];
   var value = dictionary[key];
 }

    As of C# 7, you can deconstruct objects into variables. I believe this to be the best way to iterate over a dictionary.

Example:

Create an extension method on KeyValuePair<TKey, TVal> that deconstructs it:

public static void Deconstruct<TKey, TVal>(this KeyValuePair<TKey, TVal> pair, out TKey key, out TVal value)
{
   key = pair.Key;
   value = pair.Value;
}


Iterate over any Dictionary<TKey, TVal> in the following manner

// Dictionary can be of any types, just using 'int' and 'string' as examples.
Dictionary<int, string> dict = new Dictionary<int, string>();

// Deconstructor gets called here.
foreach (var (key, value) in dict)
{
   Console.WriteLine($""{key} : {value}"");
}

    I appreciate this question has already had a lot of responses but I wanted to throw in a little research.

Iterating over a dictionary can be rather slow when compared with iterating over something like an array. In my tests an iteration over an array took 0.015003 seconds whereas an iteration over a dictionary (with the same number of elements) took 0.0365073 seconds that's 2.4 times as long! Although I have seen much bigger differences. For comparison a List was somewhere in between at 0.00215043 seconds.

However, that is like comparing apples and oranges. My point is that iterating over dictionaries is slow.

Dictionaries are optimised for lookups, so with that in mind I've created two methods. One simply does a foreach, the other iterates the keys then looks up.

public static string Normal(Dictionary<string, string> dictionary)
{
    string value;
    int count = 0;
    foreach (var kvp in dictionary)
    {
        value = kvp.Value;
        count++;
    }

    return ""Normal"";
}


This one loads the keys and iterates over them instead (I did also try pulling the keys into a string[] but the difference was negligible.

public static string Keys(Dictionary<string, string> dictionary)
{
    string value;
    int count = 0;
    foreach (var key in dictionary.Keys)
    {
        value = dictionary[key];
        count++;
    }

    return ""Keys"";
}


With this example the normal foreach test took 0.0310062 and the keys version took 0.2205441. Loading all the keys and iterating over all the lookups is clearly a LOT slower!

For a final test I've performed my iteration ten times to see if there are any benefits to using the keys here (by this point I was just curious):

Here's the RunTest method if that helps you visualise what's going on.

private static string RunTest<T>(T dictionary, Func<T, string> function)
{            
    DateTime start = DateTime.Now;
    string name = null;
    for (int i = 0; i < 10; i++)
    {
        name = function(dictionary);
    }
    DateTime end = DateTime.Now;
    var duration = end.Subtract(start);
    return string.Format(""{0} took {1} seconds"", name, duration.TotalSeconds);
}


Here the normal foreach run took 0.2820564 seconds (around ten times longer than a single iteration took - as you'd expect). The iteration over the keys took 2.2249449 seconds.

Edited To Add:
Reading some of the other answers made me question what would happen if I used Dictionary instead of Dictionary. In this example the array took 0.0120024 seconds, the list 0.0185037 seconds and the dictionary 0.0465093 seconds. It's reasonable to expect that the data type makes a difference on how much slower the dictionary is.

What are my Conclusions?


Avoid iterating over a dictionary if you can, they are substantially slower than iterating over an array with the same data in it.
If you do choose to iterate over a dictionary don't try to be too clever, although slower you could do a lot worse than using the standard foreach method.

    There are plenty of options. My personal favorite is by KeyValuePair

Dictionary<string, object> myDictionary = new Dictionary<string, object>();
// Populate your dictionary here

foreach (KeyValuePair<string,object> kvp in myDictionary)
{
     // Do some interesting things
}


You can also use the Keys and Values Collections
    Using C# 7, add this extension method to any project of your solution:

public static class IDictionaryExtensions
{
    public static IEnumerable<(TKey, TValue)> Tuples<TKey, TValue>(
        this IDictionary<TKey, TValue> dict)
    {
        foreach (KeyValuePair<TKey, TValue> kvp in dict)
            yield return (kvp.Key, kvp.Value);
    }
}



And use this simple syntax

foreach (var(id, value) in dict.Tuples())
{
    // your code using 'id' and 'value'
}



Or this one, if you prefer

foreach ((string id, object value) in dict.Tuples())
{
    // your code using 'id' and 'value'
}



In place of the traditional

foreach (KeyValuePair<string, object> kvp in dict)
{
    string id = kvp.Key;
    object value = kvp.Value;

    // your code using 'id' and 'value'
}



The extension method transforms the KeyValuePair of your IDictionary<TKey, TValue> into a strongly typed tuple, allowing you to use this new comfortable syntax.

It converts -just- the required dictionary entries to tuples, so it does NOT converts the whole dictionary to tuples, so there are no performance concerns related to that.

There is a only minor cost calling the extension method for creating a tuple in comparison with using the KeyValuePair directly, which should NOT be an issue if you are assigning the KeyValuePair's properties Key and Value to new loop variables anyway.

In practice, this new syntax suits very well for most cases, except for low-level ultra-high performance scenarios, where you still have the option to simply not use it on that specific spot.

Check this out: MSDN Blog - New features in C# 7
    Simplest form to iterate a dictionary:

foreach(var item in myDictionary)
{ 
    Console.WriteLine(item.Key);
    Console.WriteLine(item.Value);
}

    I wrote an extension to loop over a dictionary. 

public static class DictionaryExtension
{
    public static void ForEach<T1, T2>(this Dictionary<T1, T2> dictionary, Action<T1, T2> action) {
        foreach(KeyValuePair<T1, T2> keyValue in dictionary) {
            action(keyValue.Key, keyValue.Value);
        }
    }
}


Then you can call

myDictionary.ForEach((x,y) => Console.WriteLine(x + "" - "" + y));

    I know this is a very old question, but I created some extension methods that might be useful:

    public static void ForEach<T, U>(this Dictionary<T, U> d, Action<KeyValuePair<T, U>> a)
    {
        foreach (KeyValuePair<T, U> p in d) { a(p); }
    }

    public static void ForEach<T, U>(this Dictionary<T, U>.KeyCollection k, Action<T> a)
    {
        foreach (T t in k) { a(t); }
    }

    public static void ForEach<T, U>(this Dictionary<T, U>.ValueCollection v, Action<U> a)
    {
        foreach (U u in v) { a(u); }
    }


This way I can write code like this:

myDictionary.ForEach(pair => Console.Write($""key: {pair.Key}, value: {pair.Value}""));
myDictionary.Keys.ForEach(key => Console.Write(key););
myDictionary.Values.ForEach(value => Console.Write(value););

    I found this method in the documentation for the DictionaryBase class on MSDN:

foreach (DictionaryEntry de in myDictionary)
{
     //Do some stuff with de.Value or de.Key
}

This was the only one I was able to get functioning correctly in a class that inherited from the DictionaryBase.
    If say, you want to iterate over the values collection by default, I believe you can implement IEnumerable<>, Where T is the type of the values object in the dictionary, and ""this"" is a Dictionary. 

public new IEnumerator<T> GetEnumerator()
{
   return this.Values.GetEnumerator();
}

    The standard way to iterate over a Dictionary, according to official documentation on MSDN is:

foreach (DictionaryEntry entry in myDictionary)
{
     //Read entry.Key and entry.Value here
}

    Sometimes if you only needs the values to be enumerated, use the dictionary's value collection:

foreach(var value in dictionary.Values)
{
    // do something with entry.Value only
}


Reported by this post which states it is the fastest method:
http://alexpinsker.blogspot.hk/2010/02/c-fastest-way-to-iterate-over.html
    I will take the advantage of .NET 4.0+ and provide an updated answer to the originally accepted one:

foreach(var entry in MyDic)
{
    // do something with entry.Value or entry.Key
}

    The best answer is of course: Think, if you could use a more appropriate data structure than a dictionary if you plan to iterate over it- as Vikas Gupta mentioned already in the (beginning of the) discussion under the question. But that discussion as this whole thread still lacks surprisingly good alternatives. One is:
SortedList<string, string> x = new SortedList<string, string>();

x.Add(""key1"", ""value1"");
x.Add(""key2"", ""value2"");
x[""key3""] = ""value3"";
foreach( KeyValuePair<string, string> kvPair in x )
            Console.WriteLine($""{kvPair.Key}, {kvPair.Value}"");

Why it could be argued a code smell of iterating over a dictionary (e.g. by foreach(KeyValuePair<,>) ?
A basic principle of Clean Coding:
""Express intent!""
Robert C. Martin writes in ""Clean Code"": ""Choosing names that reveal intent"". Obviously naming alone is too weak. ""Express (reveal) intent with every coding decision"" expresses it better.
A related principle is ""Principle of least surprise"" (=Principle of Least Astonishment).
Why this is related to iterating over a dictionary? Choosing a dictionary expresses the intent of choosing a data structure which was made for primarily finding data by key. Nowadays there are so much alternatives in .NET, if you want to iterate through key/value pairs that you could choose something else.
Moreover: If you iterate over something, you have to reveal something about how the items are (to be) ordered and expected to be ordered!
Although the known implementations of Dictionary sort the key collection in the order of the items added-
AFAIK, Dictionary has no assured specification about ordering (has it?).
But what are the alternatives?
TLDR:
SortedList: If your collection is not getting too large, a simple solution would be to use SortedList<,> which gives you also full indexing of key/value pairs.
Microsoft has a long article about mentioning and explaining fitting collections:
Keyed collection
To mention the most important: KeyedCollection<,> and SortedDictionary<,> .
SortedDictionary<,> is a bit faster than SortedList for only inserting if it gets large, but lacks indexing and is needed only if O(log n) for inserting is preferenced over other operations. If you really need O(1) for inserting and accept slower iterating in exchange, you have to stay with simple Dictionary<,>.
Obviously there is no data structure which is the fastest for every possible operation..
Additionally there is ImmutableSortedDictionary<,>.
And if one data structure is not exactly what you need, then derivate from Dictionary<,> or even from the new ConcurrentDictionary<,> and add explicit iteration/sorting functions!
    in addition to the highest ranking posts where there is a discussion between using

foreach(KeyValuePair<string, string> entry in myDictionary)
{
    // do something with entry.Value or entry.Key
}


or 

foreach(var entry in myDictionary)
{
    // do something with entry.Value or entry.Key
}


most complete is the following because you can see the dictionary type from the initialization, kvp is KeyValuePair

var myDictionary = new Dictionary<string, string>(x);//fill dictionary with x

foreach(var kvp in myDictionary)//iterate over dictionary
{
    // do something with kvp.Value or kvp.Key
}

    var dictionary = new Dictionary<string, int>
{
    { ""Key"", 12 }
};

var aggregateObjectCollection = dictionary.Select(
    entry => new AggregateObject(entry.Key, entry.Value));

    Just wanted to add my 2 cent, as the most answers relate to foreach-loop.
Please, take a look at the following code:

Dictionary<String, Double> myProductPrices = new Dictionary<String, Double>();

//Add some entries to the dictionary

myProductPrices.ToList().ForEach(kvP => 
{
    kvP.Value *= 1.15;
    Console.Writeline(String.Format(""Product '{0}' has a new price: {1} $"", kvp.Key, kvP.Value));
});


Altought this adds a additional call of '.ToList()', there might be a slight performance-improvement (as pointed out here foreach vs someList.Foreach(){}), 
espacially when working with large Dictionaries and running in parallel is no option / won't have an effect at all.

Also, please note that you wont be able to assign values to the 'Value' property inside a foreach-loop. On the other hand, you will be able to manipulate the 'Key' as well, possibly getting you into trouble at runtime.

When you just want to ""read"" Keys and Values, you might also use IEnumerable.Select().

var newProductPrices = myProductPrices.Select(kvp => new { Name = kvp.Key, Price = kvp.Value * 1.15 } );

    Dictionary< TKey,TValue > It is a generic collection class in c# and it stores the data in the key value format.Key must be unique and it can not be null whereas value can be duplicate and null.As each item in the dictionary is treated as KeyValuePair< TKey,TValue > structure representing a key and its value. and hence we should take the element type  KeyValuePair< TKey,TValue> during the iteration of element.Below is the example.

Dictionary<int, string> dict = new Dictionary<int, string>();
dict.Add(1,""One"");
dict.Add(2,""Two"");
dict.Add(3,""Three"");

foreach (KeyValuePair<int, string> item in dict)
{
    Console.WriteLine(""Key: {0}, Value: {1}"", item.Key, item.Value);
}

    ","[3012, 4231, 1012, 183, 63, 29, 112, 91, 63, 17, 47, 19, 5, 15, 31, 30, 11, 11, 3, 6, 6, 2, 3, 6, 3, -1, -4, -1, -2, 0]",2047531,402,2008-09-26T18:20:06,2022-02-08 05:37:20Z,c 
How can I merge properties of two JavaScript objects dynamically?,"
                
I need to be able to merge two (very simple) JavaScript objects at runtime.  For example I'd like to:
var obj1 = { food: 'pizza', car: 'ford' }
var obj2 = { animal: 'dog' }

obj1.merge(obj2);

//obj1 now has three properties: food, car, and animal

Is there a built in way to do this?  I do not need recursion, and I do not need to merge functions, just methods on flat objects.
    ECMAScript 2018 Standard Method
You would use object spread:
let merged = {...obj1, ...obj2};

merged is now the union of obj1 and obj2. Properties in obj2 will overwrite those in obj1.
/** There's no limit to the number of objects you can merge.
 *  Later properties overwrite earlier properties with the same name. */
const allRules = {...obj1, ...obj2, ...obj3};

Here is also the MDN documentation for this syntax. If you're using babel you'll need the babel-plugin-transform-object-rest-spread plugin for it to work.
ECMAScript 2015 (ES6) Standard Method
/* For the case in question, you would do: */
Object.assign(obj1, obj2);

/** There's no limit to the number of objects you can merge.
 *  All objects get merged into the first object. 
 *  Only the object in the first argument is mutated and returned.
 *  Later properties overwrite earlier properties with the same name. */
const allRules = Object.assign({}, obj1, obj2, obj3, etc);

(see MDN JavaScript Reference)

Method for ES5 and Earlier
for (var attrname in obj2) { obj1[attrname] = obj2[attrname]; }

Note that this will simply add all attributes of obj2 to obj1 which might not be what you want if you still want to use the unmodified obj1.
If you're using a framework that craps all over your prototypes then you have to get fancier with checks like hasOwnProperty, but that code will work for 99% of cases.
Example function:
/**
 * Overwrites obj1's values with obj2's and adds obj2's if non existent in obj1
 * @param obj1
 * @param obj2
 * @returns obj3 a new object based on obj1 and obj2
 */
function merge_options(obj1,obj2){
    var obj3 = {};
    for (var attrname in obj1) { obj3[attrname] = obj1[attrname]; }
    for (var attrname in obj2) { obj3[attrname] = obj2[attrname]; }
    return obj3;
}

    jQuery also has a utility for this: http://api.jquery.com/jQuery.extend/.

Taken from the jQuery documentation:

// Merge options object into settings object
var settings = { validate: false, limit: 5, name: ""foo"" };
var options  = { validate: true, name: ""bar"" };
jQuery.extend(settings, options);

// Now the content of settings object is the following:
// { validate: true, limit: 5, name: ""bar"" }


The above code will mutate the existing object named settings.



If you want to create a new object without modifying either argument, use this:

var defaults = { validate: false, limit: 5, name: ""foo"" };
var options = { validate: true, name: ""bar"" };

/* Merge defaults and options, without modifying defaults */
var settings = $.extend({}, defaults, options);

// The content of settings variable is now the following:
// {validate: true, limit: 5, name: ""bar""}
// The 'defaults' and 'options' variables remained the same.

    The Harmony ECMAScript 2015 (ES6) specifies Object.assign which will do this.

Object.assign(obj1, obj2);


Current browser support is getting better, but if you're developing for browsers that don't have support, you can use a polyfill.
    I googled for code to merge object properties and ended up here. However since there wasn't any code for recursive merge I wrote it myself. (Maybe jQuery extend is recursive BTW?) Anyhow, hopefully someone else will find it useful as well.

(Now the code does not use Object.prototype :)

Code

/*
* Recursively merge properties of two objects 
*/
function MergeRecursive(obj1, obj2) {

  for (var p in obj2) {
    try {
      // Property in destination object set; update its value.
      if ( obj2[p].constructor==Object ) {
        obj1[p] = MergeRecursive(obj1[p], obj2[p]);

      } else {
        obj1[p] = obj2[p];

      }

    } catch(e) {
      // Property in destination object not set; create it and set its value.
      obj1[p] = obj2[p];

    }
  }

  return obj1;
}


An example

o1 = {  a : 1,
        b : 2,
        c : {
          ca : 1,
          cb : 2,
          cc : {
            cca : 100,
            ccb : 200 } } };

o2 = {  a : 10,
        c : {
          ca : 10,
          cb : 20, 
          cc : {
            cca : 101,
            ccb : 202 } } };

o3 = MergeRecursive(o1, o2);


Produces object o3 like

o3 = {  a : 10,
        b : 2,
        c : {
          ca : 10,
          cb : 20,
          cc : { 
            cca : 101,
            ccb : 202 } } };

    You can use the object spread syntax to achieve this. It's a part of ES2018 and beyond.
const obj1 = { food: 'pizza', car: 'ford' };
const obj2 = { animal: 'dog' };

const obj3 = { ...obj1, ...obj2 };
console.log(obj3);

    Here's my stab which


Supports deep merge
Does not mutate arguments
Takes any number of arguments
Does not extend the object prototype
Does not depend on another library (jQuery, MooTools, Underscore.js, etc.)
Includes check for hasOwnProperty
Is short :)

/*
    Recursively merge properties and return new object
    obj1 &lt;- obj2 [ &lt;- ... ]
*/
function merge () {
    var dst = {}
        ,src
        ,p
        ,args = [].splice.call(arguments, 0)
    ;

    while (args.length > 0) {
        src = args.splice(0, 1)[0];
        if (toString.call(src) == '[object Object]') {
            for (p in src) {
                if (src.hasOwnProperty(p)) {
                    if (toString.call(src[p]) == '[object Object]') {
                        dst[p] = merge(dst[p] || {}, src[p]);
                    } else {
                        dst[p] = src[p];
                    }
                }
            }
        }
    }

   return dst;
}



Example:

a = {
    ""p1"": ""p1a"",
    ""p2"": [
        ""a"",
        ""b"",
        ""c""
    ],
    ""p3"": true,
    ""p5"": null,
    ""p6"": {
        ""p61"": ""p61a"",
        ""p62"": ""p62a"",
        ""p63"": [
            ""aa"",
            ""bb"",
            ""cc""
        ],
        ""p64"": {
            ""p641"": ""p641a""
        }
    }
};

b = {
    ""p1"": ""p1b"",
    ""p2"": [
        ""d"",
        ""e"",
        ""f""
    ],
    ""p3"": false,
    ""p4"": true,
    ""p6"": {
        ""p61"": ""p61b"",
        ""p64"": {
            ""p642"": ""p642b""
        }
    }
};

c = {
    ""p1"": ""p1c"",
    ""p3"": null,
    ""p6"": {
        ""p62"": ""p62c"",
        ""p64"": {
            ""p643"": ""p641c""
        }
    }
};

d = merge(a, b, c);


/*
    d = {
        ""p1"": ""p1c"",
        ""p2"": [
            ""d"",
            ""e"",
            ""f""
        ],
        ""p3"": null,
        ""p5"": null,
        ""p6"": {
            ""p61"": ""p61b"",
            ""p62"": ""p62c"",
            ""p63"": [
                ""aa"",
                ""bb"",
                ""cc""
            ],
            ""p64"": {
                ""p641"": ""p641a"",
                ""p642"": ""p642b"",
                ""p643"": ""p641c""
            }
        },
        ""p4"": true
    };
*/

    Note that underscore.js's extend-method does this in a one-liner:

_.extend({name : 'moe'}, {age : 50});
=> {name : 'moe', age : 50}

    Wow.. this is the first StackOverflow post I've seen with multiple pages. Apologies for adding another ""answer""

ES5 & Earlier
This method is for ES5 & Earlier - there are plenty of other answers addressing ES6.
I did not see any ""deep"" object merging utilizing the arguments property. Here is my answer - compact & recursive, allowing unlimited object arguments to be passed:
function extend() {
    for (var o = {}, i = 0; i < arguments.length; i++) {
        // Uncomment to skip arguments that are not objects (to prevent errors)
        // if (arguments[i].constructor !== Object) continue;
        for (var k in arguments[i]) {
            if (arguments[i].hasOwnProperty(k)) {
                o[k] = arguments[i][k].constructor === Object
                    ? extend(o[k] || {}, arguments[i][k])
                    : arguments[i][k];
            }
        }
    }
    return o;
}


Example
/**
 * Extend objects
 */
function extend() {
    for (var o = {}, i = 0; i < arguments.length; i++) {
        for (var k in arguments[i]) {
            if (arguments[i].hasOwnProperty(k)) {
                o[k] = arguments[i][k].constructor === Object
                    ? extend(o[k] || {}, arguments[i][k])
                    : arguments[i][k];
            }
        }
    }
    return o;
}

/**
 * Example
 */
document.write(JSON.stringify(extend({
    api: 1,
    params: {
        query: 'hello'
    }
}, {
    params: {
        query: 'there'
    }
})));
// outputs {""api"": 1, ""params"": {""query"": ""there""}}


This answer is now but a drop in the ocean ...
    It seems like this should be all you need:
var obj1 = { food: 'pizza', car: 'ford' }
var obj2 = { animal: 'dog' }

var obj3 = { ...obj1, ...obj2 }

After that obj3 should now have the following value:
{food: ""pizza"", car: ""ford"", animal: ""dog""}

Try it out here:
var obj1 = { food: 'pizza', car: 'ford' }
var obj2 = { animal: 'dog' }

var obj3 = { ...obj1, ...obj2 }

console.log(obj3);

    The given solutions should be modified to check source.hasOwnProperty(property) in the for..in loops before assigning - otherwise, you end up copying the properties of the whole prototype chain, which is rarely desired...
    The following two are probably a good starting point. lodash also has a customizer function for those special needs!

_.extend (http://underscorejs.org/#extend) 
_.merge (https://lodash.com/docs#merge)
    Just by the way, what you're all doing is overwriting properties, not merging...

This is how JavaScript objects area really merged: Only keys in the to object which are not objects themselves will be overwritten by from. Everything else will be really merged. Of course you can change this behaviour to not overwrite anything which exists like only if to[n] is undefined, etc...:

var realMerge = function (to, from) {

    for (n in from) {

        if (typeof to[n] != 'object') {
            to[n] = from[n];
        } else if (typeof from[n] == 'object') {
            to[n] = realMerge(to[n], from[n]);
        }
    }
    return to;
};


Usage:

var merged = realMerge(obj1, obj2);

    You can use Object.assign method. For example:
var result = Object.assign(obj1, obj2);

Also, note that it creates a shallow copy of the object.
    Merge properties of N objects in one line of code
An Object.assign method is part of the ECMAScript 2015 (ES6) standard and does exactly what you need. (IE not supported)
var clone = Object.assign({}, obj);


The Object.assign() method is used to copy the values of all enumerable own properties from one or more source objects to a target object.

Read more...
The polyfill to support older browsers:
if (!Object.assign) {
  Object.defineProperty(Object, 'assign', {
    enumerable: false,
    configurable: true,
    writable: true,
    value: function(target) {
      'use strict';
      if (target === undefined || target === null) {
        throw new TypeError('Cannot convert first argument to object');
      }

      var to = Object(target);
      for (var i = 1; i < arguments.length; i++) {
        var nextSource = arguments[i];
        if (nextSource === undefined || nextSource === null) {
          continue;
        }
        nextSource = Object(nextSource);

        var keysArray = Object.keys(nextSource);
        for (var nextIndex = 0, len = keysArray.length; nextIndex < len; nextIndex++) {
          var nextKey = keysArray[nextIndex];
          var desc = Object.getOwnPropertyDescriptor(nextSource, nextKey);
          if (desc !== undefined && desc.enumerable) {
            to[nextKey] = nextSource[nextKey];
          }
        }
      }
      return to;
    }
  });
}

    Prototype has this:

Object.extend = function(destination,source) {
    for (var property in source)
        destination[property] = source[property];
    return destination;
}


obj1.extend(obj2) will do what you want.
    I need to merge objects today, and this question (and answers) helped me a lot. I tried some of the answers, but none of them fit my needs, so I combined some of the answers, added something myself and came up with a new merge function. Here it is:



var merge = function() {
    var obj = {},
        i = 0,
        il = arguments.length,
        key;
    for (; i < il; i++) {
        for (key in arguments[i]) {
            if (arguments[i].hasOwnProperty(key)) {
                obj[key] = arguments[i][key];
            }
        }
    }
    return obj;
};


Some example usages:

var t1 = {
    key1: 1,
    key2: ""test"",
    key3: [5, 2, 76, 21]
};
var t2 = {
    key1: {
        ik1: ""hello"",
        ik2: ""world"",
        ik3: 3
    }
};
var t3 = {
    key2: 3,
    key3: {
        t1: 1,
        t2: 2,
        t3: {
            a1: 1,
            a2: 3,
            a4: [21, 3, 42, ""asd""]
        }
    }
};

console.log(merge(t1, t2));
console.log(merge(t1, t3));
console.log(merge(t2, t3));
console.log(merge(t1, t2, t3));
console.log(merge({}, t1, { key1: 1 }));

    **Merging objects is simple using Object.assign or the spread ... operator **

var obj1 = { food: 'pizza', car: 'ford' }
var obj2 = { animal: 'dog', car: 'BMW' }
var obj3 = {a: ""A""}


var mergedObj = Object.assign(obj1,obj2,obj3)
 // or using the Spread operator (...)
var mergedObj = {...obj1,...obj2,...obj3}

console.log(mergedObj);


The objects are merged from right to left, this means that objects which have identical properties as the objects to their right will be overriden.

In this example obj2.car overrides obj1.car
    Object.assign()

ECMAScript 2015 (ES6)

This is a new technology, part of the ECMAScript 2015 (ES6) standard.
This technology's specification has been finalized, but check the compatibility table for usage and implementation status in various browsers.

The Object.assign() method is used to copy the values of all enumerable own properties from one or more source objects to a target object. It will return the target object.

var o1 = { a: 1 };
var o2 = { b: 2 };
var o3 = { c: 3 };

var obj = Object.assign(o1, o2, o3);
console.log(obj); // { a: 1, b: 2, c: 3 }
console.log(o1);  // { a: 1, b: 2, c: 3 }, target object itself is changed.

    I extended David Coallier's method:


Added the possibility to merge multiple objects
Supports deep objects
override parameter (that's detected if the last parameter is a boolean)


If override is false, no property gets overridden but new properties will be added.

Usage:
obj.merge(merges... [, override]);

Here is my code:

Object.defineProperty(Object.prototype, ""merge"", {
    enumerable: false,
    value: function () {
        var override = true,
            dest = this,
            len = arguments.length,
            props, merge, i, from;

        if (typeof(arguments[arguments.length - 1]) === ""boolean"") {
            override = arguments[arguments.length - 1];
            len = arguments.length - 1;
        }

        for (i = 0; i < len; i++) {
            from = arguments[i];
            if (from != null) {
                Object.getOwnPropertyNames(from).forEach(function (name) {
                    var descriptor;

                    // nesting
                    if ((typeof(dest[name]) === ""object"" || typeof(dest[name]) === ""undefined"")
                            && typeof(from[name]) === ""object"") {

                        // ensure proper types (Array rsp Object)
                        if (typeof(dest[name]) === ""undefined"") {
                            dest[name] = Array.isArray(from[name]) ? [] : {};
                        }
                        if (override) {
                            if (!Array.isArray(dest[name]) && Array.isArray(from[name])) {
                                dest[name] = [];
                            }
                            else if (Array.isArray(dest[name]) && !Array.isArray(from[name])) {
                                dest[name] = {};
                            }
                        }
                        dest[name].merge(from[name], override);
                    } 

                    // flat properties
                    else if ((name in dest && override) || !(name in dest)) {
                        descriptor = Object.getOwnPropertyDescriptor(from, name);
                        if (descriptor.configurable) {
                            Object.defineProperty(dest, name, descriptor);
                        }
                    }
                });
            }
        }
        return this;
    }
});


Examples and TestCases:

function clone (obj) {
    return JSON.parse(JSON.stringify(obj));
}
var obj = {
    name : ""trick"",
    value : ""value""
};

var mergeObj = {
    name : ""truck"",
    value2 : ""value2""
};

var mergeObj2 = {
    name : ""track"",
    value : ""mergeObj2"",
    value2 : ""value2-mergeObj2"",
    value3 : ""value3""
};

assertTrue(""Standard"", clone(obj).merge(mergeObj).equals({
    name : ""truck"",
    value : ""value"",
    value2 : ""value2""
}));

assertTrue(""Standard no Override"", clone(obj).merge(mergeObj, false).equals({
    name : ""trick"",
    value : ""value"",
    value2 : ""value2""
}));

assertTrue(""Multiple"", clone(obj).merge(mergeObj, mergeObj2).equals({
    name : ""track"",
    value : ""mergeObj2"",
    value2 : ""value2-mergeObj2"",
    value3 : ""value3""
}));

assertTrue(""Multiple no Override"", clone(obj).merge(mergeObj, mergeObj2, false).equals({
    name : ""trick"",
    value : ""value"",
    value2 : ""value2"",
    value3 : ""value3""
}));

var deep = {
    first : {
        name : ""trick"",
        val : ""value""
    },
    second : {
        foo : ""bar""
    }
};

var deepMerge = {
    first : {
        name : ""track"",
        anotherVal : ""wohoo""
    },
    second : {
        foo : ""baz"",
        bar : ""bam""
    },
    v : ""on first layer""
};

assertTrue(""Deep merges"", clone(deep).merge(deepMerge).equals({
    first : {
        name : ""track"",
        val : ""value"",
        anotherVal : ""wohoo""
    },
    second : {
        foo : ""baz"",
        bar : ""bam""
    },
    v : ""on first layer""
}));

assertTrue(""Deep merges no override"", clone(deep).merge(deepMerge, false).equals({
    first : {
        name : ""trick"",
        val : ""value"",
        anotherVal : ""wohoo""
    },
    second : {
        foo : ""bar"",
        bar : ""bam""
    },
    v : ""on first layer""
}));

var obj1 = {a: 1, b: ""hello""};
obj1.merge({c: 3});
assertTrue(obj1.equals({a: 1, b: ""hello"", c: 3}));

obj1.merge({a: 2, b: ""mom"", d: ""new property""}, false);
assertTrue(obj1.equals({a: 1, b: ""hello"", c: 3, d: ""new property""}));

var obj2 = {};
obj2.merge({a: 1}, {b: 2}, {a: 3});
assertTrue(obj2.equals({a: 3, b: 2}));

var a = [];
var b = [1, [2, 3], 4];
a.merge(b);
assertEquals(1, a[0]);
assertEquals([2, 3], a[1]);
assertEquals(4, a[2]);


var o1 = {};
var o2 = {a: 1, b: {c: 2}};
var o3 = {d: 3};
o1.merge(o2, o3);
assertTrue(o1.equals({a: 1, b: {c: 2}, d: 3}));
o1.b.c = 99;
assertTrue(o2.equals({a: 1, b: {c: 2}}));

// checking types with arrays and objects
var bo;
a = [];
bo = [1, {0:2, 1:3}, 4];
b = [1, [2, 3], 4];

a.merge(b);
assertTrue(""Array stays Array?"", Array.isArray(a[1]));

a = [];
a.merge(bo);
assertTrue(""Object stays Object?"", !Array.isArray(a[1]));

a = [];
a.merge(b);
a.merge(bo);
assertTrue(""Object overrides Array"", !Array.isArray(a[1]));

a = [];
a.merge(b);
a.merge(bo, false);
assertTrue(""Object does not override Array"", Array.isArray(a[1]));

a = [];
a.merge(bo);
a.merge(b);
assertTrue(""Array overrides Object"", Array.isArray(a[1]));

a = [];
a.merge(bo);
a.merge(b, false);
assertTrue(""Array does not override Object"", !Array.isArray(a[1]));


My equals method can be found here: Object comparison in JavaScript
    For not-too-complicated objects you could use JSON:

var obj1 = { food: 'pizza', car: 'ford' }
var obj2 = { animal: 'dog', car: 'chevy'}
var objMerge;

objMerge = JSON.stringify(obj1) + JSON.stringify(obj2);

// {""food"": ""pizza"",""car"":""ford""}{""animal"":""dog"",""car"":""chevy""}

objMerge = objMerge.replace(/\}\{/, "",""); //  \_ replace with comma for valid JSON

objMerge = JSON.parse(objMerge); // { food: 'pizza', animal: 'dog', car: 'chevy'}
// Of same keys in both objects, the last object's value is retained_/


Mind you that in this example ""}{"" must not occur within a string!
    There's a library called deepmerge on GitHub: That seems to be getting some traction. It's a standalone, available through both the npm and bower package managers.

I would be inclined to use or improve on this instead of copy-pasting code from answers.
    var obj1 = { food: 'pizza', car: 'ford' }
var obj2 = { animal: 'dog' }

// result
result: {food: ""pizza"", car: ""ford"", animal: ""dog""}


Using jQuery.extend() - Link

// Merge obj1 & obj2 to result
var result1 = $.extend( {}, obj1, obj2 );


Using _.merge() - Link

// Merge obj1 & obj2 to result
var result2 = _.merge( {}, obj1, obj2 );


Using _.extend() - Link

// Merge obj1 & obj2 to result
var result3 = _.extend( {}, obj1, obj2 );


Using Object.assign() ECMAScript 2015 (ES6) - Link

// Merge obj1 & obj2 to result
var result4 = Object.assign( {}, obj1, obj2 );


Output of all

obj1: { animal: 'dog' }
obj2: { food: 'pizza', car: 'ford' }
result1: {food: ""pizza"", car: ""ford"", animal: ""dog""}
result2: {food: ""pizza"", car: ""ford"", animal: ""dog""}
result3: {food: ""pizza"", car: ""ford"", animal: ""dog""}
result4: {food: ""pizza"", car: ""ford"", animal: ""dog""}

    
  Use Spread operator which follows the ES6 version


var obj1 = { food: 'pizza', car: 'ford' }
var obj2 = { animal: 'dog' }
let result = {...obj1,...obj2};
console.log(result)

output { food: 'pizza', car: 'ford', animal: 'dog' }

    Object.assign(TargetObject, Obj1, Obj2, ...);

    Similar to jQuery extend(), you have the same function in AngularJS:

// Merge the 'options' object into the 'settings' object
var settings = {validate: false, limit: 5, name: ""foo""};
var options  = {validate: true, name: ""bar""};
angular.extend(settings, options);

    let obj1 = {a:1, b:2};
let obj2 = {c:3, d:4};
let merged = {...obj1, ...obj2};
console.log(merged);

    ES2018/TypeScript: Many answers are OK but I've come up with a more elegant solution to this problem when you need to merge two objects without overwriting overlapping object keys.

My function also accepts unlimited number of objects to merge as function arguments:

(I'm using TypeScript notation here, feel free to delete the :object[] type in the function argument if you're using plain JavaScript).

const merge = (...objects: object[]) => {
  return objects.reduce((prev, next) => {
    Object.keys(prev).forEach(key => {
      next[key] = { ...next[key], ...prev[key] }
    })
    return next
  })
}

    The best way for you to do this is to add a proper property that is non-enumerable using Object.defineProperty. 

This way you will still be able to iterate over your objects properties without having the newly created ""extend"" that you would get if you were to create the property with Object.prototype.extend.

Hopefully this helps:


Object.defineProperty(Object.prototype, ""extend"", {
    enumerable: false,
    value: function(from) {
        var props = Object.getOwnPropertyNames(from);
        var dest = this;
        props.forEach(function(name) {
            if (name in dest) {
                var destination = Object.getOwnPropertyDescriptor(from, name);
                Object.defineProperty(dest, name, destination);
            }
        });
        return this;
    }
});


Once you have that working, you can do:


var obj = {
    name: 'stack',
    finish: 'overflow'
}
var replacement = {
    name: 'stock'
};

obj.extend(replacement);


I just wrote a blog post about it here: http://onemoredigit.com/post/1527191998/extending-objects-in-node-js
    With the following helper, you can merge two objects into one new object:

function extend(obj, src) {
    for (var key in src) {
        if (src.hasOwnProperty(key)) obj[key] = src[key];
    }
    return obj;
}

// example
var a = { foo: true }, b = { bar: false };
var c = extend(a, b);

console.log(c);
// { foo: true, bar: false }


This is typically useful when merging an options dict with the default settings in a function or a plugin.

If support for IE 8 is not required, you may use Object.keys for the same functionality instead:

function extend(obj, src) {
    Object.keys(src).forEach(function(key) { obj[key] = src[key]; });
    return obj;
}


This involves slightly less code and is a bit faster.
    shallow

var obj = { name : ""Jacob"" , address : [""America""] }
var obj2 = { name : ""Shaun"" , address : [""Honk Kong""] }

var merged = Object.assign({} , obj,obj2 ); //shallow merge 
obj2.address[0] = ""new city""


result.address[0] is changed to ""new city"" , i.e merged object is also changed. This is the problem with shallow merge.

deep

var obj = { name : ""Jacob"" , address : [""America""] }
var obj2 = { name : ""Shaun"" , address : [""Honk Kong""] }

var result = Object.assign({} , JSON.parse(JSON.stringify(obj)),JSON.parse(JSON.stringify(obj2)) )

obj2.address[0] = ""new city""


result.address[0] is not changed
    ","[3001, 3624, 1230, 376, 284, 61, 30, 178, 13, 5, 42, 36, 29, 4, 41, 15, 66, 13, 20, 11, 19, 17, 8, 4, 2, 83, 5, 4, 16, 1, 2]",1691116,498,2008-10-05T00:30:54,2022-04-24 05:01:39Z,javascript 
How do I select rows from a DataFrame based on column values?,"
                
How can I select rows from a DataFrame based on values in some column in Pandas?
In SQL, I would use:
SELECT *
FROM table
WHERE column_name = some_value

    To select rows whose column value equals a scalar, some_value, use ==:

df.loc[df['column_name'] == some_value]


To select rows whose column value is in an iterable, some_values, use isin:

df.loc[df['column_name'].isin(some_values)]


Combine multiple conditions with &: 

df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]


Note the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parentheses 

df['column_name'] >= A & df['column_name'] <= B


is parsed as 

df['column_name'] >= (A & df['column_name']) <= B


which results in a Truth value of a Series is ambiguous error.



To select rows whose column value does not equal some_value, use !=:

df.loc[df['column_name'] != some_value]


isin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:

df.loc[~df['column_name'].isin(some_values)]




For example,

import pandas as pd
import numpy as np
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split(),
                   'C': np.arange(8), 'D': np.arange(8) * 2})
print(df)
#      A      B  C   D
# 0  foo    one  0   0
# 1  bar    one  1   2
# 2  foo    two  2   4
# 3  bar  three  3   6
# 4  foo    two  4   8
# 5  bar    two  5  10
# 6  foo    one  6  12
# 7  foo  three  7  14

print(df.loc[df['A'] == 'foo'])


yields

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14




If you have multiple values you want to include, put them in a
list (or more generally, any iterable) and use isin:

print(df.loc[df['B'].isin(['one','three'])])


yields

     A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14




Note, however, that if you wish to do this many times, it is more efficient to
make an index first, and then use df.loc:

df = df.set_index(['B'])
print(df.loc['one'])


yields

       A  C   D
B              
one  foo  0   0
one  bar  1   2
one  foo  6  12


or, to include multiple values from the index use df.index.isin:

df.loc[df.index.isin(['one','two'])]


yields

       A  C   D
B              
one  foo  0   0
one  bar  1   2
two  foo  2   4
two  foo  4   8
two  bar  5  10
one  foo  6  12

    There are several ways to select rows from a Pandas dataframe:

Boolean indexing (df[df['col'] == value] )
Positional indexing (df.iloc[...])
Label indexing (df.xs(...))
df.query(...) API

Below I show you examples of each, with advice when to use certain techniques. Assume our criterion is column 'A' == 'foo'
(Note on performance: For each base type, we can keep things simple by using the Pandas API or we can venture outside the API, usually into NumPy, and speed things up.)

Setup
The first thing we'll need is to identify a condition that will act as our criterion for selecting rows. We'll start with the OP's case column_name == some_value, and include some other common use cases.
Borrowing from @unutbu:
import pandas as pd, numpy as np

df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split(),
                   'C': np.arange(8), 'D': np.arange(8) * 2})


1. Boolean indexing
... Boolean indexing requires finding the true value of each row's 'A' column being equal to 'foo', then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values, mask.  We'll do so here as well.
mask = df['A'] == 'foo'

We can then use this mask to slice or index the data frame
df[mask]

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14

This is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the mask.

2. Positional indexing
Positional indexing (df.iloc[...]) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.
mask = df['A'] == 'foo'
pos = np.flatnonzero(mask)
df.iloc[pos]

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14

3. Label indexing
Label indexing can be very handy, but in this case, we are again doing more work for no benefit
df.set_index('A', append=True, drop=False).xs('foo', level=1)

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14

4. df.query() API
pd.DataFrame.query is a very elegant/intuitive way to perform this task, but is often slower. However, if you pay attention to the timings below, for large data, the query is very efficient. More so than the standard approach and of similar magnitude as my best suggestion.
df.query('A == ""foo""')

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14


My preference is to use the Boolean mask
Actual improvements can be made by modifying how we create our Boolean mask.
mask alternative 1
Use the underlying NumPy array and forgo the overhead of creating another pd.Series
mask = df['A'].values == 'foo'

I'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the mask
%timeit mask = df['A'].values == 'foo'
%timeit mask = df['A'] == 'foo'

5.84 s  195 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)
166 s  4.45 s per loop (mean  std. dev. of 7 runs, 10000 loops each)

Evaluating the mask with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.
Next, we'll look at the timing for slicing with one mask versus the other.
mask = df['A'].values == 'foo'
%timeit df[mask]
mask = df['A'] == 'foo'
%timeit df[mask]

219 s  12.3 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
239 s  7.03 s per loop (mean  std. dev. of 7 runs, 1000 loops each)

The performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.

mask alternative 2
We could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframeyou must take care of the dtypes when doing so!
Instead of df[mask] we will do this
pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)

If the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.
%timeit df[m]
%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)

216 s  10.4 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
1.43 ms  39.6 s per loop (mean  std. dev. of 7 runs, 1000 loops each)

However, if the data frame is not of mixed type, this is a very useful way to do it.
Given
np.random.seed([3,1415])
d1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))

d1

   A  B  C  D  E
0  0  2  7  3  8
1  7  0  6  8  6
2  0  2  0  4  9
3  7  3  2  4  3
4  3  6  7  7  4
5  5  3  7  5  9
6  8  7  6  4  7
7  6  2  6  6  5
8  2  8  7  5  8
9  4  7  6  1  5


%%timeit
mask = d1['A'].values == 7
d1[mask]

179 s  8.73 s per loop (mean  std. dev. of 7 runs, 10000 loops each)

Versus
%%timeit
mask = d1['A'].values == 7
pd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)

87 s  5.12 s per loop (mean  std. dev. of 7 runs, 10000 loops each)

We cut the time in half.

mask alternative 3
@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.
mask = df['A'].isin(['foo'])
df[mask]

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14

However, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use np.in1d
mask = np.in1d(df['A'].values, ['foo'])
df[mask]

     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14


Timing
I'll include other concepts mentioned in other posts as well for reference.
Code Below
Each column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.
res.div(res.min())

                         10        30        100       300       1000      3000      10000     30000
mask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151
mask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103
mask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919
mask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000
query                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190
xs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255
mask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760
mask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175

You'll notice that the fastest times seem to be shared between mask_with_values and mask_with_in1d.
res.T.plot(loglog=True)


Functions
def mask_standard(df):
    mask = df['A'] == 'foo'
    return df[mask]

def mask_standard_loc(df):
    mask = df['A'] == 'foo'
    return df.loc[mask]

def mask_with_values(df):
    mask = df['A'].values == 'foo'
    return df[mask]

def mask_with_values_loc(df):
    mask = df['A'].values == 'foo'
    return df.loc[mask]

def query(df):
    return df.query('A == ""foo""')

def xs_label(df):
    return df.set_index('A', append=True, drop=False).xs('foo', level=-1)

def mask_with_isin(df):
    mask = df['A'].isin(['foo'])
    return df[mask]

def mask_with_in1d(df):
    mask = np.in1d(df['A'].values, ['foo'])
    return df[mask]


Testing
res = pd.DataFrame(
    index=[
        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',
        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'
    ],
    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],
    dtype=float
)

for j in res.columns:
    d = pd.concat([df] * j, ignore_index=True)
    for i in res.index:a
        stmt = '{}(d)'.format(i)
        setp = 'from __main__ import d, {}'.format(i)
        res.at[i, j] = timeit(stmt, setp, number=50)


Special Timing
Looking at the special case when we have a single non-object dtype for the entire data frame.
Code Below
spec.div(spec.min())

                     10        30        100       300       1000      3000      10000     30000
mask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000
mask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100
reconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735

Turns out, reconstruction isn't worth it past a few hundred rows.
spec.T.plot(loglog=True)


Functions
np.random.seed([3,1415])
d1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))

def mask_with_values(df):
    mask = df['A'].values == 'foo'
    return df[mask]

def mask_with_in1d(df):
    mask = np.in1d(df['A'].values, ['foo'])
    return df[mask]

def reconstruct(df):
    v = df.values
    mask = np.in1d(df['A'].values, ['foo'])
    return pd.DataFrame(v[mask], df.index[mask], df.columns)

spec = pd.DataFrame(
    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],
    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],
    dtype=float
)

Testing
for j in spec.columns:
    d = pd.concat([df] * j, ignore_index=True)
    for i in spec.index:
        stmt = '{}(d)'.format(i)
        setp = 'from __main__ import d, {}'.format(i)
        spec.at[i, j] = timeit(stmt, setp, number=50)

    tl;dr
The Pandas equivalent to
select * from table where column_name = some_value

is
table[table.column_name == some_value]

Multiple conditions:
table[(table.column_name == some_value) | (table.column_name2 == some_value2)]

or
table.query('column_name == some_value | column_name2 == some_value2')

Code example
import pandas as pd

# Create data set
d = {'foo':[100, 111, 222],
     'bar':[333, 444, 555]}
df = pd.DataFrame(d)

# Full dataframe:
df

# Shows:
#    bar   foo
# 0  333   100
# 1  444   111
# 2  555   222

# Output only the row(s) in df where foo is 222:
df[df.foo == 222]

# Shows:
#    bar  foo
# 2  555  222

In the above code it is the line df[df.foo == 222] that gives the rows based on the column value, 222 in this case.
Multiple conditions are also possible:
df[(df.foo == 222) | (df.bar == 444)]
#    bar  foo
# 1  444  111
# 2  555  222

But at that point I would recommend using the query function, since it's less verbose and yields the same result:
df.query('foo == 222 | bar == 444')

    More flexibility using .query with pandas >= 0.25.0:
Since pandas >= 0.25.0 we can use the query method to filter dataframes with pandas methods and even column names which have spaces. Normally the spaces in column names would give an error, but now we can solve that using a backtick (`) - see GitHub:
# Example dataframe
df = pd.DataFrame({'Sender email':['ex@example.com', ""reply@shop.com"", ""buy@shop.com""]})

     Sender email
0  ex@example.com
1  reply@shop.com
2    buy@shop.com

Using .query with method str.endswith:
df.query('`Sender email`.str.endswith(""@shop.com"")')

Output
     Sender email
1  reply@shop.com
2    buy@shop.com


Also we can use local variables by prefixing it with an @ in our query:
domain = 'shop.com'
df.query('`Sender email`.str.endswith(@domain)')

Output
     Sender email
1  reply@shop.com
2    buy@shop.com

    In newer versions of Pandas, inspired by the documentation (Viewing data):
df[df[""colume_name""] == some_value] #Scalar, True/False..

df[df[""colume_name""] == ""some_value""] #String

Combine multiple conditions by putting the clause in parentheses, (), and combining them with & and | (and/or). Like this:
df[(df[""colume_name""] == ""some_value1"") & (pd[pd[""colume_name""] == ""some_value2""])]

Other filters
pandas.notna(df[""colume_name""]) == True # Not NaN
df['colume_name'].str.contains(""text"") # Search for ""text""
df['colume_name'].str.lower().str.contains(""text"") # Search for ""text"", after converting  to lowercase

    I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the query() method in v0.13 and I much prefer it. For your question, you could do df.query('col == val')

Reproduced from http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-query

In [167]: n = 10

In [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))

In [169]: df
Out[169]: 
          a         b         c
0  0.687704  0.582314  0.281645
1  0.250846  0.610021  0.420121
2  0.624328  0.401816  0.932146
3  0.011763  0.022921  0.244186
4  0.590198  0.325680  0.890392
5  0.598892  0.296424  0.007312
6  0.634625  0.803069  0.123872
7  0.924168  0.325076  0.303746
8  0.116822  0.364564  0.454607
9  0.986142  0.751953  0.561512

# pure python
In [170]: df[(df.a < df.b) & (df.b < df.c)]
Out[170]: 
          a         b         c
3  0.011763  0.022921  0.244186
8  0.116822  0.364564  0.454607

# query
In [171]: df.query('(a < b) & (b < c)')
Out[171]: 
          a         b         c
3  0.011763  0.022921  0.244186
8  0.116822  0.364564  0.454607


You can also access variables in the environment by prepending an @.

exclude = ('red', 'orange')
df.query('color not in @exclude')

    For selecting only specific columns out of multiple columns for a given value in Pandas:
select col_name1, col_name2 from table where column_name = some_value.

Options loc:
df.loc[df['column_name'] == some_value, [col_name1, col_name2]]

or query:
df.query('column_name == some_value')[[col_name1, col_name2]]

    Faster results can be achieved using numpy.where. 

For example, with unubtu's setup -

In [76]: df.iloc[np.where(df.A.values=='foo')]
Out[76]: 
     A      B  C   D
0  foo    one  0   0
2  foo    two  2   4
4  foo    two  4   8
6  foo    one  6  12
7  foo  three  7  14


Timing comparisons:

In [68]: %timeit df.iloc[np.where(df.A.values=='foo')]  # fastest
1000 loops, best of 3: 380 s per loop

In [69]: %timeit df.loc[df['A'] == 'foo']
1000 loops, best of 3: 745 s per loop

In [71]: %timeit df.loc[df['A'].isin(['foo'])]
1000 loops, best of 3: 562 s per loop

In [72]: %timeit df[df.A=='foo']
1000 loops, best of 3: 796 s per loop

In [74]: %timeit df.query('(A==""foo"")')  # slowest
1000 loops, best of 3: 1.71 ms per loop

    Here is a simple example  

from pandas import DataFrame

# Create data set
d = {'Revenue':[100,111,222], 
     'Cost':[333,444,555]}
df = DataFrame(d)


# mask = Return True when the value in column ""Revenue"" is equal to 111
mask = df['Revenue'] == 111

print mask

# Result:
# 0    False
# 1     True
# 2    False
# Name: Revenue, dtype: bool


# Select * FROM df WHERE Revenue = 111
df[mask]

# Result:
#    Cost    Revenue
# 1  444     111

    To append to this famous question (though a bit too late): You can also do df.groupby('column_name').get_group('column_desired_value').reset_index() to make a new data frame with specified column having a particular value. E.g.

import pandas as pd
df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split()})
print(""Original dataframe:"")
print(df)

b_is_two_dataframe = pd.DataFrame(df.groupby('B').get_group('two').reset_index()).drop('index', axis = 1) 
#NOTE: the final drop is to remove the extra index column returned by groupby object
print('Sub dataframe where B is two:')
print(b_is_two_dataframe)


Run this gives:

Original dataframe:
     A      B
0  foo    one
1  bar    one
2  foo    two
3  bar  three
4  foo    two
5  bar    two
6  foo    one
7  foo  three
Sub dataframe where B is two:
     A    B
0  foo  two
1  foo  two
2  bar  two

    You can also use .apply:

df.apply(lambda row: row[df['B'].isin(['one','three'])])


It actually works row-wise (i.e., applies the function to each row).

The output is 

   A      B  C   D
0  foo    one  0   0
1  bar    one  1   2
3  bar  three  3   6
6  foo    one  6  12
7  foo  three  7  14


The results is the same as using as mentioned by @unutbu

df[[df['B'].isin(['one','three'])]]

    If you want to make query to your dataframe repeatedly and speed is important to you, the best thing is to convert your dataframe to dictionary and then by doing this you can make query thousands of times faster.
my_df = df.set_index(column_name)
my_dict = my_df.to_dict('index')

After make my_dict dictionary you can go through:
if some_value in my_dict.keys():
   my_result = my_dict[some_value]

If you have duplicated values in column_name you can't make a dictionary. but you can use:
my_result = my_df.loc[some_value]

    Great answers. Only, when the size of the dataframe approaches million rows, many of the methods tend to take ages when using df[df['col']==val]. I wanted to have all possible values of ""another_column"" that correspond to specific values in ""some_column"" (in this case in a dictionary). This worked and fast.
s=datetime.datetime.now()

my_dict={}

for i, my_key in enumerate(df['some_column'].values): 
    if i%100==0:
        print(i)  # to see the progress
    if my_key not in my_dict.keys():
        my_dict[my_key]={}
        my_dict[my_key]['values']=[df.iloc[i]['another_column']]
    else:
        my_dict[my_key]['values'].append(df.iloc[i]['another_column'])
        
e=datetime.datetime.now()

print('operation took '+str(e-s)+' seconds')```


    SQL statements on DataFrames to select rows using DuckDB
With duckdb we can query pandas DataFrames with SQL statements, in a highly performant way.
Since the question is How do I select rows from a DataFrame based on column values?, and the example in the question is a SQL query, this answer looks logical in this topic.
Example:
In [1]: import duckdb

In [2]: import pandas as pd

In [3]: con = duckdb.connect()

In [4]: df = pd.DataFrame({""A"": range(11), ""B"": range(11, 22)})

In [5]: df
Out[5]:
     A   B
0    0  11
1    1  12
2    2  13
3    3  14
4    4  15
5    5  16
6    6  17
7    7  18
8    8  19
9    9  20
10  10  21

In [6]: results = con.execute(""SELECT * FROM df where A > 2"").df()

In [7]: results
Out[7]:
    A   B
0   3  14
1   4  15
2   5  16
3   6  17
4   7  18
5   8  19
6   9  20
7  10  21

    ","[2994, 5639, 653, 329, 62, 27, 83, 36, 31, 29, 19, 11, 3, 0, 0]",4874504,1585,2013-06-12T17:42:05,2022-04-23 20:00:48Z,python 
I ran into a merge conflict. How can I abort the merge?,"
                
I used git pull and had a merge conflict:

unmerged:   _widget.html.erb

You are in the middle of a conflicted merge.


I know that the other version of the file is good and that mine is bad so all my changes should be abandoned. How can I do this?
    Since your pull was unsuccessful then HEAD (not HEAD^) is the last ""valid"" commit on your branch:

git reset --hard HEAD


The other piece you want is to let their changes over-ride your changes.  

Older versions of git allowed you to use the ""theirs"" merge strategy:

git pull --strategy=theirs remote_branch


But this has since been removed, as explained in this message by Junio Hamano (the Git maintainer).  As noted in the link, instead you would do this:

git fetch origin
git reset --hard origin

    If your git version is >= 1.6.1, you can use git reset --merge.

Also, as @Michael Johnson mentions, if your git version is >= 1.7.4, you can also use git merge --abort.

As always, make sure you have no uncommitted changes before you start a merge.

From the git merge man page

git merge --abort is equivalent to git reset --merge when MERGE_HEAD is present.

MERGE_HEAD is present when a merge is in progress.

Also, regarding uncommitted changes when starting a merge:

If you have changes you don't want to commit before starting a merge, just git stash them  before the merge and git stash pop after finishing the merge or aborting it.
    git merge --abort



  Abort the current conflict resolution process, and try to reconstruct
  the pre-merge state.
  
  If there were uncommitted worktree changes present when the merge
  started, git merge --abort will in some cases be unable to
  reconstruct these changes. It is therefore recommended to always
  commit or stash your changes before running git merge.
  
  git merge --abort is equivalent to git reset --merge when
  MERGE_HEAD is present.


http://www.git-scm.com/docs/git-merge
    You can either abort the merge step:
git merge --abort

else you can keep your changes (on which branch you are)
git checkout --ours file1 file2 ...

otherwise you can keep other branch changes
git checkout --theirs file1 file2 ...

    For git >= 1.6.1:
git merge --abort

For older versions of git, this will do the job:
git reset --merge

or
git reset --hard

    I think it's git reset you need.

Beware that git revert means something very different to, say, svn revert - in Subversion the revert will discard your (uncommitted) changes, returning the file to the current version from the repository, whereas git revert ""undoes"" a commit.

git reset should do the equivalent of svn revert, that is, discard your unwanted changes.
    Comments suggest that git reset --merge is an alias for git merge --abort. It is worth noticing that git merge --abort is only equivalent to git reset --merge given that a MERGE_HEAD is present. This can be read in the git help for merge command.


  git merge --abort is equivalent to git reset --merge when MERGE_HEAD is present.


After a failed merge, when there is no MERGE_HEAD, the failed merge can be undone with git reset --merge, but not necessarily with git merge --abort. They are not only old and new syntax for the same thing.

Personally, I find git reset --merge much more powerful for scenarios similar to the described one, and failed merges in general.
    Might not be what the OP wanted, but for me I tried to merge a stable branch to a feature branch and there were too many conflicts.
I didn't manage to reset the changes since the HEAD was changed by many commits, So the easy solution was to force checkout to a stable branch.
you can then checkout to the other branch and it will be as it was before the merge.
git checkout -f master
git checkout side-branch
    If you end up with merge conflict and doesn't have anything to commit, but still a merge error is being displayed. After applying all the below mentioned commands, 

git reset --hard HEAD
git pull --strategy=theirs remote_branch
git fetch origin
git reset --hard origin


Please remove 


  .git\index.lock


File [cut paste to some other location in case of recovery] and then enter any of below command depending on which version you want.

git reset --hard HEAD
git reset --hard origin


Hope that helps!!!
    In this particular use case, you don't really want to abort the merge, just resolve the conflict in a particular way.

There is no particular need to reset and perform a merge with a different strategy, either. The conflicts have been correctly highlighted by git and the requirement to accept the other sides changes is only for this one file.

For an unmerged file in a conflict git makes available the common base, local and remote versions of the file in the index. (This is where they are read from for use in a 3-way diff tool by git mergetool.) You can use git show to view them.

# common base:
git show :1:_widget.html.erb

# 'ours'
git show :2:_widget.html.erb

# 'theirs'
git show :3:_widget.html.erb


The simplest way to resolve the conflict to use the remote version verbatim is:

git show :3:_widget.html.erb >_widget.html.erb
git add _widget.html.erb


Or, with git >= 1.6.1:

git checkout --theirs _widget.html.erb

    To avoid getting into this sort of trouble one can expand on the git merge --abort approach and create a separate test branch before merging.
Case: You have a topic branch, it wasn't merged because you got distracted/something came up/you know but it is (or was) ready.
Now is it possible to merge this into master?
Work in a test branch to estimate / find a solution, then abandon the test branch and apply the solution in the topic branch.
# Checkout the topic branch
git checkout topic-branch-1

# Create a _test_ branch on top of this
git checkout -b test

# Attempt to merge master
git merge master

# If it fails you can abandon the merge
git merge --abort
git checkout -
git branch -D test  # we don't care about this branch really...

Work on resolving the conflict.
# Checkout the topic branch
git checkout topic-branch-1

# Create a _test_ branch on top of this
git checkout -b test

# Attempt to merge master
git merge master

# resolve conflicts, run it through tests, etc
# then
git commit <conflict-resolving>

# You *could* now even create a separate test branch on top of master
# and see if you are able to merge
git checkout master
git checkout -b master-test
git merge test

Finally checkout the topic branch again, apply the fix from the test branch and continue with the PR.
Lastly delete the test and master-test.
Involved? Yes, but it won't mess with my topic or master branch until I'm good and ready.
    An alternative, which preserves the state of the working copy is:

git stash
git merge --abort
git stash pop


I generally advise against this, because it is effectively like merging in Subversion as it throws away the branch relationships in the following commit.
    Since Git 1.6.1.3 git checkout has been able to checkout from either side of a merge:

git checkout --theirs _widget.html.erb

    I found the following worked for me (revert a single file to pre-merge state):

git reset *currentBranchIntoWhichYouMerged* -- *fileToBeReset*

    Sourcetree
Because you not commit your merge, then just double click on another branch (which mean checkout it) and when sourcetree ask you about discarding all changes then agree.
Note
This answer is addressed to those who use SourceTree as git client.
    ","[2987, 2578, 2251, 616, 70, 77, 114, 59, 12, 30, 79, 4, 21, 18, 2, -10]",2285808,508,2008-09-19T13:21:00,2021-11-28 15:41:53Z,
Understanding Python super() with __init__() methods [duplicate],"
                    
            
        
            
                
                    
                        This question already has answers here:
                        
                    
                
            
                    
                        What does 'super' do in Python? - difference between super().__init__() and explicit superclass __init__()
                            
                                (11 answers)
                            
                    
                Closed 6 years ago.
        

    

Why is super() used?
Is there a difference between using Base.__init__ and super().__init__?
class Base(object):
    def __init__(self):
        print ""Base created""
        
class ChildA(Base):
    def __init__(self):
        Base.__init__(self)
        
class ChildB(Base):
    def __init__(self):
        super(ChildB, self).__init__()
        
ChildA() 
ChildB()

    super() lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.

Note that the syntax changed in Python 3.0: you can just say super().__init__() instead of super(ChildB, self).__init__() which IMO is quite a bit nicer. The standard docs also refer to a guide to using super() which is quite explanatory.
    
I'm trying to understand super()

The reason we use super is so that child classes that may be using cooperative multiple inheritance will call the correct next parent class function in the Method Resolution Order (MRO).
In Python 3, we can call it like this:
class ChildB(Base):
    def __init__(self):
        super().__init__()

In Python 2, we were required to call super like this with the defining class's name and self, but we'll avoid this from now on because it's redundant, slower (due to the name lookups), and more verbose (so update your Python if you haven't already!):
        super(ChildB, self).__init__()

Without super, you are limited in your ability to use multiple inheritance because you hard-wire the next parent's call:
        Base.__init__(self) # Avoid this.

I further explain below.

""What difference is there actually in this code?:""

class ChildA(Base):
    def __init__(self):
        Base.__init__(self)

class ChildB(Base):
    def __init__(self):
        super().__init__()

The primary difference in this code is that in ChildB you get a layer of indirection in the __init__ with super, which uses the class in which it is defined to determine the next class's __init__ to look up in the MRO.
I illustrate this difference in an answer at the canonical question, How to use 'super' in Python?, which demonstrates dependency injection and cooperative multiple inheritance.
If Python didn't have super
Here's code that's actually closely equivalent to super (how it's implemented in C, minus some checking and fallback behavior, and translated to Python):
class ChildB(Base):
    def __init__(self):
        mro = type(self).mro()
        check_next = mro.index(ChildB) + 1 # next after *this* class.
        while check_next < len(mro):
            next_class = mro[check_next]
            if '__init__' in next_class.__dict__:
                next_class.__init__(self)
                break
            check_next += 1

Written a little more like native Python:
class ChildB(Base):
    def __init__(self):
        mro = type(self).mro()
        for next_class in mro[mro.index(ChildB) + 1:]: # slice to end
            if hasattr(next_class, '__init__'):
                next_class.__init__(self)
                break

If we didn't have the super object, we'd have to write this manual code everywhere (or recreate it!) to ensure that we call the proper next method in the Method Resolution Order!
How does super do this in Python 3 without being told explicitly which class and instance from the method it was called from?
It gets the calling stack frame, and finds the class (implicitly stored as a local free variable, __class__, making the calling function a closure over the class) and the first argument to that function, which should be the instance or class that informs it which Method Resolution Order (MRO) to use.
Since it requires that first argument for the MRO, using super with static methods is impossible as they do not have access to the MRO of the class from which they are called.
Criticisms of other answers:

super() lets you avoid referring to the base class explicitly, which can be nice. . But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.

It's rather hand-wavey and doesn't tell us much, but the point of super is not to avoid writing the parent class. The point is to ensure that the next method in line in the method resolution order (MRO) is called. This becomes important in multiple inheritance.
I'll explain here.
class Base(object):
    def __init__(self):
        print(""Base init'ed"")

class ChildA(Base):
    def __init__(self):
        print(""ChildA init'ed"")
        Base.__init__(self)

class ChildB(Base):
    def __init__(self):
        print(""ChildB init'ed"")
        super().__init__()

And let's create a dependency that we want to be called after the Child:
class UserDependency(Base):
    def __init__(self):
        print(""UserDependency init'ed"")
        super().__init__()

Now remember, ChildB uses super, ChildA does not:
class UserA(ChildA, UserDependency):
    def __init__(self):
        print(""UserA init'ed"")
        super().__init__()

class UserB(ChildB, UserDependency):
    def __init__(self):
        print(""UserB init'ed"")
        super().__init__()

And UserA does not call the UserDependency method:
>>> UserA()
UserA init'ed
ChildA init'ed
Base init'ed
<__main__.UserA object at 0x0000000003403BA8>

But UserB does in-fact call UserDependency because ChildB invokes super:
>>> UserB()
UserB init'ed
ChildB init'ed
UserDependency init'ed
Base init'ed
<__main__.UserB object at 0x0000000003403438>

Criticism for another answer
In no circumstance should you do the following, which another answer suggests, as you'll definitely get errors when you subclass ChildB:
super(self.__class__, self).__init__()  # DON'T DO THIS! EVER.

(That answer is not clever or particularly interesting, but in spite of direct criticism in the comments and over 17 downvotes, the answerer persisted in suggesting it until a kind editor fixed his problem.)
Explanation: Using self.__class__ as a substitute for the class name in super() will lead to recursion. super lets us look up the next parent in the MRO (see the first section of this answer) for child classes. If you tell super we're in the child instance's method, it will then lookup the next method in line (probably this one) resulting in recursion, probably causing a logical failure (in the answerer's example, it does) or a RuntimeError when the recursion depth is exceeded.
>>> class Polygon(object):
...     def __init__(self, id):
...         self.id = id
...
>>> class Rectangle(Polygon):
...     def __init__(self, id, width, height):
...         super(self.__class__, self).__init__(id)
...         self.shape = (width, height)
...
>>> class Square(Rectangle):
...     pass
...
>>> Square('a', 10, 10)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 3, in __init__
TypeError: __init__() missing 2 required positional arguments: 'width' and 'height'

Python 3's new super() calling method with no arguments fortunately allows us to sidestep this issue.
    It's been noted that in Python 3.0+ you can use

super().__init__()


to make your call, which is concise and does not require you to reference the parent OR class names explicitly, which can be handy. I just want to add that for Python 2.7 or under, some people implement a name-insensitive behaviour by writing self.__class__ instead of the class name, i.e.

super(self.__class__, self).__init__()  # DON'T DO THIS!


HOWEVER, this breaks calls to super for any classes that inherit from your class, where self.__class__ could return a child class. For example:

class Polygon(object):
    def __init__(self, id):
        self.id = id

class Rectangle(Polygon):
    def __init__(self, id, width, height):
        super(self.__class__, self).__init__(id)
        self.shape = (width, height)

class Square(Rectangle):
    pass


Here I have a class Square, which is a sub-class of Rectangle. Say I don't want to write a separate constructor for Square because the constructor for Rectangle is good enough, but for whatever reason I want to implement a Square so I can reimplement some other method.

When I create a Square using mSquare = Square('a', 10,10), Python calls the constructor for Rectangle because I haven't given Square its own constructor. However, in the constructor for Rectangle, the call super(self.__class__,self) is going to return the superclass of mSquare, so it calls the constructor for Rectangle again. This is how the infinite loop happens, as was mentioned by @S_C. In this case, when I run super(...).__init__() I am calling the constructor for Rectangle but since I give it no arguments, I will get an error.
    Super has no side effects
Base = ChildB

Base()

works as expected
Base = ChildA

Base()

gets into infinite recursion.
    Just a heads up... with Python 2.7, and I believe ever since super() was introduced in version 2.2, you can only call super() if one of the parents inherit from a class that eventually inherits object (new-style classes).

Personally, as for python 2.7 code, I'm going to continue using BaseClassName.__init__(self, args) until I actually get the advantage of using super().
    There isn't, really. super() looks at the next class in the MRO (method resolution order, accessed with cls.__mro__) to call the methods. Just calling the base __init__ calls the base __init__. As it happens, the MRO has exactly one item-- the base. So you're really doing the exact same thing, but in a nicer way with super() (particularly if you get into multiple inheritance later).
    The main difference is that ChildA.__init__ will unconditionally call Base.__init__ whereas ChildB.__init__ will call __init__ in whatever class happens to be ChildB ancestor in self's line of ancestors
(which may differ from what you expect).
If you add a ClassC that uses multiple inheritance:
class Mixin(Base):
  def __init__(self):
    print ""Mixin stuff""
    super(Mixin, self).__init__()

class ChildC(ChildB, Mixin):  # Mixin is now between ChildB and Base
  pass

ChildC()
help(ChildC) # shows that the Method Resolution Order is ChildC->ChildB->Mixin->Base

then Base is no longer the parent of ChildB for ChildC instances. Now super(ChildB, self) will point to Mixin if self is a ChildC instance.
You have inserted Mixin in between ChildB and Base. And you can take advantage of it with super()
So if you are designed your classes so that they can be used in a Cooperative Multiple Inheritance scenario, you use super because you don't really know who is going to be the ancestor at runtime.
The super considered super post and pycon 2015 accompanying video explain this pretty well.
    ","[2984, 2169, 1064, 272, 82, 74, 57, 40]",2198472,1097,2009-02-23T00:30:08,2022-04-01 11:47:58Z,python 
Is there a unique Android device ID?,"
                
Do Android devices have a unique ID, and if so, what is a simple way to access it using Java?
    Settings.Secure#ANDROID_ID returns the Android ID as an unique for each user 64-bit hex string.
import android.provider.Settings.Secure;

private String android_id = Secure.getString(getContext().getContentResolver(),
                                                        Secure.ANDROID_ID);

Also read Best practices for unique identifiers: https://developer.android.com/training/articles/user-data-ids
    It's a simple question, with no simple answer.
Moreover, all of the existing answers here are whether out of date or unreliable.
So if you're searching for a solution after 2020.
Here are a few things to take in mind:
All the hardware-based identifiers (IMEI, MAC, serial number, etc) are unreliable for non-google's devices (Everything except Pixels and Nexuses), which are statistically most of the android active devices worldwide. Therefore official Android identifiers best practices clearly states:

Avoid using hardware identifiers, such as IMEI, MAC address, etc...

That makes most of the answers here invalid. Also due to different android security updates, some of them require newer and stricter runtime permissions, which can be simply denied by the user.
For example CVE-2018-9489 affects all the WIFI based techniques mentioned above.
That makes those identifiers not only unreliable but also inaccessible in many cases.
So in simpler words: don't use those techniques.
Many other answers here are suggesting to use the AdvertisingIdClient, which is also incompatible, as it's by design only for ads profiling. It's also stated in the official reference

Only use an Advertising ID for user profiling or ads use cases

It's not only unreliable for device identification, but you also must follow the user privacy regarding ad tracking policy, which states clearly that users can reset or block it at any moment.
So don't use it either.
Since you cannot have the desired static globally unique and reliable device identifier. Android's official reference suggests:

Use a Firebase installation ID (FID) or a privately stored GUID whenever possible for all other use cases, except for payment fraud prevention and telephony.

It's unique for the application installation on the device, so when the user uninstalls the app - it's wiped out, so it's not 100% reliable, but it's the next best thing.
Note As of today the FirebaseInstanceId is deprecated, you should use FirebaseInstallations instead.
To use FirebaseInstallations add the latest firebase-messaging dependency into your gradle
implementation 'com.google.firebase:firebase-messaging:23.0.0'

And use the code below to get the firebase ID:
FirebaseInstallations.getInstance().getId().addOnCompleteListener(task -> {
     if (task.isSuccessful()) {
        String firebaseIdentifier = task.getResult();
        // Do what you need with firebaseIdentifier
     }
});

If you need to store the device identification on your remote server, then don't store it as is (plain text), but a hash with salt.
Today it's not only a best practice, you actually must do it by law according to GDPR - identifiers and similar regulations.
    UPDATE: As of recent versions of Android, many of the issues with ANDROID_ID have been resolved, and I believe this approach is no longer necessary. Please take a look at Anthony's answer.
Full disclosure: my app used the below approach originally but no longer uses this approach, and we now use the approach outlined in the Android Developer Blog entry that emmby's answer links to (namely, generating and saving a UUID#randomUUID()).

There are many answers to this question, most of which will only work ""some"" of the time, and unfortunately, that's not good enough.
Based on my tests of devices (all phones, at least one of which is not activated):

All devices tested returned a value for TelephonyManager.getDeviceId()
All GSM devices (all tested with a SIM) returned a value for TelephonyManager.getSimSerialNumber()
All CDMA devices returned null for getSimSerialNumber() (as expected)
All devices with a Google account added returned a value for ANDROID_ID
All CDMA devices returned the same value (or derivation of the same value) for both ANDROID_ID and TelephonyManager.getDeviceId() -- as long as a Google account has been added during setup.
I did not yet have a chance to test GSM devices with no SIM, a GSM device with no Google account added, or any of the devices in airplane mode.

So if you want something unique to the device itself, TM.getDeviceId() should be sufficient.  Obviously, some users are more paranoid than others, so it might be useful to hash 1 or more of these identifiers, so that the string is still virtually unique to the device, but does not explicitly identify the user's actual device.  For example, using String.hashCode(), combined with a UUID:
final TelephonyManager tm = (TelephonyManager) getBaseContext().getSystemService(Context.TELEPHONY_SERVICE);

final String tmDevice, tmSerial, androidId;
tmDevice = """" + tm.getDeviceId();
tmSerial = """" + tm.getSimSerialNumber();
androidId = """" + android.provider.Settings.Secure.getString(getContentResolver(), android.provider.Settings.Secure.ANDROID_ID);

UUID deviceUuid = new UUID(androidId.hashCode(), ((long)tmDevice.hashCode() << 32) | tmSerial.hashCode());
String deviceId = deviceUuid.toString();

might result in something like: 00000000-54b3-e7c7-0000-000046bffd97
It works well enough for me.
As Richard mentions below, don't forget that you need permission to read the TelephonyManager properties, so add this to your manifest:
<uses-permission android:name=""android.permission.READ_PHONE_STATE"" />

import libraries
import android.content.Context;
import android.telephony.TelephonyManager;
import android.view.View;

    #Last Updated: 6/2/15

After reading every Stack Overflow post about creating a unique ID, the Google developer blog, and Android documentation, I feel as if the 'Pseudo ID' is the best possible option.
Main Issue: Hardware vs Software
Hardware

Users can change their hardware, Android tablet, or phone, so unique IDs based on hardware are not good ideas for TRACKING USERS
For TRACKING HARDWARE, this is a great idea

Software

Users can wipe/change their ROM if they are rooted
You can track users across platforms (iOS, Android, Windows, and Web)
The best want to TRACK AN INDIVIDUAL USER with their consent is to simply have them login (make this seamless using OAuth)


#Overall breakdown with Android
###- Guarantee uniqueness (include rooted devices) for API >= 9/10 (99.5% of Android devices)
###- No extra permissions
Psuedo code:
if API >= 9/10: (99.5% of devices)

return unique ID containing serial id (rooted devices may be different)

else

return the unique ID of build information (may overlap data - API < 9)

Thanks to @stansult for posting all of our options (in this Stack Overflow question).
##List of options - reasons why/ why not to use them:

User Email - Software

User could change email - HIGHLY unlikely

API 5+ <uses-permission android:name=""android.permission.GET_ACCOUNTS"" /> or

API 14+ <uses-permission android:name=""android.permission.READ_PROFILE"" /> <uses-permission android:name=""android.permission.READ_CONTACTS"" /> (How to get the Android device's primary e-mail address)

User Phone Number - Software

Users could change phone numbers - HIGHLY unlikely

<uses-permission android:name=""android.permission.READ_PHONE_STATE"" />

IMEI - Hardware (only phones, needs android.permission.READ_PHONE_STATE)

Most users hate the fact that it says ""Phone Calls"" in the permission. Some users give bad ratings because they believe you are simply stealing their personal information when all you really want to do is track device installs. It is obvious that you are collecting data.

<uses-permission android:name=""android.permission.READ_PHONE_STATE"" />

Android ID - Hardware (can be null, can change upon factory reset, can be altered on a rooted device)

Since it can be 'null', we can check for 'null' and change its value, but this means it will no longer be unique.

If you have a user with a factory reset device, the value may have changed or altered on the rooted device so there may be duplicates entries if you are tracking user installs.

WLAN MAC Address - Hardware (needs android.permission.ACCESS_WIFI_STATE)

This could be the second-best option, but you are still collecting and storing a unique identifier that comes directly from a user. This is obvious that you are collecting data.

<uses-permission android:name=""android.permission.ACCESS_WIFI_STATE ""/>

Bluetooth MAC Address - Hardware (devices with Bluetooth, needs android.permission.BLUETOOTH)

Most applications on the market do not use Bluetooth, and so if your application doesn't use Bluetooth and you are including this, the user could become suspicious.

<uses-permission android:name=""android.permission.BLUETOOTH ""/>

Pseudo-Unique ID - Software (for all Android devices)

Very possible, may contain collisions - See my method posted below!

This allows you to have an 'almost unique' ID from the user without taking anything that is private. You can create your own anonymous ID from device information.



I know there isn't any 'perfect' way of getting a unique ID without using permissions; however, sometimes we only really need to track the device installation. When it comes to creating a unique ID, we can create a 'pseudo unique id' based solely on information that the Android API gives us without using extra permissions. This way, we can show the user respect and try to offer a good user experience as well.
With a pseudo-unique id, you really only run into the fact that there may be duplicates based on the fact that there are similar devices. You can tweak the combined method to make it more unique; however, some developers need to track device installs and this will do the trick or performance based on similar devices.
##API >= 9:
If their Android device is API 9 or over, this is guaranteed to be unique because of the 'Build.SERIAL' field.
REMEMBER, you are technically only missing out on around 0.5% of users who have API < 9. So you can focus on the rest: This is 99.5% of the users!
##API < 9:
If the user's Android device is lower than API 9; hopefully, they have not done a factory reset and their 'Secure.ANDROID_ID' will be preserved or not 'null'. (see http://developer.android.com/about/dashboards/index.html)
##If all else fails:
If all else fails, if the user does have lower than API 9 (lower than Gingerbread), has reset their device, or 'Secure.ANDROID_ID' returns 'null', then simply the ID returned will be solely based on their Android device information. This is where the collisions can happen.
Changes:

Removed 'Android.SECURE_ID' because factory resets could cause the value to change
Edited the code to change on API
Changed the Pseudo

Please take a look at the method below:
/**
 * Return pseudo unique ID
 * @return ID
 */
public static String getUniquePsuedoID() {
    // If all else fails, if the user does have lower than API 9 (lower
    // than Gingerbread), has reset their device or 'Secure.ANDROID_ID'
    // returns 'null', then simply the ID returned will be solely based
    // off their Android device information. This is where the collisions
    // can happen.
    // Thanks http://www.pocketmagic.net/?p=1662!
    // Try not to use DISPLAY, HOST or ID - these items could change.
    // If there are collisions, there will be overlapping data
    String m_szDevIDShort = ""35"" + (Build.BOARD.length() % 10) + (Build.BRAND.length() % 10) + (Build.CPU_ABI.length() % 10) + (Build.DEVICE.length() % 10) + (Build.MANUFACTURER.length() % 10) + (Build.MODEL.length() % 10) + (Build.PRODUCT.length() % 10);

    // Thanks to @Roman SL!
    // https://stackoverflow.com/a/4789483/950427
    // Only devices with API >= 9 have android.os.Build.SERIAL
    // http://developer.android.com/reference/android/os/Build.html#SERIAL
    // If a user upgrades software or roots their device, there will be a duplicate entry
    String serial = null;
    try {
        serial = android.os.Build.class.getField(""SERIAL"").get(null).toString();

        // Go ahead and return the serial for api => 9
        return new UUID(m_szDevIDShort.hashCode(), serial.hashCode()).toString();
    } catch (Exception exception) {
        // String needs to be initialized
        serial = ""serial""; // some value
    }

    // Thanks @Joe!
    // https://stackoverflow.com/a/2853253/950427
    // Finally, combine the values we have found by using the UUID class to create a unique identifier
    return new UUID(m_szDevIDShort.hashCode(), serial.hashCode()).toString();
}


#New (for apps with ads AND Google Play Services):
From the Google Play Developer's console:

Beginning August 1st, 2014, the Google Play Developer Program Policy
requires all-new app uploads and updates to use the advertising ID in
lieu of any other persistent identifiers for any advertising purposes.
Learn more

Implementation:
Permission:
<uses-permission android:name=""android.permission.INTERNET"" />

Code:
import com.google.android.gms.ads.identifier.AdvertisingIdClient;
import com.google.android.gms.ads.identifier.AdvertisingIdClient.Info;
import com.google.android.gms.common.GooglePlayServicesAvailabilityException;
import com.google.android.gms.common.GooglePlayServicesNotAvailableException;
import java.io.IOException;
...

// Do not call this function from the main thread. Otherwise, 
// an IllegalStateException will be thrown.
public void getIdThread() {

  Info adInfo = null;
  try {
    adInfo = AdvertisingIdClient.getAdvertisingIdInfo(mContext);

  } catch (IOException exception) {
    // Unrecoverable error connecting to Google Play services (e.g.,
    // the old version of the service doesn't support getting AdvertisingId).
 
  } catch (GooglePlayServicesAvailabilityException exception) {
    // Encountered a recoverable error connecting to Google Play services. 

  } catch (GooglePlayServicesNotAvailableException exception) {
    // Google Play services is not available entirely.
  }
  final String id = adInfo.getId();
  final boolean isLAT = adInfo.isLimitAdTrackingEnabled();
}

Source/Docs:
http://developer.android.com/google/play-services/id.html
http://developer.android.com/reference/com/google/android/gms/ads/identifier/AdvertisingIdClient.html
##Important:

It is intended that the advertising ID completely replace existing
usage of other identifiers for ads purposes (such as the use of ANDROID_ID
in Settings.Secure) when Google Play Services is available. Cases
where Google Play Services is unavailable are indicated by a
GooglePlayServicesNotAvailableException being thrown by
getAdvertisingIdInfo().

##Warning, users can reset:
http://en.kioskea.net/faq/34732-android-reset-your-advertising-id
I have tried to reference every link that I took information from. If you are missing and need to be included, please comment!
Google Player Services InstanceID
https://developers.google.com/instance-id/
    As Dave Webb mentions, the Android Developer Blog has an article that covers this.  Their preferred solution is to track app installs rather than devices, and that will work well for most use cases.  The blog post will show you the necessary code to make that work, and I recommend you check it out.

However, the blog post goes on to discuss solutions if you need a device identifier rather than an app installation identifier.  I spoke with someone at Google to get some additional clarification on a few items in the event that you need to do so.  Here's what I discovered about device identifiers that's NOT mentioned in the aforementioned blog post:


ANDROID_ID is the preferred device identifier.  ANDROID_ID is perfectly reliable on versions of Android <=2.1 or >=2.3.  Only 2.2 has the problems mentioned in the post.
Several devices by several manufacturers are affected by the ANDROID_ID bug in 2.2.
As far as I've been able to determine, all affected devices have the same ANDROID_ID, which is 9774d56d682e549c.  Which is also the same device id reported by the emulator, btw.
Google believes that OEMs have patched the issue for many or most of their devices, but I was able to verify that as of the beginning of April 2011, at least, it's still quite easy to find devices that have the broken ANDROID_ID.


Based on Google's recommendations, I implemented a class that will generate a unique UUID for each device, using ANDROID_ID as the seed where appropriate, falling back on TelephonyManager.getDeviceId() as necessary, and if that fails, resorting to a randomly generated unique UUID that is persisted across app restarts (but not app re-installations).

Note that for devices that have to fallback on the device ID, the unique ID WILL persist across factory resets.  This is something to be aware of.  If you need to ensure that a factory reset will reset your unique ID, you may want to consider falling back directly to the random UUID instead of the device ID.

Again, this code is for a device ID, not an app installation ID.  For most situations, an app installation ID is probably what you're looking for.  But if you do need a device ID, then the following code will probably work for you.

import android.content.Context;
import android.content.SharedPreferences;
import android.provider.Settings.Secure;
import android.telephony.TelephonyManager;

import java.io.UnsupportedEncodingException;
import java.util.UUID;

public class DeviceUuidFactory {

    protected static final String PREFS_FILE = ""device_id.xml"";
    protected static final String PREFS_DEVICE_ID = ""device_id"";
    protected volatile static UUID uuid;

    public DeviceUuidFactory(Context context) {
        if (uuid == null) {
            synchronized (DeviceUuidFactory.class) {
                if (uuid == null) {
                    final SharedPreferences prefs = context
                            .getSharedPreferences(PREFS_FILE, 0);
                    final String id = prefs.getString(PREFS_DEVICE_ID, null);
                    if (id != null) {
                        // Use the ids previously computed and stored in the
                        // prefs file
                        uuid = UUID.fromString(id);
                    } else {
                        final String androidId = Secure.getString(
                            context.getContentResolver(), Secure.ANDROID_ID);
                        // Use the Android ID unless it's broken, in which case
                        // fallback on deviceId,
                        // unless it's not available, then fallback on a random
                        // number which we store to a prefs file
                        try {
                            if (!""9774d56d682e549c"".equals(androidId)) {
                                uuid = UUID.nameUUIDFromBytes(androidId
                                        .getBytes(""utf8""));
                            } else {
                                final String deviceId = (
                                    (TelephonyManager) context
                                    .getSystemService(Context.TELEPHONY_SERVICE))
                                    .getDeviceId();
                                uuid = deviceId != null ? UUID
                                    .nameUUIDFromBytes(deviceId
                                            .getBytes(""utf8"")) : UUID
                                    .randomUUID();
                            }
                        } catch (UnsupportedEncodingException e) {
                            throw new RuntimeException(e);
                        }
                        // Write the value out to the prefs file
                        prefs.edit()
                                .putString(PREFS_DEVICE_ID, uuid.toString())
                                .commit();
                    }
                }
            }
        }
    }

    /**
     * Returns a unique UUID for the current android device. As with all UUIDs,
     * this unique ID is ""very highly likely"" to be unique across all Android
     * devices. Much more so than ANDROID_ID is.
     * 
     * The UUID is generated by using ANDROID_ID as the base key if appropriate,
     * falling back on TelephonyManager.getDeviceID() if ANDROID_ID is known to
     * be incorrect, and finally falling back on a random UUID that's persisted
     * to SharedPreferences if getDeviceID() does not return a usable value.
     * 
     * In some rare circumstances, this ID may change. In particular, if the
     * device is factory reset a new device ID may be generated. In addition, if
     * a user upgrades their phone from certain buggy implementations of Android
     * 2.2 to a newer, non-buggy version of Android, the device ID may change.
     * Or, if a user uninstalls your app on a device that has neither a proper
     * Android ID nor a Device ID, this ID may change on reinstallation.
     * 
     * Note that if the code falls back on using TelephonyManager.getDeviceId(),
     * the resulting ID will NOT change after a factory reset. Something to be
     * aware of.
     * 
     * Works around a bug in Android 2.2 for many devices when using ANDROID_ID
     * directly.
     * 
     * @see http://code.google.com/p/android/issues/detail?id=10603
     * 
     * @return a UUID that may be used to uniquely identify your device for most
     *         purposes.
     */
    public UUID getDeviceUuid() {
        return uuid;
    }
}

    Here is the code that Reto Meier used in the Google I/O presentation this year to get a unique id for the user:

private static String uniqueID = null;
private static final String PREF_UNIQUE_ID = ""PREF_UNIQUE_ID"";

public synchronized static String id(Context context) {
    if (uniqueID == null) {
        SharedPreferences sharedPrefs = context.getSharedPreferences(
                PREF_UNIQUE_ID, Context.MODE_PRIVATE);
        uniqueID = sharedPrefs.getString(PREF_UNIQUE_ID, null);
        if (uniqueID == null) {
            uniqueID = UUID.randomUUID().toString();
            Editor editor = sharedPrefs.edit();
            editor.putString(PREF_UNIQUE_ID, uniqueID);
            editor.commit();
        }
    }
    return uniqueID;
}


If you couple this with a backup strategy to send preferences to the cloud (also described in Reto's talk, you should have an id that ties to a user and sticks around after the device has been wiped, or even replaced. I plan to use this in analytics going forward (in other words, I have not done that bit yet :).
    At Google I/O Reto Meier released a robust answer to how to approach this which should meet most developers needs to track users across installations. Anthony Nolan shows the direction in his answer, but I thought I'd write out the full approach so that others can easily see how to do it (it took me a while to figure out the details).

This approach will give you an anonymous, secure user ID which will be persistent for the user across different devices (based on the primary Google account) and across installs. The basic approach is to generate a random user ID and to store this in the apps' shared preferences. You then use Google's backup agent to store the shared preferences linked to the Google account in the cloud.

Let's go through the full approach. First, we need to create a backup for our SharedPreferences using the Android Backup Service. Start by registering your app via http://developer.android.com/google/backup/signup.html.

Google will give you a backup service key which you need to add to the manifest. You also need to tell the application to use the BackupAgent as follows:

<application android:label=""MyApplication""
         android:backupAgent=""MyBackupAgent"">
    ...
    <meta-data android:name=""com.google.android.backup.api_key""
        android:value=""your_backup_service_key"" />
</application>


Then you need to create the backup agent and tell it to use the helper agent for sharedpreferences:

public class MyBackupAgent extends BackupAgentHelper {
    // The name of the SharedPreferences file
    static final String PREFS = ""user_preferences"";

    // A key to uniquely identify the set of backup data
    static final String PREFS_BACKUP_KEY = ""prefs"";

    // Allocate a helper and add it to the backup agent
    @Override
    public void onCreate() {
        SharedPreferencesBackupHelper helper = new SharedPreferencesBackupHelper(this,          PREFS);
        addHelper(PREFS_BACKUP_KEY, helper);
    }
}


To complete the backup you need to create an instance of BackupManager in your main Activity:

BackupManager backupManager = new BackupManager(context);


Finally create a user ID, if it doesn't already exist, and store it in the SharedPreferences:

  public static String getUserID(Context context) {
            private static String uniqueID = null;
        private static final String PREF_UNIQUE_ID = ""PREF_UNIQUE_ID"";
    if (uniqueID == null) {
        SharedPreferences sharedPrefs = context.getSharedPreferences(
                MyBackupAgent.PREFS, Context.MODE_PRIVATE);
        uniqueID = sharedPrefs.getString(PREF_UNIQUE_ID, null);
        if (uniqueID == null) {
            uniqueID = UUID.randomUUID().toString();
            Editor editor = sharedPrefs.edit();
            editor.putString(PREF_UNIQUE_ID, uniqueID);
            editor.commit();

            //backup the changes
            BackupManager mBackupManager = new BackupManager(context);
            mBackupManager.dataChanged();
        }
    }

    return uniqueID;
}


This User_ID will now be persistent across installations, even if the user moves device.

For more information on this approach see Reto's talk.

And for full details of how to implement the backup agent see Data Backup. I particularly recommend the section at the bottom on testing as the backup does not happen instantaneously and so to test you have to force the backup.
    Get Device UUID, model number with brand name and its version number with the help of below function.

Work in Android 10 perfectly and no need to allow read phone state permission.

Code Snippets:

private void fetchDeviceInfo() {
    String uniquePseudoID = ""35"" +
            Build.BOARD.length() % 10 +
            Build.BRAND.length() % 10 +
            Build.DEVICE.length() % 10 +
            Build.DISPLAY.length() % 10 +
            Build.HOST.length() % 10 +
            Build.ID.length() % 10 +
            Build.MANUFACTURER.length() % 10 +
            Build.MODEL.length() % 10 +
            Build.PRODUCT.length() % 10 +
            Build.TAGS.length() % 10 +
            Build.TYPE.length() % 10 +
            Build.USER.length() % 10;

    String serial = Build.getRadioVersion();
    String uuid=new UUID(uniquePseudoID.hashCode(), serial.hashCode()).toString();
    String brand=Build.BRAND;
    String modelno=Build.MODEL;
    String version=Build.VERSION.RELEASE;
    Log.e(TAG, ""fetchDeviceInfo: \n ""+
            ""\n uuid is : ""+uuid+
            ""\n brand is: ""+brand+
            ""\n model is: ""+modelno+
            ""\n version is: ""+version);
}


Call Above function and to check output of above code. please see your log cat in android studio. It look likes below:


    Theres rather useful info here.

It covers five different ID types:


IMEI (only for Android devices with Phone use; needs android.permission.READ_PHONE_STATE)
Pseudo-Unique ID (for all Android devices)
Android ID (can be null, can change upon factory reset, can be altered on rooted phone)
WLAN MAC Address string (needs android.permission.ACCESS_WIFI_STATE)
BT MAC Address string (devices with Bluetooth, needs android.permission.BLUETOOTH)

    I think this is sure fire way of building a skeleton for a unique ID...  check it out.

Pseudo-Unique ID, that works on all Android devices
Some devices don't have a phone (eg. Tablets) or for some reason, you don't want to include the READ_PHONE_STATE permission. You can still read details like ROM Version, Manufacturer name, CPU type, and other hardware details, that will be well suited if you want to use the ID for a serial key check, or other general purposes. The ID computed in this way won't be unique: it is possible to find two devices with the same ID (based on the same hardware and ROM image) but the changes in real-world applications are negligible. For this purpose you can use the Build class:

String m_szDevIDShort = ""35"" + //we make this look like a valid IMEI
            Build.BOARD.length()%10+ Build.BRAND.length()%10 +
            Build.CPU_ABI.length()%10 + Build.DEVICE.length()%10 +
            Build.DISPLAY.length()%10 + Build.HOST.length()%10 +
            Build.ID.length()%10 + Build.MANUFACTURER.length()%10 +
            Build.MODEL.length()%10 + Build.PRODUCT.length()%10 +
            Build.TAGS.length()%10 + Build.TYPE.length()%10 +
            Build.USER.length()%10 ; //13 digits


Most of the Build members are strings, what we're doing here is to take their length and transform it via modulo in a digit. We have 13 such digits and we are adding two more in front (35) to have the same size ID as the IMEI (15 digits). There are other possibilities here are well, just have a look at these strings.
Returns something like 355715565309247. No special permission is required, making this approach very convenient.



(Extra info: The technique given above was copied from an article on Pocket Magic.)
    Using the code below, you can get the unique device ID of an Android OS device as a string.

deviceId = Secure.getString(getApplicationContext().getContentResolver(), Secure.ANDROID_ID); 

    The following code returns the device serial number using a hidden Android API. But, this code don't works on Samsung Galaxy Tab because ""ro.serialno"" isn't set on this device.

String serial = null;

try {
    Class<?> c = Class.forName(""android.os.SystemProperties"");
    Method get = c.getMethod(""get"", String.class);
    serial = (String) get.invoke(c, ""ro.serialno"");
}
catch (Exception ignored) {

}

    There are 30+ answers here and some are same and some are unique. This answer is based on few of those answers. One of them being @Lenn Dolling's answer.

It combines 3 IDs and creates a 32-digit hex string. It has worked very well for me.  

3 IDs are:
Pseudo-ID - It is generated based on physical device specifications
ANDROID_ID - Settings.Secure.ANDROID_ID
Bluetooth Address - Bluetooth adapter address

It will return something like this:  551F27C060712A72730B0A0F734064B1

Note: You can always add more IDs to the longId string. For example, Serial #. wifi adapter address. IMEI. This way you are making it more unique per device.

@SuppressWarnings(""deprecation"")
@SuppressLint(""HardwareIds"")
public static String generateDeviceIdentifier(Context context) {

        String pseudoId = ""35"" +
                Build.BOARD.length() % 10 +
                Build.BRAND.length() % 10 +
                Build.CPU_ABI.length() % 10 +
                Build.DEVICE.length() % 10 +
                Build.DISPLAY.length() % 10 +
                Build.HOST.length() % 10 +
                Build.ID.length() % 10 +
                Build.MANUFACTURER.length() % 10 +
                Build.MODEL.length() % 10 +
                Build.PRODUCT.length() % 10 +
                Build.TAGS.length() % 10 +
                Build.TYPE.length() % 10 +
                Build.USER.length() % 10;

        String androidId = Settings.Secure.getString(context.getContentResolver(), Settings.Secure.ANDROID_ID);

        BluetoothAdapter bluetoothAdapter = BluetoothAdapter.getDefaultAdapter();
        String btId = """";

        if (bluetoothAdapter != null) {
            btId = bluetoothAdapter.getAddress();
        }

        String longId = pseudoId + androidId + btId;

        try {
            MessageDigest messageDigest = MessageDigest.getInstance(""MD5"");
            messageDigest.update(longId.getBytes(), 0, longId.length());

            // get md5 bytes
            byte md5Bytes[] = messageDigest.digest();

            // creating a hex string
            String identifier = """";

            for (byte md5Byte : md5Bytes) {
                int b = (0xFF & md5Byte);

                // if it is a single digit, make sure it have 0 in front (proper padding)
                if (b <= 0xF) {
                    identifier += ""0"";
                }

                // add number to string
                identifier += Integer.toHexString(b);
            }

            // hex string to uppercase
            identifier = identifier.toUpperCase();
            return identifier;
        } catch (Exception e) {
            Log.e(""TAG"", e.toString());
        }
        return """";
}

    1.Use the telephony manager, which provides a unique id(i.e. IMEI). See the example,

import android.telephony.TelephonyManager;
import android.content.Context;
// ...
TelephonyManager telephonyManager;
telephonyManager = (TelephonyManager) getSystemService(Context.
                TELEPHONY_SERVICE);
/*
* getDeviceId() returns the unique device ID.
* For example,the IMEI for GSM and the MEID or ESN for CDMA phones.
*/
String deviceId = telephonyManager.getDeviceId();
/*
* getSubscriberId() returns the unique subscriber ID,
*/
String subscriberId = telephonyManager.getSubscriberId();


This requires android.permission.READ_PHONE_STATE to your user which can be hard to justify following the type of application you have made.


Devices without telephony services like tablets must report a unique device ID that is available via android.os.Build.SERIAL since Android 2.3 Gingerbread. Some phones having telephony services can also define a serial number. Like not all Android devices have a Serial Number, this solution is not reliable.
On a device first boot, a random value is generated and stored. This value is available via Settings.Secure.ANDROID_ID. Its a 64-bit number that should remain constant for the lifetime of a device. ANDROID_ID seems a good choice for a unique device identifier because its available for smartphones and tablets. To retrieve the value, you can use the following code,

String androidId = Settings.Secure.getString(getContentResolver(),
                 Settings.Secure.ANDROID_ID);


However, the value may change if a factory reset is performed on the device. There is also a known bug with a popular handset from a manufacturer where every instance has the same ANDROID_ID. Clearly, the solution is not 100% reliable.


Use UUID. As the requirement for most of the applications is to identify a particular installation and not a physical device, a good solution to get the unique id for a user if to use UUID class. The following solution has been presented by Reto Meier from Google in a Google I/O presentation,


SharedPreferences sharedPrefs = context.getSharedPreferences(
         PREF_UNIQUE_ID, Context.MODE_PRIVATE);
uniqueID = sharedPrefs.getString(PREF_UNIQUE_ID, null);



  Update : The option #1 and #2 are no longer available after android 10 as the privacy updates by google. as option 2 and 3 requires critical permission.

    The official Android Developers Blog now has a full article just about this very subject, Identifying App Installations.
    One thing I'll add - I have one of those unique situations.

Using:

deviceId = Secure.getString(this.getContext().getContentResolver(), Secure.ANDROID_ID);


Turns out that even though my Viewsonic G Tablet reports a DeviceID that is not Null, every single G Tablet reports the same number.

Makes it interesting playing ""Pocket Empires"" which gives you instant access to someone's account based on the ""unique"" DeviceID.

My device does not have a cell radio.
    For detailed instructions on how to get a unique identifier for each Android device your application is installed from, see the official Android Developers Blog posting Identifying App Installations.

It seems the best way is for you to generate one yourself upon installation and subsequently read it when the application is re-launched.

I personally find this acceptable but not ideal. No one identifier provided by Android works in all instances as most are dependent on the phone's radio states (Wi-Fi on/off, cellular on/off, Bluetooth on/off). The others, like Settings.Secure.ANDROID_ID must be implemented by the manufacturer and are not guaranteed to be unique.

The following is an example of writing data to an installation file that would be stored along with any other data the application saves locally.

public class Installation {
    private static String sID = null;
    private static final String INSTALLATION = ""INSTALLATION"";

    public synchronized static String id(Context context) {
        if (sID == null) {
            File installation = new File(context.getFilesDir(), INSTALLATION);
            try {
                if (!installation.exists())
                    writeInstallationFile(installation);
                sID = readInstallationFile(installation);
            } 
            catch (Exception e) {
                throw new RuntimeException(e);
            }
        }
        return sID;
    }

    private static String readInstallationFile(File installation) throws IOException {
        RandomAccessFile f = new RandomAccessFile(installation, ""r"");
        byte[] bytes = new byte[(int) f.length()];
        f.readFully(bytes);
        f.close();
        return new String(bytes);
    }

    private static void writeInstallationFile(File installation) throws IOException {
        FileOutputStream out = new FileOutputStream(installation);
        String id = UUID.randomUUID().toString();
        out.write(id.getBytes());
        out.close();
    }
}

    There are a lot of different approaches to work around those ANDROID_ID issues (may be null sometimes or devices of a specific model always return the same ID) with pros and cons:


Implementing a custom ID generation algorithm (based on device properties that are supposed to be static and won't change -> who knows)
Abusing other IDs like IMEI, serial number, Wi-Fi/Bluetooth-MAC address (they won't exist on all devices or additional permissions become necessary)


I myself prefer using an existing OpenUDID implementation (see https://github.com/ylechelle/OpenUDID) for Android (see https://github.com/vieux/OpenUDID). It is easy to integrate and makes use of the ANDROID_ID with fallbacks for those issues mentioned above.
    My two cents - NB this is for a device (err) unique ID - not the installation one as discussed in the Android developers's blog.

Of note that the solution provided by @emmby falls back in a per application ID as the SharedPreferences are not synchronized across processes (see here and here). So I avoided this altogether.

Instead, I encapsulated the various strategies for getting a (device) ID in an enum - changing the order of the enum constants affects the priority of the various ways of getting the ID. The first non-null ID is returned or an exception is thrown (as per good Java practices of not giving null a meaning). So for instance I have the TELEPHONY one first - but a good default choice would be the ANDROID_ID
beta:

import android.Manifest.permission;
import android.bluetooth.BluetoothAdapter;
import android.content.Context;
import android.content.pm.PackageManager;
import android.net.wifi.WifiManager;
import android.provider.Settings.Secure;
import android.telephony.TelephonyManager;
import android.util.Log;

// TODO : hash
public final class DeviceIdentifier {

    private DeviceIdentifier() {}

    /** @see http://code.google.com/p/android/issues/detail?id=10603 */
    private static final String ANDROID_ID_BUG_MSG = ""The device suffers from ""
        + ""the Android ID bug - its ID is the emulator ID : ""
        + IDs.BUGGY_ANDROID_ID;
    private static volatile String uuid; // volatile needed - see EJ item 71
    // need lazy initialization to get a context

    /**
     * Returns a unique identifier for this device. The first (in the order the
     * enums constants as defined in the IDs enum) non null identifier is
     * returned or a DeviceIDException is thrown. A DeviceIDException is also
     * thrown if ignoreBuggyAndroidID is false and the device has the Android ID
     * bug
     *
     * @param ctx
     *            an Android constant (to retrieve system services)
     * @param ignoreBuggyAndroidID
     *            if false, on a device with the android ID bug, the buggy
     *            android ID is not returned instead a DeviceIDException is
     *            thrown
     * @return a *device* ID - null is never returned, instead a
     *         DeviceIDException is thrown
     * @throws DeviceIDException
     *             if none of the enum methods manages to return a device ID
     */
    public static String getDeviceIdentifier(Context ctx,
            boolean ignoreBuggyAndroidID) throws DeviceIDException {
        String result = uuid;
        if (result == null) {
            synchronized (DeviceIdentifier.class) {
                result = uuid;
                if (result == null) {
                    for (IDs id : IDs.values()) {
                        try {
                            result = uuid = id.getId(ctx);
                        } catch (DeviceIDNotUniqueException e) {
                            if (!ignoreBuggyAndroidID)
                                throw new DeviceIDException(e);
                        }
                        if (result != null) return result;
                    }
                    throw new DeviceIDException();
                }
            }
        }
        return result;
    }

    private static enum IDs {
        TELEPHONY_ID {

            @Override
            String getId(Context ctx) {
                // TODO : add a SIM based mechanism ? tm.getSimSerialNumber();
                final TelephonyManager tm = (TelephonyManager) ctx
                        .getSystemService(Context.TELEPHONY_SERVICE);
                if (tm == null) {
                    w(""Telephony Manager not available"");
                    return null;
                }
                assertPermission(ctx, permission.READ_PHONE_STATE);
                return tm.getDeviceId();
            }
        },
        ANDROID_ID {

            @Override
            String getId(Context ctx) throws DeviceIDException {
                // no permission needed !
                final String andoidId = Secure.getString(
                    ctx.getContentResolver(),
                    android.provider.Settings.Secure.ANDROID_ID);
                if (BUGGY_ANDROID_ID.equals(andoidId)) {
                    e(ANDROID_ID_BUG_MSG);
                    throw new DeviceIDNotUniqueException();
                }
                return andoidId;
            }
        },
        WIFI_MAC {

            @Override
            String getId(Context ctx) {
                WifiManager wm = (WifiManager) ctx
                        .getSystemService(Context.WIFI_SERVICE);
                if (wm == null) {
                    w(""Wifi Manager not available"");
                    return null;
                }
                assertPermission(ctx, permission.ACCESS_WIFI_STATE); // I guess
                // getMacAddress() has no java doc !!!
                return wm.getConnectionInfo().getMacAddress();
            }
        },
        BLUETOOTH_MAC {

            @Override
            String getId(Context ctx) {
                BluetoothAdapter ba = BluetoothAdapter.getDefaultAdapter();
                if (ba == null) {
                    w(""Bluetooth Adapter not available"");
                    return null;
                }
                assertPermission(ctx, permission.BLUETOOTH);
                return ba.getAddress();
            }
        }
        // TODO PSEUDO_ID
        // http://www.pocketmagic.net/2011/02/android-unique-device-id/
        ;

        static final String BUGGY_ANDROID_ID = ""9774d56d682e549c"";
        private final static String TAG = IDs.class.getSimpleName();

        abstract String getId(Context ctx) throws DeviceIDException;

        private static void w(String msg) {
            Log.w(TAG, msg);
        }

        private static void e(String msg) {
            Log.e(TAG, msg);
        }
    }

    private static void assertPermission(Context ctx, String perm) {
        final int checkPermission = ctx.getPackageManager().checkPermission(
            perm, ctx.getPackageName());
        if (checkPermission != PackageManager.PERMISSION_GRANTED) {
            throw new SecurityException(""Permission "" + perm + "" is required"");
        }
    }

    // =========================================================================
    // Exceptions
    // =========================================================================
    public static class DeviceIDException extends Exception {

        private static final long serialVersionUID = -8083699995384519417L;
        private static final String NO_ANDROID_ID = ""Could not retrieve a ""
            + ""device ID"";

        public DeviceIDException(Throwable throwable) {
            super(NO_ANDROID_ID, throwable);
        }

        public DeviceIDException(String detailMessage) {
            super(detailMessage);
        }

        public DeviceIDException() {
            super(NO_ANDROID_ID);
        }
    }

    public static final class DeviceIDNotUniqueException extends
            DeviceIDException {

        private static final long serialVersionUID = -8940090896069484955L;

        public DeviceIDNotUniqueException() {
            super(ANDROID_ID_BUG_MSG);
        }
    }
}

    Also you might consider the Wi-Fi adapter's MAC address. Retrieved like this:

WifiManager wm = (WifiManager)Ctxt.getSystemService(Context.WIFI_SERVICE);
return wm.getConnectionInfo().getMacAddress();


Requires permission android.permission.ACCESS_WIFI_STATE in the manifest.

Reported to be available even when Wi-Fi is not connected. If Joe from the answer above gives this one a try on his many devices, that'd be nice.

On some devices, it's not available when Wi-Fi is turned off.

NOTE: From Android 6.x, it returns consistent fake mac address: 02:00:00:00:00:00
    A Serial field was added to the Build class in API level 9 (Android 2.3 - Gingerbread). Documentation says it represents the hardware serial number. Thus it should be unique, if it exists on the device. 

I don't know whether it is actually supported (=not null) by all devices with API level >= 9 though.
    Google Instance ID

Released at I/O 2015; on Android requires play services 7.5.

https://developers.google.com/instance-id/
https://developers.google.com/instance-id/guides/android-implementation

InstanceID iid = InstanceID.getInstance( context );   // Google docs are wrong - this requires context
String id = iid.getId();  // blocking call


It seems that Google intends for this ID to be used to identify installations across Android, Chrome, and iOS.

It identifies an installation rather then a device, but then again, ANDROID_ID (which is the accepted answer) now no longer identifies devices either.  With the ARC runtime a new ANDROID_ID is generated for every installation (details here), just like this new instance ID.  Also, I think that identifying installations (not devices) is what most of us are actually looking for.

The advantages of instance ID

It appears to me that Google intends for it to be used for this purpose (identifying your installations), it is cross-platform, and can be used for a number of other purposes (see the links above).

If you use GCM, then you will eventually need to use this instance ID because you need it in order to get the GCM token (which replaces the old GCM registration ID).

The disadvantages/issues

In the current implementation (GPS 7.5) the instance ID is retrieved from a server when your app requests it.  This means that the call above is a blocking call - in my unscientific testing it takes 1-3 seconds if the device is online, and 0.5 - 1.0 seconds if off-line (presumably this is how long it waits before giving up and generating a random ID).  This was tested in North America on Nexus 5 with Android 5.1.1 and GPS 7.5.

If you use the ID for the purposes they intend - eg. app authentication, app identification, GCM - I think this 1-3 seconds could be a nuisance (depending on your app, of course).
    TelephonyManger.getDeviceId() Returns the unique device ID, for example, the IMEI for GSM and the MEID or ESN for CDMA phones.

final TelephonyManager mTelephony = (TelephonyManager) getSystemService(Context.TELEPHONY_SERVICE);            
String myAndroidDeviceId = mTelephony.getDeviceId(); 


But i recommend to use:

Settings.Secure.ANDROID_ID that returns the Android ID as an unique 64-bit hex string.

    String   myAndroidDeviceId = Secure.getString(getApplicationContext().getContentResolver(), Secure.ANDROID_ID); 


Sometimes TelephonyManger.getDeviceId() will return null, so to assure an unique id you will use this method:

public String getUniqueID(){    
    String myAndroidDeviceId = """";
    TelephonyManager mTelephony = (TelephonyManager) getSystemService(Context.TELEPHONY_SERVICE);
    if (mTelephony.getDeviceId() != null){
        myAndroidDeviceId = mTelephony.getDeviceId(); 
    }else{
         myAndroidDeviceId = Secure.getString(getApplicationContext().getContentResolver(), Secure.ANDROID_ID); 
    }
    return myAndroidDeviceId;
}

    In order to include Android 9 I have only one idea that could still work that (probably) doesn't violate any terms, requires permissions, and works across installations and apps. 

Fingerprinting involving a server should be able to identify a device uniquely.
The combination of hardware information + installed apps and the installation times should do the trick. 
First installation times do not change unless an app is uninstalled and installed again. But this would have to be done for all apps on device in order to not be able to identify the device (ie. after a factory reset).  

This is how I would go about it:


Extract hardware information, application package names and first installation times. 


This is how you extract all applications from Android (no permissions needed):

final PackageManager pm = application.getPackageManager();
List<ApplicationInfo> packages = 
pm.getInstalledApplications(PackageManager.GET_META_DATA);

for (ApplicationInfo packageInfo : packages) {
    try {
        Log.d(TAG, ""Installed package :"" + packageInfo.packageName);
        Log.d(TAG, ""Installed :"" + pm.getPackageInfo(packageInfo.packageName, 0).firstInstallTime);
    } catch (PackageManager.NameNotFoundException e) {
        e.printStackTrace();
    }
}



You may want to make a hash of the each package name and installation timestamp combination, before sending it to the server, as it may or may not be any of your business what the user has installed on the device.
Some apps (a lot actually) are system apps. These are likely to have the same installation timestamp, matching the latest system update after a factory reset. Because they have the same installation timestamp they are cannot be installed by the user, and can be filtered out.
Send the info to the server and let it look for nearest match amongst previously stored info. You need to make a threshold when comparing with previously stored device info as apps are installed and uninstalled. But my guess is that this threshold can be very low, as any package name and first time installation timestamp combination alone will be pretty unique for a device, and apps are not that frequently installed and uninstalled. Having multiple apps just increases the probability of being unique.  
Return the generated unique id for the match, or generate a unique id, store with device info and return this new id.


NB: This is a non-tested and non-proved method! I am confident it will work, but I am also pretty sure that if this catches on, they will close it down one way or another. 
    How about the IMEI. That is unique for Android or other mobile devices.
    To understand the available Unique Ids in Android devices. Use this official guide. 

Best practices for unique identifiers:

IMEI, Mac Addresses, Instance Id, GUIDs, SSAID, Advertising Id, Safety Net API to verify devices. 

https://developer.android.com/training/articles/user-data-ids
    Add Below code in class file:

final TelephonyManager tm = (TelephonyManager) getBaseContext()
            .getSystemService(SplashActivity.TELEPHONY_SERVICE);
    final String tmDevice, tmSerial, androidId;
    tmDevice = """" + tm.getDeviceId();
    Log.v(""DeviceIMEI"", """" + tmDevice);
    tmSerial = """" + tm.getSimSerialNumber();
    Log.v(""GSM devices Serial Number[simcard] "", """" + tmSerial);
    androidId = """" + android.provider.Settings.Secure.getString(getContentResolver(),
            android.provider.Settings.Secure.ANDROID_ID);
    Log.v(""androidId CDMA devices"", """" + androidId);
    UUID deviceUuid = new UUID(androidId.hashCode(),
            ((long) tmDevice.hashCode() << 32) | tmSerial.hashCode());
    String deviceId = deviceUuid.toString();
    Log.v(""deviceIdUUID universally unique identifier"", """" + deviceId);
    String deviceModelName = android.os.Build.MODEL;
    Log.v(""Model Name"", """" + deviceModelName);
    String deviceUSER = android.os.Build.USER;
    Log.v(""Name USER"", """" + deviceUSER);
    String devicePRODUCT = android.os.Build.PRODUCT;
    Log.v(""PRODUCT"", """" + devicePRODUCT);
    String deviceHARDWARE = android.os.Build.HARDWARE;
    Log.v(""HARDWARE"", """" + deviceHARDWARE);
    String deviceBRAND = android.os.Build.BRAND;
    Log.v(""BRAND"", """" + deviceBRAND);
    String myVersion = android.os.Build.VERSION.RELEASE;
    Log.v(""VERSION.RELEASE"", """" + myVersion);
    int sdkVersion = android.os.Build.VERSION.SDK_INT;
    Log.v(""VERSION.SDK_INT"", """" + sdkVersion);


Add in AndroidManifest.xml:

<uses-permission android:name=""android.permission.READ_PHONE_STATE"" />

    Just a heads up for everyone reading looking for more up to date info. With Android O there are some changes to how the system manages these ids. 

https://android-developers.googleblog.com/2017/04/changes-to-device-identifiers-in.html

tl;dr Serial will require PHONE permission and Android ID will change for different apps, based on their package name and signature.

And also Google has put together a nice document which provides suggestions about when to use the hardware and software ids.

https://developer.android.com/training/articles/user-data-ids.html
    Normally, I use device unique id for my apps. But sometime I use IMEI. Both are unique numbers.

to get IMEI (international mobile equipment identifier)

public String getIMEI(Activity activity) {
    TelephonyManager telephonyManager = (TelephonyManager) activity
            .getSystemService(Context.TELEPHONY_SERVICE);
    return telephonyManager.getDeviceId();
}


to get device unique id

public String getDeviceUniqueID(Activity activity){
    String device_unique_id = Secure.getString(activity.getContentResolver(),
            Secure.ANDROID_ID);
    return device_unique_id;
}

    Here is how I am generating the unique id:

public static String getDeviceId(Context ctx)
{
    TelephonyManager tm = (TelephonyManager) ctx.getSystemService(Context.TELEPHONY_SERVICE);

    String tmDevice = tm.getDeviceId();
    String androidId = Secure.getString(ctx.getContentResolver(), Secure.ANDROID_ID);
    String serial = null;
    if(Build.VERSION.SDK_INT > Build.VERSION_CODES.FROYO) serial = Build.SERIAL;

    if(tmDevice != null) return ""01"" + tmDevice;
    if(androidId != null) return ""02"" + androidId;
    if(serial != null) return ""03"" + serial;
    // other alternatives (i.e. Wi-Fi MAC, Bluetooth MAC, etc.)

    return null;
}

    ","[2979, 2170, 106, 1178, 462, 349, 186, 49, 7, 90, 43, 31, 41, 13, 8, 53, 21, 19, 13, 11, 105, 23, 9, 9, 4, 11, 8, 14, 4, 4, 10]",1170455,1027,2010-05-07T00:47:28,2022-03-08 10:23:11Z,
What does ** (double star/asterisk) and * (star/asterisk) do for parameters?,"
                
What do *args and **kwargs mean?
def foo(x, y, *args):
def bar(x, y, **kwargs):

    The *args and **kwargs is a common idiom to allow arbitrary number of arguments to functions as described in the section more on defining functions in the Python documentation.
The *args will give you all function parameters as a tuple:
def foo(*args):
    for a in args:
        print(a)        

foo(1)
# 1

foo(1,2,3)
# 1
# 2
# 3

The **kwargs will give you all
keyword arguments except for those corresponding to a formal parameter as a dictionary.
def bar(**kwargs):
    for a in kwargs:
        print(a, kwargs[a])  

bar(name='one', age=27)
# name one
# age 27

Both idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:
def foo(kind, *args, **kwargs):
   pass

It is also possible to use this the other way around:
def foo(a, b, c):
    print(a, b, c)

obj = {'b':10, 'c':'lee'}

foo(100,**obj)
# 100 10 lee

Another usage of the *l idiom is to unpack argument lists when calling a function.
def foo(bar, lee):
    print(bar, lee)

l = [1,2]

foo(*l)
# 1 2

In Python 3 it is possible to use *l on the left side of an assignment (Extended Iterable Unpacking), though it gives a list instead of a tuple in this context:
first, *rest = [1,2,3,4]
first, *l, last = [1,2,3,4]

Also Python 3 adds new semantic (refer PEP 3102):
def func(arg1, arg2, arg3, *, kwarg1, kwarg2):
    pass

Such function accepts only 3 positional arguments, and everything after * can only be passed as keyword arguments.
Note:

A Python dict, semantically used for keyword argument passing, are arbitrarily ordered. However, in Python 3.6, keyword arguments are guaranteed to remember insertion order.
""The order of elements in **kwargs now corresponds to the order in which keyword arguments were passed to the function."" - Whats New In Python 3.6
In fact, all dicts in CPython 3.6 will remember insertion order as an implementation detail, this becomes standard in Python 3.7.

    It's also worth noting that you can use * and ** when calling functions as well. This is a shortcut that allows you to pass multiple arguments to a function directly using either a list/tuple or a dictionary. For example, if you have the following function:

def foo(x,y,z):
    print(""x="" + str(x))
    print(""y="" + str(y))
    print(""z="" + str(z))


You can do things like:

>>> mylist = [1,2,3]
>>> foo(*mylist)
x=1
y=2
z=3

>>> mydict = {'x':1,'y':2,'z':3}
>>> foo(**mydict)
x=1
y=2
z=3

>>> mytuple = (1, 2, 3)
>>> foo(*mytuple)
x=1
y=2
z=3


Note: The keys in mydict have to be named exactly like the parameters of function foo. Otherwise it will throw a TypeError:

>>> mydict = {'x':1,'y':2,'z':3,'badnews':9}
>>> foo(**mydict)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: foo() got an unexpected keyword argument 'badnews'

    
What does ** (double star) and * (star) do for parameters?

They allow for functions to be defined to accept and for users to pass any number of arguments, positional (*) and keyword (**).
Defining Functions
*args allows for any number of optional positional arguments (parameters), which will be assigned to a tuple named args.
**kwargs allows for any number of optional keyword arguments (parameters), which will be in a dict named kwargs.
You can (and should) choose any appropriate name, but if the intention is for the arguments to be of non-specific semantics, args and kwargs are standard names.
Expansion, Passing any number of arguments
You can also use *args and **kwargs to pass in parameters from lists (or any iterable) and dicts (or any mapping), respectively.
The function recieving the parameters does not have to know that they are being expanded.
For example, Python 2's xrange does not explicitly expect *args, but since it takes 3 integers as arguments:
>>> x = xrange(3) # create our *args - an iterable of 3 integers
>>> xrange(*x)    # expand here
xrange(0, 2, 2)

As another example, we can use dict expansion in str.format:
>>> foo = 'FOO'
>>> bar = 'BAR'
>>> 'this is foo, {foo} and bar, {bar}'.format(**locals())
'this is foo, FOO and bar, BAR'

New in Python 3: Defining functions with keyword only arguments
You can have keyword only arguments after the *args - for example, here, kwarg2 must be given as a keyword argument - not positionally:
def foo(arg, kwarg=None, *args, kwarg2=None, **kwargs): 
    return arg, kwarg, args, kwarg2, kwargs

Usage:
>>> foo(1,2,3,4,5,kwarg2='kwarg2', bar='bar', baz='baz')
(1, 2, (3, 4, 5), 'kwarg2', {'bar': 'bar', 'baz': 'baz'})

Also, * can be used by itself  to indicate that keyword only arguments follow, without allowing for unlimited positional arguments.
def foo(arg, kwarg=None, *, kwarg2=None, **kwargs): 
    return arg, kwarg, kwarg2, kwargs

Here, kwarg2 again must be an explicitly named, keyword argument:
>>> foo(1,2,kwarg2='kwarg2', foo='foo', bar='bar')
(1, 2, 'kwarg2', {'foo': 'foo', 'bar': 'bar'})

And we can no longer accept unlimited positional arguments because we don't have *args*:
>>> foo(1,2,3,4,5, kwarg2='kwarg2', foo='foo', bar='bar')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: foo() takes from 1 to 2 positional arguments 
    but 5 positional arguments (and 1 keyword-only argument) were given

Again, more simply, here we require kwarg to be given by name, not positionally:
def bar(*, kwarg=None): 
    return kwarg

In this example, we see that if we try to pass kwarg positionally, we get an error:
>>> bar('kwarg')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: bar() takes 0 positional arguments but 1 was given

We must explicitly pass the kwarg parameter as a keyword argument.
>>> bar(kwarg='kwarg')
'kwarg'

Python 2 compatible demos
*args (typically said ""star-args"") and **kwargs (stars can be implied by saying ""kwargs"", but be explicit with ""double-star kwargs"") are common idioms of Python for using the * and ** notation. These specific variable names aren't required (e.g. you could use *foos and **bars), but a departure from convention is likely to enrage your fellow Python coders.
We typically use these when we don't know what our function is going to receive or how many arguments we may be passing, and sometimes even when naming every variable separately would get very messy and redundant (but this is a case where usually explicit is better than implicit).
Example 1
The following function describes how they can be used, and demonstrates behavior. Note the named b argument will be consumed by the second positional argument before :
def foo(a, b=10, *args, **kwargs):
    '''
    this function takes required argument a, not required keyword argument b
    and any number of unknown positional arguments and keyword arguments after
    '''
    print('a is a required argument, and its value is {0}'.format(a))
    print('b not required, its default value is 10, actual value: {0}'.format(b))
    # we can inspect the unknown arguments we were passed:
    #  - args:
    print('args is of type {0} and length {1}'.format(type(args), len(args)))
    for arg in args:
        print('unknown arg: {0}'.format(arg))
    #  - kwargs:
    print('kwargs is of type {0} and length {1}'.format(type(kwargs),
                                                        len(kwargs)))
    for kw, arg in kwargs.items():
        print('unknown kwarg - kw: {0}, arg: {1}'.format(kw, arg))
    # But we don't have to know anything about them 
    # to pass them to other functions.
    print('Args or kwargs can be passed without knowing what they are.')
    # max can take two or more positional args: max(a, b, c...)
    print('e.g. max(a, b, *args) \n{0}'.format(
      max(a, b, *args))) 
    kweg = 'dict({0})'.format( # named args same as unknown kwargs
      ', '.join('{k}={v}'.format(k=k, v=v) 
                             for k, v in sorted(kwargs.items())))
    print('e.g. dict(**kwargs) (same as {kweg}) returns: \n{0}'.format(
      dict(**kwargs), kweg=kweg))

We can check the online help for the function's signature, with help(foo), which tells us
foo(a, b=10, *args, **kwargs)

Let's call this function with foo(1, 2, 3, 4, e=5, f=6, g=7)
which prints:
a is a required argument, and its value is 1
b not required, its default value is 10, actual value: 2
args is of type <type 'tuple'> and length 2
unknown arg: 3
unknown arg: 4
kwargs is of type <type 'dict'> and length 3
unknown kwarg - kw: e, arg: 5
unknown kwarg - kw: g, arg: 7
unknown kwarg - kw: f, arg: 6
Args or kwargs can be passed without knowing what they are.
e.g. max(a, b, *args) 
4
e.g. dict(**kwargs) (same as dict(e=5, f=6, g=7)) returns: 
{'e': 5, 'g': 7, 'f': 6}

Example 2
We can also call it using another function, into which we just provide a:
def bar(a):
    b, c, d, e, f = 2, 3, 4, 5, 6
    # dumping every local variable into foo as a keyword argument 
    # by expanding the locals dict:
    foo(**locals()) 

bar(100) prints:
a is a required argument, and its value is 100
b not required, its default value is 10, actual value: 2
args is of type <type 'tuple'> and length 0
kwargs is of type <type 'dict'> and length 4
unknown kwarg - kw: c, arg: 3
unknown kwarg - kw: e, arg: 5
unknown kwarg - kw: d, arg: 4
unknown kwarg - kw: f, arg: 6
Args or kwargs can be passed without knowing what they are.
e.g. max(a, b, *args) 
100
e.g. dict(**kwargs) (same as dict(c=3, d=4, e=5, f=6)) returns: 
{'c': 3, 'e': 5, 'd': 4, 'f': 6}

Example 3: practical usage in decorators
OK, so maybe we're not seeing the utility yet. So imagine you have several functions with redundant code before and/or after the differentiating code. The following named functions are just pseudo-code for illustrative purposes.
def foo(a, b, c, d=0, e=100):
    # imagine this is much more code than a simple function call
    preprocess() 
    differentiating_process_foo(a,b,c,d,e)
    # imagine this is much more code than a simple function call
    postprocess()

def bar(a, b, c=None, d=0, e=100, f=None):
    preprocess()
    differentiating_process_bar(a,b,c,d,e,f)
    postprocess()

def baz(a, b, c, d, e, f):
    ... and so on

We might be able to handle this differently, but we can certainly extract the redundancy with a decorator, and so our below example demonstrates how *args and **kwargs can be very useful:
def decorator(function):
    '''function to wrap other functions with a pre- and postprocess'''
    @functools.wraps(function) # applies module, name, and docstring to wrapper
    def wrapper(*args, **kwargs):
        # again, imagine this is complicated, but we only write it once!
        preprocess()
        function(*args, **kwargs)
        postprocess()
    return wrapper

And now every wrapped function can be written much more succinctly, as we've factored out the redundancy:
@decorator
def foo(a, b, c, d=0, e=100):
    differentiating_process_foo(a,b,c,d,e)

@decorator
def bar(a, b, c=None, d=0, e=100, f=None):
    differentiating_process_bar(a,b,c,d,e,f)

@decorator
def baz(a, b, c=None, d=0, e=100, f=None, g=None):
    differentiating_process_baz(a,b,c,d,e,f, g)

@decorator
def quux(a, b, c=None, d=0, e=100, f=None, g=None, h=None):
    differentiating_process_quux(a,b,c,d,e,f,g,h)

And by factoring out our code, which *args and **kwargs allows us to do, we reduce lines of code, improve readability and maintainability, and have sole canonical locations for the logic in our program. If we need to change any part of this structure, we have one place in which to make each change.
    The single * means that there can be any number of extra positional arguments. foo() can be invoked like foo(1,2,3,4,5). In the body of foo() param2 is a sequence containing 2-5.

The double ** means there can be any number of extra named parameters. bar() can be invoked like bar(1, a=2, b=3). In the body of bar() param2 is a dictionary containing {'a':2, 'b':3 }

With the following code:

def foo(param1, *param2):
    print(param1)
    print(param2)

def bar(param1, **param2):
    print(param1)
    print(param2)

foo(1,2,3,4,5)
bar(1,a=2,b=3)


the output is

1
(2, 3, 4, 5)
1
{'a': 2, 'b': 3}

    TL;DR

Below are 6 different use cases for * and ** in python programming:


To accept any number of positional arguments using *args: def foo(*args): pass, here foo accepts any number of positional arguments, i. e., the following calls are valid foo(1), foo(1, 'bar')
To accept any number of keyword arguments using **kwargs: def foo(**kwargs): pass, here 'foo' accepts any number of keyword arguments, i. e., the following calls are valid foo(name='Tom'), foo(name='Tom', age=33)
To accept any number of positional and keyword arguments using *args, **kwargs: def foo(*args, **kwargs): pass, here foo accepts any number of positional and keyword arguments, i. e., the following calls are valid foo(1,name='Tom'), foo(1, 'bar', name='Tom', age=33)
To enforce keyword only arguments using *: def foo(pos1, pos2, *, kwarg1): pass, here * means that foo only accept keyword arguments after pos2, hence foo(1, 2, 3) raises TypeError but foo(1, 2, kwarg1=3) is ok.
To express no further interest in more positional arguments using *_ (Note: this is a convention only): def foo(bar, baz, *_): pass means (by convention) foo only uses bar and baz arguments in its working and will ignore others.
To express no further interest in more keyword arguments using \**_ (Note: this is a convention only): def foo(bar, baz, **_): pass means (by convention) foo only uses bar and baz arguments in its working and will ignore others.


BONUS: From python 3.8 onward, one can use / in function definition to enforce  positional only parameters. In the following example, parameters a and b are positional-only, while c or d can be positional or keyword, and e or f are required to be keywords:

def f(a, b, /, c, d, *, e, f):
    pass

    This table is handy for using * and ** in function construction and function call:

            In function construction         In function call
=======================================================================
          |  def f(*args):                 |  def f(a, b):
*args     |      for arg in args:          |      return a + b
          |          print(arg)            |  args = (1, 2)
          |  f(1, 2)                       |  f(*args)
----------|--------------------------------|---------------------------
          |  def f(a, b):                  |  def f(a, b):
**kwargs  |      return a + b              |      return a + b
          |  def g(**kwargs):              |  kwargs = dict(a=1, b=2)
          |      return f(**kwargs)        |  f(**kwargs)
          |  g(a=1, b=2)                   |
-----------------------------------------------------------------------


This really just serves to summarize Lorin Hochstein's answer but I find it helpful.

Relatedly: uses for the star/splat operators have been expanded in Python 3
    Let us first understand what are positional arguments and keyword arguments.
Below is an example of function definition with Positional arguments.

def test(a,b,c):
     print(a)
     print(b)
     print(c)

test(1,2,3)
#output:
1
2
3


So this is a function definition with positional arguments.
You can call it with keyword/named arguments as well:

def test(a,b,c):
     print(a)
     print(b)
     print(c)

test(a=1,b=2,c=3)
#output:
1
2
3


Now let us study an example of function definition with keyword arguments:

def test(a=0,b=0,c=0):
     print(a)
     print(b)
     print(c)
     print('-------------------------')

test(a=1,b=2,c=3)
#output :
1
2
3
-------------------------


You can call this function with positional arguments as well:

def test(a=0,b=0,c=0):
    print(a)
    print(b)
    print(c)
    print('-------------------------')

test(1,2,3)
# output :
1
2
3
---------------------------------


So we now know function definitions with positional as well as keyword arguments.

Now let us study the '*' operator and '**' operator.

Please note these operators can be used in 2 areas:

a) function call

b) function definition

The use of '*' operator and '**' operator in function call. 

Let us get straight to an example and then discuss it.

def sum(a,b):  #receive args from function calls as sum(1,2) or sum(a=1,b=2)
    print(a+b)

my_tuple = (1,2)
my_list = [1,2]
my_dict = {'a':1,'b':2}

# Let us unpack data structure of list or tuple or dict into arguments with help of '*' operator
sum(*my_tuple)   # becomes same as sum(1,2) after unpacking my_tuple with '*'
sum(*my_list)    # becomes same as sum(1,2) after unpacking my_list with  '*'
sum(**my_dict)   # becomes same as sum(a=1,b=2) after unpacking by '**' 

# output is 3 in all three calls to sum function.


So remember 

when the '*' or '**' operator is used in a function call -

'*' operator unpacks data structure such as a list or tuple  into arguments needed by function definition.

'**' operator unpacks a dictionary into arguments needed by function definition.

Now let us study the '*' operator use in function definition.
Example:

def sum(*args): #pack the received positional args into data structure of tuple. after applying '*' - def sum((1,2,3,4))
    sum = 0
    for a in args:
        sum+=a
    print(sum)

sum(1,2,3,4)  #positional args sent to function sum
#output:
10


In function definition the '*' operator packs the received arguments into a tuple.

Now let us see an example of '**' used in function definition:

def sum(**args): #pack keyword args into datastructure of dict after applying '**' - def sum({a:1,b:2,c:3,d:4})
    sum=0
    for k,v in args.items():
        sum+=v
    print(sum)

sum(a=1,b=2,c=3,d=4) #positional args sent to function sum


In function definition The '**' operator packs the received arguments into a dictionary.

So remember:

In a function call the '*' unpacks data structure of tuple or list into positional or keyword arguments to be received by function definition.

In a function call the '**' unpacks data structure of dictionary into positional or keyword arguments to be received by function definition.

In a function definition the '*' packs positional arguments into a tuple.

In a function definition the '**' packs keyword arguments into a dictionary.
    TL;DR
It packs arguments passed to the function into list and dict respectively inside the function body. When you define a function signature like this:
def func(*args, **kwds):
    # do stuff

it can be called with any number of arguments and keyword arguments. The non-keyword arguments get packed into a list called args inside the function body and the keyword arguments get packed into a dict called kwds inside the function body.
func(""this"", ""is a list of"", ""non-keyowrd"", ""arguments"", keyword=""ligma"", options=[1,2,3])

now inside the function body, when the function is called, there are two local variables, args which is a list having value [""this"", ""is a list of"", ""non-keyword"", ""arguments""] and kwds which is a dict having value {""keyword"" : ""ligma"", ""options"" : [1,2,3]}

This also works in reverse, i.e. from the caller side. for example if you have a function defined as:
def f(a, b, c, d=1, e=10):
    # do stuff

you can call it with by unpacking iterables or mappings you have in the calling scope:
iterable = [1, 20, 500]
mapping = {""d"" : 100, ""e"": 3}
f(*iterable, **mapping)
# That call is equivalent to
f(1, 20, 500, d=100, e=3)

    * means receive variable arguments as tuple

** means receive variable arguments as dictionary

Used like the following:

1) single *

def foo(*args):
    for arg in args:
        print(arg)

foo(""two"", 3)


Output:

two
3


2) Now **

def bar(**kwargs):
    for key in kwargs:
        print(key, kwargs[key])

bar(dic1=""two"", dic2=3)


Output:

dic1 two
dic2 3

    For those of you who learn by examples!


The purpose of *  is to give you the ability to define a function that can take an arbitrary number of arguments provided as a list (e.g. f(*myList) ).
The purpose of ** is to give you the ability to feed a function's arguments by providing a dictionary (e.g. f(**{'x' : 1, 'y' : 2}) ).


Let us show this by defining a function that takes two normal variables x, y, and can accept more arguments as myArgs, and can accept even more arguments as myKW. Later, we will show how to feed y using myArgDict.

def f(x, y, *myArgs, **myKW):
    print(""# x      = {}"".format(x))
    print(""# y      = {}"".format(y))
    print(""# myArgs = {}"".format(myArgs))
    print(""# myKW   = {}"".format(myKW))
    print(""# ----------------------------------------------------------------------"")

# Define a list for demonstration purposes
myList    = [""Left"", ""Right"", ""Up"", ""Down""]
# Define a dictionary for demonstration purposes
myDict    = {""Wubba"": ""lubba"", ""Dub"": ""dub""}
# Define a dictionary to feed y
myArgDict = {'y': ""Why?"", 'y0': ""Why not?"", ""q"": ""Here is a cue!""}

# The 1st elem of myList feeds y
f(""myEx"", *myList, **myDict)
# x      = myEx
# y      = Left
# myArgs = ('Right', 'Up', 'Down')
# myKW   = {'Wubba': 'lubba', 'Dub': 'dub'}
# ----------------------------------------------------------------------

# y is matched and fed first
# The rest of myArgDict becomes additional arguments feeding myKW
f(""myEx"", **myArgDict)
# x      = myEx
# y      = Why?
# myArgs = ()
# myKW   = {'y0': 'Why not?', 'q': 'Here is a cue!'}
# ----------------------------------------------------------------------

# The rest of myArgDict becomes additional arguments feeding myArgs
f(""myEx"", *myArgDict)
# x      = myEx
# y      = y
# myArgs = ('y0', 'q')
# myKW   = {}
# ----------------------------------------------------------------------

# Feed extra arguments manually and append even more from my list
f(""myEx"", 4, 42, 420, *myList, *myDict, **myDict)
# x      = myEx
# y      = 4
# myArgs = (42, 420, 'Left', 'Right', 'Up', 'Down', 'Wubba', 'Dub')
# myKW   = {'Wubba': 'lubba', 'Dub': 'dub'}
# ----------------------------------------------------------------------

# Without the stars, the entire provided list and dict become x, and y:
f(myList, myDict)
# x      = ['Left', 'Right', 'Up', 'Down']
# y      = {'Wubba': 'lubba', 'Dub': 'dub'}
# myArgs = ()
# myKW   = {}
# ----------------------------------------------------------------------


Caveats


** is exclusively reserved for dictionaries.
Non-optional argument assignment happens first.
You cannot use a non-optional argument twice.
If applicable, ** must come after *, always.

    * and ** have special usage in the function argument list. *
implies that the argument is a list and ** implies that the argument
is a dictionary. This allows functions to take arbitrary number of
arguments
    In Python 3.5, you can also use this syntax in list, dict, tuple, and set displays (also sometimes called literals). See PEP 488: Additional Unpacking Generalizations.

>>> (0, *range(1, 4), 5, *range(6, 8))
(0, 1, 2, 3, 5, 6, 7)
>>> [0, *range(1, 4), 5, *range(6, 8)]
[0, 1, 2, 3, 5, 6, 7]
>>> {0, *range(1, 4), 5, *range(6, 8)}
{0, 1, 2, 3, 5, 6, 7}
>>> d = {'one': 1, 'two': 2, 'three': 3}
>>> e = {'six': 6, 'seven': 7}
>>> {'zero': 0, **d, 'five': 5, **e}
{'five': 5, 'seven': 7, 'two': 2, 'one': 1, 'three': 3, 'six': 6, 'zero': 0}


It also allows multiple iterables to be unpacked in a single function call.

>>> range(*[1, 10], *[2])
range(1, 10, 2)


(Thanks to mgilson for the PEP link.)
    Building on nickd's answer...

def foo(param1, *param2):
    print(param1)
    print(param2)


def bar(param1, **param2):
    print(param1)
    print(param2)


def three_params(param1, *param2, **param3):
    print(param1)
    print(param2)
    print(param3)


foo(1, 2, 3, 4, 5)
print(""\n"")
bar(1, a=2, b=3)
print(""\n"")
three_params(1, 2, 3, 4, s=5)


Output:

1
(2, 3, 4, 5)

1
{'a': 2, 'b': 3}

1
(2, 3, 4)
{'s': 5}


Basically, any number of positional arguments can use *args and any named arguments (or kwargs aka keyword arguments) can use **kwargs.
    From the Python documentation:


  If there are more positional arguments than there are formal parameter slots, a TypeError exception is raised, unless a formal parameter using the syntax ""*identifier"" is present; in this case, that formal parameter receives a tuple containing the excess positional arguments (or an empty tuple if there were no excess positional arguments). 
  
  If any keyword argument does not correspond to a formal parameter name, a TypeError exception is raised, unless a formal parameter using the syntax ""**identifier"" is present; in this case, that formal parameter receives a dictionary containing the excess keyword arguments (using the keywords as keys and the argument values as corresponding values), or a (new) empty dictionary if there were no excess keyword arguments. 

    Given a function that has 3 items as argument

sum = lambda x, y, z: x + y + z
sum(1,2,3) # sum 3 items

sum([1,2,3]) # error, needs 3 items, not 1 list

x = [1,2,3][0]
y = [1,2,3][1]
z = [1,2,3][2]
sum(x,y,z) # ok

sum(*[1,2,3]) # ok, 1 list becomes 3 items


Imagine this toy with a bag of a triangle, a circle and a rectangle item. That bag does not directly fit. You need to unpack the bag to take those 3 items and now they fit. The Python * operator does this unpack process.


    *args ( or *any ) means every parameters
def any_param(*param):
    pass

any_param(1)
any_param(1,1)
any_param(1,1,1)
any_param(1,...)

NOTICE : you can don't pass parameters to *args
def any_param(*param):
    pass

any_param() # will work correct

The *args is in type tuple
def any_param(*param):
    return type(param)

any_param(1) #tuple
any_param() # tuple

for access to elements don't use of *
def any(*param):
    param[0] # correct

def any(*param):
    *param[0] # incorrect

The **kwd
**kwd or **any
This is a dict type
def func(**any):
    return type(any) # dict

def func(**any):
    return any

func(width=""10"",height=""20"") # {width=""10"",height=""20"")



    I want to give an example which others haven't  mentioned

* can also unpack a generator

An example from Python3 Document

x = [1, 2, 3]
y = [4, 5, 6]

unzip_x, unzip_y = zip(*zip(x, y))


unzip_x will be [1, 2, 3], unzip_y will be [4, 5, 6]

The zip() receives multiple iretable args, and return a generator. 

zip(*zip(x,y)) -> zip((1, 4), (2, 5), (3, 6))

    *args and **kwargs: allow you to pass a variable number of arguments to a function. 

*args: is used to send a non-keyworded variable length argument list to the function:

def args(normal_arg, *argv):
    print(""normal argument:"", normal_arg)

    for arg in argv:
        print(""Argument in list of arguments from *argv:"", arg)

args('animals', 'fish', 'duck', 'bird')


Will produce:

normal argument: animals
Argument in list of arguments from *argv: fish
Argument in list of arguments from *argv: duck
Argument in list of arguments from *argv: bird


**kwargs*

**kwargs allows you to pass keyworded variable length of arguments to a function. You should use **kwargs if you want to handle named arguments in a function. 

def who(**kwargs):
    if kwargs is not None:
        for key, value in kwargs.items():
            print(""Your %s is %s."" % (key, value))

who(name=""Nikola"", last_name=""Tesla"", birthday=""7.10.1856"", birthplace=""Croatia"")  


Will produce:

Your name is Nikola.
Your last_name is Tesla.
Your birthday is 7.10.1856.
Your birthplace is Croatia.

    In addition to function calls, *args and **kwargs are useful in class hierarchies and also avoid having to write __init__ method in Python. Similar usage can seen in frameworks like Django code.

For example,

def __init__(self, *args, **kwargs):
    for attribute_name, value in zip(self._expected_attributes, args):
        setattr(self, attribute_name, value)
        if kwargs.has_key(attribute_name):
            kwargs.pop(attribute_name)

    for attribute_name in kwargs.viewkeys():
        setattr(self, attribute_name, kwargs[attribute_name])


A subclass can then be

class RetailItem(Item):
    _expected_attributes = Item._expected_attributes + ['name', 'price', 'category', 'country_of_origin']

class FoodItem(RetailItem):
    _expected_attributes = RetailItem._expected_attributes +  ['expiry_date']


The subclass then be instantiated as 

food_item = FoodItem(name = 'Jam', 
                     price = 12.0, 
                     category = 'Foods', 
                     country_of_origin = 'US', 
                     expiry_date = datetime.datetime.now())


Also, a subclass with a new attribute which makes sense only to that subclass instance can call the Base class __init__ to offload the attributes setting.
This is done through *args and **kwargs. kwargs mainly used so that code is readable using named arguments. For example,

class ElectronicAccessories(RetailItem):
    _expected_attributes = RetailItem._expected_attributes +  ['specifications']
    # Depend on args and kwargs to populate the data as needed.
    def __init__(self, specifications = None, *args, **kwargs):
        self.specifications = specifications  # Rest of attributes will make sense to parent class.
        super(ElectronicAccessories, self).__init__(*args, **kwargs)


which can be instatiated as

usb_key = ElectronicAccessories(name = 'Sandisk', 
                                price = '$6.00', 
                                category = 'Electronics',
                                country_of_origin = 'CN',
                                specifications = '4GB USB 2.0/USB 3.0')


The complete code is here
    A good example of using both in a function is:

>>> def foo(*arg,**kwargs):
...     print arg
...     print kwargs
>>>
>>> a = (1, 2, 3)
>>> b = {'aa': 11, 'bb': 22}
>>>
>>>
>>> foo(*a,**b)
(1, 2, 3)
{'aa': 11, 'bb': 22}
>>>
>>>
>>> foo(a,**b) 
((1, 2, 3),)
{'aa': 11, 'bb': 22}
>>>
>>>
>>> foo(a,b) 
((1, 2, 3), {'aa': 11, 'bb': 22})
{}
>>>
>>>
>>> foo(a,*b)
((1, 2, 3), 'aa', 'bb')
{}

    Context


python 3.x
unpacking with **
use with string formatting


Use with string formatting

In addition to the answers in this thread, here is another detail that was not mentioned elsewhere. This expands on the answer by Brad Solomon

Unpacking with ** is also useful when using python str.format.  

This is somewhat similar to what you can do with python f-strings f-string but with the added overhead of declaring a dict to hold the variables (f-string does not require a dict).

Quick Example

  ## init vars
  ddvars = dict()
  ddcalc = dict()
  pass
  ddvars['fname']     = 'Huomer'
  ddvars['lname']     = 'Huimpson'
  ddvars['motto']     = 'I love donuts!'
  ddvars['age']       = 33
  pass
  ddcalc['ydiff']     = 5
  ddcalc['ycalc']     = ddvars['age'] + ddcalc['ydiff']
  pass
  vdemo = []

  ## ********************
  ## single unpack supported in py 2.7
  vdemo.append('''
  Hello {fname} {lname}!

  Today you are {age} years old!

  We love your motto ""{motto}"" and we agree with you!
  '''.format(**ddvars)) 
  pass

  ## ********************
  ## multiple unpack supported in py 3.x
  vdemo.append('''
  Hello {fname} {lname}!

  In {ydiff} years you will be {ycalc} years old!
  '''.format(**ddvars,**ddcalc)) 
  pass

  ## ********************
  print(vdemo[-1])


    This example would help you remember *args, **kwargs and even super and inheritance in Python at once.

class base(object):
    def __init__(self, base_param):
        self.base_param = base_param


class child1(base): # inherited from base class
    def __init__(self, child_param, *args) # *args for non-keyword args
        self.child_param = child_param
        super(child1, self).__init__(*args) # call __init__ of the base class and initialize it with a NON-KEYWORD arg

class child2(base):
    def __init__(self, child_param, **kwargs):
        self.child_param = child_param
        super(child2, self).__init__(**kwargs) # call __init__ of the base class and initialize it with a KEYWORD arg

c1 = child1(1,0)
c2 = child2(1,base_param=0)
print c1.base_param # 0
print c1.child_param # 1
print c2.base_param # 0
print c2.child_param # 1

    
def foo(param1, *param2): is a method can accept arbitrary number of values for *param2,
def bar(param1, **param2): is a method can accept arbitrary number of values with keys for *param2
param1 is a simple parameter.


For example, the syntax for implementing varargs in Java as follows:

accessModifier methodName(datatype arg) {
    // method body
}

    ","[2979, 2881, 756, 185, 201, 20, 40, 56, 9, 15, 21, 25, 12, 8, 17, 4, 2, 10, 4, 7, 3, 2, 3, 0]",1065117,1226,2008-08-31T15:04:35,2022-04-01 01:47:59Z,python 
How can you find out which process is listening on a TCP or UDP port on Windows? [closed],"
                    
            
        
            
                
                    
                        Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    
                
            
        
            
        
                
                    
                
            
                
                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                
                    Closed last month.
            The community reviewed whether to reopen this question last month and left it closed:
            
                    Original close reason(s) were not resolved
            

            
        
            
                    
                        Improve this question
                    
            

    

How can you find out which process is listening on a TCP or UDP port on Windows?
    New answer, powershell
TCP
Get-Process -Id (Get-NetTCPConnection -LocalPort YourPortNumberHere).OwningProcess

UDP
Get-Process -Id (Get-NetUDPEndpoint -LocalPort YourPortNumberHere).OwningProcess

Old answer, cmd
 C:\> netstat -a -b

(Add -n to stop it trying to resolve hostnames, which will make it a lot faster.)
Note Dane's recommendation for TCPView. It looks very useful!
-a  Displays all connections and listening ports.
-b  Displays the executable involved in creating each connection or listening port. In some cases well-known executables host multiple independent components, and in these cases the sequence of components involved in creating the connection        or listening port is displayed. In this case the executable name is in [] at the bottom, on top is the component it called, and so forth until TCP/IP was reached. Note that this option can be time-consuming and will fail unless you have sufficient permissions.
-n  Displays addresses and port numbers in numerical form.
-o  Displays the owning process ID associated with each connection.
    There's a native GUI for Windows:

Start menu  All Programs  Accessories  System Tools  Resource Monitor

or run resmon.exe,

or from TaskManager  Performance tab.



    For Windows:

netstat -aon | find /i ""listening""

    The -b switch mentioned in most answers requires you to have administrative privileges on the machine. You don't really need elevated rights to get the process name!

Find the pid of the process running in the port number (e.g., 8080)

netstat -ano | findStr ""8080""


Find the process name by pid

tasklist /fi ""pid eq 2216""



    Use TCPView if you want a GUI for this. It's the old Sysinternals application that Microsoft bought out.
    netstat -aof | findstr :8080 (Change 8080 for any port)
    You can get more information if you run the following command:

netstat -aon | find /i ""listening"" |find ""port""


using the 'Find' command allows you to filter the results. find /i ""listening"" will display only ports that are 'Listening'. Note, you need the /i to ignore case, otherwise you would type find ""LISTENING"". | find ""port"" will limit the results to only those containing the specific port number. Note, on this it will also filter in results that have the port number anywhere in the response string.
    
Open a command prompt window (as Administrator) From ""Start\Search box"" Enter ""cmd"" then right-click on ""cmd.exe"" and select ""Run as Administrator""
Enter the following text then hit Enter.

netstat -abno

-a          Displays all connections and listening ports.

-b          Displays the executable involved in creating each connection or
            listening port. In some cases well-known executables host
            multiple independent components, and in these cases the
            sequence of components involved in creating the connection
            or listening port is displayed. In this case the executable
            name is in [] at the bottom, on top is the component it called,
            and so forth until TCP/IP was reached. Note that this option
            can be time-consuming and will fail unless you have sufficient
            permissions.

-n          Displays addresses and port numbers in numerical form.

-o          Displays the owning process ID associated with each connection.
Find the Port that you are listening on under ""Local Address""
Look at the process name directly under that.


NOTE: To find the process under Task Manager


Note the PID (process identifier) next to the port you are looking at.
Open Windows Task Manager. 
Select the Processes tab. 
Look for the PID you noted when you did the netstat in step 1.


If you dont see a PID column, click on View / Select Columns. Select PID.
Make sure Show processes from all users is selected.


    In case someone need an equivalent for macOS like I did, here is it:
lsof -i tcp:8080
After you get the PID of the process, you can kill it with:
kill -9 <PID>
    With PowerShell 5 on Windows 10 or Windows Server 2016, run Get-NetTCPConnection cmdlet. I guess that it should also work on older Windows versions.
The default output of Get-NetTCPConnection does not include Process ID for some reason and it is a bit confusing. However, you could always get it by formatting the output. The property you are looking for is OwningProcess.

If you want to find out the ID of the process that is listening on port 443, run this command:
  PS C:\> Get-NetTCPConnection -LocalPort 443 | Format-List

  LocalAddress   : ::
  LocalPort      : 443
  RemoteAddress  : ::
  RemotePort     : 0
  State          : Listen
  AppliedSetting :
  OwningProcess  : 4572
  CreationTime   : 02.11.2016 21:55:43
  OffloadState   : InHost


Format the output to a table with the properties you look for:
  PS C:\> Get-NetTCPConnection -LocalPort 443 | Format-Table -Property LocalAddress, LocalPort, State, OwningProcess

  LocalAddress LocalPort  State OwningProcess
  ------------ ---------  ----- -------------
  ::                 443 Listen          4572
  0.0.0.0            443 Listen          4572


If you want to find out a name of the process, run this command:
  PS C:\> Get-Process -Id (Get-NetTCPConnection -LocalPort 443).OwningProcess

  Handles  NPM(K)    PM(K)      WS(K)     CPU(s)     Id  SI ProcessName
  -------  ------    -----      -----     ------     --  -- -----------
  143      15     3448      11024              4572   0 VisualSVNServer



    First we find the process id of that particular task which we need to eliminate in order to get the port free:

Type  

netstat -n -a -o


After executing this command in the Windows command line prompt (cmd), select the pid which I think the last column. Suppose this is 3312.

Now type

taskkill /F /PID 3312


You can now cross check by typing the netstat command.

NOTE: sometimes Windows doesnt allow you to run this command directly on CMD, so first you need to go with these steps:

From the start menu -> command prompt (right click on command prompt, and run as administrator)
    Get PID and Image Name

Use only one command:

for /f ""tokens=5"" %a in ('netstat -aon ^| findstr 9000') do tasklist /FI ""PID eq %a""


where 9000 should be replaced by your port number.

The output will contain something like this:

Image Name                     PID Session Name        Session#    Mem Usage
========================= ======== ================ =========== ============
java.exe                      5312 Services                   0    130,768 K




Explanation:


it iterates through every line from the output of the following command: 

netstat -aon | findstr 9000

from every line, the PID (%a - the name is not important here) is extracted (PID is the 5th element in that line) and passed to the following command

tasklist /FI ""PID eq 5312""





If you want to skip the header and the return of the command prompt, you can use:

echo off & (for /f ""tokens=5"" %a in ('netstat -aon ^| findstr 9000') do tasklist /NH /FI ""PID eq %a"") & echo on


Output:

java.exe                      5312 Services                   0    130,768 K

    It is very simple to get the port number from a PID in Windows.

The following are the steps:


Go to run  type cmd  press Enter.
Write the following command...

netstat -aon | findstr [port number]


(Note: Don't include square brackets.)
Press Enter...
Then cmd will give you the detail of the service running on that port along with the PID.
Open Task Manager and hit the service tab and match the PID with that of the cmd, and that's it.

    To find out which specific process (PID) is using which port:

netstat -anon | findstr 1234


Where 1234 is the PID of your process. [Go to Task Manager  Services/Processes tab to find out the PID of your application.]
    To find pid who using port 8000
netstat -aon | findstr '8000'

To Kill that Process in windows
taskkill /pid pid /f

where pid is the process id which you get form first command
    To get a list of all the owning process IDs associated with each connection:

netstat -ao |find /i ""listening""


If want to kill any process have the ID and use this command, so that port becomes free

Taskkill /F /IM PID of a process

    You can also check the reserved ports with the command below. Hyper-V reserve some ports, for instance.
netsh int ipv4 show excludedportrange protocol=tcp
    
Open the command prompt - start  Run  cmd, or start menu  All Programs  Accessories  Command Prompt.
Type

netstat -aon | findstr '[port_number]'



Replace the [port_number] with the actual port number that you want to check and hit Enter.


If the port is being used by any application, then that applications detail will be shown. The number, which is shown at the last column of the list, is the PID (process ID) of that application. Make note of this.
Type

tasklist | findstr '[PID]'



Replace the [PID] with the number from the above step and hit Enter.


Youll be shown the application name that is using your port number.

    Netstat:


-a displays all connection and listening ports
-b displays executables
-n stop resolve hostnames (numerical form)
-o owning process

netstat -bano | findstr ""7002""

netstat -ano > ano.txt 



The Currports tool helps to search and filter
    A single-line solution that helps me is this one. Just substitute 3000 with your port:

$P = Get-Process -Id (Get-NetTCPConnection -LocalPort 3000).OwningProcess; Stop-Process $P.Id


Edit: Changed kill to Stop-Process for more PowerShell-like language
    Using Windows' default shell (powershell) and without external apps
For those using PowerShell, try Get-NetworkStatistics:
> Get-NetworkStatistics | where Localport -eq 8000


ComputerName  : DESKTOP-JL59SC6
Protocol      : TCP
LocalAddress  : 0.0.0.0
LocalPort     : 8000
RemoteAddress : 0.0.0.0
RemotePort    : 0
State         : LISTENING
ProcessName   : node
PID           : 11552

    netstat -ao and netstat -ab tell you the application, but if you're not a system administrator  you'll get ""The requested operation requires elevation"".

It's not ideal, but if you use Sysinternals' Process Explorer you can go to specific processes' properties and look at the TCP tab to see if they're using the port you're interested in. It is a bit of a needle and haystack thing, but maybe it'll help someone...
    Just open a command shell and type (saying your port is 123456):

netstat -a -n -o | find ""123456""


You will see everything you need.

The headers are:

 Proto  Local Address          Foreign Address        State           PID
 TCP    0.0.0.0:37             0.0.0.0:0              LISTENING       1111


This is as mentioned here.
    Follow these tools: From cmd: C:\> netstat -anob with Administrator privileges.
Process Explorer
Process Dump
Port Monitor
All from sysinternals.com.
If you just want to know process running and threads under each process, I recommend learning about wmic. It is a wonderful command-line tool, which gives you much more than you can know.
Example:
c:\> wmic process list brief /every:5

The above command will show an all process list in brief every 5 seconds. To know more, you can just go with /? command of windows , for example,
c:\> wmic /?
c:\> wmic process /?
c:\> wmic prcess list /?

And so on and so forth. :)
    Type in the command: netstat -aon | findstr :DESIRED_PORT_NUMBER

For example, if I want to find port 80: netstat -aon | findstr :80

This answer was originally posted to this question.
    Use:

netstat -a -o


This shows the PID of the process running on a particular port.

Keep in mind the process ID and go to Task Manager and services or details tab and end the process which has the same PID.

Thus you can kill a process running on a particular port in Windows.
    Powershell
if you want to have a good overview, you can use this:
Get-NetTCPConnection -State Listen | Select-Object -Property *, `
    @{'Name' = 'ProcessName';'Expression'={(Get-Process -Id $_.OwningProcess).Name}} `
    | select ProcessName,LocalAddress,LocalPort

than you get a table like this:
ProcessName              LocalAddress  LocalPort
-----------              ------------  ---------
services                 ::                49755
jhi_service              ::1               49673
svchost                  ::                  135
services                 0.0.0.0           49755
spoolsv                  0.0.0.0           49672

for udp it is:
Get-NetUDPEndpoint | Select-Object -Property *, `
   @{'Name' = 'ProcessName';'Expression'={(Get-Process -Id $_.OwningProcess).Name}} `
   | select ProcessName,LocalAddress,LocalPort

    If you'd like to use a GUI tool to do this there's Sysinternals' TCPView.
    Programmatically, you need stuff from iphlpapi.h, for example GetTcpTable2(). Structures like MIB_TCP6ROW2 contain the owner PID.
    Using PowerShell...
...this would be your friend (replace 8080 with your port number):

 netstat -abno | Select-String -Context 0,1 -Pattern 8080


Sample output

>   TCP    0.0.0.0:8080           0.0.0.0:0              LISTENING         2920
   [tnslsnr.exe]
>   TCP    [::]:8080              [::]:0                 LISTENING         2920
   [tnslsnr.exe]


So in this example tnslsnr.exe (OracleXE database) is listening on port 8080.

Quick explanation


Select-String is used to filter the lengthy output of netstat for the relevant lines.
-Pattern tests each line against a regular expression.
-Context 0,1 will output 0 leading lines and 1 trailing line for each pattern match.

    ","[2976, 3426, 2702, 303, 202, 274, 25, 99, 88, 17, 31, 59, 63, 35, 24, 10, 36, 7, 17, 16, 11, 10, 13, 20, 10, 15, 9, 2, 17, 7, 7]",4616095,859,2008-09-07T06:26:12,2022-03-07 01:08:13Z,
Should I use the datetime or timestamp data type in MySQL?,"
                
Would you recommend using a datetime or a timestamp field, and why (using MySQL)? 

I'm working with PHP on the server side.
    Timestamps in MySQL are generally used to track changes to records, and are often updated every time the record is changed. If you want to store a specific value you should use a datetime field.

If you meant that you want to decide between using a UNIX timestamp or a native MySQL datetime field, go with the native format. You can do calculations within MySQL that way 
(""SELECT DATE_ADD(my_datetime, INTERVAL 1 DAY)"") and it is simple to change the format of the value to a UNIX timestamp (""SELECT UNIX_TIMESTAMP(my_datetime)"") when you query the record if you want to operate on it with PHP.
    In MySQL 5 and above, TIMESTAMP values are converted from the current time zone to UTC for storage, and converted back from UTC to the current time zone for retrieval. (This occurs only for the TIMESTAMP data type, and not for other types such as DATETIME.)

By default, the current time zone for each connection is the server's time. The time zone can be set on a per-connection basis, as described in MySQL Server Time Zone Support.
    I always use DATETIME fields for anything other than row metadata (date created or modified).

As mentioned in the MySQL documentation:


  The DATETIME type is used when you need values that contain both date and time information. MySQL retrieves and displays DATETIME values in 'YYYY-MM-DD HH:MM:SS' format. The supported range is '1000-01-01 00:00:00' to '9999-12-31 23:59:59'.
  
  ...
  
  The TIMESTAMP data type has a range of '1970-01-01 00:00:01' UTC to '2038-01-09 03:14:07' UTC. It has varying properties, depending on the MySQL version and the SQL mode the server is running in.


You're quite likely to hit the lower limit on TIMESTAMPs in general use -- e.g. storing birthdate.
    The below examples show how the TIMESTAMP date type changed the values after changing the time-zone to 'america/new_york' where DATETIMEis unchanged.

mysql> show variables like '%time_zone%';
+------------------+---------------------+
| Variable_name    | Value               |
+------------------+---------------------+
| system_time_zone | India Standard Time |
| time_zone        | Asia/Calcutta       |
+------------------+---------------------+

mysql> create table datedemo(
    -> mydatetime datetime,
    -> mytimestamp timestamp
    -> );

mysql> insert into datedemo values ((now()),(now()));

mysql> select * from datedemo;
+---------------------+---------------------+
| mydatetime          | mytimestamp         |
+---------------------+---------------------+
| 2011-08-21 14:11:09 | 2011-08-21 14:11:09 |
+---------------------+---------------------+

mysql> set time_zone=""america/new_york"";

mysql> select * from datedemo;
+---------------------+---------------------+
| mydatetime          | mytimestamp         |
+---------------------+---------------------+
| 2011-08-21 14:11:09 | 2011-08-21 04:41:09 |
+---------------------+---------------------+


I've converted my answer into article so more people can find this useful, MySQL: Datetime Versus Timestamp Data Types.
    Neither. The DATETIME and TIMESTAMP types are fundamentally broken for generic use cases. MySQL will change them in the future. You should use BIGINT and UNIX timestamps unless you have a specific reason to use something else.
Special cases
Here are some specific situations where your choice is easier and you don't need the analysis and general recommendation in this answer.

Date only  if you only care about the date (like the date of the next Lunar New Year, 2022-02-01) AND you have a clear understanding of what timezone that date applies (or don't care, as in the case of Lunar New Year) then use the DATE column type.

Record insert times  if you are logging the insert dates/times for rows in your database AND you don't care that your application will break in the next 17 years, then go ahead and use TIMESTAMP with a default value of CURRENT_TIMESTAMP().


Why is TIMESTAMP broken?
The TIMESTAMP type is stored on disk in UTC timezone. This means that if you physically move your server, it does not break. That's good . But timestamps as currently defined will stop working entirely in the year 2038 .
Every time you INSERT INTO or SELECT FROM a TIMESTAMP column, the physical location (i.e. timezone configuration) of your client/application server is taken into account. If you move your application server then your dates break .
(Update 2022-04-29 MySQL fixed this in 8.0.28 but if your production environment is on CentOS 7 or many other flavors your migration path will be a long time until you get this support.)
Why is VARCHAR broken?
The VARCHAR type allows to unambiguously store a non-local date/time/both in ISO 8601 format and it works for dates past 2037. It is common to use Zulu time, but ISO 8601 allows to encode any offset. This is less useful because while MySQL date and time functions do support string as input anywhere date/time/both are expected, the result is incorrect if the input uses timezone offsets.
Also VARCHAR uses extra bytes of storage.
Why is DATETIME broken?
A DATETIME stores a DATE and a TIME in the same column. Neither of these things have any meaning unless the timezone is understood, and the timezone is not stored anywhere . You should put the intended timezone as a comment in the column because the timezone is inextricably linked to the data. So few people use column comments, therefore this is mistake waiting to happen. I inherited a server from Arizona, so I always need to convert all timestamps FROM Arizona time and then TO another time.
(Update 2021-12-08 I restarted the server after years of uptime and the database client (with upgrades) reset to UTC. That means my application needs to handle dates before and after the reset differently. Hardcode!)
The only situation a DATETIME is correct is to complete this sentence:

Your year 2020 solar new year starts at exactly DATETIME(""2020-01-01 00:00:00"").

There is no other good use for DATETIMEs. Perhaps you will imagine a web server for a city government in Delaware. Surely the timezone for this server and all the people accessing this server can be implied to be in Delaware, with Eastern Time Zone, right? Wrong! In this millennium, we all think of servers as existing in ""the cloud"". So it is always wrong to think of your server in any specific timezone, because your server will be moved some day.
Note: MySQL now supports time zone offsets in DATETIME literals (thanks @Marko). This may make inserting DATETIMEs more convenient for you but does not address the incomplete and therefore useless meaning of the data, this fatal issue identifies ("""") above.
How to use BIGINT?
Define:
CREATE TEMPORARY TABLE good_times (
    a_time BIGINT
)

Insert a specific value:
INSERT INTO good_times VALUES (
    UNIX_TIMESTAMP(CONVERT_TZ(""2014-12-03 12:24:54"", '+00:00', @@global.time_zone))
);

Or of course this is much better from your application, like:
$statement = $myDB->prepare('INSERT INTO good_times VALUES (?)');
$statement->execute([$someTime->getTimestamp()]);

Select:
SELECT a_time FROM good_times;

There are techniques for filtering relative times (select posts within the past 30 days, find users that bought within 10 minutes of registering) beyond the scope here.
    
TIMESTAMP is four bytes vs eight bytes for DATETIME.

Timestamps are also lighter on the database and indexed faster.

The DATETIME type is used when you need values that contain both date and time information. MySQL retrieves and displays DATETIME values in YYYY-MM-DD HH:MM:SS format. The supported range is 1000-01-01 00:00:00 to 9999-12-31 23:59:59. The TIMESTAMP data type has a range of 1970-01-01 00:00:01 UTC to 2038-01-09 03:14:07 UTC. It has varying properties, depending on the MySQL version and the SQL mode the server is running in.

DATETIME is constant while TIMESTAMP is affected by the time_zone setting.


    I recommend using neither a DATETIME or a TIMESTAMP field.  If you want to represent a specific day as a whole (like a birthday), then use a DATE type, but if you're being more specific than that, you're probably interested in recording an actual moment as opposed to a unit of time (day,week,month,year).  Instead of using a DATETIME or TIMESTAMP, use a BIGINT, and simply store the number of milliseconds since the epoch (System.currentTimeMillis() if you're using Java).  This has several advantages:


You avoid vendor lock-in.  Pretty much every database supports integers in the relatively similar fashion.  Suppose you want to move to another database.  Do you want to worry about the differences between MySQL's DATETIME values and how Oracle defines them?  Even among different versions of MySQL, TIMESTAMPS have a different level of precision.  It was only just recently that MySQL supported milliseconds in the timestamps. 
No timezone issues.  There's been some insightful comments on here on what happens with timezones with the different data types.  But is this common knowledge, and will your co-workers all take the time to learn it?  On the other hand, it's pretty hard to mess up changing a BigINT into a java.util.Date.  Using a BIGINT causes a lot of issues with timezones to fall by the wayside.
No worries about ranges or precision.  You don't have to worry about what being cut short by future date ranges (TIMESTAMP only goes to 2038).  
Third-party tool integration.  By using an integer, it's trivial for 3rd party tools (e.g. EclipseLink) to interface with the database.  Not every third-party tool is going to have the same understanding of a ""datetime"" as MySQL does.  Want to try and figure out in Hibernate whether you should use a java.sql.TimeStamp or java.util.Date object if you're using these custom data types?  Using your base data types make's use with 3rd-party tools trivial. 


This issue is closely related how you should store a money value (i.e. $1.99) in a database.  Should you use a Decimal, or the database's Money type, or worst of all a Double? All 3 of these options are terrible, for many of the same reasons listed above.  The solution is to store the value of money in cents using BIGINT, and then convert cents to dollars when you display the value to the user.  The database's job is to store data, and NOT to intrepret that data.  All these fancy data-types you see in databases(especially Oracle) add little, and start you down the road to vendor lock-in.  
    The main difference is that DATETIME is constant while TIMESTAMP is affected by the time_zone setting.

So it only matters when you have  or may in the future have  synchronized clusters across time zones.

In simpler words: If I have a database in Australia, and take a dump of that database to synchronize/populate a database in America, then the TIMESTAMP would update to reflect the real time of the event in the new time zone, while DATETIME would still reflect the time of the event in the au time zone.

A great example of DATETIME being used where TIMESTAMP should have been used is in Facebook, where their servers are never quite sure what time stuff happened across time zones. Once I was having a conversation in which the time said I was replying to messages before the message was actually sent. (This, of course, could also have been caused by bad time zone translation in the messaging software if the times were being posted rather than synchronized.)
    I make this decision on a semantic base.

I use a timestamp when I need to record a (more or less) fixed point in time. For example when a record was inserted into the database or when some user action took place.

I use a datetime field when the date/time can be set and changed arbitrarily. For example when a user can save later change appointments.
    TIMESTAMP is 4 bytes Vs 8 bytes for DATETIME. 

http://dev.mysql.com/doc/refman/5.0/en/storage-requirements.html

But like scronide said it does have a lower limit of the year 1970. It's great for anything that might happen in the future though ;)
    2016 +: what I advise is to set your Mysql timezone to UTC and use DATETIME:

Any recent front-end framework (Angular 1/2, react, Vue,...) can easily and automatically convert your UTC datetime to local time.

Additionally:


DATETIME can now be automatically set to the current time value How do you set a default value for a MySQL Datetime column?
Contrary to what one might think, DATETIME is FASTER THAN TIMESTAMP,
http://gpshumano.blogs.dri.pt/2009/07/06/mysql-datetime-vs-timestamp-vs-int-performance-and-benchmarking-with-myisam/
TIMESTAMP is still limited to 1970-2038


(Unless you are likely to change the timezone of your servers)



Example with AngularJs

// back-end: format for angular within the sql query
SELECT DATE_FORMAT(my_datetime, ""%Y-%m-%dT%TZ"")...

// font-end Output the localised time
{{item.my_datetime | date :'medium' }}


All localised time format available here:
https://docs.angularjs.org/api/ng/filter/date
    Depends on application, really.

Consider setting a timestamp by a user to a server in New York, for an appointment in Sanghai. Now when the user connects in Sanghai, he accesses the same appointment timestamp from a mirrored server in Tokyo. He will see the appointment in Tokyo time, offset from the original New York time.

So for values that represent user time like an appointment or a schedule, datetime is better. It allows the user to control the exact date and time desired, regardless of the server settings. The set time is the set time, not affected by the server's time zone, the user's time zone, or by changes in the way daylight savings time is calculated (yes it does change).

On the other hand, for values that represent system time like payment transactions, table modifications or logging, always use timestamps. The system will not be affected by moving the server to another time zone, or when comparing between servers in different timezones.

Timestamps are also lighter on the database and indexed faster.
    Comparison between DATETIME, TIMESTAMP and DATE



What is that [.fraction]?


A DATETIME or TIMESTAMP value can include a trailing fractional
seconds part in up to microseconds (6 digits) precision. In
particular, any fractional part in a value inserted into a DATETIME
or TIMESTAMP column is stored rather than discarded. This is of course optional.


Sources:


MySQL Date/Time data types reference
MySQL Storage Requirements reference

    TIMESTAMP is always in UTC (that is, elapsed seconds since 1970-01-01, in UTC), and your MySQL server auto-converts it to the date/time for the connection timezone. In the long-term, TIMESTAMP is the way to go because you know your temporal data will always be in UTC. For example, you won't screw your dates up if you migrate to a different server or if you change the timezone settings on your server.

Note: default connection timezone is the server timezone, but this can (should) be changed per session (see SET time_zone = ...).
    DATETIME vs TIMESTAMP:

TIMESTAMP used to track changes of records, and update every time when the record is changed.

DATETIME used to store specific and static value which is not affected by any changes in records.

TIMESTAMP also affected by different TIME ZONE related setting.
DATETIME is constant.

TIMESTAMP internally converted a current time zone to UTC for storage, and during retrieval convert the back to the current time zone.

DATETIME can not do this.

TIMESTAMP is 4 bytes and DATETIME is 8 bytes.

TIMESTAMP supported range:
1970-01-01 00:00:01 UTC to 2038-01-19 03:14:07 UTC
DATETIME supported range:
1000-01-01 00:00:00 to 9999-12-31 23:59:59
    A timestamp field is a special case of the datetime field. You can create timestamp columns to have special properties; it can be set to update itself on either create and/or update.

In ""bigger"" database terms, timestamp has a couple of special-case triggers on it.

What the right one is depends entirely on what you want to do.
    It is worth noting in MySQL you can use something along the lines of the below when creating your table columns:

on update CURRENT_TIMESTAMP


This will update the time at each instance you modify a row and is sometimes very helpful for stored last edit information. This only works with timestamp, not datetime however.
    +---------------------------------------------------------------------------------------+--------------------------------------------------------------------------+
|                                       TIMESTAMP                                       |                                 DATETIME                                 |
+---------------------------------------------------------------------------------------+--------------------------------------------------------------------------+
| TIMESTAMP requires 4 bytes.                                                           | DATETIME requires 8 bytes.                                               |
| Timestamp is the number of seconds that have elapsed since January 1, 1970 00:00 UTC. | DATETIME is a text displays 'YYYY-MM-DD HH:MM:SS' format.                |
| TIMESTAMP supported range: 1970-01-01 00:00:01 UTC to 2038-01-19 03:14:07 UTC.    | DATETIME supported range: 1000-01-01 00:00:00 to 9999-12-31 23:59:59 |
| TIMESTAMP during retrieval converted back to the current time zone.                   | DATETIME can not do this.                                                |
| TIMESTAMP is used mostly for metadata i.e. row created/modified and audit purpose.    | DATETIME is used mostly for user-data.                                   |
+---------------------------------------------------------------------------------------+--------------------------------------------------------------------------+

    I would always use a Unix timestamp when working with MySQL and PHP. The main reason for this being the default date method in PHP uses a timestamp as the parameter, so there would be no parsing needed.
To get the current Unix timestamp in PHP, just do time();
and in MySQL do SELECT UNIX_TIMESTAMP();.
    In my case, I set UTC as a time zone for everything: the system, the database server, etc. every time that I can. If my customer requires another time zone, then I configure it on the app.

I almost always prefer timestamps rather than datetime fields, because timestamps include the timezone implicitly. So, since the moment that the app will be accessed from users from different time zones and you want them to see dates and times in their local timezone, this field type makes it pretty easy to do it than if the data were saved in datetime fields.

As a plus, in the case of a migration of the database to a system with another timezone, I would feel more confident using timestamps. Not to say possible issues when calculating differences between two moments with a sumer time change in between and needing a precision of 1 hour or less.

So, to summarize, I value this advantages of timestamp:


ready to use on international (multi time zone) apps
easy migrations between time zones
pretty easy to calculate diferences (just subtract both timestamps)
no worry about dates in/out a summer time period


For all this reasons, I choose UTC & timestamp fields where posible. And I avoid headaches ;)
    From my experiences, if you want a date field in which insertion happens only once and you don't want to have any update or any other action on that particular field, go with date time.

For example, consider a user table with a REGISTRATION DATE field. In that user table, if you want to know the last logged in time of a particular user, go with a field of timestamp type so that the field gets updated.

If you are creating the table from phpMyAdmin the default setting will update the timestamp field when a row update happens. If your timestamp filed is not updating with row update, you can use the following query to make a timestamp field get auto updated.

ALTER TABLE your_table
      MODIFY COLUMN ts_activity TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;

    Reference taken from this Article:

The main differences:

TIMESTAMP used to track changes to records, and update every time when the record is changed.
DATETIME used to store specific and static value which is not affected by any changes in records.

TIMESTAMP also affected by different TIME ZONE related setting.
DATETIME is constant.

TIMESTAMP internally converted current time zone to UTC for storage, and during retrieval converted back to the current time zone.
DATETIME can not do this.

TIMESTAMP supported range:
1970-01-01 00:00:01 UTC to 2038-01-19 03:14:07 UTC
DATETIME supported range:
1000-01-01 00:00:00 to 9999-12-31 23:59:59
    I stopped using datetime in my applications after facing many problems and bugs related to time zones. IMHO using timestamp is better than datetime in most of the cases.

When you ask what is the time ? and the answer comes as something like '2019-02-05 21:18:30', that is not completed, not defined answer because it lacks another part, in which timezone  ? Washington ? Moscow ? Beijing ?

Using datetimes without the timezone means that your application is dealing with only 1 timezone, however timestamps give you the benefits of datetime plus the flexibility of showing the same exact point of time in different timezones.

Here are some cases that will make you regret using datetime and wish that you stored your data in timestamps.


For your clients comfort you want to show them the times based on their preferred time zones without making them doing the math and convert the time to their meaningful timezone. all you need is to change the timezone and all your application code will be the same.(Actually you should always define the timezone at the start of the application, or request processing in case of PHP applications)

SET time_zone = '+2:00';

you changed the country you stay in, and continue your work of maintaining the data while seeing it in a different timezone (without changing the actual data).
you accept data from different clients around the world, each of them inserts the time in his timezone. 


In short

datetime = application supports 1 timezone (for both inserting and selecting)

timestamp = application supports any timezone (for both inserting and selecting)



This answer is only for putting some highlight on the flexibility and ease of timestamps when it comes to time zones , it is not covering any other differences like the column size or range or fraction.
    A DATETIME carries no timezone information with it and will always display the same independent of the timezone that is in effect for the session, which defaults to the server's timezone unless you have explicitly changed it. However, if I initialize a DATETIME column with a function such as NOW() rather than a literal such as '2020-01-16 12:15:00', then the value stored will, of course, be the current date and time localized to the session's timezone.

A TIMESTAMP by contrast does implicitly carry timezone information: When you initialize a  TIMESTAMP column with a value, that value is converted to UTC before it is stored. If the value being stored is a literal such as '2020-01-16 12:15:00', it is interpreted as being in the session's current timezone for conversion purposes. Conversely, when a TIMESTAMP column is displayed, it will first be converted from UTC to the session's current timezone.

When to use one or the other? A Case Study

A Website for a community theater group is presenting several performances of a play for which it is selling tickets. The dates and times of these performances will appear in a drop down from which a customer wishing to buy tickets for a performance will select one. It would make sense for database column performance_date_and_time to be a DATETIME type. If the performance is in New York, there is an understanding that there is an implicit timezone involved (""New York local time"") and ideally we would want the date and time to display as 'December 12, 2019 at 8:00 PM' regardless of the session's timezone and without having to go to the trouble of having to do any timezone conversions.

On the other hand, once the December 12th, 2019 8 PM performance began, we might no longer want to sell tickets for it and thus no longer display that performance in the drop down. So, we would like to be able to know whether '2019-12-12 20:00:00' has occurred or not. That would argue for having a TIMESTAMP column, setting the timezone for the session to 'America/New_York' with set session time_zone='America/New_York' and then storing '2019-12-12 20:00:00' into the TIMESTAMP column. Henceforth we can test for whether the performance has begun by comparing this column with NOW() independent of the current session timezone.

Or it might make sense to have a DATETIME and a TIMESTAMP column for these two separate purposes. Or not. Clearly, either one could serve both purposes. If you go with just a DATETIME column, then you must set the current timezone to your local timezone before comparing with NOW(). If you go with just a TIMESTAMP column, you must set the session timezone to your local timezone before displaying the column.
    Beware of timestamp changing when you do a UPDATE statement on a table.  If you have a table with columns 'Name' (varchar), 'Age' (int), and 'Date_Added' (timestamp) and you run the following DML statement

UPDATE table
SET age = 30


then every single value in your 'Date_Added' column would be changed to the current timestamp. 
    The timestamp data type stores date and time, but in UTC format, not in the current timezone format as datetime does. And when you fetch data, timestamp again converts that into the current timezone time.

So suppose you are in USA and getting data from a server which has a time zone of USA. Then you will get the date and time according to the USA time zone. The timestamp data type column always get updated automatically when its row gets updated. So it can be useful to track when a particular row was updated last time.

For more details you can read the blog post Timestamp Vs Datetime .
    I always use a Unix timestamp, simply to maintain sanity when dealing with a lot of datetime information, especially when performing adjustments for timezones, adding/subtracting dates, and the like. When comparing timestamps, this excludes the complicating factors of timezone and allows you to spare resources in your server side processing (Whether it be application code or database queries) in that you make use of light weight arithmetic rather then heavier date-time add/subtract functions. 

Another thing worth considering:

If you're building an application, you never know how your data might have to be used down the line. If you wind up having to, say, compare a bunch of records in your data set, with, say, a bunch of items from a third-party API, and say, put them in chronological order, you'll be happy to have Unix timestamps for your rows. Even if you decide to use MySQL timestamps, store a Unix timestamp as insurance.
    I found unsurpassed usefulness in TIMESTAMP's ability to auto update itself based on the current time without the use of unnecessary triggers. That's just me though, although TIMESTAMP is UTC like it was said.

It can keep track across different timezones, so if you need to display a relative time for instance, UTC time is what you would want.
    TIMESTAMP is useful when you have visitors from different countries with different time zones. you can easily convert the TIMESTAMP to any country time zone
    I prefer using timestamp so to keep everything in one common raw format and format the data in PHP code or in your SQL query. There are instances where it comes in handy in your code to keep everything in plain seconds.
    ","[2976, 2015, 1011, 590, 362, 46, 126, 133, 216, 135, 105, 50, 54, 35, 43, 12, 41, 29, 14, 27, 16, 20, 16, 10, 5, 15, 17, 16, 12, 6, 8]",1019296,798,2009-01-03T16:14:37,2022-04-29 15:04:39Z,
How can I know which radio button is selected via jQuery?,"
                
I have two radio buttons and want to post the value of the selected one.
How can I get the value with jQuery?

I can get all of them like this:

$(""form :radio"")


How do I know which one is selected?
    To get the value of the selected radioName item of a form with id myForm:

$('input[name=radioName]:checked', '#myForm').val()


Here's an example:

$('#myForm input').on('change', function() {
   alert($('input[name=radioName]:checked', '#myForm').val()); 
});<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<form id=""myForm"">
  <input type=""radio"" name=""radioName"" value=""1"" /> 1 <br />
  <input type=""radio"" name=""radioName"" value=""2"" /> 2 <br />
  <input type=""radio"" name=""radioName"" value=""3"" /> 3 <br />
</form>

    Use this..  

$(""#myform input[type='radio']:checked"").val();

    If you already have a reference to a radio button group, for example:

var myRadio = $(""input[name=myRadio]"");


Use the filter() function, not find(). (find() is for locating child/descendant elements, whereas filter() searches top-level elements in your selection.)

var checkedValue = myRadio.filter("":checked"").val();




Notes: This answer was originally correcting another answer that recommended using find(), which seems to have since been changed. find() could still be useful for the situation where you already had a reference to a container element, but not to the radio buttons, e.g.:

var form = $(""#mainForm"");
...
var checkedValue = form.find(""input[name=myRadio]:checked"").val();

    If you want just the boolean value, i.e. if it's checked or not try this:

$(""#Myradio"").is("":checked"")

    This should work: 

$(""input[name='radioName']:checked"").val()


Note the """" usaged around the input:checked and not '' like the Peter J's solution
    try this one.
it worked for me
$('input[type=""radio""][name=""name""]:checked').val();

    You can use the :checked selector along with the radio selector.

 $(""form:radio:checked"").val();

    Another option is:

$('input[name=radioName]:checked').val()

    Get all radios:

var radios = jQuery(""input[type='radio']"");


Filter to get the one thats checked

radios.filter("":checked"")

    In my case I have two radio buttons in one form and I wanted to know the status of each button.
This below worked for me:

// get radio buttons value
console.log( ""radio1: "" +  $('input[id=radio1]:checked', '#toggle-form').val() );
console.log( ""radio2: "" +  $('input[id=radio2]:checked', '#toggle-form').val() );


    <script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<form id=""toggle-form"">
  <div id=""radio"">
    <input type=""radio"" id=""radio1"" name=""radio"" checked=""checked"" /><label for=""radio1"">Plot single</label>
    <input type=""radio"" id=""radio2"" name=""radio""/><label for=""radio2"">Plot all</label>
  </div>
</form>

    $(""input:radio:checked"").val();

    Here's how I would write the form and handle the getting of the checked radio.

Using a form called myForm:

<form id='myForm'>
    <input type='radio' name='radio1' class='radio1' value='val1' />
    <input type='radio' name='radio1' class='radio1' value='val2' />
    ...
</form>


Get the value from the form:

$('#myForm .radio1:checked').val();


If you're not posting the form, I would simplify it further by using:

<input type='radio' class='radio1' value='val1' />
<input type='radio' class='radio1' value='val2' />


Then getting the checked value becomes:

    $('.radio1:checked').val();


Having a class name on the input allows me to easily style the inputs...
    Another way to get it:

 $(""#myForm input[type=radio]"").on(""change"",function(){
   if(this.checked) {
    alert(this.value);
    }
  });<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<form id=""myForm"">
   <span><input type=""radio"" name=""q12_3"" value=""1"">1</span><br>
   <span><input type=""radio"" name=""q12_3"" value=""2"">2</span>
</form>

    If you have Multiple radio buttons in single form then 

var myRadio1 = $('input[name=radioButtonName1]');
var value1 = myRadio1.filter(':checked').val();

var myRadio2 = $('input[name=radioButtonName2]');
var value2 = myRadio2.filter(':checked').val();


This is working for me.
    This works fine

$('input[type=""radio""][class=""className""]:checked').val()


Working Demo

The :checked selector works for checkboxes, radio buttons, and select elements. For select elements only, use the :selected selector.

API for :checked Selector
     $("".Stat"").click(function () {
     var rdbVal1 = $(""input[name$=S]:checked"").val();
 }

    JQuery to get all the radio buttons in the form and the checked value.

$.each($(""input[type='radio']"").filter("":checked""), function () {
  console.log(""Name:"" + this.name);
  console.log(""Value:"" + $(this).val());
});

    Also, check if the user does not select anything.

var radioanswer = 'none';
if ($('input[name=myRadio]:checked').val() != null) {           
   radioanswer = $('input[name=myRadio]:checked').val();
}

    In a JSF generated radio button (using <h:selectOneRadio> tag), you can do this:

radiobuttonvalue = jQuery(""input[name='form_id\:radiobutton_id']:checked"").val();


where selectOneRadio ID is radiobutton_id and form ID is form_id.

Be sure to use name instead id, as indicated, because jQuery uses this attribute (name is generated automatically by JSF resembling control ID).
    I use this simple script

$('input[name=""myRadio""]').on('change', function() {
  var radioValue = $('input[name=""myRadio""]:checked').val();        
  alert(radioValue); 
});

    Try

myForm.myOption.value


function check() {
  console.log( myForm.myOption.value );
}<form id=""myForm"">
  <input type=""radio"" name=""myOption"" value=""1""> 1 <br>
  <input type=""radio"" name=""myOption"" value=""2""> 2 <br>
  <input type=""radio"" name=""myOption"" value=""3""> 3 <br>
</form>
<button onclick=""check()"">check</button>

    I wrote a jQuery plugin for setting and getting radio-button values. It also respects the ""change"" event on them.

(function ($) {

    function changeRadioButton(element, value) {
        var name = $(element).attr(""name"");
        $(""[type=radio][name="" + name + ""]:checked"").removeAttr(""checked"");
        $(""[type=radio][name="" + name + ""][value="" + value + ""]"").attr(""checked"", ""checked"");
        $(""[type=radio][name="" + name + ""]:checked"").change();
    }

    function getRadioButton(element) {
        var name = $(element).attr(""name"");
        return $(""[type=radio][name="" + name + ""]:checked"").attr(""value"");
    }

    var originalVal = $.fn.val;
    $.fn.val = function(value) {

        //is it a radio button? treat it differently.
        if($(this).is(""[type=radio]"")) {

            if (typeof value != 'undefined') {

                //setter
                changeRadioButton(this, value);
                return $(this);

            } else {

                //getter
                return getRadioButton(this);

            }

        } else {

            //it wasn't a radio button - let's call the default val function.
            if (typeof value != 'undefined') {
                return originalVal.call(this, value);
            } else {
                return originalVal.call(this);
            }

        }
    };
})(jQuery);


Put the code anywhere to enable the addin. Then enjoy! It just overrides the default val function without breaking anything.

You can visit this jsFiddle to try it in action, and see how it works.

Fiddle
    DEMO : https://jsfiddle.net/ipsjolly/xygr065w/

	$(function(){
	    $(""#submit"").click(function(){      
	        alert($('input:radio:checked').val());
	    });
	 });<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<table>
       <tr>
         <td>Sales Promotion</td>
         <td><input type=""radio"" name=""q12_3"" value=""1"">1</td>
         <td><input type=""radio"" name=""q12_3"" value=""2"">2</td>
         <td><input type=""radio"" name=""q12_3"" value=""3"">3</td>
         <td><input type=""radio"" name=""q12_3"" value=""4"">4</td>
         <td><input type=""radio"" name=""q12_3"" value=""5"">5</td>
      </tr>
    </table>
<button id=""submit"">submit</button>

    This solution does not require jQuery.
const RADIO_NAME = ""radioName"";
const radios = Array.from(document.getElementsByName(RADIO_NAME));
const checkedRadio = radios.filter(e=>e.checked);

This uses jQuery:
const radios = Array.from($(`[name=${RADIO_NAME}`));
const checkedRadio = radios.filter(e=>e.checked);

jQuery adds an extra layer of abstraction that isn't needed here.
You could also use:
const radios = Array.from(document.querySelectorAll(`[name=${RADIO_NAME}`));
const checkedRadio = radios.filter(e=>e.checked)[0];

But getElementsByName is simple and clear enough.
    To get the value of the selected radio that uses a class:

$('.class:checked').val()

    try it-

var radioVal = $(""#myform"").find(""input[type='radio']:checked"").val();

console.log(radioVal);

    Use this:

value = $('input[name=button-name]:checked').val();

    $(function () {
// Someone has clicked one of the radio buttons
var myform= 'form.myform';
$(myform).click(function () {
    var radValue= """";
    $(this).find('input[type=radio]:checked').each(function () {
        radValue= $(this).val();
    });
  })
});

    You need access with the :checked selector:

Check this doc:


  
  https://api.jquery.com/checked-selector/
  


a example:

$('input[name=radioName]:checked', '#myForm').val()
$('#myForm input').on('change', function() {
	$('#val').text($('input[name=radioName]:checked', '#myForm').val());
});#val {
  color: #EB0054;
  font-size: 1.5em;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>

<h3>Radio value: <span id='val'><span></h3>
<form id=""myForm"">
  <input type=""radio"" name=""radioName"" value=""a""> a <br>
  <input type=""radio"" name=""radioName"" value=""b""> b <br>
  <input type=""radio"" name=""radioName"" value=""c""> c <br>
</form>

    How about this?

Using change and get the value of radio type is checked...

$('#my-radio-form').on('change', function() {
  console.log($('[type=""radio""]:checked').val());
});<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.0.0/jquery.min.js""></script>
<form id=""my-radio-form"">
  <input type=""radio"" name=""input-radio"" value=""a"" />a
  <input type=""radio"" name=""input-radio"" value=""b"" />b
  <input type=""radio"" name=""input-radio"" value=""c"" />c
  <input type=""radio"" name=""input-radio"" value=""d"" />d
</form>

    ","[2954, 4191, 449, 335, 75, 155, 13, 86, 55, 58, 27, 33, 26, 4, 17, 14, 14, 5, 17, 18, 11, 2, 16, 8, 1, 12, 4, 10, 2, 1, 1]",2395613,392,2009-02-27T19:53:27,2021-02-28 20:02:46Z,javascript html 
"How do you set, clear, and toggle a single bit?","
                
How do you set, clear, and toggle a bit?
    Setting a bit
Use the bitwise OR operator (|) to set a bit.
number |= 1UL << n;

That will set the nth bit of number. n should be zero, if you want to set the 1st bit and so on upto n-1, if you want to set the nth bit.
Use 1ULL if number is wider than unsigned long; promotion of 1UL << n doesn't happen until after evaluating 1UL << n where it's undefined behaviour to shift by more than the width of a long.  The same applies to all the rest of the examples.
Clearing a bit
Use the bitwise AND operator (&) to clear a bit.
number &= ~(1UL << n);

That will clear the nth bit of number. You must invert the bit string with the bitwise NOT operator (~), then AND it.
Toggling a bit
The XOR operator (^) can be used to toggle a bit.
number ^= 1UL << n;

That will toggle the nth bit of number.
Checking a bit
You didn't ask for this, but I might as well add it.
To check a bit, shift the number n to the right, then bitwise AND it:
bit = (number >> n) & 1U;

That will put the value of the nth bit of number into the variable bit.
Changing the nth bit to x
Setting the nth bit to either 1 or 0 can be achieved with the following on a 2's complement C++ implementation:
number ^= (-x ^ number) & (1UL << n);

Bit n will be set if x is 1, and cleared if x is 0.  If x has some other value, you get garbage.  x = !!x will booleanize it to 0 or 1.
To make this independent of 2's complement negation behaviour (where -1 has all bits set, unlike on a 1's complement or sign/magnitude C++ implementation), use unsigned negation.
number ^= (-(unsigned long)x ^ number) & (1UL << n);

or
unsigned long newbit = !!x;    // Also booleanize to force 0 or 1
number ^= (-newbit ^ number) & (1UL << n);

It's generally a good idea to use unsigned types for portable bit manipulation.
or
number = (number & ~(1UL << n)) | (x << n);

(number & ~(1UL << n)) will clear the nth bit and (x << n) will set the nth bit to x.
It's also generally a good idea to not to copy/paste code in general and so many people use preprocessor macros (like the community wiki answer further down) or some sort of encapsulation.
    Using the Standard C++ Library: std::bitset<N>.

Or the Boost version: boost::dynamic_bitset.

There is no need to roll your own:

#include <bitset>
#include <iostream>

int main()
{
    std::bitset<5> x;

    x[1] = 1;
    x[2] = 0;
    // Note x[0-4]  valid

    std::cout << x << std::endl;
}




[Alpha:] > ./a.out
00010


The Boost version allows a runtime sized bitset compared with a standard library compile-time sized bitset.
    I use macros defined in a header file to handle bit set and clear:
/* a=target variable, b=bit number to act upon 0-n */
#define BIT_SET(a,b) ((a) |= (1ULL<<(b)))
#define BIT_CLEAR(a,b) ((a) &= ~(1ULL<<(b)))
#define BIT_FLIP(a,b) ((a) ^= (1ULL<<(b)))
#define BIT_CHECK(a,b) (!!((a) & (1ULL<<(b))))        // '!!' to make sure this returns 0 or 1

#define BITMASK_SET(x, mask) ((x) |= (mask))
#define BITMASK_CLEAR(x, mask) ((x) &= (~(mask)))
#define BITMASK_FLIP(x, mask) ((x) ^= (mask))
#define BITMASK_CHECK_ALL(x, mask) (!(~(x) & (mask)))
#define BITMASK_CHECK_ANY(x, mask) ((x) & (mask))

    The other option is to use bit fields:

struct bits {
    unsigned int a:1;
    unsigned int b:1;
    unsigned int c:1;
};

struct bits mybits;


defines a 3-bit field (actually, it's three 1-bit felds). Bit operations now become a bit (haha) simpler:

To set or clear a bit:

mybits.b = 1;
mybits.c = 0;


To toggle a bit:

mybits.a = !mybits.a;
mybits.b = ~mybits.b;
mybits.c ^= 1;  /* all work */


Checking a bit:

if (mybits.c)  //if mybits.c is non zero the next line below will execute


This only works with fixed-size bit fields. Otherwise you have to resort to the bit-twiddling techniques described in previous posts.
    It is sometimes worth using an enum to name the bits:

enum ThingFlags = {
  ThingMask  = 0x0000,
  ThingFlag0 = 1 << 0,
  ThingFlag1 = 1 << 1,
  ThingError = 1 << 8,
}


Then use the names later on. I.e. write 

thingstate |= ThingFlag1;
thingstate &= ~ThingFlag0;
if (thing & ThingError) {...}


to set, clear and test. This way you hide the magic numbers from the rest of your code. 

Other than that I endorse Jeremy's solution.
    
  Let suppose few things first
  num = 55 Integer to perform bitwise operations (set, get, clear, toggle).
  n = 4 0 based bit position to perform bitwise operations.   


How to get a bit?


To get the nth bit of num right shift num, n times. Then perform bitwise AND & with 1.


bit = (num >> n) & 1;


How it works?

       0011 0111 (55 in decimal)
    >>         4 (right shift 4 times)
-----------------
       0000 0011
     & 0000 0001 (1 in decimal)
-----------------
    => 0000 0001 (final result)


How to set a bit?


To set a particular bit of number. Left shift 1 n times. Then perform bitwise OR | operation with num.


num |= (1 << n);    // Equivalent to; num = (1 << n) | num;


How it works?

       0000 0001 (1 in decimal)
    <<         4 (left shift 4 times)
-----------------
       0001 0000
     | 0011 0111 (55 in decimal)
-----------------
    => 0001 0000 (final result)


How to clear a bit?


Left shift 1, n times i.e. 1 << n.
Perform bitwise complement with the above result. So that the nth bit becomes unset and rest of bit becomes set i.e. ~ (1 << n).
Finally, perform bitwise AND & operation with the above result and num. The above three steps together can be written as num & (~ (1 << n));




num &= (~(1 << n));    // Equivalent to; num = num & (~(1 << n));


How it works?

       0000 0001 (1 in decimal)
    <<         4 (left shift 4 times)
-----------------
     ~ 0001 0000
-----------------
       1110 1111
     & 0011 0111 (55 in decimal)
-----------------
    => 0010 0111 (final result)


How to toggle a bit?

To toggle a bit we use bitwise XOR ^ operator. Bitwise XOR operator evaluates to 1 if corresponding bit of both operands are different, otherwise evaluates to 0.  

Which means to toggle a bit, we need to perform XOR operation with the bit you want to toggle and 1. 

num ^= (1 << n);    // Equivalent to; num = num ^ (1 << n);


How it works? 


If the bit to toggle is 0 then, 0 ^ 1 => 1.  
If the bit to toggle is 1 then, 1 ^ 1 => 0.


       0000 0001 (1 in decimal)
    <<         4 (left shift 4 times)
-----------------
       0001 0000
     ^ 0011 0111 (55 in decimal)
-----------------
    => 0010 0111 (final result)


Recommended reading - Bitwise operator exercises
    For the beginner I would like to explain a bit more with an example:

Example:

value is 0x55;
bitnum : 3rd.


The & operator is used check the bit:

0101 0101
&
0000 1000
___________
0000 0000 (mean 0: False). It will work fine if the third bit is 1 (then the answer will be True)


Toggle or Flip:

0101 0101
^
0000 1000
___________
0101 1101 (Flip the third bit without affecting other bits)


| operator: set the bit

0101 0101
|
0000 1000
___________
0101 1101 (set the third bit without affecting other bits)

    From snip-c.zip's bitops.h:

/*
**  Bit set, clear, and test operations
**
**  public domain snippet by Bob Stout
*/

typedef enum {ERROR = -1, FALSE, TRUE} LOGICAL;

#define BOOL(x) (!(!(x)))

#define BitSet(arg,posn) ((arg) | (1L << (posn)))
#define BitClr(arg,posn) ((arg) & ~(1L << (posn)))
#define BitTst(arg,posn) BOOL((arg) & (1L << (posn)))
#define BitFlp(arg,posn) ((arg) ^ (1L << (posn)))


OK, let's analyze things...

The common expression that you seem to be having problems with in all of these is ""(1L << (posn))"". All this does is create a mask with a single bit on
and which will work with any integer type. The ""posn"" argument specifies the
position where you want the bit. If posn==0, then this expression will
evaluate to:

0000 0000 0000 0000 0000 0000 0000 0001 binary.


If posn==8, it will evaluate to:

0000 0000 0000 0000 0000 0001 0000 0000 binary.


In other words, it simply creates a field of 0's with a 1 at the specified
position. The only tricky part is in the BitClr() macro where we need to set
a single 0 bit in a field of 1's. This is accomplished by using the 1's
complement of the same expression as denoted by the tilde (~) operator.

Once the mask is created it's applied to the argument just as you suggest,
by use of the bitwise and (&), or (|), and xor (^) operators. Since the mask
is of type long, the macros will work just as well on char's, short's, int's,
or long's.

The bottom line is that this is a general solution to an entire class of
problems. It is, of course, possible and even appropriate to rewrite the
equivalent of any of these macros with explicit mask values every time you
need one, but why do it? Remember, the macro substitution occurs in the
preprocessor and so the generated code will reflect the fact that the values
are considered constant by the compiler - i.e. it's just as efficient to use
the generalized macros as to ""reinvent the wheel"" every time you need to do
bit manipulation. 

Unconvinced? Here's some test code - I used Watcom C with full optimization
and without using _cdecl so the resulting disassembly would be as clean as
possible:

----[ TEST.C ]----------------------------------------------------------------

#define BOOL(x) (!(!(x)))

#define BitSet(arg,posn) ((arg) | (1L << (posn)))
#define BitClr(arg,posn) ((arg) & ~(1L << (posn)))
#define BitTst(arg,posn) BOOL((arg) & (1L << (posn)))
#define BitFlp(arg,posn) ((arg) ^ (1L << (posn)))

int bitmanip(int word)
{
      word = BitSet(word, 2);
      word = BitSet(word, 7);
      word = BitClr(word, 3);
      word = BitFlp(word, 9);
      return word;
}


----[ TEST.OUT (disassembled) ]-----------------------------------------------

Module: C:\BINK\tst.c
Group: 'DGROUP' CONST,CONST2,_DATA,_BSS

Segment: _TEXT  BYTE   00000008 bytes  
 0000  0c 84             bitmanip_       or      al,84H    ; set bits 2 and 7
 0002  80 f4 02                          xor     ah,02H    ; flip bit 9 of EAX (bit 1 of AH)
 0005  24 f7                             and     al,0f7H
 0007  c3                                ret     

No disassembly errors


----[ finis ]----------------------------------------------------------------- 
    As this is tagged ""embedded"" I'll assume you're using a microcontroller. All of the above suggestions are valid & work (read-modify-write, unions, structs, etc.).

However, during a bout of oscilloscope-based debugging I was amazed to find that these methods have a considerable overhead in CPU cycles compared to writing a value directly to the micro's PORTnSET / PORTnCLEAR registers which makes a real difference where there are tight loops / high-frequency ISR's toggling pins.

For those unfamiliar: In my example, the micro has a general pin-state register PORTn which reflects the output pins, so doing PORTn |= BIT_TO_SET results in a read-modify-write to that register. However, the PORTnSET / PORTnCLEAR registers take a '1' to mean ""please make this bit 1"" (SET) or ""please make this bit zero"" (CLEAR) and a '0' to mean ""leave the pin alone"". so, you end up with two port addresses depending whether you're setting or clearing the bit (not always convenient) but a much faster reaction and smaller assembled code.
    The bitfield approach has other advantages in the embedded arena. You can define a struct that maps directly onto the bits in a particular hardware register.

struct HwRegister {
    unsigned int errorFlag:1;  // one-bit flag field
    unsigned int Mode:3;       // three-bit mode field
    unsigned int StatusCode:4;  // four-bit status code
};

struct HwRegister CR3342_AReg;


You need to be aware of the bit packing order - I think it's MSB first, but this may be implementation-dependent. Also, verify how your compiler handlers fields crossing byte boundaries.

You can then read, write, test the individual values as before.
    Here's my favorite bit arithmetic macro, which works for any type of unsigned integer array from unsigned char up to size_t (which is the biggest type that should be efficient to work with):

#define BITOP(a,b,op) \
 ((a)[(size_t)(b)/(8*sizeof *(a))] op ((size_t)1<<((size_t)(b)%(8*sizeof *(a)))))


To set a bit:

BITOP(array, bit, |=);


To clear a bit:

BITOP(array, bit, &=~);


To toggle a bit:

BITOP(array, bit, ^=);


To test a bit:

if (BITOP(array, bit, &)) ...


etc.
    If you want to perform this all operation with C programming in the Linux kernel then I suggest to use standard APIs of the Linux kernel.

See https://www.kernel.org/doc/htmldocs/kernel-api/ch02s03.html

set_bit  Atomically set a bit in memory
clear_bit  Clears a bit in memory
change_bit  Toggle a bit in memory
test_and_set_bit  Set a bit and return its old value
test_and_clear_bit  Clear a bit and return its old value
test_and_change_bit  Change a bit and return its old value
test_bit  Determine whether a bit is set


Note: Here the whole operation happens in a single step. So these all are  guaranteed to be atomic even on SMP computers and are useful
to keep coherence across processors.
    This program is based out of @Jeremy's above solution. If someone wish to quickly play around.

public class BitwiseOperations {

    public static void main(String args[]) {

        setABit(0, 4); // set the 4th bit, 0000 -> 1000 [8]
        clearABit(16, 5); // clear the 5th bit, 10000 -> 00000 [0]
        toggleABit(8, 4); // toggle the 4th bit, 1000 -> 0000 [0]
        checkABit(8,4); // check the 4th bit 1000 -> true 
    }

    public static void setABit(int input, int n) {
        input = input | ( 1 << n-1);
        System.out.println(input);
    }


    public static void clearABit(int input, int n) {
        input = input & ~(1 << n-1);
        System.out.println(input);
    }

    public static void toggleABit(int input, int n) {
        input = input ^ (1 << n-1);
        System.out.println(input);
    }

    public static void checkABit(int input, int n) {
        boolean isSet = ((input >> n-1) & 1) == 1; 
        System.out.println(isSet);
    }
}


Output :
8
0
0
true

    Here is a routine in C to perform the basic bitwise operations:
#define INT_BIT (unsigned int) (sizeof(unsigned int) * 8U) //number of bits in unsigned int

int main(void)
{
    
    unsigned int k = 5; //k is the bit position; here it is the 5th bit from the LSb (0th bit)
    
    unsigned int regA = 0x00007C7C; //we perform bitwise operations on regA
    
    regA |= (1U << k);    //Set kth bit
    
    regA &= ~(1U << k);   //Clear kth bit
    
    regA ^= (1U << k);    //Toggle kth bit
    
    regA = (regA << k) | regA >> (INT_BIT - k); //Rotate left by k bits
    
    regA = (regA >> k) | regA << (INT_BIT - k); //Rotate right by k bits

    return 0;   
}


    Check a bit at an arbitrary location in a variable of arbitrary type:

#define bit_test(x, y)  ( ( ((const char*)&(x))[(y)>>3] & 0x80 >> ((y)&0x07)) >> (7-((y)&0x07) ) )


Sample usage:

int main(void)
{
    unsigned char arr[8] = { 0x01, 0x23, 0x45, 0x67, 0x89, 0xAB, 0xCD, 0xEF };

    for (int ix = 0; ix < 64; ++ix)
        printf(""bit %d is %d\n"", ix, bit_test(arr, ix));

    return 0;
}


Notes:
This is designed to be fast (given its flexibility) and non-branchy.  It results in efficient SPARC machine code when compiled Sun Studio 8; I've also tested it using MSVC++ 2008 on amd64.  It's possible to make similar macros for setting and clearing bits.  The key difference of this solution compared with many others here is that it works for any location in pretty much any type of variable.
    Expanding on the bitset answer:

#include <iostream>
#include <bitset>
#include <string>

using namespace std;
int main() {
  bitset<8> byte(std::string(""10010011"");

  // Set Bit
  byte.set(3); // 10010111

  // Clear Bit
  byte.reset(2); // 10010101

  // Toggle Bit
  byte.flip(7); // 00010101

  cout << byte << endl;

  return 0;
}

    More general, for arbitrary sized bitmaps:

#define BITS 8
#define BIT_SET(  p, n) (p[(n)/BITS] |=  (0x80>>((n)%BITS)))
#define BIT_CLEAR(p, n) (p[(n)/BITS] &= ~(0x80>>((n)%BITS)))
#define BIT_ISSET(p, n) (p[(n)/BITS] &   (0x80>>((n)%BITS)))

    This program is to change any data bit from 0 to 1 or 1 to 0:

{
    unsigned int data = 0x000000F0;
    int bitpos = 4;
    int bitvalue = 1;
    unsigned int bit = data;
    bit = (bit>>bitpos)&0x00000001;
    int invbitvalue = 0x00000001&(~bitvalue);
    printf(""%x\n"",bit);

    if (bitvalue == 0)
    {
        if (bit == 0)
            printf(""%x\n"", data);
        else
        {
             data = (data^(invbitvalue<<bitpos));
             printf(""%x\n"", data);
        }
    }
    else
    {
        if (bit == 1)
            printf(""elseif %x\n"", data);
        else
        {
            data = (data|(bitvalue<<bitpos));
            printf(""else %x\n"", data);
        }
    }
}

    int set_nth_bit(int num, int n){    
    return (num | 1 << n);
}

int clear_nth_bit(int num, int n){    
    return (num & ~( 1 << n));
}

int toggle_nth_bit(int num, int n){    
    return num ^ (1 << n);
}

int check_nth_bit(int num, int n){    
    return num & (1 << n);
}

    Use this:

int ToggleNthBit ( unsigned char n, int num )
{
    if(num & (1 << n))
        num &= ~(1 << n);
    else
        num |= (1 << n);

    return num;
}

    Visual C 2010, and perhaps many other compilers, have direct support for boolean operations built in. A bit has two possible values, just like a boolean, so we can use booleans instead - even if they take up more space than a single bit in memory in this representation. This works, even the sizeof() operator works properly. 

bool    IsGph[256], IsNotGph[256];

//  Initialize boolean array to detect printable characters
for(i=0; i<sizeof(IsGph); i++)  {
    IsGph[i] = isgraph((unsigned char)i);
}


So, to your question, IsGph[i] =1, or IsGph[i] =0 make setting and clearing bools easy.

To find unprintable characters:

//  Initialize boolean array to detect UN-printable characters, 
//  then call function to toggle required bits true, while initializing a 2nd
//  boolean array as the complement of the 1st.
for(i=0; i<sizeof(IsGph); i++)  {
    if(IsGph[i])    {
         IsNotGph[i] = 0;
    }   else   {
         IsNotGph[i] = 1;
    }
}


Note there is nothing ""special"" about this code. It treats a bit like an integer - which technically, it is. A 1 bit integer that can hold 2 values, and 2 values only.

I once used this approach to find duplicate loan records, where loan_number was the ISAM key, using the 6-digit loan number as an index into the bit array. Savagely fast, and after 8 months, proved that the mainframe system we were getting the data from was in fact malfunctioning. The simplicity of bit arrays makes confidence in their correctness very high - vs a searching approach for example.
    If you're doing a lot of bit twiddling you might want to use masks which will make the whole thing quicker. The following functions are very fast and are still flexible (they allow bit twiddling in bit maps of any size).

const unsigned char TQuickByteMask[8] =
{
   0x01, 0x02, 0x04, 0x08,
   0x10, 0x20, 0x40, 0x80,
};


/** Set bit in any sized bit mask.
 *
 * @return    none
 *
 * @param     bit    - Bit number.
 * @param     bitmap - Pointer to bitmap.
 */
void TSetBit( short bit, unsigned char *bitmap)
{
    short n, x;

    x = bit / 8;        // Index to byte.
    n = bit % 8;        // Specific bit in byte.

    bitmap[x] |= TQuickByteMask[n];        // Set bit.
}


/** Reset bit in any sized mask.
 *
 * @return  None
 *
 * @param   bit    - Bit number.
 * @param   bitmap - Pointer to bitmap.
 */
void TResetBit( short bit, unsigned char *bitmap)
{
    short n, x;

    x = bit / 8;        // Index to byte.
    n = bit % 8;        // Specific bit in byte.

    bitmap[x] &= (~TQuickByteMask[n]);    // Reset bit.
}


/** Toggle bit in any sized bit mask.
 *
 * @return   none
 *
 * @param   bit    - Bit number.
 * @param   bitmap - Pointer to bitmap.
 */
void TToggleBit( short bit, unsigned char *bitmap)
{
    short n, x;

    x = bit / 8;        // Index to byte.
    n = bit % 8;        // Specific bit in byte.

    bitmap[x] ^= TQuickByteMask[n];        // Toggle bit.
}


/** Checks specified bit.
 *
 * @return  1 if bit set else 0.
 *
 * @param   bit    - Bit number.
 * @param   bitmap - Pointer to bitmap.
 */
short TIsBitSet( short bit, const unsigned char *bitmap)
{
    short n, x;

    x = bit / 8;    // Index to byte.
    n = bit % 8;    // Specific bit in byte.

    // Test bit (logigal AND).
    if (bitmap[x] & TQuickByteMask[n])
        return 1;

    return 0;
}


/** Checks specified bit.
 *
 * @return  1 if bit reset else 0.
 *
 * @param   bit    - Bit number.
 * @param   bitmap - Pointer to bitmap.
 */
short TIsBitReset( short bit, const unsigned char *bitmap)
{
    return TIsBitSet(bit, bitmap) ^ 1;
}


/** Count number of bits set in a bitmap.
 *
 * @return   Number of bits set.
 *
 * @param    bitmap - Pointer to bitmap.
 * @param    size   - Bitmap size (in bits).
 *
 * @note    Not very efficient in terms of execution speed. If you are doing
 *        some computationally intense stuff you may need a more complex
 *        implementation which would be faster (especially for big bitmaps).
 *        See (http://graphics.stanford.edu/~seander/bithacks.html).
 */
int TCountBits( const unsigned char *bitmap, int size)
{
    int i, count = 0;

    for (i=0; i<size; i++)
        if (TIsBitSet(i, bitmap))
            count++;

    return count;
}


Note, to set bit 'n' in a 16 bit integer you do the following:

TSetBit( n, &my_int);


It's up to you to ensure that the bit number is within the range of the bit map that you pass. Note that for little endian processors that bytes, words, dwords, qwords, etc., map correctly to each other in memory (main reason that little endian processors are 'better' than big-endian processors, ah, I feel a flame war coming on...).
    
  How do you set, clear, and toggle a single bit?


To address a common coding pitfall when attempting to form the mask:
1 is not always wide enough

What problems happen when number is a wider type than 1?
x may be too great for the shift 1 << x leading to undefined behavior (UB).  Even if x is not too great, ~ may not flip enough most-significant-bits.

// assume 32 bit int/unsigned
unsigned long long number = foo();

unsigned x = 40; 
number |= (1 << x);  // UB
number ^= (1 << x);  // UB
number &= ~(1 << x); // UB

x = 10;
number &= ~(1 << x); // Wrong mask, not wide enough




To insure 1 is wide enough:

Code could use 1ull or pedantically (uintmax_t)1 and let the compiler optimize.

number |= (1ull << x);
number |= ((uintmax_t)1 << x);


Or cast - which makes for coding/review/maintenance issues keeping the cast correct and up-to-date.

number |= (type_of_number)1 << x;


Or gently promote the 1 by forcing a math operation that is as least as wide as the type of number.

number |= (number*0 + 1) << x;




As with most bit manipulations, best to work with unsigned types rather than signed ones
    A templated version (put in a header file) with support for changing multiple bits (works on AVR microcontrollers btw):
namespace bit {
  template <typename T1, typename T2>
  constexpr inline T1 bitmask(T2 bit) 
  {return (T1)1 << bit;}
  template <typename T1, typename T3, typename ...T2>
  constexpr inline T1 bitmask(T3 bit, T2 ...bits) 
  {return ((T1)1 << bit) | bitmask<T1>(bits...);}

  /** Set these bits (others retain their state) */
  template <typename T1, typename ...T2>
  constexpr inline void set (T1 &variable, T2 ...bits) 
  {variable |= bitmask<T1>(bits...);}
  /** Set only these bits (others will be cleared) */
  template <typename T1, typename ...T2>
  constexpr inline void setOnly (T1 &variable, T2 ...bits) 
  {variable = bitmask<T1>(bits...);}
  /** Clear these bits (others retain their state) */
  template <typename T1, typename ...T2>
  constexpr inline void clear (T1 &variable, T2 ...bits) 
  {variable &= ~bitmask<T1>(bits...);}
  /** Flip these bits (others retain their state) */
  template <typename T1, typename ...T2>
  constexpr inline void flip (T1 &variable, T2 ...bits) 
  {variable ^= bitmask<T1>(bits...);}
  /** Check if any of these bits are set */
  template <typename T1, typename ...T2>
  constexpr inline bool isAnySet(const T1 &variable, T2 ...bits) 
  {return variable & bitmask<T1>(bits...);}
  /** Check if all these bits are set */
  template <typename T1, typename ...T2>
  constexpr inline bool isSet (const T1 &variable, T2 ...bits) 
  {return ((variable & bitmask<T1>(bits...)) == bitmask<T1>(bits...));}
  /** Check if all these bits are not set */
  template <typename T1, typename ...T2>
  constexpr inline bool isNotSet (const T1 &variable, T2 ...bits) 
  {return ((variable & bitmask<T1>(bits...)) != bitmask<T1>(bits...));}
}

Example of use:
#include <iostream>
#include <bitset> // for console output of binary values

// and include the code above of course

using namespace std;

int main() {
  uint8_t v = 0b1111'1100;
  bit::set(v, 0);
  cout << bitset<8>(v) << endl;

  bit::clear(v, 0,1);
  cout << bitset<8>(v) << endl;

  bit::flip(v, 0,1);
  cout << bitset<8>(v) << endl;

  bit::clear(v, 0,1,2,3,4,5,6,7);
  cout << bitset<8>(v) << endl;

  bit::flip(v, 0,7);
  cout << bitset<8>(v) << endl;
}

BTW: It turns out that constexpr and inline is not used if not sending the optimizer argument (e.g.: -O3) to the compiler. Feel free to try the code at https://godbolt.org/ and look at the ASM output.
    Here are some macros I use:

SET_FLAG(Status, Flag)            ((Status) |= (Flag))
CLEAR_FLAG(Status, Flag)          ((Status) &= ~(Flag))
INVALID_FLAGS(ulFlags, ulAllowed) ((ulFlags) & ~(ulAllowed))
TEST_FLAGS(t,ulMask, ulBit)       (((t)&(ulMask)) == (ulBit))
IS_FLAG_SET(t,ulMask)             TEST_FLAGS(t,ulMask,ulMask)
IS_FLAG_CLEAR(t,ulMask)           TEST_FLAGS(t,ulMask,0)

    Try one of these functions in the C language to change n bit:

char bitfield;

// Start at 0th position

void chang_n_bit(int n, int value)
{
    bitfield = (bitfield | (1 << n)) & (~( (1 << n) ^ (value << n) ));
}


Or

void chang_n_bit(int n, int value)
{
    bitfield = (bitfield | (1 << n)) & ((value << n) | ((~0) ^ (1 << n)));
}


Or

void chang_n_bit(int n, int value)
{
    if(value)
        bitfield |= 1 << n;
    else
        bitfield &= ~0 ^ (1 << n);
}

char get_n_bit(int n)
{
    return (bitfield & (1 << n)) ? 1 : 0;
}

    Setting the nth bit to x (bit value) without using -1
Sometimes when you are not sure what -1 or the like will result in, you may wish to set the nth bit without using -1:
number = (((number | (1 << n)) ^ (1 << n))) | (x << n);

Explanation: ((number | (1 << n) sets the nth bit to 1 (where | denotes bitwise OR), then with (...) ^ (1 << n) we set the nth bit to 0, and finally with (...) | x << n) we set the nth bit that was 0, to (bit value) x.
This also works in golang.
    ","[2951, 4105, 523, 230, 273, 133, 22, 40, 53, 28, 26, 28, 12, 4, 2, 21, 12, 21, 15, 7, 13, 11, 13, 4, 3, 5, -2, 0]",1469184,1762,2008-09-07T00:42:17,2021-08-12 20:06:22Z,c c 
What does cherry-picking a commit with Git mean?,"
                
Recently, I have been asked to cherry-pick a commit. 

So what does cherry-picking a commit in git mean? How do you do it? 
    Cherry picking in Git means to choose a commit from one branch and apply it onto another.
This is in contrast with other ways such as merge and rebase which normally apply many commits onto another branch.

Make sure you are on the branch you want to apply the commit to.
 git switch master


Execute the following:
 git cherry-pick <commit-hash>



N.B.:

If you cherry-pick from a public branch, you should consider using
 git cherry-pick -x <commit-hash>

This will generate a standardized commit message. This way, you (and your co-workers) can still keep track of the origin of the commit and may avoid merge conflicts in the future.

If you have notes attached to the commit they do not follow the cherry-pick. To bring them over as well, You have to use:
 git notes copy <from> <to>



Additional links:

git official guide page

    This quote is taken from: Version Control with Git

Using git cherry-pick The command  git cherry-pick commit applies the
changes introduced by the named commit on the current branch. It will
introduce a new, distinct commit. Strictly speaking, using  git
cherry-pick doesnt alter the existing history within a repository;
instead, it adds to the history. As with other Git operations that
introduce changes via the process of applying a diff, you may need to
resolve conflicts to fully apply the changes from the given  commit .
The command  git cherry-pick is typically used to introduce particular
commits from one branch within a repository onto a different branch. A
common use is to forward- or back-port commits from a maintenance
branch to a development branch.

$ git checkout rel_2.3
$ git cherry-pick dev~2 # commit F, below

before:

after:

Also, here is a very nice in action video tutorial about it: Youtube: Introduction to Git cherry-pick
    I prepared step-by-step illustrations what cherry-pick does  and an animation of these illustrations (near the end).


Before cherry-picking
(we are going to do a cherry-pick of the commit L from the branch feature):






Starting the command git cherry-pick feature~2
(feature~2 is the 2nd commit before
feature, i.e. the commit L):






After performing the command (git cherry-pick feature~2):





The same animated:




Note:

The commit L' is from the user's point of view (commit = snapshot) the exact copy of the commit L.  

Technically (internally), it's a new, different commit (because e.g. L contains a pointer to K (as its parent), while L' contains a pointer to E). 
    Cherry picking in Git is designed to apply some commit from one branch into another branch. It can be done if you eg. made a mistake and committed a change into wrong branch, but do not want to merge the whole branch. You can just eg. revert the commit and cherry-pick it on another branch.

To use it, you just need git cherry-pick hash, where hash is a commit hash from other branch.

For full procedure see: http://technosophos.com/2009/12/04/git-cherry-picking-move-small-code-patches-across-branches.html
    Short example of situation, when you need cherry pick


  Consider following scenario. You have two branches.
  
  a) release1 - This branch is going to your customer, but there are
  still some bugs to be fixed.
  
  b) master - Classic master branch, where you can for example add
  functionality for release2.


NOW: You fix something in release1. Of course you need this fix also in master. And that is a typical use-case for cherry picking. So cherry pick in this scenario means that you take a commit from release1 branch and include it into the master branch.
    cherry-pick is a Git feature. If someone wants to Commit specific commits in one branch to a target branch, then cherry-pick is used.
        git cherry-pick 
steps are as below.


checkout (switch to) target branch.
git cherry-pick <commit id>


Here commit id is activity id of another branch.Eg.

git cherry-pick 9772dd546a3609b06f84b680340fb84c5463264f

push to target branch


Visit https://git-scm.com/docs/git-cherry-pick
    It will apply a particular commit to your current branch.

This means :


all files added by this commit will be added
all files deleted by this commit will be deleted
all files modified by this commit will be merged. This means the whole file from the commit, not only the changes from this commit!


Ex: Consider commit A

added newFileA
modified main:
+ import './newFileA'


commit B

added newFileB
modified main:
+ import './newFileB'


If you cherry-pick commit B on another branch, you'll end up with :

/newFileB
/main :
   import './newFileA'
   import './newFileB'


since commit B contains newFileB and main, but no newFileA, resulting in a bug, so use with caution.
    You can think if a cherry pick as similar to a rebase, or rather it's managed like a rebase. By this, I mean that it takes an existing commit and regenerates it taking, as the starting point, the head of the branch you're currently on.

A rebase takes a commit that had a parent X and regenerates the commit as if it actually had a parent Y, and this is precisely what a cherry-pick does.

Cherry pick is more about how you select the commits. With pull (rebase), git implicitly regenerates your local commits on top of what's pulled to your branch, but with cherry-pick you explicitly choose some commit(s), and implicitly regenerate it (them) on top of your current branch.

So the way you do it differs, but under the hood they are very similar operations - the regeneration of commits.
    It's kind of like Copy (from somewhere) and Paste (to somewhere), but for specific commits.  

If you want to do a hot fix, for example, then you can use the cherry-pick feature.  

Do your cherry-pick in a development branch, and merge that commit to a release branch. Likewise, do a cherry-pick from a release branch to master.  Voila
    If you want to merge without commit ids you can use this command

git cherry-pick master~2 master~0


The above command will merge last three commits of master from 1 to 3

If you want to do this for single commit just remove last option

git cherry-pick master~2


This way you will merge 3rd commit from the end of master.
    When you are working with a team of developers on a project, managing the changes between a number of git branches can become a complex task. Sometimes you don't want to merge a whole branch into another, and only need to pick one or two specific commits. This process is called 'cherry picking'.

Found a great article on cherry picking, check it out for in-depth details: https://www.previousnext.com.au/blog/intro-cherry-picking-git
    Excerpt from the official docs:


  Given one or more existing commits, apply the change each one
  introduces, recording a new commit for each. This requires your
  working tree to be clean (no modifications from the HEAD commit).
  
  When it is not obvious how to apply a change, the following happens:
  
  
  The current branch and HEAD pointer stay at the last commit
  successfully made.
  The CHERRY_PICK_HEAD ref is set to point at the commit that introduced
  the change that is difficult to apply.
  Paths in which the change applied cleanly are updated both in the
  index file and in your working tree.
  For conflicting paths, the index file records up to three versions, as
  described in the ""TRUE MERGE"" section of git-merge. The working
  tree files will include a description of the conflict bracketed by the
  usual conflict markers <<<<<<< and >>>>>>>.
  
  
  No other modifications are made.


Read more...
    ","[2937, 3563, 415, 107, 186, 115, 67, 20, 26, 16, 9, 12, 1]",1680837,530,2012-02-18T07:20:47,2021-10-18 03:20:04Z,
Git is not working after macOS Update (xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools),"
                
I updated to macOS Mojave (this happens on Catalina update too, and seems to potentially occur on every major update thereafter)
This morning I navigated to my work's codebase in the Command Line on my MacBook pro, typed in ""git status"" in the repository and received the error:

xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun

How do I fix git, and command line tools?
    The problem is that Xcode Command-line Tools needs to be updated.
Solution #1
Go back to your terminal and enter:
xcode-select --install
You'll then receive the following output:
xcode-select: note: install requested for command line developer tools
You will then be prompted in a window to update Xcode Command Line tools. (which may take a while)
Open a new terminal window and your development tools should be returned.
Addition: With any major or semi-major update you'll need to update the command line tools in order to get them functioning properly again. Check Xcode with any update. This goes beyond Mojave...
After that restart your terminal
Alternatively, IF that fails, and it very well might.... you'll get a pop-up box saying ""Software not found on server"", see below!
Solution #2
and you hit xcode-select --install and it doesn't find the software, log into Apple Developer, and install it via webpage.
Log in or sign up here:
https://developer.apple.com/download/more/
Look for: ""Command Line Tools for Xcode 12.x"" in the list of downloads
Then click the dmg and download.

    I got some errors that the software was unavailable from the update server when trying 

xcode-select --install


What fixed it for me was going here https://developer.apple.com/download/more/ and downloading Command Line Tools (macOS 10.14) for Xcode 10 and then installing it manually.

After that, the errors should be gone when you open up a new terminal.
    For me xcode-select --reset was the solution on Mojave.
    updated from Mojave to Big Sur and got the same error :
the command
xcode-select --install

worked like a charm
    If you use xcode then install it (~12GB)
xcode-select --install

Otherwise install latest command line tools (~500MB)

    In addition to dustbuster's answer I needed to set path to the Xcode folder with this command:

sudo xcode-select -switch /Library/Developer/CommandLineTools

    After upgrade to Mac Catalina I faced the same issue, I had to run couple of commands to get this fixed. 

First started with: 

xcode-select --install 

It didn't fix the problem, had to run the following in sudo 

sudo xcode-select --reset 

Then, finally got fixed after I switched and set the path explicitly for active developer directory: 

sudo xcode-select -s /Library/Developer/CommandLineTools

Note: In case you have Xcode installed, you may need to specify Xcode directory in this case, it should be something like this

xcode-select -s /Applications/Xcode.app
    Mac OS : Big Sur
First Priority
sudo xcode-select --reset

sudo xcodebuild -license

Second Priority
xcode-select --install

    
Run this command:

xcode-select --install

Hit return for a progress indicator on the Command Line Tools download.

After installation of the Command Line Tools has been completed, your Mac should be rebooted. If youre getting xcrun error invalid active developer path while working in Terminal, refresh the application or relaunch it.


Even after following the above-mentioned steps, if you see the error: invalid active developer path (/Library/Developer/CommandLineTools). The next step would be to try and install Command Line Tools using a DMG file that can be downloaded directly from the Apple website.
Again, if you are using Homebrew, you need to update it. You dont need to uninstall and again install Homebrew on Mac.
NOTE: If you are using Homebrew, try updating it after re-installing Command Line tools.
Credits: Git not working after macOS Update
    Following worked on M1
ProductName:    macOS
ProductVersion: 11.2.1
BuildVersion:   20D74

% xcode-select --install

Agree the Terms and Conditions prompt, it will return following message on success.
% xcode-select: note: install requested for command line developer tools

    For me what worked is the following:

sudo xcode-select --reset


Then like in @High6's answer:

sudo xcodebuild -license


This will reveal a license which I assume is some Xcode license. Scroll to the bottom using space (or the mouse) then tap agree. 

This is what worked for me on MacOS Mojave v 10.14.
    in my case it wasn't checked in xcode 
After installation process , 

you can do that as following :
xcode -> Preferences and tap Locations then select  , as the followng  image 


    I've used xcode-select --install given in the accepted answer in previous major releases.

I've just upgraded to OS X 10.15 Catalina and run the Software Update tool from preferences again after the OS upgrade completed. The Xcode utilities update was available there, which also sorted the issue using git which had just output 
xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools)
    If you have Xcode downloaded manually (i.e. not from the App Store) or don't have Xcode at all:


sudo rm -rf /Library/Developer/CommandLineTools
Go to https://developer.apple.com/download/more/ to download Command Line Tools (macOS 10.14) for Xcode 10
Setup Command Line Tools 


If you have Xcode installed from the App Store:


xcode-select --install

    I figured out the Xcode Command Line Tools part from the error message, but after running Xcode and getting the prompt to install the additional tools it did claim to install them, but still I got the same error after opening a new terminal.

So I did the xcode-select --install manually and after that it worked for me.
    Open Terminal:

install XCode developer tools and fix the problem.

$ xcode-select --install


Reset the path to Xcode if you have several versions:

$ xcode-select --switch /Applications/Xcode.app
$ xcode-select --switch /Library/Developer/CommandLineTools

    I observed in the Catalina privacy setting if Xcode not added in Full access disk I will get the same error, Xcode does not run scripts. Add your Xcode the same as in the attached image. After that clean build and run. Hope so it will work.

    I found that my version of Xcode was too outdated and installing command-line-tools wasn't helping. Here's what I did:


I completely uninstalled the outdated XCode
I reinstalled the most recent XCode from the app store
That was all. Git was restored.

    I updated my macOS yesterdayfrom macOS Mojave10.14.6to macOS Catalina10.15.7I was executing ""git"" command in my project. I get same errors
run
xcode-select --install

    This works for me 

sudo xcode-select --reset 
sudo xcodebuild -license


X-code must be installed.  
    For me, I didn't have xcode installed (on Mojave OS). I went to the App Store on my mac and downloaded it, then went back to terminal and typed git and hit enter, then it worked.
    I had the same issue and couldn't use SVN after the update,

Just in case if doing  xcode-select --install didn't fix the issue,

You might see,


  svn: error: The subversion command line tools are no longer provided by Xcode.


Refer : https://developer.apple.com/documentation/macos_release_notes/macos_catalina_10_15_release_notes

Try installing the svn by brew

brew install svn


This should get you going.
    For those using Catalina and Xcode-beta:

sudo xcode-select -s /Applications/Xcode-beta.app/Contents/Developer

    If you created a new Applications folder in an external drive and installed Xcode there:
sudo xcode-select --switch /Volumes/MyExternalStorageName/Applications/Xcode.app/Contents/Developer

    Edge case, but still worth writing down: when migrating from and older Mac with Migration Assistant, you may have selected the option to transfer all your applications from your older Mac. Applications, like Xcode needs to be updated if it was transferred and preserved OR delete from your other user's Applications folder. This is because Xcode has not been set up properly after the first start with the new OS version.
    For me It happened after Mac OS update to Mojave and git was not functioning in Intellij 

Solution:-
Go to Settings, then File | Settings | Version Control | Git and edit Path to Git executable field which is /usr/local/bin/git
    After Updating macOS to Monterey (12.3) from BigSur getting such issue
only worked-

xcode-select --install

Done!
    ","[2925, 4528, 217, 156, 78, 40, 88, 52, 39, 23, 33, 53, 46, 48, 31, 36, 28, 7, 7, 3, 7, 7, 3, 4, 1, 1, 3, 0]",1086699,358,2018-09-26T16:43:18,2022-04-27 10:38:38Z,
"Why is printing ""B"" dramatically slower than printing ""#""?","
                
I generated two matrices of 1000 x 1000:

First Matrix: O and #.
Second Matrix: O and B.

Using the following code, the first matrix took 8.52 seconds to complete:

Random r = new Random();
for (int i = 0; i < 1000; i++) {
    for (int j = 0; j < 1000; j++) {
        if(r.nextInt(4) == 0) {
            System.out.print(""O"");
        } else {
            System.out.print(""#"");
        }
    }

   System.out.println("""");
 }


With this code, the second matrix took 259.152 seconds to complete:

Random r = new Random();
for (int i = 0; i < 1000; i++) {
    for (int j = 0; j < 1000; j++) {
        if(r.nextInt(4) == 0) {
            System.out.print(""O"");
        } else {
            System.out.print(""B""); //only line changed
        }
    }

    System.out.println("""");
}


What is the reason behind the dramatically different run times?



As suggested in the comments, printing only System.out.print(""#""); takes 7.8871 seconds, whereas System.out.print(""B""); gives still printing....

As others who pointed out that it works for them normally, I tried Ideone.com for instance, and both pieces of code execute at the same speed.

Test Conditions:


I ran this test from Netbeans 7.2, with the output into its console
I used System.nanoTime() for measurements

    Pure speculation is that you're using a terminal that attempts to do word-wrapping rather than character-wrapping, and treats B as a word character but # as a non-word character. So when it reaches the end of a line and searches for a place to break the line, it sees a # almost immediately and happily breaks there; whereas with the B, it has to keep searching for longer, and may have more text to wrap (which may be expensive on some terminals, e.g., outputting backspaces, then outputting spaces to overwrite the letters being wrapped).

But that's pure speculation.
    I performed tests on Eclipse vs Netbeans 8.0.2, both with Java version 1.8;
I used System.nanoTime() for measurements.
Eclipse:
I got the same time on both cases - around 1.564 seconds.
Netbeans:

Using ""#"": 1.536 seconds
Using ""B"": 44.164 seconds

So, it looks like Netbeans has bad performance on print to console.
After more research I realized that the problem is line-wrapping of the max buffer of Netbeans (it's not restricted to System.out.println command), demonstrated by this code:
for (int i = 0; i < 1000; i++) {
    long t1 = System.nanoTime();
    System.out.print(""BBB......BBB""); \\<-contain 1000 ""B""
    long t2 = System.nanoTime();
    System.out.println(t2-t1);
    System.out.println("""");
}

The time results are less then 1 millisecond every iteration except every fifth iteration, when the time result is around 225 millisecond. Something like (in nanoseconds):
BBB...31744
BBB...31744
BBB...31744
BBB...31744
BBB...226365807
BBB...31744
BBB...31744
BBB...31744
BBB...31744
BBB...226365807
.
.
.

And so on..
Summary:

Eclipse works perfectly with ""B""
Netbeans has a line-wrapping problem that can be solved (because the problem does not occur in eclipse)(without adding space after B (""B "")).

    Yes the culprit is definitely word-wrapping. When I tested your two programs, NetBeans IDE 8.2 gave me the following result.


First Matrix: O and # = 6.03 seconds
Second Matrix: O and B = 50.97 seconds


Looking at your code closely you have used a line break at the end of first loop. But you didn't use any line break in second loop. So you are going to print a word with 1000 characters in the second loop. That causes a word-wrapping problem. If we use a non-word character "" "" after B, it takes only 5.35 seconds to compile the program. And If we use a line break in the second loop after passing 100 values or 50 values, it takes only 8.56 seconds and 7.05 seconds respectively. 

Random r = new Random();
for (int i = 0; i < 1000; i++) {
    for (int j = 0; j < 1000; j++) {
        if(r.nextInt(4) == 0) {
            System.out.print(""O"");
        } else {
            System.out.print(""B"");
        }
        if(j%100==0){               //Adding a line break in second loop      
            System.out.println();
        }                    
    }
    System.out.println("""");                
}


Another advice is that to change settings of NetBeans IDE. First of all, go to NetBeans Tools and click Options. After that click Editor and go to Formatting tab. Then select Anywhere in Line Wrap Option. It will take almost 6.24% less time to compile the program. 


    ","[2914, 4210, 244, 21]",249044,492,2014-02-21T23:45:43,2020-03-03 06:14:58Z,java 
How to store objects in HTML5 localStorage,"
                
I'd like to store a JavaScript object in HTML5 localStorage, but my object is apparently being converted to a string.
I can store and retrieve primitive JavaScript types and arrays using localStorage, but objects don't seem to work.  Should they?
Here's my code:
var testObject = { 'one': 1, 'two': 2, 'three': 3 };
console.log('typeof testObject: ' + typeof testObject);
console.log('testObject properties:');
for (var prop in testObject) {
    console.log('  ' + prop + ': ' + testObject[prop]);
}

// Put the object into storage
localStorage.setItem('testObject', testObject);

// Retrieve the object from storage
var retrievedObject = localStorage.getItem('testObject');

console.log('typeof retrievedObject: ' + typeof retrievedObject);
console.log('Value of retrievedObject: ' + retrievedObject);

The console output is
typeof testObject: object
testObject properties:
  one: 1
  two: 2
  three: 3
typeof retrievedObject: string
Value of retrievedObject: [object Object]

It looks to me like the setItem method is converting the input to a string before storing it.
I see this behavior in Safari, Chrome, and Firefox, so I assume it's my misunderstanding of the HTML5 Web Storage specification, not a browser-specific bug or limitation.
I've tried to make sense of the structured clone algorithm described in 2 Common infrastructure.  I don't fully understand what it's saying, but maybe my problem has to do with my object's properties not being enumerable (???).
Is there an easy workaround?

Update: The W3C eventually changed their minds about the structured-clone specification, and decided to change the spec to match the implementations.  See 12111  spec for Storage object getItem(key) method does not match implementation behavior. So this question is no longer 100% valid, but the answers still may be of interest.
    Looking at the Apple, Mozilla and Mozilla again documentation, the functionality seems to be limited to handle only string key/value pairs.

A workaround can be to stringify your object before storing it, and later parse it when you retrieve it:

var testObject = { 'one': 1, 'two': 2, 'three': 3 };

// Put the object into storage
localStorage.setItem('testObject', JSON.stringify(testObject));

// Retrieve the object from storage
var retrievedObject = localStorage.getItem('testObject');

console.log('retrievedObject: ', JSON.parse(retrievedObject));

    A minor improvement on a variant:

Storage.prototype.setObject = function(key, value) {
    this.setItem(key, JSON.stringify(value));
}

Storage.prototype.getObject = function(key) {
    var value = this.getItem(key);
    return value && JSON.parse(value);
}


Because of short-circuit evaluation, getObject() will immediately return null if key is not in Storage. It also will not throw a SyntaxError exception if value is """" (the empty string; JSON.parse() cannot handle that).
    You might find it useful to extend the Storage object with these handy methods:

Storage.prototype.setObject = function(key, value) {
    this.setItem(key, JSON.stringify(value));
}

Storage.prototype.getObject = function(key) {
    return JSON.parse(this.getItem(key));
}


This way you get the functionality that you really wanted even though underneath the API only supports strings.
    Stringify doesn't solve all problems
It seems that the answers here don't cover all types that are possible in JavaScript, so here are some short examples on how to deal with them correctly:
// Objects and Arrays:
    var obj = {key: ""value""};
    localStorage.object = JSON.stringify(obj);  // Will ignore private members
    obj = JSON.parse(localStorage.object);

// Boolean:
    var bool = false;
    localStorage.bool = bool;
    bool = (localStorage.bool === ""true"");

// Numbers:
    var num = 42;
    localStorage.num = num;
    num = +localStorage.num;    // Short for ""num = parseFloat(localStorage.num);""

// Dates:
    var date = Date.now();
    localStorage.date = date;
    date = new Date(parseInt(localStorage.date));

// Regular expressions:
    var regex = /^No\.[\d]*$/i;     // Usage example: ""No.42"".match(regex);
    localStorage.regex = regex;
    var components = localStorage.regex.match(""^/(.*)/([a-z]*)$"");
    regex = new RegExp(components[1], components[2]);

// Functions (not recommended):
    function func() {}

    localStorage.func = func;
    eval(localStorage.func);      // Recreates the function with the name ""func""

I do not recommend to store functions, because eval() is evil and can lead to issues regarding security, optimisation and debugging.
In general, eval() should never be used in JavaScript code.
Private members
The problem with using JSON.stringify() for storing objects is, that this function can not serialise private members.
This issue can be solved by overwriting the .toString() method (which is called implicitly when storing data in web storage):
// Object with private and public members:
    function MyClass(privateContent, publicContent) {
        var privateMember = privateContent || ""defaultPrivateValue"";
        this.publicMember = publicContent  || ""defaultPublicValue"";

        this.toString = function() {
            return '{""private"": ""' + privateMember + '"", ""public"": ""' + this.publicMember + '""}';
        };
    }
    MyClass.fromString = function(serialisedString) {
        var properties = JSON.parse(serialisedString || ""{}"");
        return new MyClass(properties.private, properties.public);
    };

// Storing:
    var obj = new MyClass(""invisible"", ""visible"");
    localStorage.object = obj;

// Loading:
    obj = MyClass.fromString(localStorage.object);

Circular references
Another problem stringify can't deal with are circular references:
var obj = {};
obj[""circular""] = obj;
localStorage.object = JSON.stringify(obj);  // Fails

In this example, JSON.stringify() will throw a TypeError ""Converting circular structure to JSON"".
If storing circular references should be supported, the second parameter of JSON.stringify() might be used:
var obj = {id: 1, sub: {}};
obj.sub[""circular""] = obj;
localStorage.object = JSON.stringify(obj, function(key, value) {
    if(key == 'circular') {
        return ""$ref"" + value.id + ""$"";
    } else {
        return value;
    }
});

However, finding an efficient solution for storing circular references highly depends on the tasks that need to be solved, and restoring such data is not trivial either.
There are already some question on StackOverflow dealing with this problem: Stringify (convert to JSON) a JavaScript object with circular reference
    Creating a facade for the Storage object is an awesome solution. That way, you can implement your own get and set methods. For my API, I have created a facade for localStorage and then check if it is an object or not while setting and getting.
var data = {
  set: function(key, value) {
    if (!key || !value) {return;}

    if (typeof value === ""object"") {
      value = JSON.stringify(value);
    }
    localStorage.setItem(key, value);
  },
  get: function(key) {
    var value = localStorage.getItem(key);

    if (!value) {return;}

    // assume it is an object that has been stringified
    if (value[0] === ""{"") {
      value = JSON.parse(value);
    }

    return value;
  }
}

    I arrived at this post after hitting on another post that has been closed as a duplicate of this - titled 'how to store an array in localstorage?'.  Which is fine except neither thread actually provides a full answer as to how you can maintain an array in localStorage - however I have managed to craft a solution based on information contained in both threads.

So if anyone else is wanting to be able to push/pop/shift items within an array, and they want that array stored in localStorage or indeed sessionStorage, here you go:

Storage.prototype.getArray = function(arrayName) {
  var thisArray = [];
  var fetchArrayObject = this.getItem(arrayName);
  if (typeof fetchArrayObject !== 'undefined') {
    if (fetchArrayObject !== null) { thisArray = JSON.parse(fetchArrayObject); }
  }
  return thisArray;
}

Storage.prototype.pushArrayItem = function(arrayName,arrayItem) {
  var existingArray = this.getArray(arrayName);
  existingArray.push(arrayItem);
  this.setItem(arrayName,JSON.stringify(existingArray));
}

Storage.prototype.popArrayItem = function(arrayName) {
  var arrayItem = {};
  var existingArray = this.getArray(arrayName);
  if (existingArray.length > 0) {
    arrayItem = existingArray.pop();
    this.setItem(arrayName,JSON.stringify(existingArray));
  }
  return arrayItem;
}

Storage.prototype.shiftArrayItem = function(arrayName) {
  var arrayItem = {};
  var existingArray = this.getArray(arrayName);
  if (existingArray.length > 0) {
    arrayItem = existingArray.shift();
    this.setItem(arrayName,JSON.stringify(existingArray));
  }
  return arrayItem;
}

Storage.prototype.unshiftArrayItem = function(arrayName,arrayItem) {
  var existingArray = this.getArray(arrayName);
  existingArray.unshift(arrayItem);
  this.setItem(arrayName,JSON.stringify(existingArray));
}

Storage.prototype.deleteArray = function(arrayName) {
  this.removeItem(arrayName);
}


example usage - storing simple strings in localStorage array:

localStorage.pushArrayItem('myArray','item one');
localStorage.pushArrayItem('myArray','item two');


example usage - storing objects in sessionStorage array:

var item1 = {}; item1.name = 'fred'; item1.age = 48;
sessionStorage.pushArrayItem('myArray',item1);

var item2 = {}; item2.name = 'dave'; item2.age = 22;
sessionStorage.pushArrayItem('myArray',item2);


common methods to manipulate arrays:

.pushArrayItem(arrayName,arrayItem); -> adds an element onto end of named array
.unshiftArrayItem(arrayName,arrayItem); -> adds an element onto front of named array
.popArrayItem(arrayName); -> removes & returns last array element
.shiftArrayItem(arrayName); -> removes & returns first array element
.getArray(arrayName); -> returns entire array
.deleteArray(arrayName); -> removes entire array from storage

    You cannot store a key value without a string format.
LocalStorage only supports string formats for keys/values.
That is why you should convert your data to string whatever it is an array or object.
To store data in localStorage, first of all stringify it using the JSON.stringify() method.
var myObj = [{name:""test"", time:""Date 2017-02-03T08:38:04.449Z""}];
localStorage.setItem('item', JSON.stringify(myObj));

Then when you want to retrieve data, you need to parse the string to object again.
var getObj = JSON.parse(localStorage.getItem('item'));

    You can use localDataStorage to transparently store JavaScript data types (Array, Boolean, Date, Float, Integer, String and Object). It also provides lightweight data obfuscation, automatically compresses strings, facilitates query by key (name) as well as query by (key) value, and helps to enforce segmented shared storage within the same domain by prefixing keys.
[DISCLAIMER] I am the author of the utility [/DISCLAIMER]
Examples:
localDataStorage.set( 'key1', 'Belgian' )
localDataStorage.set( 'key2', 1200.0047 )
localDataStorage.set( 'key3', true )
localDataStorage.set( 'key4', { 'RSK' : [1,'3',5,'7',9] } )
localDataStorage.set( 'key5', null )

localDataStorage.get( 'key1' )  // -->   'Belgian'
localDataStorage.get( 'key2' )  // -->   1200.0047
localDataStorage.get( 'key3' )  // -->   true
localDataStorage.get( 'key4' )  // -->   Object {RSK: Array(5)}
localDataStorage.get( 'key5' )  // -->   null

As you can see, the primitive values are respected.
    In theory, it is possible to store objects with functions:
function store (a)
{
  var c = {f: {}, d: {}};
  for (var k in a)
  {
    if (a.hasOwnProperty(k) && typeof a[k] === 'function')
    {
      c.f[k] = encodeURIComponent(a[k]);
    }
  }

  c.d = a;
  var data = JSON.stringify(c);
  window.localStorage.setItem('CODE', data);
}

function restore ()
{
  var data = window.localStorage.getItem('CODE');
  data = JSON.parse(data);
  var b = data.d;

  for (var k in data.f)
  {
    if (data.f.hasOwnProperty(k))
    {
      b[k] = eval(""("" + decodeURIComponent(data.f[k]) + "")"");
    }
  }

  return b;
}

However, function serialization/deserialization is unreliable because it is implementation-dependent.
    There is a great library that wraps many solutions so it even supports older browsers called jStorage

You can set an object

$.jStorage.set(key, value)


And retrieve it easily

value = $.jStorage.get(key)
value = $.jStorage.get(key, ""default value"")

    For TypeScript users willing to set and get typed properties:
/**
 * Silly wrapper to be able to type the storage keys
 */
export class TypedStorage<T> {

    public removeItem(key: keyof T): void {
        localStorage.removeItem(key);
    }

    public getItem<K extends keyof T>(key: K): T[K] | null {
        const data: string | null =  localStorage.getItem(key);
        return JSON.parse(data);
    }

    public setItem<K extends keyof T>(key: K, value: T[K]): void {
        const data: string = JSON.stringify(value);
        localStorage.setItem(key, data);
    }
}

Example usage:
// write an interface for the storage
interface MyStore {
   age: number,
   name: string,
   address: {city:string}
}

const storage: TypedStorage<MyStore> = new TypedStorage<MyStore>();

storage.setItem(""wrong key"", """"); // error unknown key
storage.setItem(""age"", ""hello""); // error, age should be number
storage.setItem(""address"", {city:""Here""}); // ok

const address: {city:string} = storage.getItem(""address"");

    localStorage.setItem('obj',JSON.stringify({name:'Akash'})); // Set Object in localStorage
localStorage.getItem('obj'); // Get Object from localStorage

sessionStorage.setItem('obj',JSON.stringify({name:'Akash'})); // Set Object in sessionStorage
sessionStorage.getItem('obj'); // Get Object from sessionStorage

    It is recommended using an abstraction library for many of the features discussed here, as well as better compatibility. There are lots of options:

jStorage or simpleStorage  my preference
localForage
alekseykulikov/storage
Lawnchair
Store.js  another good option
OMG

    You can use ejson to store the objects as strings.


  EJSON is an extension of JSON to support more types. It supports all JSON-safe types, as well as:
  
  
  Date (JavaScript Date)
  Binary (JavaScript Uint8Array or the result of EJSON.newBinary)
  User-defined types (see EJSON.addType. For example, Mongo.ObjectID is implemented this way.)
  
  
  All EJSON serializations are also valid JSON. For example an object with a date and a binary buffer would be serialized in EJSON as:

{
  ""d"": {""$date"": 1358205756553},
  ""b"": {""$binary"": ""c3VyZS4=""}
}



Here is my localStorage wrapper using ejson

https://github.com/UziTech/storage.js

I added some types to my wrapper including regular expressions and functions
    https://github.com/adrianmay/rhaboo is a localStorage sugar layer that lets you write things like this:

var store = Rhaboo.persistent('Some name');
store.write('count', store.count ? store.count+1 : 1);
store.write('somethingfancy', {
  one: ['man', 'went'],
  2: 'mow',
  went: [  2, { mow: ['a', 'meadow' ] }, {}  ]
});
store.somethingfancy.went[1].mow.write(1, 'lawn');


It doesn't use JSON.stringify/parse because that would be inaccurate and slow on big objects. Instead, each terminal value has its own localStorage entry.

You can probably guess that I might have something to do with rhaboo.
    This question has been answered sufficiently from the JavaScript-only perspective, and others have already noted that both localStorage.getItem and localStorage.setItem have no concept of objectsthey handle strings and strings only. This answer provides a TypeScript-friendly solution that incorporates what others have suggested in JavaScript-only solutions.
TypeScript 4.2.3
Storage.prototype.setObject = function (key: string, value: unknown) {
  this.setItem(key, JSON.stringify(value));
};

Storage.prototype.getObject = function (key: string) {
  const value = this.getItem(key);
  if (!value) {
    return null;
  }

  return JSON.parse(value);
};

declare global {
  interface Storage {
    setObject: (key: string, value: unknown) => void;
    getObject: (key: string) => unknown;
  }
}

Usage
localStorage.setObject('ages', [23, 18, 33, 22, 58]);
localStorage.getObject('ages');

Explanation
We declare both setObject and getObject functions on the Storage prototypelocalStorage is an instance of this type. There's nothing special we really need to note besides the null handling in getObject. Since getItem can return null, we must exit early since calling JSON.parse on a null value will throw a runtime exception.
After declaring the functions on the Storage prototype, we include their type definitions on the Storage type in the global namespace.
Note: If we defined these functions with arrow functions, we'd need to assume that the storage object we're calling is always localStorage, which might not be true. For instance, the above code will add setObject and getObject support to sessionStorage as well.
    Another option would be to use an existing plugin.

For example persisto is an open source project that provides an easy interface to localStorage/sessionStorage and automates persistence for form fields (input, radio buttons, and checkboxes).



(Disclaimer: I am the author.)
    I found a way to make it work with objects that have cyclic references.

Let's make an object with cyclic references.

obj = {
    L: {
        L: { v: 'lorem' },
        R: { v: 'ipsum' }
    },
    R: {
        L: { v: 'dolor' },
        R: {
            L: { v: 'sit' },
            R: { v: 'amet' }
        }
    }
}
obj.R.L.uncle = obj.L;
obj.R.R.uncle = obj.L;
obj.R.R.L.uncle = obj.R.L;
obj.R.R.R.uncle = obj.R.L;
obj.L.L.uncle = obj.R;
obj.L.R.uncle = obj.R;


We can't do JSON.stringify here, because of the circular references.



LOCALSTORAGE.CYCLICJSON has .stringify and .parse just like normal JSON, but works with objects with circular references. (""Works"" meaning parse(stringify(obj)) and obj are deep equal AND have identical sets of 'inner equalities')

But we can just use the shortcuts:

LOCALSTORAGE.setObject('latinUncles', obj)
recovered = LOCALSTORAGE.getObject('latinUncles')


Then, recovered will be ""the same"" to obj, in the following sense:

[
obj.L.L.v === recovered.L.L.v,
obj.L.R.v === recovered.L.R.v,
obj.R.L.v === recovered.R.L.v,
obj.R.R.L.v === recovered.R.R.L.v,
obj.R.R.R.v === recovered.R.R.R.v,
obj.R.L.uncle === obj.L,
obj.R.R.uncle === obj.L,
obj.R.R.L.uncle === obj.R.L,
obj.R.R.R.uncle === obj.R.L,
obj.L.L.uncle === obj.R,
obj.L.R.uncle === obj.R,
recovered.R.L.uncle === recovered.L,
recovered.R.R.uncle === recovered.L,
recovered.R.R.L.uncle === recovered.R.L,
recovered.R.R.R.uncle === recovered.R.L,
recovered.L.L.uncle === recovered.R,
recovered.L.R.uncle === recovered.R
]


Here is the implementation of LOCALSTORAGE

LOCALSTORAGE = (function(){
  ""use strict"";
  var ignore = [Boolean, Date, Number, RegExp, String];
  function primitive(item){
    if (typeof item === 'object'){
      if (item === null) { return true; }
      for (var i=0; i<ignore.length; i++){
        if (item instanceof ignore[i]) { return true; }
      }
      return false;
    } else {
      return true;
    }
  }
  function infant(value){
    return Array.isArray(value) ? [] : {};
  }
  function decycleIntoForest(object, replacer) {
    if (typeof replacer !== 'function'){
      replacer = function(x){ return x; }
    }
    object = replacer(object);
    if (primitive(object)) return object;
    var objects = [object];
    var forest  = [infant(object)];
    var bucket  = new WeakMap(); // bucket = inverse of objects 
    bucket.set(object, 0);    
    function addToBucket(obj){
      var result = objects.length;
      objects.push(obj);
      bucket.set(obj, result);
      return result;
    }
    function isInBucket(obj){ return bucket.has(obj); }
    function processNode(source, target){
      Object.keys(source).forEach(function(key){
        var value = replacer(source[key]);
        if (primitive(value)){
          target[key] = {value: value};
        } else {
          var ptr;
          if (isInBucket(value)){
            ptr = bucket.get(value);
          } else {
            ptr = addToBucket(value);
            var newTree = infant(value);
            forest.push(newTree);
            processNode(value, newTree);
          }
          target[key] = {pointer: ptr};
        }
      });
    }
    processNode(object, forest[0]);
    return forest;
  };
  function deForestIntoCycle(forest) {
    var objects = [];
    var objectRequested = [];
    var todo = [];
    function processTree(idx) {
      if (idx in objects) return objects[idx];
      if (objectRequested[idx]) return null;
      objectRequested[idx] = true;
      var tree = forest[idx];
      var node = Array.isArray(tree) ? [] : {};
      for (var key in tree) {
        var o = tree[key];
        if ('pointer' in o) {
          var ptr = o.pointer;
          var value = processTree(ptr);
          if (value === null) {
            todo.push({
              node: node,
              key: key,
              idx: ptr
            });
          } else {
            node[key] = value;
          }
        } else {
          if ('value' in o) {
            node[key] = o.value;
          } else {
            throw new Error('unexpected')
          }
        }
      }
      objects[idx] = node;
      return node;
    }
    var result = processTree(0);
    for (var i = 0; i < todo.length; i++) {
      var item = todo[i];
      item.node[item.key] = objects[item.idx];
    }
    return result;
  };
  var console = {
    log: function(x){
      var the = document.getElementById('the');
      the.textContent = the.textContent + '\n' + x;
	},
	delimiter: function(){
      var the = document.getElementById('the');
      the.textContent = the.textContent +
		'\n*******************************************';
	}
  }
  function logCyclicObjectToConsole(root) {
    var cycleFree = decycleIntoForest(root);
    var shown = cycleFree.map(function(tree, idx) {
      return false;
    });
    var indentIncrement = 4;
    function showItem(nodeSlot, indent, label) {
      var leadingSpaces = ' '.repeat(indent);
      var leadingSpacesPlus = ' '.repeat(indent + indentIncrement);
      if (shown[nodeSlot]) {
        console.log(leadingSpaces + label + ' ... see above (object #' + nodeSlot + ')');
      } else {
        console.log(leadingSpaces + label + ' object#' + nodeSlot);
        var tree = cycleFree[nodeSlot];
        shown[nodeSlot] = true;
        Object.keys(tree).forEach(function(key) {
          var entry = tree[key];
          if ('value' in entry) {
            console.log(leadingSpacesPlus + key + "": "" + entry.value);
          } else {
            if ('pointer' in entry) {
              showItem(entry.pointer, indent + indentIncrement, key);
            }
          }
        });
      }
    }
	console.delimiter();
    showItem(0, 0, 'root');
  };
  function stringify(obj){
    return JSON.stringify(decycleIntoForest(obj));
  }
  function parse(str){
    return deForestIntoCycle(JSON.parse(str));
  }
  var CYCLICJSON = {
    decycleIntoForest: decycleIntoForest,
    deForestIntoCycle : deForestIntoCycle,
    logCyclicObjectToConsole: logCyclicObjectToConsole,
    stringify : stringify,
    parse : parse
  }
  function setObject(name, object){
    var str = stringify(object);
    localStorage.setItem(name, str);
  }
  function getObject(name){
    var str = localStorage.getItem(name);
    if (str===null) return null;
    return parse(str);
  }
  return {
    CYCLICJSON : CYCLICJSON,
    setObject  : setObject,
    getObject  : getObject
  }
})();
obj = {
	L: {
		L: { v: 'lorem' },
		R: { v: 'ipsum' }
	},
	R: {
		L: { v: 'dolor' },
		R: {
			L: { v: 'sit' },
			R: { v: 'amet' }
		}
	}
}
obj.R.L.uncle = obj.L;
obj.R.R.uncle = obj.L;
obj.R.R.L.uncle = obj.R.L;
obj.R.R.R.uncle = obj.R.L;
obj.L.L.uncle = obj.R;
obj.L.R.uncle = obj.R;

// LOCALSTORAGE.setObject('latinUncles', obj)
// recovered = LOCALSTORAGE.getObject('latinUncles')
// localStorage not available inside fiddle ):
LOCALSTORAGE.CYCLICJSON.logCyclicObjectToConsole(obj)
putIntoLS = LOCALSTORAGE.CYCLICJSON.stringify(obj);
recovered = LOCALSTORAGE.CYCLICJSON.parse(putIntoLS);
LOCALSTORAGE.CYCLICJSON.logCyclicObjectToConsole(recovered);

var the = document.getElementById('the');
the.textContent = the.textContent + '\n\n' +
JSON.stringify(
[
obj.L.L.v === recovered.L.L.v,
obj.L.R.v === recovered.L.R.v,
obj.R.L.v === recovered.R.L.v,
obj.R.R.L.v === recovered.R.R.L.v,
obj.R.R.R.v === recovered.R.R.R.v,
obj.R.L.uncle === obj.L,
obj.R.R.uncle === obj.L,
obj.R.R.L.uncle === obj.R.L,
obj.R.R.R.uncle === obj.R.L,
obj.L.L.uncle === obj.R,
obj.L.R.uncle === obj.R,
recovered.R.L.uncle === recovered.L,
recovered.R.R.uncle === recovered.L,
recovered.R.R.L.uncle === recovered.R.L,
recovered.R.R.R.uncle === recovered.R.L,
recovered.L.L.uncle === recovered.R,
recovered.L.R.uncle === recovered.R
]
)<pre id='the'></pre>

    I made a thing that doesn't break the existing Storage objects, but creates a wrapper so you can do what you want. The result is a normal object, no methods, with access like any object.

The thing I made.

If you want 1 localStorage property to be magic:

var prop = ObjectStorage(localStorage, 'prop');


If you need several:

var storage = ObjectStorage(localStorage, ['prop', 'more', 'props']);


Everything you do to prop, or the objects inside storage will be automatically saved into localStorage. You're always playing with a real object, so you can do stuff like this:

storage.data.list.push('more data');
storage.another.list.splice(1, 2, {another: 'object'});


And every new object inside a tracked object will be automatically tracked.

The very big downside: it depends on Object.observe() so it has very limited browser support. And it doesn't look like it'll be coming for Firefox or Edge anytime soon.
    Here is some extended version of the code posted by danott:
It'll also implement a delete value from localstorage and shows how to adds a Getter and Setter layer so instead of,
localstorage.setItem(preview, true)
you can write
config.preview = true
Okay, here were go:
var PT=Storage.prototype

if (typeof PT._setItem >='u')
  PT._setItem = PT.setItem;
PT.setItem = function(key, value)
{
  if (typeof value >='u') //..undefined
    this.removeItem(key)
  else
    this._setItem(key, JSON.stringify(value));
}

if (typeof PT._getItem >='u')
  PT._getItem = PT.getItem;
PT.getItem = function(key)
{
  var ItemData = this._getItem(key)
  try
  {
    return JSON.parse(ItemData);
  }
  catch(e)
  {
    return ItemData;
  }
}

// Aliases for localStorage.set/getItem
get = localStorage.getItem.bind(localStorage)
set = localStorage.setItem.bind(localStorage)

// Create ConfigWrapperObject
var config = {}

// Helper to create getter & setter
function configCreate(PropToAdd){
    Object.defineProperty( config, PropToAdd, {
      get: function ()    { return (get(PropToAdd)    )},
      set: function (val) {         set(PropToAdd, val)}
    })
}
//------------------------------

// Usage Part
// Create properties
configCreate('preview')
configCreate('notification')
//...

// Configuration Data transfer
// Set
config.preview = true

// Get
config.preview

// Delete
config.preview = undefined

Well, you may strip the aliases part with .bind(...). However, I just put it in since it's really good to know about this. I took me hours to find out why a simple get = localStorage.getItem; don't work.
    I made another minimalistic wrapper with only 20 lines of code to allow using it like it should:

localStorage.set('myKey',{a:[1,2,5], b: 'ok'});
localStorage.has('myKey');   // --> true
localStorage.get('myKey');   // --> {a:[1,2,5], b: 'ok'}
localStorage.keys();         // --> ['myKey']
localStorage.remove('myKey');


https://github.com/zevero/simpleWebstorage
    localStorage.setItem('user', JSON.stringify(user));

Then to retrieve it from the store and convert to an object again:
var user = JSON.parse(localStorage.getItem('user'));

If we need to delete all entries of the store we can simply do:

localStorage.clear();

    Circular References
In this answer I focus on data-only objects (without functions, etc.) with circular references and develop ideas mentioned by maja and mathheadinclouds (I use his test case and
my code is several times shorter).
Actually, we can use JSON.stringify with a proper replacer - if the source object contains multi-references to some object, or contains circular references then we reference it by special path-string (similar to JSONPath).
// JSON.strigify replacer for objects with circ ref
function refReplacer() {
  let m = new Map(), v = new Map(), init = null;

  return function(field, value) {
    let p = m.get(this) + (Array.isArray(this) ? `[${field}]` : '.' + field);
    let isComplex = value === Object(value)

    if (isComplex) m.set(value, p);

    let pp = v.get(value)||'';
    let path = p.replace(/undefined\.\.?/, '');
    let val = pp ? `#REF:${pp[0] == '[' ? '$':'$.'}${pp}` : value;

    !init ? (init=value) : (val===init ? val=""#REF:$"" : 0);
    if(!pp && isComplex) v.set(value, path);

    return val;
  }
}


// ---------------
// TEST
// ---------------

// Generate obj with duplicate/circular references
let obj = {
    L: {
        L: { v: 'lorem' },
        R: { v: 'ipsum' }
    },
    R: {
        L: { v: 'dolor' },
        R: {
            L: { v: 'sit' },
            R: { v: 'amet' }
        }
    }
}
obj.R.L.uncle = obj.L;
obj.R.R.uncle = obj.L;
obj.R.R.L.uncle = obj.R.L;
obj.R.R.R.uncle = obj.R.L;
obj.L.L.uncle = obj.R;
obj.L.R.uncle = obj.R;
testObject = obj;

let json = JSON.stringify(testObject, refReplacer(), 4);

console.log(""Test Object\n"", testObject);
console.log(""JSON with JSONpath references\n"", json);

Parse such JSON content with JSONpath-like references:
// Parse JSON content with JSONpath references to object
function parseRefJSON(json) {
  let objToPath = new Map();
  let pathToObj = new Map();
  let o = JSON.parse(json);

  let traverse = (parent, field) => {
    let obj = parent;
    let path = '#REF:$';

    if (field !== undefined) {
      obj = parent[field];
      path = objToPath.get(parent) + (Array.isArray(parent) ? `[${field}]` : `${field ? '.' + field : ''}`);
    }

    objToPath.set(obj, path);
    pathToObj.set(path, obj);

    let ref = pathToObj.get(obj);
    if (ref) parent[field] = ref;

    for (let f in obj) if (obj === Object(obj)) traverse(obj, f);
  }

  traverse(o);
  return o;
}


// ---------------
// TEST 1
// ---------------

let json = `
{
    ""L"": {
        ""L"": {
            ""v"": ""lorem"",
            ""uncle"": {
                ""L"": {
                    ""v"": ""dolor"",
                    ""uncle"": ""#REF:$.L""
                },
                ""R"": {
                    ""L"": {
                        ""v"": ""sit"",
                        ""uncle"": ""#REF:$.L.L.uncle.L""
                    },
                    ""R"": {
                        ""v"": ""amet"",
                        ""uncle"": ""#REF:$.L.L.uncle.L""
                    },
                    ""uncle"": ""#REF:$.L""
                }
            }
        },
        ""R"": {
            ""v"": ""ipsum"",
            ""uncle"": ""#REF:$.L.L.uncle""
        }
    },
    ""R"": ""#REF:$.L.L.uncle""
}`;

let testObject = parseRefJSON(json);

console.log(""Test Object\n"", testObject);


// ---------------
// TEST 2
// ---------------

console.log('Tests from mathheadinclouds answer: ');

let recovered = testObject;

let obj = { // Original object
    L: {
        L: { v: 'lorem' },
        R: { v: 'ipsum' }
    },
    R: {
        L: { v: 'dolor' },
        R: {
            L: { v: 'sit' },
            R: { v: 'amet' }
        }
    }
}
obj.R.L.uncle = obj.L;
obj.R.R.uncle = obj.L;
obj.R.R.L.uncle = obj.R.L;
obj.R.R.R.uncle = obj.R.L;
obj.L.L.uncle = obj.R;
obj.L.R.uncle = obj.R;

[
  obj.L.L.v === recovered.L.L.v,
  obj.L.R.v === recovered.L.R.v,
  obj.R.L.v === recovered.R.L.v,
  obj.R.R.L.v === recovered.R.R.L.v,
  obj.R.R.R.v === recovered.R.R.R.v,
  obj.R.L.uncle === obj.L,
  obj.R.R.uncle === obj.L,
  obj.R.R.L.uncle === obj.R.L,
  obj.R.R.R.uncle === obj.R.L,
  obj.L.L.uncle === obj.R,
  obj.L.R.uncle === obj.R,
  recovered.R.L.uncle === recovered.L,
  recovered.R.R.uncle === recovered.L,
  recovered.R.R.L.uncle === recovered.R.L,
  recovered.R.R.R.uncle === recovered.R.L,
  recovered.L.L.uncle === recovered.R,
  recovered.L.R.uncle === recovered.R
].forEach(x => console.log('test pass: ' + x));

To load/save the resulting JSON content into storage, use the following code:
localStorage.myObject = JSON.stringify(testObject, refReplacer());  // Save
testObject = parseRefJSON(localStorage.myObject);                   // Load

    I suggest using Jackson-js. It is a library that handles serializing and deserializing of Objects while retaining their structure, based on decorators.
The library handles all the pitfalls such as cyclic reference, attributes aliasing, etc.
Simply describe your class using the @JsonProperty() and @JsonClassType() decorators.
Serialize your object using:
const objectMapper = new ObjectMapper();
localstore.setItem(key, objectMapper.stringify<yourObjectType>(yourObject));

For slightly more detailed explanation, check my answer here:
Typescript objects serialization?
And the Jackson-js tutorial here:
Jackson-js: Powerful JavaScript decorators to serialize/deserialize objects into JSON and vice versa (Part 1)
    ","[2903, 3638, 662, 241, 74, 82, 33, 7, 11, 31, 55, 4, 2, 16, 6, 4, 1, 5, 1, 1, 1, 2, -5, 0, 0]",1318962,868,2010-01-06T04:05:51,2022-04-28 21:58:28Z,javascript html 
How do I delete a file or folder in Python?,"
                
How do I delete a file or folder?
    
os.remove() removes a file.
os.rmdir() removes an empty directory.
shutil.rmtree() deletes a directory and all its contents.




Path objects from the Python 3.4+ pathlib module also expose these instance methods:


pathlib.Path.unlink() removes a file or symbolic link.
pathlib.Path.rmdir() removes an empty directory.

    Python syntax to delete a file
import os
os.remove(""/tmp/<file_name>.txt"")

Or
import os
os.unlink(""/tmp/<file_name>.txt"")

Or
pathlib Library for Python version >= 3.4
file_to_rem = pathlib.Path(""/tmp/<file_name>.txt"")
file_to_rem.unlink()

Path.unlink(missing_ok=False)
Unlink method used to remove the file or the symbolik link.

If missing_ok is false (the default), FileNotFoundError is raised if the path does not exist.
If missing_ok is true, FileNotFoundError exceptions will be ignored (same behavior as the POSIX rm -f command).
Changed in version 3.8: The missing_ok parameter was added.

Best practice

First, check whether the file or folder exists or not then only delete that file. This can be achieved in two ways :
a. os.path.isfile(""/path/to/file"")
b. Use exception handling.

EXAMPLE for os.path.isfile
#!/usr/bin/python
import os
myfile=""/tmp/foo.txt""

## If file exists, delete it ##
if os.path.isfile(myfile):
    os.remove(myfile)
else:    ## Show an error ##
    print(""Error: %s file not found"" % myfile)

Exception Handling
#!/usr/bin/python
import os

## Get input ##
myfile= raw_input(""Enter file name to delete: "")

## Try to delete the file ##
try:
    os.remove(myfile)
except OSError as e:  ## if failed, report it back to the user ##
    print (""Error: %s - %s."" % (e.filename, e.strerror))

RESPECTIVE OUTPUT

Enter file name to delete : demo.txt
Error: demo.txt - No such file or directory.

Enter file name to delete : rrr.txt
Error: rrr.txt - Operation not permitted.

Enter file name to delete : foo.txt

Python syntax to delete a folder
shutil.rmtree()

Example for shutil.rmtree()
#!/usr/bin/python
import os
import sys
import shutil

# Get directory name
mydir= raw_input(""Enter directory name: "")

## Try to remove tree; if failed show an error using try...except on screen
try:
    shutil.rmtree(mydir)
except OSError as e:
    print (""Error: %s - %s."" % (e.filename, e.strerror))

    Here is a robust function that uses both os.remove and shutil.rmtree:

def remove(path):
    """""" param <path> could either be relative or absolute. """"""
    if os.path.isfile(path) or os.path.islink(path):
        os.remove(path)  # remove the file
    elif os.path.isdir(path):
        shutil.rmtree(path)  # remove dir and all contains
    else:
        raise ValueError(""file {} is not a file or dir."".format(path))

    Deleting a file or folder in Python
There are multiple ways to Delete a File in Python but the best ways are the following:

os.remove() removes a file.
os.unlink() removes a file. it is a Unix name of remove() method.
shutil.rmtree() deletes a directory and all its contents.
pathlib.Path.unlink() deletes a single file The pathlib module is available in Python 3.4 and above.

os.remove()
Example 1: Basic Example to Remove a File Using os.remove() Method.
import os
os.remove(""test_file.txt"")
print(""File removed successfully"")

Example 2: Checking if File Exists using os.path.isfile and Deleting it With os.remove
import os
#checking if file exist or not
if(os.path.isfile(""test.txt"")):
    #os.remove() function to remove the file
    os.remove(""test.txt"")
    #Printing the confirmation message of deletion
    print(""File Deleted successfully"")
else:
print(""File does not exist"")
#Showing the message instead of throwig an error

Example 3: Python Program to Delete all files with a specific extension
import os 
from os import listdir
my_path = 'C:\Python Pool\Test\'
for file_name in listdir(my_path):
    if file_name.endswith('.txt'):
        os.remove(my_path + file_name)

Example 4: Python Program to Delete All Files Inside a Folder
To delete all files inside a particular directory, you simply have to use the * symbol as the pattern string.
#Importing os and glob modules
import os, glob
#Loop Through the folder projects all files and deleting them one by one
for file in glob.glob(""pythonpool/*""):
os.remove(file)
print(""Deleted "" + str(file))
os.unlink()
os.unlink() is an alias or another name of os.remove() . As in the Unix OS remove is also known as unlink.
Note: All the functionalities and syntax is the same of os.unlink() and os.remove(). Both of them are used to delete the Python file path.
Both are methods in the os module in Pythons standard libraries which performs the deletion function.
shutil.rmtree()
Example 1: Python Program to Delete a File Using shutil.rmtree()
import shutil 
import os 
# location 
location = ""E:/Projects/PythonPool/""
# directory 
dir = ""Test""
# path 
path = os.path.join(location, dir) 
# removing directory 
shutil.rmtree(path) 

Example 2: Python Program to Delete a File Using shutil.rmtree()
import shutil 
import os 
location = ""E:/Projects/PythonPool/""
dir = ""Test""    
path = os.path.join(location, dir) 
shutil.rmtree(path) 

pathlib.Path.rmdir() to remove Empty Directory
Pathlib module provides different ways to interact with your files. Rmdir is one of the path functions which allows you to delete an empty folder. Firstly, you need to select the Path() for the directory, and then calling rmdir() method will check the folder size. If its empty, itll delete it.
This is a good way to deleting empty folders without any fear of losing actual data.
from pathlib import Path
q = Path('foldername')
q.rmdir()

    Use 

shutil.rmtree(path[, ignore_errors[, onerror]])


(See complete documentation on shutil) and/or

os.remove


and

os.rmdir


(Complete documentation on os.)
    You can use the built-in pathlib module (requires Python 3.4+, but there are backports for older versions on PyPI: pathlib, pathlib2). 

To remove a file there is the unlink method:

import pathlib
path = pathlib.Path(name_of_file)
path.unlink()


Or the rmdir method to remove an empty folder:

import pathlib
path = pathlib.Path(name_of_folder)
path.rmdir()

    
  How do I delete a file or folder in Python?


For Python 3, to remove the file and directory individually, use the unlink and rmdir Path object methods respectively:

from pathlib import Path
dir_path = Path.home() / 'directory' 
file_path = dir_path / 'file'

file_path.unlink() # remove file

dir_path.rmdir()   # remove directory


Note that you can also use relative paths with Path objects, and you can check your current working directory with Path.cwd.

For removing individual files and directories in Python 2, see the section so labeled below.

To remove a directory with contents, use shutil.rmtree, and note that this is available in Python 2 and 3:

from shutil import rmtree

rmtree(dir_path)


Demonstration

New in Python 3.4 is the Path object. 

Let's use one to create a directory and file to demonstrate usage. Note that we use the / to join the parts of the path, this works around issues between operating systems and issues from using backslashes on Windows (where you'd need to either double up your backslashes like \\ or use raw strings, like r""foo\bar""):

from pathlib import Path

# .home() is new in 3.5, otherwise use os.path.expanduser('~')
directory_path = Path.home() / 'directory'
directory_path.mkdir()

file_path = directory_path / 'file'
file_path.touch()


and now:

>>> file_path.is_file()
True


Now let's delete them. First the file:

>>> file_path.unlink()     # remove file
>>> file_path.is_file()
False
>>> file_path.exists()
False


We can use globbing to remove multiple files - first let's create a few files for this:

>>> (directory_path / 'foo.my').touch()
>>> (directory_path / 'bar.my').touch()


Then just iterate over the glob pattern:

>>> for each_file_path in directory_path.glob('*.my'):
...     print(f'removing {each_file_path}')
...     each_file_path.unlink()
... 
removing ~/directory/foo.my
removing ~/directory/bar.my


Now, demonstrating removing the directory:

>>> directory_path.rmdir() # remove directory
>>> directory_path.is_dir()
False
>>> directory_path.exists()
False


What if we want to remove a directory  and everything in it? 
For this use-case, use shutil.rmtree

Let's recreate our directory and file:

file_path.parent.mkdir()
file_path.touch()


and note that rmdir fails unless it's empty, which is why rmtree is so convenient:

>>> directory_path.rmdir()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""~/anaconda3/lib/python3.6/pathlib.py"", line 1270, in rmdir
    self._accessor.rmdir(self)
  File ""~/anaconda3/lib/python3.6/pathlib.py"", line 387, in wrapped
    return strfunc(str(pathobj), *args)
OSError: [Errno 39] Directory not empty: '/home/username/directory'


Now, import rmtree and pass the directory to the funtion:

from shutil import rmtree
rmtree(directory_path)      # remove everything 


and we can see the whole thing has been removed:

>>> directory_path.exists()
False


Python 2

If you're on Python 2, there's a backport of the pathlib module called pathlib2, which can be installed with pip:

$ pip install pathlib2


And then you can alias the library to pathlib

import pathlib2 as pathlib


Or just directly import the Path object (as demonstrated here):

from pathlib2 import Path


If that's too much, you can remove files with os.remove or os.unlink

from os import unlink, remove
from os.path import join, expanduser

remove(join(expanduser('~'), 'directory/file'))


or

unlink(join(expanduser('~'), 'directory/file'))


and you can remove directories with os.rmdir:

from os import rmdir

rmdir(join(expanduser('~'), 'directory'))


Note that there is also a os.removedirs - it only removes empty directories recursively, but it may suit your use-case.
    This is my function for deleting dirs. The ""path"" requires the full pathname.
import os

def rm_dir(path):
    cwd = os.getcwd()
    if not os.path.exists(os.path.join(cwd, path)):
        return False
    os.chdir(os.path.join(cwd, path))

    for file in os.listdir():
        print(""file = "" + file)
        os.remove(file)
    print(cwd)
    os.chdir(cwd)
    os.rmdir(os.path.join(cwd, path))

    shutil.rmtree is the asynchronous function, 
so if you want to check when it complete, you can use while...loop

import os
import shutil

shutil.rmtree(path)

while os.path.exists(path):
  pass

print('done')

    For deleting files:

os.unlink(path, *, dir_fd=None)


or 

os.remove(path, *, dir_fd=None)


Both functions are semantically same. This functions removes (deletes) the file path. If path is not a file and it is directory, then exception is raised.

For deleting folders:

shutil.rmtree(path, ignore_errors=False, onerror=None)


or

os.rmdir(path, *, dir_fd=None)


In order to remove whole directory trees, shutil.rmtree() can be used. os.rmdir only works when the directory is empty and exists.

For deleting folders recursively towards parent:

os.removedirs(name)


It remove every empty parent directory with self until parent which has some content


  ex. os.removedirs('abc/xyz/pqr') will remove the directories by order 'abc/xyz/pqr', 'abc/xyz' and 'abc' if they are empty. 


For more info check official doc: os.unlink , os.remove, os.rmdir , shutil.rmtree, os.removedirs
    import os

folder = '/Path/to/yourDir/'
fileList = os.listdir(folder)

for f in fileList:
    filePath = folder + '/'+f

    if os.path.isfile(filePath):
        os.remove(filePath)

    elif os.path.isdir(filePath):
        newFileList = os.listdir(filePath)
        for f1 in newFileList:
            insideFilePath = filePath + '/' + f1

            if os.path.isfile(insideFilePath):
                os.remove(insideFilePath)

    My personal preference is to work with pathlib objects - it offers a more pythonic and less error-prone way to interact with the filesystem, especially if You develop cross-platform code.
In that case, You might use pathlib3x - it offers a backport of the latest (at the date of writing this answer Python 3.10.a0) Python pathlib for Python 3.6 or newer, and a few additional functions like ""copy"", ""copy2"", ""copytree"", ""rmtree"" etc ...
It also wraps shutil.rmtree:
$> python -m pip install pathlib3x
$> python
>>> import pathlib3x as pathlib

# delete a directory tree
>>> my_dir_to_delete=pathlib.Path('c:/temp/some_dir')
>>> my_dir_to_delete.rmtree(ignore_errors=True)

# delete a file
>>> my_file_to_delete=pathlib.Path('c:/temp/some_file.txt')
>>> my_file_to_delete.unlink(missing_ok=True)


you can find it on github or PyPi

Disclaimer: I'm the author of the pathlib3x library.
    To avoid the TOCTOU issue highlighted by ric Araujo's comment, you can catch an exception to call the correct method:

def remove_file_or_dir(path: str) -> None:
    """""" Remove a file or directory """"""
    try:
        shutil.rmtree(path)
    except NotADirectoryError:
        os.remove(path)


Since shutil.rmtree() will only remove directories and os.remove() or os.unlink() will only remove files.
    To remove all files in folder

import os
import glob

files = glob.glob(os.path.join('path/to/folder/*'))
files = glob.glob(os.path.join('path/to/folder/*.csv')) // It will give all csv files in folder
for file in files:
    os.remove(file)


To remove all folders in a directory

from shutil import rmtree
import os

// os.path.join()  # current working directory.

for dirct in os.listdir(os.path.join('path/to/folder')):
    rmtree(os.path.join('path/to/folder',dirct))

    I recommend using subprocess if writing a beautiful and readable code is your cup of tea:

import subprocess
subprocess.Popen(""rm -r my_dir"", shell=True)


And if you are not a software engineer, then maybe consider using Jupyter; you can simply type bash commands:

!rm -r my_dir


Traditionally, you use shutil:

import shutil
shutil.rmtree(my_dir) 

    ","[2892, 4297, 547, 72, 22, 105, 43, 37, 3, 7, 5, 6, 1, 1, 1, -1]",2705802,383,2011-08-09T13:05:42,2022-04-01 12:08:25Z,python 
How to append something to an array?,"
                    
            
        
            
                    
                        
                    
                
                    
                        This question's answers are a community effort. Edit existing answers to improve this post. It is not currently accepting new answers or interactions.
                        
                    
                
            
        

    

How do I append an object (such as a string or number) to an array in JavaScript?          
    Use the Array.prototype.push method to append values to the end of an array:
// initialize array
var arr = [
  ""Hi"",
  ""Hello"",
  ""Bonjour""
];

// append new value to the array
arr.push(""Hola"");

console.log(arr);


You can use the push() function to append more than one value to an array in a single call:
// initialize array
var arr = [""Hi"", ""Hello"", ""Bonjour"", ""Hola""];

// append multiple values to the array
arr.push(""Salut"", ""Hey"");

// display all values
for (var i = 0; i < arr.length; i++) {
  console.log(arr[i]);
}


Update
If you want to add the items of one array to another array, you can use firstArray.concat(secondArray):
var arr = [
  ""apple"",
  ""banana"",
  ""cherry""
];

// Do not forget to assign the result as, unlike push, concat does not change the existing array
arr = arr.concat([
  ""dragonfruit"",
  ""elderberry"",
  ""fig""
]);

console.log(arr);

Update
Just an addition to this answer if you want to prepend any value to the start of an array (i.e. first index) then you can use Array.prototype.unshift for this purpose.
var arr = [1, 2, 3];
arr.unshift(0);
console.log(arr);

It also supports appending multiple values at once just like push.

Update
Another way with ES6 syntax is to return a new array with the spread syntax. This leaves the original array unchanged, but returns a new array with new items appended, compliant with the spirit of functional programming.
const arr = [
  ""Hi"",
  ""Hello"",
  ""Bonjour"",
];

const newArr = [
  ...arr,
  ""Salut"",
];

console.log(newArr);

    If you're only appending a single variable, then push() works just fine. If you need to append another array, use concat():
var ar1 = [1, 2, 3];
var ar2 = [4, 5, 6];

var ar3 = ar1.concat(ar2);

alert(ar1);
alert(ar2);
alert(ar3);

The concat does not affect ar1 and ar2 unless reassigned, for example:
var ar1 = [1, 2, 3];
var ar2 = [4, 5, 6];

ar1 = ar1.concat(ar2);
alert(ar1);

There is a lot of great information on JavaScript Reference.
    I think it's worth mentioning that push can be called with multiple arguments, which will be appended to the array in order. For example:
var arr = ['first'];
arr.push('second', 'third');
console.log(arr);

As a result of this you can use push.apply to append an array to another array like so:
var arr = ['first'];
arr.push('second', 'third');
arr.push.apply(arr, ['forth', 'fifth']);
console.log(arr);

Annotated ES5 has more info on exactly what push and apply do.
2016 update: with spread, you don't need that apply anymore, like:
var arr = ['first'];
arr.push('second', 'third');

arr.push(...['fourth', 'fifth']);
console.log(arr) ;

    Some quick benchmarking (each test = 500k appended elements and the results are averages of multiple runs) showed the following:

Firefox 3.6 (Mac):


Small arrays: arr[arr.length] = b is faster (300ms vs. 800ms)
Large arrays: arr.push(b) is faster (500ms vs. 900ms)


Safari 5.0 (Mac):


Small arrays: arr[arr.length] = b is faster (90ms vs. 115ms)
Large arrays: arr[arr.length] = b is faster (160ms vs. 185ms)


Google Chrome 6.0 (Mac):


Small arrays: No significant difference (and Chrome is FAST! Only ~38ms !!)
Large arrays: No significant difference (160ms)


I like the arr.push() syntax better, but I think I'd be better off with the arr[arr.length] Version, at least in raw speed. I'd love to see the results of an IE run though.



My benchmarking loops:

function arrpush_small() {
    var arr1 = [];
    for (a = 0; a < 100; a++)
    {
        arr1 = [];
        for (i = 0; i < 5000; i++)
        {
            arr1.push('elem' + i);
        }
    }
}

function arrlen_small() {
    var arr2 = [];
    for (b = 0; b < 100; b++)
    {
        arr2 = [];
        for (j = 0; j < 5000; j++)
        {
            arr2[arr2.length] = 'elem' + j;
        }
    }
}


function arrpush_large() {
    var arr1 = [];
    for (i = 0; i < 500000; i++)
    {
        arr1.push('elem' + i);
    }
}

function arrlen_large() {
    var arr2 = [];
    for (j = 0; j < 500000; j++)
    {
        arr2[arr2.length] = 'elem' + j;
    }
}

    With the new ES6 spread operator, joining two arrays using push becomes even easier:

var arr = [1, 2, 3, 4, 5];
var arr2 = [6, 7, 8, 9, 10];
arr.push(...arr2);
console.log(arr);


This adds the contents of arr2 onto the end of arr.

Babel REPL Example
    You can do it using JavaScript Spread Operator Syntax:
// Initialize the array

var arr = [
    ""Hi"",
    ""Hello"",
    ""Bangladesh""
];

// Append a new value to the array

arr = [...arr, ""Feni""]; 

// Or you can add a variable value

var testValue = ""Cool"";

arr = [...arr, testValue ];

console.log(arr);

// Final output [ 'Hi', 'Hello', 'Bangladesh', 'Feni', 'Cool' ]

    If arr is an array, and val is the value you wish to add use:

arr.push(val);


E.g.

var arr = ['a', 'b', 'c'];
arr.push('d');
console.log(arr);

    You can use the push and apply functions to append two arrays.
var array1 = [11, 32, 75];
var array2 = [99, 67, 34];

Array.prototype.push.apply(array1, array2);
console.log(array1);

It will append array2 to array1. Now array1 contains [11, 32, 75, 99, 67, 34].
This code is much simpler than writing for loops to copy each and every items in the array.
    Append a single element
// Append to the end
arrName.push('newName1');

// Prepend to the start
arrName.unshift('newName1');

// Insert at index 1
arrName.splice(1, 0,'newName1');
// 1: index number, 0: number of element to remove, newName1: new element


// Replace index 3 (of exists), add new element otherwise.
arrName[3] = 'newName1';

Append multiple elements
// Insert from index number 1
arrName.splice(1, 0,'newElemenet1', 'newElemenet2', 'newElemenet3');
// 1: index number from where insert starts,
// 0: number of element to remove,
//newElemenet1,2,3: new elements

Append an array
// Join two or more arrays
arrName.concat(newAry1, newAry2);
//newAry1,newAry2: Two different arrays which are to be combined (concatenated) to an existing array

    Use concat:

a = [1, 2, 3];
b = [3, 4, 5];
a = a.concat(b);
console.log(a);

    There are a couple of ways to append an array in JavaScript:

1) The push() method adds one or more elements to the end of an array and returns the new length of the array.

var a = [1, 2, 3];
a.push(4, 5);
console.log(a);


Output:

[1, 2, 3, 4, 5]


2) The unshift() method adds one or more elements to the beginning of an array and returns the new length of the array:

var a = [1, 2, 3];
a.unshift(4, 5);
console.log(a); 


Output:

[4, 5, 1, 2, 3]


3) The concat() method is used to merge two or more arrays. This method does not change the existing arrays, but instead returns a new array.

var arr1 = [""a"", ""b"", ""c""];
var arr2 = [""d"", ""e"", ""f""];
var arr3 = arr1.concat(arr2);
console.log(arr3);


Output:

[ ""a"", ""b"", ""c"", ""d"", ""e"", ""f"" ]


4) You can use the array's .length property to add an element to the end of the array:

var ar = ['one', 'two', 'three'];
ar[ar.length] = 'four';
console.log( ar ); 


Output:

 [""one"", ""two"", ""three"", ""four""]


5) The splice() method changes the content of an array by removing existing elements and/or adding new elements:

var myFish = [""angel"", ""clown"", ""mandarin"", ""surgeon""];
myFish.splice(4, 0, ""nemo"");
//array.splice(start, deleteCount, item1, item2, ...)
console.log(myFish);


Output:

[""angel"", ""clown"", ""mandarin"", ""surgeon"",""nemo""]


6) You can also add a new element to an array simply by specifying a new index and assigning a value:

var ar = ['one', 'two', 'three'];
ar[3] = 'four'; // add new element to ar
console.log(ar);


Output:

[""one"", ""two"",""three"",""four""]

    Now, you can take advantage of ES6 syntax and just do
let array = [1, 2];
console.log([...array, 3]);

keeping the original array immutable.
    JavaScript with the ECMAScript 5 (ES5) standard which is supported by most browsers now, you can use apply() to append array1 to array2.
var array1 = [3, 4, 5];
var array2 = [1, 2];

Array.prototype.push.apply(array2, array1);

console.log(array2); // [1, 2, 3, 4, 5]

JavaScript with ECMAScript 6 (ES6) standard which is supported by Chrome, Firefox, Internet Explorer, and Edge, you can use the spread operator:
""use strict"";
let array1 = [3, 4, 5];
let array2 = [1, 2];

array2.push(...array1);

console.log(array2); // [1, 2, 3, 4, 5]

The spread operator will replace array2.push(...array1); with array2.push(3, 4, 5); when the browser is thinking the logic.
Bonus point
If you'd like to create another variable to store all the items from both arrays, you can do this:
ES5 var combinedArray = array1.concat(array2);
ES6 const combinedArray = [...array1, ...array2]
The spread operator (...) is to spread out all items from a collection.
    If you want to append two arrays -

var a = ['a', 'b'];
var b = ['c', 'd'];


then you could use:

var c = a.concat(b);


And if you want to add record g to array  (var a=[]) then you could use:

a.push('g');

    You can use the push() if you want to add values,
e.g. arr.push(""Test1"", ""Test2"");.
If you have array you can use concat(), e.g. Array1.concat(Array2).
If you have just one element to add, you can also try the length method, e.g. array[aray.length] = 'test';.
    Appending items on an array
let fruits = [""orange"", ""banana"", ""apple"", ""lemon""]; /* Array declaration */

fruits.push(""avacado""); /* Adding an element to the array */

/* Displaying elements of the array */

for(var i=0; i < fruits.length; i++){
  console.log(fruits[i]);
}

    Append a single item

To append a single item to an array, use the push() method provided by the Array object:

const fruits = ['banana', 'pear', 'apple']
fruits.push('mango')
console.log(fruits)


push() mutates the original array.

To create a new array instead, use the concat() Array method:

const fruits = ['banana', 'pear', 'apple']
const allfruits = fruits.concat('mango')
console.log(allfruits)


Notice that concat() does not actually add an item to the array, but creates a new array, which you can assign to another variable, or reassign to the original array (declaring it as let, as you cannot reassign a const):

const fruits = ['banana', 'pear', 'apple']
const allfruits = fruits.concat('mango')
console.log(allfruits)


let fruits = ['banana', 'pear', 'apple']
fruits = fruits.concat('mango')


Append multiple items

To append a multiple item to an array, you can use push() by calling it with multiple arguments:

const fruits = ['banana', 'pear', 'apple']
fruits.push('mango', 'melon', 'avocado')
console.log(fruits)


You can also use the concat() method you saw before, passing a list of items separated by a comma:

const fruits = ['banana', 'pear', 'apple']
const allfruits = fruits.concat('mango', 'melon', 'avocado')
console.log(allfruits)


or an array:

const fruits = ['banana', 'pear', 'apple']
const allfruits = fruits.concat(['mango', 'melon', 'avocado'])
console.log(allfruits)


Remember that as described previously this method does not mutate the original array, but it  returns a new array.


  Originally posted at 

    We don't have an append function for Array in JavaScript, but we have push and unshift. Imagine you have the array below:
var arr = [1, 2, 3, 4, 5];

And we like to append a value to this array. We can do arr.push(6), and it will add 6 to the end of the array:
arr.push(6); // Returns [1, 2, 3, 4, 5, 6];

Also we can use unshift, look at how we can apply this:
arr.unshift(0); // Returns [0, 1, 2, 3, 4, 5];

They are main functions to add or append new values to the arrays.
    Append a value to an array

Since Array.prototype.push adds one or more elements to the end of an array and returns the new length of the array, sometimes we want just to get the new up-to-date array so we can do something like so:

const arr = [1, 2, 3];
const val = 4;

arr.concat([val]); // [1, 2, 3, 4]


Or just:

[...arr, val] // [1, 2, 3, 4]

    concat(), of course, can be used with two-dimensional arrays as well. No looping required.
var a = [
    [1, 2],
    [3, 4] ];

var b = [
    [""a"", ""b""],
    [""c"", ""d""] ];

b = b.concat(a);

alert(b[2][1]); // Result: 2

    The push() method adds new items to the end of an array, and returns the new length. Example:

var fruits = [""Banana"", ""Orange"", ""Apple"", ""Mango""];
fruits.push(""Kiwi"");

// The result of fruits will be:
Banana, Orange, Apple, Mango, Kiwi


The exact answer to your question is already answered, but let's look at some other ways to add items to an array.

The unshift() method adds new items to the beginning of an array, and returns the new length. Example:

var fruits = [""Banana"", ""Orange"", ""Apple"", ""Mango""];
fruits.unshift(""Lemon"", ""Pineapple"");

// The result of fruits will be:
Lemon, Pineapple, Banana, Orange, Apple, Mango


And lastly, the concat() method is used to join two or more arrays. Example:

var fruits = [""Banana"", ""Orange""];
var moreFruits = [""Apple"", ""Mango"", ""Lemon""];
var allFruits = fruits.concat(moreFruits);

// The values of the children array will be:
Banana, Orange, Apple, Mango, Lemon

    If you know the highest index (such as stored in a variable ""i"") then you can do
myArray[i + 1] = someValue;

However, if you don't know then you can either use
myArray.push(someValue);

as other answers suggested, or you can use
myArray[myArray.length] = someValue;

Note that the array is zero based so .length returns the highest index plus one.
Also note that you don't have to add in order and you can actually skip values, as in
myArray[myArray.length + 1000] = someValue;

In which case the values in between will have a value of undefined.
It is therefore a good practice when looping through a JavaScript to verify that a value actually exists at that point.
This can be done by something like the following:
if(myArray[i] === ""undefined""){ continue; }

If you are certain that you don't have any zeros in the array then you can just do:
if(!myArray[i]){ continue; }

Of course, make sure in this case that you don't use as the condition myArray[i] (as some people over the Internet suggest based on the end that as soon as i is greater than the highest index, it will return undefined which evaluates to false).
    
  Just want to add a snippet for non-destructive addition of an element.


var newArr = oldArr.concat([newEl]);

    push() adds a new element to the end of an array.
pop() removes an element from the end of an array.
To append an object (such as a string or number) to an array, use:
array.push(toAppend);

    Let the array length property do the work:
myarray[myarray.length] = 'new element value added to the end of the array';

myarray.length returns the number of strings in the array.
JavaScript is zero-based, so the next element key of the array will be the current length of the array.
Example:
var myarray = [0, 1, 2, 3],
    myarrayLength = myarray.length; // myarrayLength is set to 4

    If you are using ES6 you can use spread operator to do it.
var arr = [
    ""apple"",
    ""banana"",
    ""cherry""
];

var arr2 = [
    ""dragonfruit"",
    ""elderberry"",
    ""fig""
];

arr.push(...arr2);

    You .push() that value in. 
Example: array.push(value);
    If you want to append a single value into an array, simply use the push method. It will add a new element at the end of the array.

But if you intend to add multiple elements then store the elements in a new array and concat the second array with the first array...either way you wish.

arr=['a','b','c'];
arr.push('d');
//now print the array in console.log and it will contain 'a','b','c','d' as elements.
console.log(array);

    If you want to combine two arrays without the duplicate you may try the code below:
array_merge = function (arr1, arr2) {
  return arr1.concat(arr2.filter(function(item){
    return arr1.indexOf(item) < 0;
  }))
}

Usage:
array1 = ['1', '2', '3']
array2 = ['2', '3', '4', '5']
combined_array = array_merge(array1, array2)

Output:
[1,2,3,4,5]
    You can use the push method.
Array.prototype.append = function(destArray){
    destArray = destArray || [];
    this.push.call(this, ...destArray);
    return this;
}
var arr = [1,2,5,67];
var arr1 = [7,4,7,8];
console.log(arr.append(arr1)); // [7, 4, 7, 8, 1, 4, 5, 67, 7]
console.log(arr.append(""Hola"")) // [1, 2, 5, 67, 7, 4, 7, 8, ""H"", ""o"", ""l"", ""a""]

    ","[2891, 5019, 1097, 320, 421, 69, 16, 57, 84, 20, 52, 30, 24, 36, 33, 3, 2, 6, 3, 8, 12, 25, 17, 10, 1, 11, 7, 4, 4, 5, 1]",4432572,423,2008-12-09T00:20:05,2022-03-29 21:09:18Z,javascript 
Deleting an element from an array in PHP,"
                
Is there an easy way to delete an element from an array using PHP, such that foreach ($array) no longer includes that element?

I thought that setting it to null would do it, but apparently it does not work.
    There are different ways to delete an array element, where some are more useful for some specific tasks than others.
Deleting a single array element
If you want to delete just one array element you can use unset() or alternatively \array_splice().
If you know the value and dont know the key to delete the element you can use \array_search() to get the key. This only works if the element does not occur more than once, since \array_search returns the first hit only.
unset()
Note that when you use unset() the array keys wont change. If you want to reindex the keys you can use \array_values() after unset(), which will convert all keys to numerically enumerated keys starting from 0.
Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c""];
unset($array[1]);
          //  Key which you want to delete

Output:
[
    [0] => a
    [2] => c
]

\array_splice() method
If you use \array_splice() the keys will automatically be reindexed, but the associative keys wont change  as opposed to \array_values(), which will convert all keys to numerical keys.
\array_splice() needs the offset, not the key, as the second parameter.
Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c""];
\array_splice($array, 1, 1);
                   //  Offset which you want to delete

Output:
[
    [0] => a
    [1] => c
]

array_splice(), same as unset(), take the array by reference. You dont assign the return values of those functions back to the array.
Deleting multiple array elements
If you want to delete multiple array elements and dont want to call unset() or \array_splice() multiple times you can use the functions \array_diff() or \array_diff_key() depending on whether you know the values or the keys of the elements which you want to delete.
\array_diff() method
If you know the values of the array elements which you want to delete, then you can use \array_diff(). As before with unset() it wont change the keys of the array.
Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c"", 3 => ""c""];
$array = \array_diff($array, [""a"", ""c""]);
                          // 
                          // Array values which you want to delete

Output:
[
    [1] => b
]

\array_diff_key() method
If you know the keys of the elements which you want to delete, then you want to use \array_diff_key(). You have to make sure you pass the keys as keys in the second parameter and not as values. Keys wont reindex.
Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c""];
$array = \array_diff_key($array, [0 => ""xy"", ""2"" => ""xy""]);
                               //            
                               // Array keys which you want to delete

Output:
[
    [1] => b
]

If you want to use unset() or \array_splice() to delete multiple elements with the same value you can use \array_keys() to get all the keys for a specific value and then delete all elements.
\array_filter() method
If you want to delete all elements with a specific value in the array you can use \array_filter().
Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c""];
$array = \array_filter($array, static function ($element) {
    return $element !== ""b"";
    //                   
    // Array value which you want to delete
});

Output:
[
    [0] => a
    [1] => c
]

    It should be noted that unset() will keep indexes untouched, which is what you'd expect when using string indexes (array as hashtable), but can be quite surprising when dealing with integer indexed arrays:

$array = array(0, 1, 2, 3);
unset($array[2]);
var_dump($array);
/* array(3) {
  [0]=>
  int(0)
  [1]=>
  int(1)
  [3]=>
  int(3)
} */

$array = array(0, 1, 2, 3);
array_splice($array, 2, 1);
var_dump($array);
/* array(3) {
  [0]=>
  int(0)
  [1]=>
  int(1)
  [2]=>
  int(3)
} */


So array_splice() can be used if you'd like to normalize your integer keys. Another option is using array_values() after unset():

$array = array(0, 1, 2, 3);

unset($array[2]);
$array = array_values($array);
var_dump($array);
/* array(3) {
  [0]=>
  int(0)
  [1]=>
  int(1)
  [2]=>
  int(3)
} */

      // Our initial array
  $arr = array(""blue"", ""green"", ""red"", ""yellow"", ""green"", ""orange"", ""yellow"", ""indigo"", ""red"");
  print_r($arr);

  // Remove the elements who's values are yellow or red
  $arr = array_diff($arr, array(""yellow"", ""red""));
  print_r($arr);


This is the output from the code above:

Array
(
    [0] => blue
    [1] => green
    [2] => red
    [3] => yellow
    [4] => green
    [5] => orange
    [6] => yellow
    [7] => indigo
    [8] => red
)

Array
(
    [0] => blue
    [1] => green
    [4] => green
    [5] => orange
    [7] => indigo
)


Now, array_values() will reindex a numerical array nicely, but it will remove all key strings from the array and replace them with numbers. If you need to preserve the key names (strings), or reindex the array if all keys are numerical, use array_merge():

$arr = array_merge(array_diff($arr, array(""yellow"", ""red"")));
print_r($arr);


Outputs

Array
(
    [0] => blue
    [1] => green
    [2] => green
    [3] => orange
    [4] => indigo
)

    $key = array_search($needle, $array);
if ($key !== false) {
    unset($array[$key]);
}

    unset($array[$index]);

    If the index is specified:
$arr = ['a', 'b', 'c'];
$index = 0;    
unset($arr[$index]);  // $arr = ['b', 'c']

If we have value instead of index:
$arr = ['a', 'b', 'c'];

// search the value to find index
// Notice! this will only find the first occurrence of value
$index = array_search('a', $arr);

if($index !== false){
   unset($arr[$index]);  // $arr = ['b', 'c']
}

The if condition is necessary
because if index is not found, unset() will automatically delete
the first element of the array which is not what we want.
    Also, for a named element:

unset($array[""elementName""]);

    If you have to delete multiple values in an array and the entries in that array are objects or structured data, array_filter() is your best bet. Those entries that return a true from the callback function will be retained.
$array = [
    ['x'=>1,'y'=>2,'z'=>3], 
    ['x'=>2,'y'=>4,'z'=>6], 
    ['x'=>3,'y'=>6,'z'=>9]
];

$results = array_filter($array, function($value) {
    return $value['x'] > 2; 
}); //=> [['x'=>3,'y'=>6,z=>'9']]

    Edit
If you can't take it as given that the object is in that array you need to add a check:
if(in_array($object,$array)) unset($array[array_search($object,$array)]);

Original Answer
if you want to remove a specific object of an array by reference of that object you can do following:
unset($array[array_search($object,$array)]);

Example:
<?php
class Foo
{
    public $id;
    public $name;
}

$foo1 = new Foo();
$foo1->id = 1;
$foo1->name = 'Name1';

$foo2 = new Foo();
$foo2->id = 2;
$foo2->name = 'Name2';

$foo3 = new Foo();
$foo3->id = 3;
$foo3->name = 'Name3';


$array = array($foo1,$foo2,$foo3);
unset($array[array_search($foo2,$array)]);

echo '<pre>';
var_dump($array);
echo '</pre>';
?>

Result:
array(2) {
[0]=>
    object(Foo)#1 (2) {
        [""id""]=>
        int(1)
        [""name""]=>
        string(5) ""Name1""
    }
[2]=>
    object(Foo)#3 (2) {
        [""id""]=>
        int(3)
        [""name""]=>
        string(5) ""Name3""
    }
}

Note that if the object occures several times it will only be removed the first occurence!
    If you have a numerically indexed array where all values are unique (or they are non-unique but you wish to remove all instances of a particular value), you can simply use array_diff() to remove a matching element, like this:

$my_array = array_diff($my_array, array('Value_to_remove'));


For example:

$my_array = array('Andy', 'Bertha', 'Charles', 'Diana');
echo sizeof($my_array) . ""\n"";
$my_array = array_diff($my_array, array('Charles'));
echo sizeof($my_array);


This displays the following:

4
3


In this example, the element with the value 'Charles' is removed as can be verified by the sizeof() calls that report a size of 4 for the initial array, and 3 after the removal.
    Destroy a single element of an array

unset()

$array1 = array('A', 'B', 'C', 'D', 'E');
unset($array1[2]); // Delete known index(2) value from array
var_dump($array1);


The output will be:

array(4) {
  [0]=>
  string(1) ""A""
  [1]=>
  string(1) ""B""
  [3]=>
  string(1) ""D""
  [4]=>
  string(1) ""E""
}


If you need to re index the array:

$array1 = array_values($array1);
var_dump($array1);


Then the output will be:

array(4) {
  [0]=>
  string(1) ""A""
  [1]=>
  string(1) ""B""
  [2]=>
  string(1) ""D""
  [3]=>
  string(1) ""E""
}


Pop the element off the end of array - return the value of the removed element

mixed array_pop(array &$array)

$stack = array(""orange"", ""banana"", ""apple"", ""raspberry"");
$last_fruit = array_pop($stack);
print_r($stack);
print_r('Last Fruit:'.$last_fruit); // Last element of the array


The output will be

Array
(
    [0] => orange
    [1] => banana
    [2] => apple
)
Last Fruit: raspberry


Remove the first element (red) from an array, - return the value of the removed element

mixed array_shift ( array &$array )

$color = array(""a"" => ""red"", ""b"" => ""green"" , ""c"" => ""blue"");
$first_color = array_shift($color);
print_r ($color);
print_r ('First Color: '.$first_color);


The output will be:

Array
(
    [b] => green
    [c] => blue
)
First Color: red

    <?php
    $stack = [""fruit1"", ""fruit2"", ""fruit3"", ""fruit4""];
    $fruit = array_shift($stack);
    print_r($stack);

    echo $fruit;
?>


Output: 

[
    [0] => fruit2
    [1] => fruit3
    [2] => fruit4
]

fruit1

    If you need to remove multiple elements from an associative array, you can use array_diff_key() (here used with array_flip()):

$my_array = array(
  ""key1"" => ""value 1"",
  ""key2"" => ""value 2"",
  ""key3"" => ""value 3"",
  ""key4"" => ""value 4"",
  ""key5"" => ""value 5"",
);

$to_remove = array(""key2"", ""key4"");

$result = array_diff_key($my_array, array_flip($to_remove));

print_r($result);


Output:

Array ( [key1] => value 1 [key3] => value 3 [key5] => value 5 ) 

    Solutions:


To delete one element, use unset():



unset($array[3]);
unset($array['foo']);




To delete multiple noncontiguous elements, also use unset():



unset($array[3], $array[5]);
unset($array['foo'], $array['bar']);




To delete multiple contiguous elements, use array_splice():



array_splice($array, $offset, $length);



Further explanation:

Using these functions removes all references to these elements from PHP. If you want to keep a key in the array, but with an empty value, assign the empty string to the element:

$array[3] = $array['foo'] = '';


Besides syntax, there's a logical difference between using unset() and assigning '' to the element. The first says This doesn't exist anymore, while the second says This still exists, but its value is the empty string.

If you're dealing with numbers, assigning 0 may be a better alternative. So, if a company stopped production of the model XL1000 sprocket, it would update its inventory with:

unset($products['XL1000']);


However, if it temporarily ran out of XL1000 sprockets, but was planning to receive a new shipment from the plant later this week, this is better:

$products['XL1000'] = 0;


If you unset() an element, PHP adjusts the array so that looping still works correctly. It doesn't compact the array to fill in the missing holes. This is what we mean when we say that all arrays are associative, even when they appear to be numeric. Here's an example:

// Create a ""numeric"" array
$animals = array('ant', 'bee', 'cat', 'dog', 'elk', 'fox');
print $animals[1];  // Prints 'bee'
print $animals[2];  // Prints 'cat'
count($animals);    // Returns 6

// unset()
unset($animals[1]); // Removes element $animals[1] = 'bee'
print $animals[1];  // Prints '' and throws an E_NOTICE error
print $animals[2];  // Still prints 'cat'
count($animals);    // Returns 5, even though $array[5] is 'fox'

// Add a new element
$animals[ ] = 'gnu'; // Add a new element (not Unix)
print $animals[1];  // Prints '', still empty
print $animals[6];  // Prints 'gnu', this is where 'gnu' ended up
count($animals);    // Returns 6

// Assign ''
$animals[2] = '';   // Zero out value
print $animals[2];  // Prints ''
count($animals);    // Returns 6, count does not decrease


To compact the array into a densely filled numeric array, use array_values():

$animals = array_values($animals);


Alternatively, array_splice() automatically reindexes arrays to avoid leaving holes:

// Create a ""numeric"" array
$animals = array('ant', 'bee', 'cat', 'dog', 'elk', 'fox');
array_splice($animals, 2, 2);
print_r($animals);
Array
(
    [0] => ant
    [1] => bee
    [2] => elk
    [3] => fox
)


This is useful if you're using the array as a queue and want to remove items from the queue while still allowing random access. To safely remove the first or last element from an array, use array_shift() and array_pop(), respectively.
    // Remove by value
function removeFromArr($arr, $val)
{
    unset($arr[array_search($val, $arr)]);
    return array_values($arr);
}

    Associative arrays

For associative arrays, use unset:

$arr = array('a' => 1, 'b' => 2, 'c' => 3);
unset($arr['b']);

// RESULT: array('a' => 1, 'c' => 3)




Numeric arrays

For numeric arrays, use array_splice:

$arr = array(1, 2, 3);
array_splice($arr, 1, 1);

// RESULT: array(0 => 1, 1 => 3)


Note

Using unset for numeric arrays will not produce an error, but it will mess up your indexes:

$arr = array(1, 2, 3);
unset($arr[1]);

// RESULT: array(0 => 1, 2 => 3)

    Two ways for removing the first item of an array with keeping order of the index and also if you don't know the key name of the first item.

Solution #1

// 1 is the index of the first object to get
// NULL to get everything until the end
// true to preserve keys
$array = array_slice($array, 1, null, true);


Solution #2

// Rewinds the array's internal pointer to the first element
// and returns the value of the first array element.
$value = reset($array);
// Returns the index element of the current array position
$key = key($array);
unset($array[$key]);




For this sample data:

$array = array(10 => ""a"", 20 => ""b"", 30 => ""c"");


You must have this result:

array(2) {
  [20]=>
  string(1) ""b""
  [30]=>
  string(1) ""c""
}

    unset() destroys the specified variables.

The behavior of unset() inside of a function can vary depending on what type of variable you are attempting to destroy.

If a globalized variable is unset() inside of a function, only the local variable is destroyed. The variable in the calling environment will retain the same value as before unset() was called.

<?php
    function destroy_foo()
    {
        global $foo;
        unset($foo);
    }

    $foo = 'bar';
    destroy_foo();
    echo $foo;
?>


The answer of the above code will be bar.

To unset() a global variable inside of a function:

<?php
    function foo()
    {
        unset($GLOBALS['bar']);
    }

    $bar = ""something"";
    foo();
?>

    unset() multiple, fragmented elements from an array

While unset() has been mentioned here several times, it has yet to be mentioned that unset() accepts multiple variables making it easy to delete multiple, noncontiguous elements from an array in one operation:

// Delete multiple, noncontiguous elements from an array
$array = [ 'foo', 'bar', 'baz', 'quz' ];
unset( $array[2], $array[3] );
print_r($array);
// Output: [ 'foo', 'bar' ]


unset() dynamically

unset() does not accept an array of keys to remove, so the code below will fail (it would have made it slightly easier to use unset() dynamically though).

$array = range(0,5);
$remove = [1,2];
$array = unset( $remove ); // FAILS: ""unexpected 'unset'""
print_r($array);


Instead, unset() can be used dynamically in a foreach loop:

$array = range(0,5);
$remove = [1,2];
foreach ($remove as $k=>$v) {
    unset($array[$v]);
}
print_r($array);
// Output: [ 0, 3, 4, 5 ]


Remove array keys by copying the array

There is also another practice that has yet to be mentioned.
Sometimes, the simplest way to get rid of certain array keys is to simply copy $array1 into $array2.

$array1 = range(1,10);
foreach ($array1 as $v) {
    // Remove all even integers from the array
    if( $v % 2 ) {
        $array2[] = $v;
    }
}
print_r($array2);
// Output: [ 1, 3, 5, 7, 9 ];


Obviously, the same practice applies to text strings:

$array1 = [ 'foo', '_bar', 'baz' ];
foreach ($array1 as $v) {
    // Remove all strings beginning with underscore
    if( strpos($v,'_')===false ) {
        $array2[] = $v;
    }
}
print_r($array2);
// Output: [ 'foo', 'baz' ]

    Follow the default functions:

PHP: unset

unset() destroys the specified variables. For more info, you can refer to PHP unset
$Array = array(""test1"", ""test2"", ""test3"", ""test3"");

unset($Array[2]);


PHP: array_pop

The array_pop() function deletes the last element of an array. For more info, you can refer to PHP array_pop
$Array = array(""test1"", ""test2"", ""test3"", ""test3"");

array_pop($Array);


PHP: array_splice

The array_splice() function removes selected elements from an array and replaces it with new elements. For more info, you can refer to PHP array_splice
$Array = array(""test1"", ""test2"", ""test3"", ""test3"");

array_splice($Array,1,2);


PHP: array_shift

The array_shift() function removes the first element from an array. For more info, you can refer to PHP array_shift
$Array = array(""test1"", ""test2"", ""test3"", ""test3"");

array_shift($Array);

    I'd just like to say I had a particular object that had variable attributes (it was basically mapping a table and I was changing the columns in the table, so the attributes in the object, reflecting the table would vary as well):

class obj {
    protected $fields = array('field1','field2');
    protected $field1 = array();
    protected $field2 = array();
    protected loadfields(){}
    // This will load the $field1 and $field2 with rows of data for the column they describe
    protected function clearFields($num){
        foreach($fields as $field) {
            unset($this->$field[$num]);
            // This did not work the line below worked
            unset($this->{$field}[$num]); // You have to resolve $field first using {}
        }
    }
}


The whole purpose of $fields was just, so I don't have to look everywhere in the code when they're changed, I just look at the beginning of the class and change the list of attributes and the $fields array content to reflect the new attributes.
    Remove an array element based on a key:

Use the unset function like below:

$a = array(
       'salam',
       '10',
       1
);

unset($a[1]);

print_r($a);

/*

    Output:

        Array
        (
            [0] => salam
            [2] => 1
        )

*/


Remove an array element based on value:

Use the array_search function to get an element key and use the above manner to remove an array element like below:

$a = array(
       'salam',
       '10',
       1
);

$key = array_search(10, $a);

if ($key !== false) {
    unset($a[$key]);
}

print_r($a);

/*

    Output:

        Array
        (
            [0] => salam
            [2] => 1
        )

*/

    Suppose you have the following array:

Array
(
    [user_id] => 193
    [storage] => 5
)


To delete storage, do:

unset($attributes['storage']);
$attributes = array_filter($attributes);


And you get:

Array
(
    [user_id] => 193
)

    <?php
    // If you want to remove a particular array element use this method
    $my_array = array(""key1""=>""value 1"", ""key2""=>""value 2"", ""key3""=>""value 3"");

    print_r($my_array);
    if (array_key_exists(""key1"", $my_array)) {
        unset($my_array['key1']);
        print_r($my_array);
    }
    else {
        echo ""Key does not exist"";
    }
?>

<?php
    //To remove first array element
    $my_array = array(""key1""=>""value 1"", ""key2""=>""value 2"", ""key3""=>""value 3"");
    print_r($my_array);
    $new_array = array_slice($my_array, 1);
    print_r($new_array);
?>


<?php
    echo ""<br/>    "";
    // To remove first array element to length
    // starts from first and remove two element
    $my_array = array(""key1""=>""value 1"", ""key2""=>""value 2"", ""key3""=>""value 3"");
    print_r($my_array);
    $new_array = array_slice($my_array, 1, 2);
    print_r($new_array);
?>


   

Output

 Array ( [key1] => value 1 [key2] => value 2 [key3] =>
 value 3 ) Array (    [key2] => value 2 [key3] => value 3 )
 Array ( [key1] => value 1 [key2] => value 2 [key3] => value 3 )
 Array ( [key2] => value 2 [key3] => value 3 )
 Array ( [key1] => value 1 [key2] => value 2 [key3] => value 3 )
 Array ( [key2] => value 2 [key3] => value 3 )

    Use the following code:

$arr = array('orange', 'banana', 'apple', 'raspberry');
$result = array_pop($arr);
print_r($result);

    I came here because I wanted to see if there was a more elegant solution to this problem than using unset($arr[$i]).  To my disappointment these answers are either wrong or do not cover every edge case.
Here is why array_diff() does not work.  Keys are unique in the array, while elements are not always unique.
$arr = [1,2,2,3];

foreach($arr as $i => $n){
    $b = array_diff($arr,[$n]);
    echo ""\n"".json_encode($b);
}

Results...
[2,2,3]
[1,3]
[1,2,2] 

If two elements are the same they will be remove.  This also applies for array_search() and array_flip().
I saw a lot of answers with array_slice() and array_splice(), but these functions only work with numeric arrays.  All the answers I am aware if here does not answer the question, and so here is a solution that will work.
$arr = [1,2,3];

foreach($arr as $i => $n){
    $b = array_merge(array_slice($arr,0,$i),array_slice($arr,$i+1));
    echo ""\n"".json_encode($b);
}

Results...

[2,3];
[1,3];
[1,2];

Since unset($arr[$i]) will work on both associative array and numeric arrays this still does not answer the question.
This solution is to compare the keys and with a tool that will handle both numeric and associative arrays. I use array_diff_uassoc() for this.  This function compares the keys in a call back function.
$arr = [1,2,2,3];
//$arr = ['a'=>'z','b'=>'y','c'=>'x','d'=>'w'];
foreach($arr as $key => $n){
    $b = array_diff_uassoc($arr, [$key=>$n], function($a,$b) {
        if($a != $b){
            return 1;
        }
    });
    echo ""\n"".json_encode($b);
}    

Results.....
[2,2,3];
[1,2,3];
[1,2,2];

['b'=>'y','c'=>'x','d'=>'w'];
['a'=>'z','c'=>'x','d'=>'w'];
['a'=>'z','b'=>'y','d'=>'w'];
['a'=>'z','b'=>'y','c'=>'x'];

    ","[2884, 3347, 1397, 404, 230, 100, 29, 69, 26, 7, 70, 38, 37, 22, 13, 19, 22, 9, 21, 8, 11, 10, 7, 9, 6, 6, 1]",3289817,431,2008-12-15T20:28:55,2022-03-23 14:24:08Z,php 
"When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used?","
                
What are the proper uses of:


static_cast
dynamic_cast
const_cast
reinterpret_cast
C-style cast (type)value
Function-style cast type(value)


How does one decide which to use in which specific cases?
    static_cast is the first cast you should attempt to use. It does things like implicit conversions between types (such as int to float, or pointer to void*), and it can also call explicit conversion functions (or implicit ones). In many cases, explicitly stating static_cast isn't necessary, but it's important to note that the T(something) syntax is equivalent to (T)something and should be avoided (more on that later). A T(something, something_else) is safe, however, and guaranteed to call the constructor.

static_cast can also cast through inheritance hierarchies. It is unnecessary when casting upwards (towards a base class), but when casting downwards it can be used as long as it doesn't cast through virtual inheritance. It does not do checking, however, and it is undefined behavior to static_cast down a hierarchy to a type that isn't actually the type of the object.



const_cast can be used to remove or add const to a variable; no other C++ cast is capable of removing it (not even reinterpret_cast). It is important to note that modifying a formerly const value is only undefined if the original variable is const; if you use it to take the const off a reference to something that wasn't declared with const, it is safe. This can be useful when overloading member functions based on const, for instance. It can also be used to add const to an object, such as to call a member function overload.

const_cast also works similarly on volatile, though that's less common.



dynamic_cast is exclusively used for handling polymorphism. You can cast a pointer or reference to any polymorphic type to any other class type (a polymorphic type has at least one virtual function, declared or inherited). You can use it for more than just casting downwards  you can cast sideways or even up another chain. The dynamic_cast will seek out the desired object and return it if possible. If it can't, it will return nullptr in the case of a pointer, or throw std::bad_cast in the case of a reference.

dynamic_cast has some limitations, though. It doesn't work if there are multiple objects of the same type in the inheritance hierarchy (the so-called 'dreaded diamond') and you aren't using virtual inheritance. It also can only go through public inheritance - it will always fail to travel through protected or private inheritance. This is rarely an issue, however, as such forms of inheritance are rare.



reinterpret_cast is the most dangerous cast, and should be used very sparingly. It turns one type directly into another  such as casting the value from one pointer to another, or storing a pointer in an int, or all sorts of other nasty things. Largely, the only guarantee you get with reinterpret_cast is that normally if you cast the result back to the original type, you will get the exact same value (but not if the intermediate type is smaller than the original type). There are a number of conversions that reinterpret_cast cannot do, too. It's used primarily for particularly weird conversions and bit manipulations, like turning a raw data stream into actual data, or storing data in the low bits of a pointer to aligned data.



C-style cast and function-style cast are casts using (type)object or type(object), respectively, and are functionally equivalent. They are defined as the first of the following which succeeds:


const_cast
static_cast (though ignoring access restrictions)
static_cast (see above), then const_cast
reinterpret_cast
reinterpret_cast, then const_cast


It can therefore be used as a replacement for other casts in some instances, but can be extremely dangerous because of the ability to devolve into a reinterpret_cast, and the latter should be preferred when explicit casting is needed, unless you are sure static_cast will succeed or reinterpret_cast will fail. Even then, consider the longer, more explicit option.

C-style casts also ignore access control when performing a static_cast, which means that they have the ability to perform an operation that no other cast can. This is mostly a kludge, though, and in my mind is just another reason to avoid C-style casts.
    
Use dynamic_cast for converting pointers/references within an inheritance hierarchy.

Use static_cast for ordinary type conversions.

Use reinterpret_cast for low-level reinterpreting of bit patterns.  Use with extreme caution.

Use const_cast for casting away const/volatile.  Avoid this unless you are stuck using a const-incorrect API.


    It might help if you know little bit of internals...
static_cast

C++ compiler already knows how to convert between scaler types such as float to int. Use static_cast for them.
When you ask compiler to convert from type A to B, static_cast calls B's constructor passing A as param. Alternatively, A could have a conversion operator (i.e. A::operator B()).  If B doesn't have such constructor, or A doesn't have a conversion operator, then you get compile time error.
Cast from A* to B* always succeeds if A and B are in inheritance hierarchy (or void) otherwise you get compile error.
Gotcha: If you cast base pointer to derived pointer but if actual object is not really derived type then you don't get error. You get bad pointer and  very likely a segfault at runtime. Same goes for A& to B&.
Gotcha: Cast from Derived to Base or viceversa creates new copy! For people coming from C#/Java, this can be a huge surprise because the result is basically a chopped off object created from Derived.

dynamic_cast

dynamic_cast uses runtime type information to figure out if cast is valid. For example, (Base*) to (Derived*) may fail if pointer is not actually of derived type.
This means, dynamic_cast is very expensive compared to static_cast!
For A* to B*, if cast is invalid then dynamic_cast will return nullptr.
For A& to B& if cast is invalid then dynamic_cast will throw bad_cast exception.
Unlike other casts, there is runtime overhead.

const_cast

While static_cast can do non-const to const it can't go other way around. The const_cast can do both ways.
One example where this comes handy is iterating through some container like set<T> which only returns its elements as const to make sure you don't change its key. However if your intent is to modify object's non-key members then it should be ok. You can use const_cast to remove constness.
Another example is when you want to implement T& SomeClass::foo() as well as const T& SomeClass::foo() const. To avoid code duplication, you can apply const_cast to return value of one function from another.

reinterpret_cast

This basically says that take these bytes at this memory location and think of it as given object.
For example, you can load 4 bytes of float to 4 bytes of int to see how bits in float looks like.
Obviously, if data is not correct for the type, you may get segfault.
There is no runtime overhead for this cast.

    (A lot of theoretical and conceptual explanation has been given above) 

Below are some of the practical examples when I used static_cast, dynamic_cast, const_cast, reinterpret_cast.

(Also referes this to understand the explaination : http://www.cplusplus.com/doc/tutorial/typecasting/)

static_cast :

OnEventData(void* pData)

{
  ......

  //  pData is a void* pData, 

  //  EventData is a structure e.g. 
  //  typedef struct _EventData {
  //  std::string id;
  //  std:: string remote_id;
  //  } EventData;

  // On Some Situation a void pointer *pData
  // has been static_casted as 
  // EventData* pointer 

  EventData *evtdata = static_cast<EventData*>(pData);
  .....
}


dynamic_cast :

void DebugLog::OnMessage(Message *msg)
{
    static DebugMsgData *debug;
    static XYZMsgData *xyz;

    if(debug = dynamic_cast<DebugMsgData*>(msg->pdata)){
        // debug message
    }
    else if(xyz = dynamic_cast<XYZMsgData*>(msg->pdata)){
        // xyz message
    }
    else/* if( ... )*/{
        // ...
    }
}


const_cast :

// *Passwd declared as a const

const unsigned char *Passwd


// on some situation it require to remove its constness

const_cast<unsigned char*>(Passwd)


reinterpret_cast :

typedef unsigned short uint16;

// Read Bytes returns that 2 bytes got read. 

bool ByteBuffer::ReadUInt16(uint16& val) {
  return ReadBytes(reinterpret_cast<char*>(&val), 2);
}

    static_cast vs dynamic_cast vs reinterpret_cast internals view on a downcast/upcast
In this answer, I want to compare these three mechanisms on a concrete upcast/downcast example and analyze what happens to the underlying pointers/memory/assembly to give a concrete understanding of how they compare.
I believe that this will give a good intuition on how those casts are different:

static_cast: does one address offset at runtime (low runtime impact) and no safety checks that a downcast is correct.

dyanamic_cast: does the same address offset at runtime like static_cast, but also and an expensive safety check that a downcast is correct using RTTI.
This safety check allows you to query if a base class pointer is of a given type at runtime by checking a return of nullptr which indicates an invalid downcast.
Therefore, if your code is not able to check for that nullptr and take a valid non-abort action, you should just use static_cast instead of dynamic cast.
If an abort is the only action your code can take, maybe you only want to enable the dynamic_cast in debug builds (-NDEBUG), and use static_cast otherwise, e.g. as done here, to not slow down your fast runs.

reinterpret_cast: does nothing at runtime, not even the address offset. The pointer must point exactly to the correct type, not even a base class works. You generally don't want this unless raw byte streams are involved.


Consider the following code example:
main.cpp
#include <iostream>

struct B1 {
    B1(int int_in_b1) : int_in_b1(int_in_b1) {}
    virtual ~B1() {}
    void f0() {}
    virtual int f1() { return 1; }
    int int_in_b1;
};

struct B2 {
    B2(int int_in_b2) : int_in_b2(int_in_b2) {}
    virtual ~B2() {}
    virtual int f2() { return 2; }
    int int_in_b2;
};

struct D : public B1, public B2 {
    D(int int_in_b1, int int_in_b2, int int_in_d)
        : B1(int_in_b1), B2(int_in_b2), int_in_d(int_in_d) {}
    void d() {}
    int f2() { return 3; }
    int int_in_d;
};

int main() {
    B2 *b2s[2];
    B2 b2{11};
    D *dp;
    D d{1, 2, 3};

    // The memory layout must support the virtual method call use case.
    b2s[0] = &b2;
    // An upcast is an implicit static_cast<>().
    b2s[1] = &d;
    std::cout << ""&d           "" << &d           << std::endl;
    std::cout << ""b2s[0]       "" << b2s[0]       << std::endl;
    std::cout << ""b2s[1]       "" << b2s[1]       << std::endl;
    std::cout << ""b2s[0]->f2() "" << b2s[0]->f2() << std::endl;
    std::cout << ""b2s[1]->f2() "" << b2s[1]->f2() << std::endl;

    // Now for some downcasts.

    // Cannot be done implicitly
    // error: invalid conversion from B2* to D* [-fpermissive]
    // dp = (b2s[0]);

    // Undefined behaviour to an unrelated memory address because this is a B2, not D.
    dp = static_cast<D*>(b2s[0]);
    std::cout << ""static_cast<D*>(b2s[0])            "" << dp           << std::endl;
    std::cout << ""static_cast<D*>(b2s[0])->int_in_d  "" << dp->int_in_d << std::endl;

    // OK
    dp = static_cast<D*>(b2s[1]);
    std::cout << ""static_cast<D*>(b2s[1])            "" << dp           << std::endl;
    std::cout << ""static_cast<D*>(b2s[1])->int_in_d  "" << dp->int_in_d << std::endl;

    // Segfault because dp is nullptr.
    dp = dynamic_cast<D*>(b2s[0]);
    std::cout << ""dynamic_cast<D*>(b2s[0])           "" << dp           << std::endl;
    //std::cout << ""dynamic_cast<D*>(b2s[0])->int_in_d "" << dp->int_in_d << std::endl;

    // OK
    dp = dynamic_cast<D*>(b2s[1]);
    std::cout << ""dynamic_cast<D*>(b2s[1])           "" << dp           << std::endl;
    std::cout << ""dynamic_cast<D*>(b2s[1])->int_in_d "" << dp->int_in_d << std::endl;

    // Undefined behaviour to an unrelated memory address because this
    // did not calculate the offset to get from B2* to D*.
    dp = reinterpret_cast<D*>(b2s[1]);
    std::cout << ""reinterpret_cast<D*>(b2s[1])           "" << dp           << std::endl;
    std::cout << ""reinterpret_cast<D*>(b2s[1])->int_in_d "" << dp->int_in_d << std::endl;
}


Compile, run and disassemble with:
g++ -ggdb3 -O0 -std=c++11 -Wall -Wextra -pedantic -o main.out main.cpp
setarch `uname -m` -R ./main.out
gdb -batch -ex ""disassemble/rs main"" main.out

where setarch is used to disable ASLR to make it easier to compare runs.
Possible output:
&d           0x7fffffffc930
b2s[0]       0x7fffffffc920
b2s[1]       0x7fffffffc940
b2s[0]->f2() 2
b2s[1]->f2() 3
static_cast<D*>(b2s[0])            0x7fffffffc910
static_cast<D*>(b2s[0])->int_in_d  1
static_cast<D*>(b2s[1])            0x7fffffffc930
static_cast<D*>(b2s[1])->int_in_d  3
dynamic_cast<D*>(b2s[0])           0
dynamic_cast<D*>(b2s[1])           0x7fffffffc930
dynamic_cast<D*>(b2s[1])->int_in_d 3
reinterpret_cast<D*>(b2s[1])           0x7fffffffc940
reinterpret_cast<D*>(b2s[1])->int_in_d 32767

Now, as mentioned at: https://en.wikipedia.org/wiki/Virtual_method_table in order to support the virtual method calls efficiently, supposing that the memory data structures of B1 is of form:
B1:
  +0: pointer to virtual method table of B1
  +4: value of int_in_b1

and B2 is of form:
B2:
  +0: pointer to virtual method table of B2
  +4: value of int_in_b2

then memory data structure of D has to look something like:
D:
  +0: pointer to virtual method table of D (for B1)
  +4: value of int_in_b1
  +8: pointer to virtual method table of D (for B2)
 +12: value of int_in_b2
 +16: value of int_in_d

The key fact is that the memory data structure of D contains inside it memory structure identical to that of B1 and B2, i.e.:

+0 looks exactly like a B1, with the B1 vtable for D followed by int_in_b1
+8 looks exactly like a B2, with the B2 vtable for D followed by int_in_b2

Therefore we reach the critical conclusion:

an upcast or downcast only needs to shift the pointer value by a value known at compile time

This way, when D gets passed to the base type array, the type cast actually calculates that offset and points something that looks exactly like a valid B2 in memory, except that this one has the vtable for D instead of B2, and therefore all virtual calls work transparently.
E.g.:
b2s[1] = &d;

simply needs to get the address of d + 8 to reach the corresponding B2-like data structure.
Now, we can finally get back to type casting and the analysis of our concrete example.
From the stdout output we see:
&d           0x7fffffffc930
b2s[1]       0x7fffffffc940

Therefore, the implicit static_cast done there did correctly calculate the offset from the full D data structure at 0x7fffffffc930 to the B2 like one which is at 0x7fffffffc940. We also infer that what lies between 0x7fffffffc930 and 0x7fffffffc940 is likely be the B1 data and vtable.
Then, on the downcast sections, it is now easy to understand how the invalid ones fail and why:

static_cast<D*>(b2s[0])            0x7fffffffc910: the compiler just went up 0x10 at compile time bytes to try and go from a B2 to the containing D
But because b2s[0] was not a D, it now points to an undefined memory region.
The disassembly is:
49          dp = static_cast<D*>(b2s[0]);
   0x0000000000000fc8 <+414>:   48 8b 45 d0     mov    -0x30(%rbp),%rax
   0x0000000000000fcc <+418>:   48 85 c0        test   %rax,%rax
   0x0000000000000fcf <+421>:   74 0a   je     0xfdb <main()+433>
   0x0000000000000fd1 <+423>:   48 8b 45 d0     mov    -0x30(%rbp),%rax
   0x0000000000000fd5 <+427>:   48 83 e8 10     sub    $0x10,%rax
   0x0000000000000fd9 <+431>:   eb 05   jmp    0xfe0 <main()+438>
   0x0000000000000fdb <+433>:   b8 00 00 00 00  mov    $0x0,%eax
   0x0000000000000fe0 <+438>:   48 89 45 98     mov    %rax,-0x68(%rbp)

so we see that GCC does:

check if pointer is NULL, and if yes return NULL
otherwise, subtract 0x10 from it to reach the D which does not exist


dynamic_cast<D*>(b2s[0])           0: C++ actually found that the cast was invalid and returned nullptr!
There is no way this can be done at compile time, and we will confirm that from the disassembly:
59          dp = dynamic_cast<D*>(b2s[0]);
   0x00000000000010ec <+706>:   48 8b 45 d0     mov    -0x30(%rbp),%rax
   0x00000000000010f0 <+710>:   48 85 c0        test   %rax,%rax
   0x00000000000010f3 <+713>:   74 1d   je     0x1112 <main()+744>
   0x00000000000010f5 <+715>:   b9 10 00 00 00  mov    $0x10,%ecx
   0x00000000000010fa <+720>:   48 8d 15 f7 0b 20 00    lea    0x200bf7(%rip),%rdx        # 0x201cf8 <_ZTI1D>
   0x0000000000001101 <+727>:   48 8d 35 28 0c 20 00    lea    0x200c28(%rip),%rsi        # 0x201d30 <_ZTI2B2>
   0x0000000000001108 <+734>:   48 89 c7        mov    %rax,%rdi
   0x000000000000110b <+737>:   e8 c0 fb ff ff  callq  0xcd0 <__dynamic_cast@plt>
   0x0000000000001110 <+742>:   eb 05   jmp    0x1117 <main()+749>
   0x0000000000001112 <+744>:   b8 00 00 00 00  mov    $0x0,%eax
   0x0000000000001117 <+749>:   48 89 45 98     mov    %rax,-0x68(%rbp)

First there is a NULL check, and it returns NULL if th einput is NULL.
Otherwise, it sets up some arguments in the RDX, RSI and RDI and calls __dynamic_cast.
I don't have the patience to analyze this further now, but as others said, the only way for this to work is for __dynamic_cast to access some extra RTTI in-memory data structures that represent the class hierarchy.
It must therefore start from the B2 entry for that table, then walk this class hierarchy until it finds that the vtable for a D typecast from b2s[0].
This is why dynamic cast is potentially expensive! Here is an example where a one liner patch converting a dynamic_cast to a static_cast in a complex project reduced runtime by 33%!.

reinterpret_cast<D*>(b2s[1])           0x7fffffffc940 this one just believes us blindly: we said there is a D at address b2s[1], and the compiler does no offset calculations.
But this is wrong, because D is actually at 0x7fffffffc930, what is at 0x7fffffffc940 is the B2-like structure inside D! So trash gets accessed.
We can confirm this from the horrendous -O0 assembly that just moves the value around:
70          dp = reinterpret_cast<D*>(b2s[1]);
   0x00000000000011fa <+976>:   48 8b 45 d8     mov    -0x28(%rbp),%rax
   0x00000000000011fe <+980>:   48 89 45 98     mov    %rax,-0x68(%rbp)



Related questions:

When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used?
How is dynamic_cast implemented
Downcasting using the 'static_cast' in C++

Tested on Ubuntu 18.04 amd64, GCC 7.4.0.
    In addition to the other answers so far, here is unobvious example where static_cast is not sufficient so that reinterpret_cast is needed. Suppose there is a function which in an output parameter returns pointers to objects of different classes (which do not share a common base class). A real example of such function is CoCreateInstance() (see the last parameter, which is in fact void**). Suppose you request particular class of object from this function, so you know in advance the type for the pointer (which you often do for COM objects). In this case you cannot cast pointer to your pointer into void** with static_cast: you need reinterpret_cast<void**>(&yourPointer). 

In code:

#include <windows.h>
#include <netfw.h>
.....
INetFwPolicy2* pNetFwPolicy2 = nullptr;
HRESULT hr = CoCreateInstance(__uuidof(NetFwPolicy2), nullptr,
    CLSCTX_INPROC_SERVER, __uuidof(INetFwPolicy2),
    //static_cast<void**>(&pNetFwPolicy2) would give a compile error
    reinterpret_cast<void**>(&pNetFwPolicy2) );


However, static_cast works for simple pointers (not pointers to pointers), so the above code can be rewritten to avoid reinterpret_cast (at a price of an extra variable) in the following way:

#include <windows.h>
#include <netfw.h>
.....
INetFwPolicy2* pNetFwPolicy2 = nullptr;
void* tmp = nullptr;
HRESULT hr = CoCreateInstance(__uuidof(NetFwPolicy2), nullptr,
    CLSCTX_INPROC_SERVER, __uuidof(INetFwPolicy2),
    &tmp );
pNetFwPolicy2 = static_cast<INetFwPolicy2*>(tmp);

    While other answers nicely described all differences between C++ casts, I would like to add a short note why you should not use C-style casts (Type) var and Type(var).

For C++ beginners C-style casts look like being the superset operation over C++ casts (static_cast<>(), dynamic_cast<>(), const_cast<>(), reinterpret_cast<>()) and someone could prefer them over the C++ casts. In fact C-style cast is the superset and shorter to write.

The main problem of C-style casts is that they hide developer real intention of the cast. The C-style casts can do virtually all types of casting from normally safe casts done by static_cast<>() and dynamic_cast<>() to potentially dangerous casts like const_cast<>(), where const modifier can be removed so the const variables can be modified and reinterpret_cast<>() that can even reinterpret integer values to pointers.

Here is the sample.

int a=rand(); // Random number.

int* pa1=reinterpret_cast<int*>(a); // OK. Here developer clearly expressed he wanted to do this potentially dangerous operation.

int* pa2=static_cast<int*>(a); // Compiler error.
int* pa3=dynamic_cast<int*>(a); // Compiler error.

int* pa4=(int*) a; // OK. C-style cast can do such cast. The question is if it was intentional or developer just did some typo.

*pa4=5; // Program crashes.


The main reason why C++ casts were added to the language was to allow a developer to clarify his intentions - why he is going to do that cast. By using C-style casts which are perfectly valid in C++ you are making your code less readable and more error prone especially for other developers who didn't create your code. So to make your code more readable and explicit you should always prefer C++ casts over C-style casts.

Here is a short quote from Bjarne Stroustrup's (the author of C++) book The C++ Programming Language 4th edition - page 302.


  This C-style cast is far more dangerous than the named conversion operators
  because the notation is harder to spot in a large program and the kind of conversion intended by the programmer is not explicit.

    Does this answer your question?

I have never used reinterpret_cast, and wonder whether running into a case that needs it isn't a smell of bad design. In the code base I work on dynamic_cast is used a lot. The difference with  static_cast is that a dynamic_cast does runtime checking which may (safer) or may not (more overhead) be what you want (see msdn).
    To understand, let's consider below code snippet:

struct Foo{};
struct Bar{};

int main(int argc, char** argv)
{
    Foo* f = new Foo;

    Bar* b1 = f;                              // (1)
    Bar* b2 = static_cast<Bar*>(f);           // (2)
    Bar* b3 = dynamic_cast<Bar*>(f);          // (3)
    Bar* b4 = reinterpret_cast<Bar*>(f);      // (4)
    Bar* b5 = const_cast<Bar*>(f);            // (5)

    return 0;
}


Only line (4) compiles without error. Only reinterpret_cast can be used to convert a pointer to an object to a pointer to an any unrelated object type.

One this to be noted is: The dynamic_cast would fail at run-time, however on most compilers it will also fail to compile because there are no virtual functions in the struct of the pointer being casted, meaning dynamic_cast will work with only polymorphic class pointers.

When to use C++ cast:


Use static_cast as the equivalent of a C-style cast that does value conversion, or when we need to explicitly up-cast a pointer from a class to its superclass.
Use const_cast to remove the const qualifier.  
Use reinterpret_cast to do unsafe conversions of pointer types to and from integer and other pointer types. Use this only if we know what we are doing and we understand the aliasing issues.

    Nice feature of reinterpret_cast, not mentioned in the other answers, is that it allows us to create a sort of void* pointer for function types. Normally, for object types one uses static_cast to retrieve the original type of a pointer stored in void*:
  int i = 13;
  void *p = &i;
  auto *pi = static_cast<int*>(p);

For functions, we must use reinterpret_cast twice:
#include<iostream>

using any_fcn_ptr_t = void(*)();


void print(int i)
{
   std::cout << i <<std::endl;
}

int main()
{     
  //Create type-erased pointer to function:
  auto any_ptr = reinterpret_cast<any_fcn_ptr_t>(&print);
  
  //Retrieve the original pointer:
  auto ptr = reinterpret_cast< void(*)(int) >(any_ptr);
  
  ptr(7);
}

With reinterpret_cast we can even get a similar sort-of-void* pointer for pointers to member functions.
As with plain void* and static_cast, C++ guarantees that ptr points to print function (as long as we pass the correct type to reinterpret_cast).
    ","[2874, 2865, 391, 128, 228, 15, 17, 12, 18, 5, 0]",664672,1810,2008-12-01T20:11:07,2022-04-16 21:36:05Z,c 
How do you push a tag to a remote repository using Git?,"
                
I have cloned a remote Git repository to my laptop, then I wanted to add a tag so I ran

git tag mytag master


When I run git tag on my laptop the tag mytag is shown. I then want to push this to the remote repository so I have this tag on all my clients, so I run git push but I got the message:


  Everything up-to-date


And if I go to my desktop and run git pull and then git tag no tags are shown. 

I have also tried to do a minor change on a file in the project, then push it to the server. After that I could pull the change from the server to my Desktop computer, but there's still no tag when running git tag on my desktop computer.

How can I push my tag to the remote repository so that all client computers can see it?
    To push a single tag:
git push origin <tag_name>

And the following command should push all tags (not recommended):
# not recommended
git push --tags

    git push --follow-tags
This is a sane option introduced in Git 1.8.3:
git push --follow-tags

It pushes both commits and only tags that are both:

annotated
reachable (an ancestor) from the pushed commits

This is sane because:

you should only push annotated tags to the remote, and keep lightweight tags for local development to avoid tag clashes. See also: What is the difference between an annotated and unannotated tag?
it won't push annotated tags on unrelated branches

It is for those reasons that --tags should be avoided.
Git 2.4 has added the push.followTags option to turn that flag on by default which you can set with:
git config --global push.followTags true

or by adding followTags = true to the [push] section of your ~/.gitconfig file.
    To push specific, one tag do following
git push origin tag_name
    You can push all local tags by simply git push --tags command.  

$ git tag                         # see tag lists
$ git push origin <tag-name>      # push a single tag
$ git push --tags                 # push all local tags 

    To expand on Trevor's answer, you can push a single tag or all of your
tags at once.

Push a Single Tag

git push <remote> <tag>


This is a summary of the relevant documentation that explains this (some
command options omitted for brevity):


git push [[<repository> [<refspec>]]

<refspec>...

  
  The format of a <refspec> parameter isthe source ref <src>,
  followed by a colon :, followed by the destination ref <dst>
  
  The <dst> tells which ref on the remote side is updated with this
  pushIf :<dst> is omitted, the same ref as <src> will be
  updated
  
  tag <tag> means the same as refs/tags/<tag>:refs/tags/<tag>.


Push All of Your Tags at Once

git push --tags <remote>
# Or
git push <remote> --tags


Here is a summary of the relevant documentation (some command options
omitted for brevity):


git push [--all | --mirror | --tags] [<repository> [<refspec>]]

--tags

  
  All refs under refs/tags are pushed, in addition to refspecs explicitly
  listed on the command line.

    Add a tag in your current branch
git tag tag_name

Check if it's created or not
git tag

Push in your remote origin
git push origin tag_name

    
How can I push my tag to the remote repository so that all client computers can see it?

Run this to push mytag to your git origin (eg: GitHub or GitLab)
git push origin refs/tags/mytag

It's better to use the full ""refspec"" as shown above (literally refs/tags/mytag) just in-case mytag is actually v1.0.0 and is ambiguous (eg: because there's a branch also named v1.0.0).
    Tags are not sent to the remote repository by the git push command. We need to explicitly send these tags to the remote server by using the following command:

git push origin <tagname>


We can push all the tags at once by using the below command:

git push origin --tags


Here are some resources for complete details on git tagging:

http://www.cubearticle.com/articles/more/git/git-tag

http://wptheming.com/2011/04/add-remove-github-tags
    I am using git push <remote-name> tag <tag-name> to ensure that I am pushing a tag.  I use it like: git push origin tag v1.0.1.  This pattern is based upon the documentation (man git-push):

OPTIONS
   ...
   <refspec>...
       ...
       tag <tag> means the same as refs/tags/<tag>:refs/tags/<tag>.

    You can push the tags like this git push --tags
    I did something like this :
git push --tags origin <branch-name> <tag-name>

e.g. : git push --tags origin master v2.0

    ","[2859, 4663, 1420, 293, 78, 118, 15, 22, 63, 20, 51, 2]",1510769,523,2011-03-04T15:37:24,2021-12-01 09:48:14Z,
How do I change the size of figures drawn with Matplotlib?,"
                
How do I change the size of figure drawn with Matplotlib?
    figure tells you the call signature:
from matplotlib.pyplot import figure

figure(figsize=(8, 6), dpi=80)

figure(figsize=(1,1)) would create an inch-by-inch image, which would be 80-by-80 pixels unless you also give a different dpi argument.
    Using plt.rcParams
There is also this workaround in case you want to change the size without using the figure environment. So in case you are using plt.plot() for example, you can set a tuple with width and height.
import matplotlib.pyplot as plt
plt.rcParams[""figure.figsize""] = (20,3)

This is very useful when you plot inline (e.g., with IPython Notebook). As asmaier noticed, it is preferable to not put this statement in the same cell of the imports statements.
To reset the global figure size back to default for subsequent plots:
plt.rcParams[""figure.figsize""] = plt.rcParamsDefault[""figure.figsize""]

Conversion to cm
The figsize tuple accepts inches, so if you want to set it in centimetres you have to divide them by 2.54. Have a look at this question.
    If you've already got the figure created, you can use figure.set_size_inches to adjust the figure size:
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(18.5, 10.5)
fig.savefig('test2png.png', dpi=100)

To propagate the size change to an existing GUI window, add forward=True:
fig.set_size_inches(18.5, 10.5, forward=True)

Additionally as Erik Shilts mentioned in the comments you can also use figure.set_dpi to ""[s]et the resolution of the figure in dots-per-inch""
fig.set_dpi(100)

    Please try a simple code as following:

from matplotlib import pyplot as plt
plt.figure(figsize=(1,1))
x = [1,2,3]
plt.plot(x, x)
plt.show()


You need to set the figure size before you plot.
    In case you're looking for a way to change the figure size in Pandas, you could do:
df['some_column'].plot(figsize=(10, 5))

where df is a Pandas dataframe. Or, to use an existing figure or axes:
fig, ax = plt.subplots(figsize=(10, 5))
df['some_column'].plot(ax=ax)

If you want to change the default settings, you could do the following:
import matplotlib

matplotlib.rc('figure', figsize=(10, 5))

For more details, check out the docs: pd.DataFrame.plot.
    
  Deprecation note:
  As per the official Matplotlib guide, usage of the pylab module is no longer recommended. Please consider using the matplotlib.pyplot module instead, as described by this other answer.


The following seems to work:

from pylab import rcParams
rcParams['figure.figsize'] = 5, 10


This makes the figure's width 5 inches, and its height 10 inches. 

The Figure class then uses this as the default value for one of its arguments.
    import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
plt.plot(x,y) ## This is your plot
plt.show()


You can also use:

fig, ax = plt.subplots(figsize=(20, 10))

    Comparison of different approaches to set exact image sizes in pixels
This answer will focus on:

savefig: how to save to a file, not just show on screen
setting the size in pixels

Here is a quick comparison of some of the approaches I've tried with images showing what the give.
Summary of current status: things are messy, not sure if it is a fundamental limitation, or if the use case just didn't get enough attention from devs, I couldn't easily find an upstream discussion about this.
Baseline example without trying to set the image dimensions
Just to have a comparison point:
base.py
#!/usr/bin/env python3

import sys

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

fig, ax = plt.subplots()
print('fig.dpi = {}'.format(fig.dpi))
print('fig.get_size_inches() = ' + str(fig.get_size_inches())
t = np.arange(-10., 10., 1.)
plt.plot(t, t, '.')
plt.plot(t, t**2, '.')
ax.text(0., 60., 'Hello', fontdict=dict(size=25))
plt.savefig('base.png', format='png')

run:
./base.py
identify base.png

outputs:
fig.dpi = 100.0
fig.get_size_inches() = [6.4 4.8]
base.png PNG 640x480 640x480+0+0 8-bit sRGB 13064B 0.000u 0:00.000


My best approach so far: plt.savefig(dpi=h/fig.get_size_inches()[1] height-only control
I think this is what I'll go with most of the time, as it is simple and scales:
get_size.py
#!/usr/bin/env python3

import sys

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

height = int(sys.argv[1])
fig, ax = plt.subplots()
t = np.arange(-10., 10., 1.)
plt.plot(t, t, '.')
plt.plot(t, t**2, '.')
ax.text(0., 60., 'Hello', fontdict=dict(size=25))
plt.savefig(
    'get_size.png',
    format='png',
    dpi=height/fig.get_size_inches()[1]
)

run:
./get_size.py 431

outputs:
get_size.png PNG 574x431 574x431+0+0 8-bit sRGB 10058B 0.000u 0:00.000


and
./get_size.py 1293

outputs:
main.png PNG 1724x1293 1724x1293+0+0 8-bit sRGB 46709B 0.000u 0:00.000


I tend to set just the height because I'm usually most concerned about how much vertical space the image is going to take up in the middle of my text.
plt.savefig(bbox_inches='tight' changes image size
I always feel that there is too much white space around images, and tended to add bbox_inches='tight' from:
Removing white space around a saved image
However, that works by cropping the image, and you won't get the desired sizes with it.
Instead, this other approach proposed in the same question seems to work well:
plt.tight_layout(pad=1)
plt.savefig(...

which gives the exact desired height for height equals 431:

Fixed height, set_aspect, automatically sized width and small margins
Ermmm, set_aspect messes things up again and prevents plt.tight_layout from actually removing the margins... this is an important use case that I don't have a great solution for yet.
Asked at: How to obtain a fixed height in pixels, fixed data x/y aspect ratio and automatically remove remove horizontal whitespace margin in Matplotlib?
plt.savefig(dpi=h/fig.get_size_inches()[1] + width control
If you really need a specific width in addition to height, this seems to work OK:
width.py
#!/usr/bin/env python3

import sys

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

h = int(sys.argv[1])
w = int(sys.argv[2])
fig, ax = plt.subplots()
wi, hi = fig.get_size_inches()
fig.set_size_inches(hi*(w/h), hi)
t = np.arange(-10., 10., 1.)
plt.plot(t, t, '.')
plt.plot(t, t**2, '.')
ax.text(0., 60., 'Hello', fontdict=dict(size=25))
plt.savefig(
    'width.png',
    format='png',
    dpi=h/hi
)

run:
./width.py 431 869

output:
width.png PNG 869x431 869x431+0+0 8-bit sRGB 10965B 0.000u 0:00.000


and for a small width:
./width.py 431 869

output:
width.png PNG 211x431 211x431+0+0 8-bit sRGB 6949B 0.000u 0:00.000


So it does seem that fonts are scaling correctly, we just get some trouble for very small widths with labels getting cut off, e.g. the 100 on the top left.
I managed to work around those with Removing white space around a saved image
plt.tight_layout(pad=1)

which gives:
width.png PNG 211x431 211x431+0+0 8-bit sRGB 7134B 0.000u 0:00.000


From this, we also see that tight_layout removes a lot of the empty space at the top of the image, so I just generally always use it.
Fixed magic base height, dpi on fig.set_size_inches and plt.savefig(dpi= scaling
I believe that this is equivalent to the approach mentioned at: https://stackoverflow.com/a/13714720/895245
magic.py
#!/usr/bin/env python3

import sys

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

magic_height = 300
w = int(sys.argv[1])
h = int(sys.argv[2])
dpi = 80
fig, ax = plt.subplots(dpi=dpi)
fig.set_size_inches(magic_height*w/(h*dpi), magic_height/dpi)
t = np.arange(-10., 10., 1.)
plt.plot(t, t, '.')
plt.plot(t, t**2, '.')
ax.text(0., 60., 'Hello', fontdict=dict(size=25))
plt.savefig(
    'magic.png',
    format='png',
    dpi=h/magic_height*dpi,
)

run:
./magic.py 431 231

outputs:
magic.png PNG 431x231 431x231+0+0 8-bit sRGB 7923B 0.000u 0:00.000


And to see if it scales nicely:
./magic.py 1291 693

outputs:
magic.png PNG 1291x693 1291x693+0+0 8-bit sRGB 25013B 0.000u 0:00.000


So we see that this approach also does work well. The only problem I have with it is that you have to set that magic_height parameter or equivalent.
Fixed DPI + set_size_inches
This approach gave a slightly wrong pixel size, and it makes it is hard to scale everything seamlessly.
set_size_inches.py
#!/usr/bin/env python3

import sys

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

w = int(sys.argv[1])
h = int(sys.argv[2])
fig, ax = plt.subplots()
fig.set_size_inches(w/fig.dpi, h/fig.dpi)
t = np.arange(-10., 10., 1.)
plt.plot(t, t, '.')
plt.plot(t, t**2, '.')
ax.text(
    0,
    60.,
    'Hello',
    # Keep font size fixed independently of DPI.
    # https://stackoverflow.com/questions/39395616/matplotlib-change-figsize-but-keep-fontsize-constant
    fontdict=dict(size=10*h/fig.dpi),
)
plt.savefig(
    'set_size_inches.png',
    format='png',
)

run:
./set_size_inches.py 431 231

outputs:
set_size_inches.png PNG 430x231 430x231+0+0 8-bit sRGB 8078B 0.000u 0:00.000

so the height is slightly off, and the image:

The pixel sizes are also correct if I make it 3 times larger:
./set_size_inches.py 1291 693

outputs:
set_size_inches.png PNG 1291x693 1291x693+0+0 8-bit sRGB 19798B 0.000u 0:00.000


We understand from this however that for this approach to scale nicely, you need to make every DPI-dependant setting proportional to the size in inches.
In the previous example, we only made the ""Hello"" text proportional, and it did retain its height between 60 and 80 as we'd expect. But everything for which we didn't do that, looks tiny, including:

line width of axes
tick labels
point markers

SVG
I could not find how to set it for SVG images, my approaches only worked for PNG e.g.:
get_size_svg.py
#!/usr/bin/env python3

import sys

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

height = int(sys.argv[1])
fig, ax = plt.subplots()
t = np.arange(-10., 10., 1.)
plt.plot(t, t, '.')
plt.plot(t, t**2, '.')
ax.text(0., 60., 'Hello', fontdict=dict(size=25))
plt.savefig(
    'get_size_svg.svg',
    format='svg',
    dpi=height/fig.get_size_inches()[1]
)

run:
./get_size_svg.py 431

and the generated output contains:
<svg height=""345.6pt"" version=""1.1"" viewBox=""0 0 460.8 345.6"" width=""460.8pt""

and identify says:
get_size_svg.svg SVG 614x461 614x461+0+0 8-bit sRGB 17094B 0.000u 0:00.000

and if I open it in Chromium 86 the browser debug tools mouse image hover confirm that height as 460.79.
But of course, since SVG is a vector format, everything should in theory scale, so you can just convert to any fixed sized format without loss of resolution, e.g.:
inkscape -h 431 get_size_svg.svg -b FFF -e get_size_svg.png

gives the exact height:

I use Inkscape instead of Imagemagick's convert here because you need to mess with -density as well to get sharp SVG resizes with ImageMagick:

https://superuser.com/questions/598849/imagemagick-convert-how-to-produce-sharp-resized-png-files-from-svg-files/1602059#1602059
How to convert a SVG to a PNG with ImageMagick?

And setting <img height="""" on the HTML should also just work for the browser.
Tested on matplotlib==3.2.2.
    This works well for me:
from matplotlib import pyplot as plt

F = plt.gcf()
Size = F.get_size_inches()
F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.
plt.show() # Or plt.imshow(z_array) if using an animation, where z_array is a matrix or NumPy array

This forum post might also help: Resizing figure windows
    You can simply use (from matplotlib.figure.Figure):

fig.set_size_inches(width,height)


As of Matplotlib 2.0.0, changes to your canvas will be visible immediately, as the forward keyword defaults to True.

If you want to just change the width or height instead of both, you can use 

fig.set_figwidth(val) or fig.set_figheight(val)

These will also immediately update your canvas, but only in Matplotlib 2.2.0 and newer.

For Older Versions

You need to specify forward=True explicitly in order to live-update your canvas in versions older than what is specified above. Note that the set_figwidth and set_figheight functions dont support the forward parameter in versions older than Matplotlib 1.5.0.
    Try commenting out the fig = ... line

%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

N = 50
x = np.random.rand(N)
y = np.random.rand(N)
area = np.pi * (15 * np.random.rand(N))**2

fig = plt.figure(figsize=(18, 18))
plt.scatter(x, y, s=area, alpha=0.5)
plt.show()

    The following will surely work, but make sure you add the line plt.figure(figsize=(20,10)) above plt.plot(x,y), plt.pie(), etc.
import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
plt.plot(x,y) ## This is your plot
plt.show()

The code is copied from amalik2205.
    The first link in Google for 'matplotlib figure size' is AdjustingImageSize (Google cache of the page).

Here's a test script from the above page. It creates test[1-3].png files of different sizes of the same image:

#!/usr/bin/env python
""""""
This is a small demo file that helps teach how to adjust figure sizes
for matplotlib

""""""

import matplotlib
print ""using MPL version:"", matplotlib.__version__
matplotlib.use(""WXAgg"") # do this before pylab so you don'tget the default back end.

import pylab
import numpy as np

# Generate and plot some simple data:
x = np.arange(0, 2*np.pi, 0.1)
y = np.sin(x)

pylab.plot(x,y)
F = pylab.gcf()

# Now check everything with the defaults:
DPI = F.get_dpi()
print ""DPI:"", DPI
DefaultSize = F.get_size_inches()
print ""Default size in Inches"", DefaultSize
print ""Which should result in a %i x %i Image""%(DPI*DefaultSize[0], DPI*DefaultSize[1])
# the default is 100dpi for savefig:
F.savefig(""test1.png"")
# this gives me a 797 x 566 pixel image, which is about 100 DPI

# Now make the image twice as big, while keeping the fonts and all the
# same size
F.set_size_inches( (DefaultSize[0]*2, DefaultSize[1]*2) )
Size = F.get_size_inches()
print ""Size in Inches"", Size
F.savefig(""test2.png"")
# this results in a 1595x1132 image

# Now make the image twice as big, making all the fonts and lines
# bigger too.

F.set_size_inches( DefaultSize )# resetthe size
Size = F.get_size_inches()
print ""Size in Inches"", Size
F.savefig(""test3.png"", dpi = (200)) # change the dpi
# this also results in a 1595x1132 image, but the fonts are larger.


Output:

using MPL version: 0.98.1
DPI: 80
Default size in Inches [ 8.  6.]
Which should result in a 640 x 480 Image
Size in Inches [ 16.  12.]
Size in Inches [ 16.  12.]


Two notes:


The module comments and the actual output differ.
This answer allows easily to combine all three images in one image file to see the difference in sizes.

    Use this:
plt.figure(figsize=(width,height))

The width and height are in inches.
If not provided, it defaults to rcParams[""figure.figsize""] = [6.4, 4.8]. See more here.
    Generalizing and simplifying psihodelia's answer:
If you want to change the current size of the figure by a factor sizefactor:
import matplotlib.pyplot as plt

# Here goes your code

fig_size = plt.gcf().get_size_inches() # Get current size
sizefactor = 0.8 # Set a zoom factor
# Modify the current size by the factor
plt.gcf().set_size_inches(sizefactor * fig_size) 

After changing the current size, it might occur that you have to fine tune the subplot layout. You can do that in the figure window GUI, or by means of the command subplots_adjust
For example,
plt.subplots_adjust(left=0.16, bottom=0.19, top=0.82)

    This is How  I printed my Custom Graph with Custom Size
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

figure(figsize=(16, 8), dpi=80)
plt.plot(x_test, color = 'red', label = 'Predicted Price')
plt.plot(y_test, color = 'blue', label = 'Actual Price')
plt.title('Dollar to PKR Prediction')
plt.xlabel('Predicted Price')
plt.ylabel('Actual Dollar Price')
plt.legend()
plt.show()
           

    When creating a new figure, you can specify the size (in inches) with the figsize argument:

import matplotlib.pyplot as plt
fig = plt.figure(figsize=(w,h))


If you want to modify an existing figure, use the set_size_inches() method:

fig.set_size_inches(w,h)


If you want to change the default figure size (6.4"" x 4.8""), use ""run commands"" rc:

plt.rc('figure', figsize=(w,h))


    To increase the size of your figure N times, you need to insert this just before your pl.show():
N = 2
params = pl.gcf()
plSize = params.get_size_inches()
params.set_size_inches((plSize[0]*N, plSize[1]*N))

It also works well with an IPython notebook.
    import random
import math
import matplotlib.pyplot as plt
start=-20
end=20
x=[v for v in range(start,end)]
#sigmoid function
def sigmoid(x):
    return 1/(1+math.exp(x))
plt.figure(figsize=(8,5))#setting the figure size
plt.scatter([abs(v) for v in x],[sigmoid(v) for v in x])
plt.scatter(x,[sigmoid(sigmoid(v)) for v in x])


    This resizes the figure immediately even after the figure has been drawn (at least using Qt4Agg/TkAgg - but not MacOSX - with Matplotlib 1.4.0):
matplotlib.pyplot.get_current_fig_manager().resize(width_px, height_px)

    Since Matplotlib isn't able to use the metric system natively, if you want to specify the size of your figure in a reasonable unit of length such as centimeters, you can do the following (code from gns-ank):

def cm2inch(*tupl):
    inch = 2.54
    if isinstance(tupl[0], tuple):
        return tuple(i/inch for i in tupl[0])
    else:
        return tuple(i/inch for i in tupl)


Then you can use:

plt.figure(figsize=cm2inch(21, 29.7))

    Another option is to use the rc() function in Matplotlib (the unit is inch):
import matplotlib
matplotlib.rc('figure', figsize=[10,5])

    Here is an example from my own.
Below I've given you your answer, and I've expanded for you to experiment with.
Also note fig size values are measured in inches
import matplotlib.pyplot as plt

data = [2,5,8,10,15] # Random data, can use existing data frame column

fig, axs = plt.subplots(figsize = (20,6)) # This is your answer to resize the figure

# The below will help you expand on your question and resize individual elements within your figure. Experiement with the below parameters.
axs.set_title(""Data"", fontsize = 17.5)
axs.tick_params(axis = 'x', labelsize = 14)
axs.set_xlabel('X Label Here', size = 15)
axs.tick_params(axis = 'y', labelsize =14)
axs.set_ylabel('Y Label Here', size = 15)

plt.plot(data)

Output:

    You could also try to initialize this:
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rcParams['figure.figsize'] = (8, 6)
mpl.rcParams['axes.grid'] = False

    I always use the following pattern:
x_inches = 150*(1/25.4)     # [mm]*constant
y_inches = x_inches*(0.8)
dpi = 96

fig = plt.figure(1, figsize = (x_inches,y_inches), dpi = dpi, constrained_layout = True)

With this example you are able to set figure dimensions in inches or in millimetres. When setting constrained_layout to True, plots fill your figure without borders.
    ","[2854, 1608, 609, 1008, 404, 173, 423, 84, 23, 43, 58, 48, 16, 84, 14, 16, 8, 7, 21, 3, 12, 14, 8, 2, 1, 1]",4813159,535,2008-12-01T21:24:44,2022-04-25 00:44:48Z,python 
Manually raising (throwing) an exception in Python,"
                
How can I raise an exception in Python so that it can later be caught via an except block?
    
How do I manually throw/raise an exception in Python?

Use the most specific Exception constructor that semantically fits your issue.
Be specific in your message, e.g.:
raise ValueError('A very specific bad thing happened.')

Don't raise generic exceptions
Avoid raising a generic Exception. To catch it, you'll have to catch all other more specific exceptions that subclass it.
Problem 1: Hiding bugs
raise Exception('I know Python!') # Don't! If you catch, likely to hide bugs.

For example:
def demo_bad_catch():
    try:
        raise ValueError('Represents a hidden bug, do not catch this')
        raise Exception('This is the exception you expect to handle')
    except Exception as error:
        print('Caught this error: ' + repr(error))

>>> demo_bad_catch()
Caught this error: ValueError('Represents a hidden bug, do not catch this',)

Problem 2: Won't catch
And more specific catches won't catch the general exception:
def demo_no_catch():
    try:
        raise Exception('general exceptions not caught by specific handling')
    except ValueError as e:
        print('we will not catch exception: Exception')
 

>>> demo_no_catch()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 3, in demo_no_catch
Exception: general exceptions not caught by specific handling

Best Practices: raise statement
Instead, use the most specific Exception constructor that semantically fits your issue.
raise ValueError('A very specific bad thing happened')

which also handily allows an arbitrary number of arguments to be passed to the constructor:
raise ValueError('A very specific bad thing happened', 'foo', 'bar', 'baz') 

These arguments are accessed by the args attribute on the Exception object. For example:
try:
    some_code_that_may_raise_our_value_error()
except ValueError as err:
    print(err.args)

prints
('message', 'foo', 'bar', 'baz')    

In Python 2.5, an actual message attribute was added to BaseException in favor of encouraging users to subclass Exceptions and stop using args, but the introduction of message and the original deprecation of args has been retracted.
Best Practices: except clause
When inside an except clause, you might want to, for example, log that a specific type of error happened, and then re-raise. The best way to do this while preserving the stack trace is to use a bare raise statement. For example:
logger = logging.getLogger(__name__)

try:
    do_something_in_app_that_breaks_easily()
except AppError as error:
    logger.error(error)
    raise                 # just this!
    # raise AppError      # Don't do this, you'll lose the stack trace!

Don't modify your errors... but if you insist.
You can preserve the stacktrace (and error value) with sys.exc_info(), but this is way more error prone and has compatibility problems between Python 2 and 3, prefer to use a bare raise to re-raise.
To explain - the sys.exc_info() returns the type, value, and traceback.
type, value, traceback = sys.exc_info()

This is the syntax in Python 2 - note this is not compatible with Python 3:
raise AppError, error, sys.exc_info()[2] # avoid this.
# Equivalently, as error *is* the second object:
raise sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]

If you want to, you can modify what happens with your new raise - e.g. setting new args for the instance:
def error():
    raise ValueError('oops!')

def catch_error_modify_message():
    try:
        error()
    except ValueError:
        error_type, error_instance, traceback = sys.exc_info()
        error_instance.args = (error_instance.args[0] + ' <modification>',)
        raise error_type, error_instance, traceback

And we have preserved the whole traceback while modifying the args. Note that this is not a best practice and it is invalid syntax in Python 3 (making keeping compatibility much harder to work around).
>>> catch_error_modify_message()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 3, in catch_error_modify_message
  File ""<stdin>"", line 2, in error
ValueError: oops! <modification>

In Python 3:
raise error.with_traceback(sys.exc_info()[2])

Again: avoid manually manipulating tracebacks. It's less efficient and more error prone. And if you're using threading and sys.exc_info you may even get the wrong traceback (especially if you're using exception handling for control flow - which I'd personally tend to avoid.)
Python 3, Exception chaining
In Python 3, you can chain Exceptions, which preserve tracebacks:
raise RuntimeError('specific message') from error

Be aware:

this does allow changing the error type raised, and
this is not compatible with Python 2.

Deprecated Methods:
These can easily hide and even get into production code. You want to raise an exception, and doing them will raise an exception, but not the one intended!
Valid in Python 2, but not in Python 3 is the following:
raise ValueError, 'message' # Don't do this, it's deprecated!

Only valid in much older versions of Python (2.4 and lower), you may still see people raising strings:
raise 'message' # really really wrong. don't do this.

In all modern versions, this will actually raise a TypeError, because you're not raising a BaseException type. If you're not checking for the right exception and don't have a reviewer that's aware of the issue, it could get into production.
Example Usage
I raise Exceptions to warn consumers of my API if they're using it incorrectly:
def api_func(foo):
    '''foo should be either 'baz' or 'bar'. returns something very useful.'''
    if foo not in _ALLOWED_ARGS:
        raise ValueError('{foo} wrong, use ""baz"" or ""bar""'.format(foo=repr(foo)))

Create your own error types when apropos

""I want to make an error on purpose, so that it would go into the except""

You can create your own error types, if you want to indicate something specific is wrong with your application, just subclass the appropriate point in the exception hierarchy:
class MyAppLookupError(LookupError):
    '''raise this when there's a lookup error for my app'''

and usage:
if important_key not in resource_dict and not ok_to_be_missing:
    raise MyAppLookupError('resource is missing, and that is not ok.')

    
DON'T DO THIS. Raising a bare Exception is absolutely not the right thing to do; see Aaron Hall's excellent answer instead.

Can't get much more pythonic than this:
raise Exception(""I know python!"")

Replace Exception with the specific type of exception you want to throw.
See the raise statement docs for python if you'd like more info.
    In Python3 there are 4 different syntaxes for raising exceptions:
1. raise exception 
2. raise exception (args) 
3. raise
4. raise exception (args) from original_exception


1. raise exception vs. 2. raise exception (args)

If you use raise exception (args)  to raise an exception then the   args will be printed when you print the exception object - as shown in the example below.
  #raise exception (args)
    try:
        raise ValueError(""I have raised an Exception"")
    except ValueError as exp:
        print (""Error"", exp)     # Output -> Error I have raised an Exception 



  #raise exception 
    try:
        raise ValueError
    except ValueError as exp:
        print (""Error"", exp)     # Output -> Error 


3.raise

The raise statement without any arguments re-raises the last exception.
This is useful if you need to perform some actions after catching the exception and  then want to re-raise it. But if there was no exception before, raise statement raises  TypeError Exception.
def somefunction():
    print(""some cleaning"")

a=10
b=0 
result=None

try:
    result=a/b
    print(result)
    
except Exception:            #Output ->
    somefunction()           #some cleaning
    raise                    #Traceback (most recent call last):
                             #File ""python"", line 8, in <module>
                             #ZeroDivisionError: division by zero


4. raise exception (args) from original_exception

This statement is used to create exception chaining in which an exception that is raised in response to another exception can contain the details of the original exception - as shown in the example below.
class MyCustomException(Exception):
pass

a=10
b=0 
reuslt=None
try:
    try:
        result=a/b

    except ZeroDivisionError as exp:
        print(""ZeroDivisionError -- "",exp)
        raise MyCustomException(""Zero Division "") from exp
    
except MyCustomException as exp:
        print(""MyException"",exp)
        print(exp.__cause__)
        

Output:
ZeroDivisionError --  division by zero
MyException Zero Division 
division by zero

    To catch all the exceptions, use the BaseException, it's on the top of the  Exception hierarchy.
#1 Catch the Exception
try:
    #Do something
except BaseException as error:
    print('An exception occurred: {}'.format(error))

#2 Raise the Exception
try:
    #Do something
except BaseException as error:
    raise 'An exception occurred: {}'.format(error)

The above code supports Python 2.7 to latest version.
Reference: https://docs.python.org/3.9/library/exceptions.html#exception-hierarchy
    Read the existing answers first, this is just an addendum.

Notice that you can raise exceptions with or without arguments.

Example:

raise SystemExit


exits the program but you might want to know what happened.So you can use this.

raise SystemExit(""program exited"")


this will print ""program exited"" to stderr before closing the program.
    For the common case where you need to throw an exception in response to some unexpected conditions, and that you never intend to catch, but simply to fail fast to enable you to debug from there if it ever happens  the most logical one seems to be AssertionError:

if 0 < distance <= RADIUS:
    #Do something.
elif RADIUS < distance:
    #Do something.
else:
    raise AssertionError(""Unexpected value of 'distance'!"", distance)

    Just to note: there are times when you DO want to handle generic exceptions. If you're processing a bunch of files and logging your errors, you might want to catch any error that occurs for a file, log it, and continue processing the rest of the files. In that case, a
try:
    foo() 
except Exception as e:
    print(e) # Print out handled error

block is a good way to do it. You'll still want to raise specific exceptions so you know what they mean, though.
    Another way to throw an exceptions is assert. You can use assert to verify a condition is being fulfilled if not then it will raise AssertionError. For more details have a look here.

def avg(marks):
    assert len(marks) != 0,""List is empty.""
    return sum(marks)/len(marks)

mark2 = [55,88,78,90,79]
print(""Average of mark2:"",avg(mark2))

mark1 = []
print(""Average of mark1:"",avg(mark1))

    You might also want to raise custom exceptions. For example, if you're writing a library, it's a very good practice to make a base exception class for your module, and then have custom sub-exceptions to be more specific.
You can achieve that like this:
class MyModuleBaseClass(Exception):
    pass

class MoreSpecificException(MyModuleBaseClass):
    pass


# To raise custom exceptions, you can just
# use the raise keyword
raise MoreSpecificException
raise MoreSpecificException('message')

If you're not interested in having a custom base class, you can just inherit your custom exception classes from an ordinary exception class like Exception, TypeError, ValueError, etc.
    if you don`t care about the raised exception do:
def crash(): return 0/0

the good old division be 0
    If you don't care about which error to raise, you could use assert to raise an AssertionError:
>>> assert False, ""Manually raised error""
Traceback (most recent call last):
  File ""<pyshell#24>"", line 1, in <module>
    assert False, ""Manually raised error""
AssertionError: Manually raised error
>>> 

The assert keyword raises an AssertionError if the condition is False, in this case we specified False directly so it raises the error, but to have it have a text we want it to raise to, we add a comma and specify the error text we want, in this case I wrote Manually raised error and it this raise it with that text.
    You should learn the raise statement of python for that.
It should be kept inside the try block.
Example -

try:
    raise TypeError            #remove TypeError by any other error if you want
except TypeError:
    print('TypeError raised')

    ","[2849, 3745, 558, 76, 6, 17, 40, 6, 5, 2, 1, 1, 2]",2441697,808,2010-01-12T21:07:40,2022-04-29 06:02:38Z,python 
"How can I check for ""undefined"" in JavaScript? [duplicate]","
                    
            
        
            
                
                    
                        This question already has answers here:
                        
                    
                
            
                    
                        Detecting an undefined object property
                            
                                (50 answers)
                            
                    
                    
                        How to check a not-defined variable in JavaScript
                            
                                (15 answers)
                            
                    
                    
                        How to handle 'undefined' in JavaScript [duplicate]
                            
                                (3 answers)
                            
                    
                    
                        How can I check if a variable exist in JavaScript?
                            
                                (8 answers)
                            
                    
                Closed 7 years ago.
        

    

What is the most appropriate way to test if a variable is undefined in JavaScript?
I've seen several possible ways:
if (window.myVariable)

Or
if (typeof(myVariable) != ""undefined"")

Or
if (myVariable) // This throws an error if undefined. Should this be in Try/Catch?

    If you are interested in finding out whether a variable has been declared regardless of its value, then using the in operator is the safest way to go. Consider this example:

// global scope
var theFu; // theFu has been declared, but its value is undefined
typeof theFu; // ""undefined""


But this may not be the intended result for some cases, since the variable or property was declared but just not initialized. Use the in operator for a more robust check.

""theFu"" in window; // true
""theFoo"" in window; // false


If you are interested in knowing whether the variable hasn't been declared or has the value undefined, then use the typeof operator, which is guaranteed to return a string:

if (typeof myVar !== 'undefined')


Direct comparisons against undefined are troublesome as undefined can be overwritten. 

window.undefined = ""foo"";
""foo"" == undefined // true


As @CMS pointed out, this has been patched in ECMAScript 5th ed., and undefined is non-writable.

if (window.myVar) will also include these falsy values, so it's not very robust:


false
0
""""
NaN
null
undefined


Thanks to @CMS for pointing out that your third case - if (myVariable) can also throw an error in two cases. The first is when the variable hasn't been defined which throws a ReferenceError. 

// abc was never declared.
if (abc) {
    // ReferenceError: abc is not defined
} 


The other case is when the variable has been defined, but has a getter function which throws an error when invoked. For example,

// or it's a property that can throw an error
Object.defineProperty(window, ""myVariable"", { 
    get: function() { throw new Error(""W00t?""); }, 
    set: undefined 
});
if (myVariable) {
    // Error: W00t?
}

    I personally use 

myVar === undefined


Warning: Please note that === is used over == and that myVar has been previously declared (not defined).



I do not like typeof myVar === ""undefined"". I think it is long winded and unnecessary. (I can get the same done in less code.)

Now some people will keel over in pain when they read this, screaming: ""Wait! WAAITTT!!! undefined can be redefined!""

Cool. I know this. Then again, most variables in Javascript can be redefined. Should you never use any built-in identifier that can be redefined?

If you follow this rule, good for you: you aren't a hypocrite.

The thing is, in order to do lots of real work in JS, developers need to rely on redefinable identifiers to be what they are. I don't hear people telling me that I shouldn't use setTimeout because someone can

window.setTimeout = function () {
    alert(""Got you now!"");
};


Bottom line, the ""it can be redefined"" argument to not use a raw === undefined is bogus. 

(If you are still scared of undefined being redefined, why are you blindly integrating untested library code into your code base? Or even simpler: a linting tool.)



Also, like the typeof approach, this technique can ""detect"" undeclared variables: 

if (window.someVar === undefined) {
    doSomething();
}


But both these techniques leak in their abstraction. I urge you not to use this or even 

if (typeof myVar !== ""undefined"") {
    doSomething();
}


Consider:

var iAmUndefined;


To catch whether or not that variable is declared or not, you may need to resort to the in operator. (In many cases, you can simply read the code O_o).

if (""myVar"" in window) {
    doSomething();
}


But wait! There's more! What if some prototype chain magic is happening? Now even the superior in operator does not suffice. (Okay, I'm done here about this part except to say that for 99% of the time, === undefined (and ****cough**** typeof) works just fine. If you really care, you can read about this subject on its own.)
    2020 Update

One of my reasons for preferring a typeof check (namely, that undefined can be redefined) became irrelevant with the mass adoption of ECMAScript 5. The other, that you can use typeof to check the type of an undeclared variable, was always niche. Therefore, I'd now recommend using a direct comparison in most situations:

myVariable === undefined


Original answer from 2010

Using typeof is my preference. It will work when the variable has never been declared, unlike any comparison with the == or === operators or type coercion using if. (undefined, unlike null, may also be redefined in ECMAScript 3 environments, making it unreliable for comparison, although nearly all common environments now are compliant with ECMAScript 5 or above).

if (typeof someUndeclaredVariable == ""undefined"") {
    // Works
}

if (someUndeclaredVariable === undefined) { 
    // Throws an error
}

    You can use typeof, like this:

if (typeof something != ""undefined"") {
    // ...
}

    Update 2018-07-25

It's been nearly five years since this post was first made, and JavaScript has come a long way.  In repeating the tests in the original post, I found no consistent difference between the following test methods:


abc === undefined
abc === void 0
typeof abc == 'undefined'
typeof abc === 'undefined'


Even when I modified the tests to prevent Chrome from optimizing them away, the differences were insignificant.  As such, I'd now recommend abc === undefined for clarity.

Relevant content from chrome://version:


Google Chrome: 67.0.3396.99 (Official Build) (64-bit) (cohort: Stable)
Revision: a337fbf3c2ab8ebc6b64b0bfdce73a20e2e2252b-refs/branch-heads/3396@{#790}
OS: Windows
JavaScript: V8 6.7.288.46
User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36


Original post 2013-11-01

In Google Chrome, the following was ever so slightly faster than a typeof test:

if (abc === void 0) {
    // Undefined
}


The difference was negligible.  However, this code is more concise, and clearer at a glance to someone who knows what void 0 means. Note, however, that abc must still be declared.

Both typeof and void were significantly faster than comparing directly against undefined.  I used the following test format in the Chrome developer console:

var abc;
start = +new Date();
for (var i = 0; i < 10000000; i++) {
    if (TEST) {
        void 1;
    }
}
end = +new Date();
end - start;


The results were as follows:

Test: | abc === undefined      abc === void 0      typeof abc == 'undefined'
------+---------------------------------------------------------------------
x10M  |     13678 ms               9854 ms                 9888 ms
  x1  |    1367.8 ns              985.4 ns                988.8 ns


Note that the first row is in milliseconds, while the second row is in nanoseconds.  A difference of 3.4 nanoseconds is nothing.  The times were pretty consistent in subsequent tests.
    if (typeof foo == 'undefined') {
 // Do something
};


Note that strict comparison (!==) is not necessary in this case, since typeof will always return a string.
    If it is undefined, it will not be equal to a string that contains the characters ""undefined"", as the string is not undefined.

You can check the type of the variable:

if (typeof(something) != ""undefined"") ...


Sometimes you don't even have to check the type. If the value of the variable can't evaluate to false when it's set (for example if it's a function), then you can just evalue the variable. Example:

if (something) {
  something(param);
}

    Since none of the other answers helped me, I suggest doing this. It worked for me in InternetExplorer8:

if (typeof variable_name.value === 'undefined') {
    // variable_name is undefined
}

    Personally, I always use the following:

var x;
if( x === undefined) {
    //Do something here
}
else {
   //Do something else here
}


The window.undefined property is non-writable in all modern browsers (JavaScript 1.8.5 or later). From Mozilla's documentation: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/undefined, I see this: One reason to use typeof() is that it does not throw an error if the variable has not been defined.

I prefer to have the approach of using

x === undefined 


because it fails and blows up in my face rather than silently passing/failing if x has not been declared before. This alerts me that x is not declared. I believe all variables used in JavaScript should be declared.
    In this article I read that frameworks like Underscore.js use this function:

function isUndefined(obj){
    return obj === void 0;
}

    The most reliable way I know of checking for undefined is to use void 0.

This is compatible with newer and older browsers, alike, and cannot be overwritten like window.undefined can in some cases.

if( myVar === void 0){
    //yup it's undefined
}

    Some scenarios illustrating the results of the various answers:
http://jsfiddle.net/drzaus/UVjM4/

(Note that the use of var for in tests make a difference when in a scoped wrapper)

Code for reference:

(function(undefined) {
    var definedButNotInitialized;
    definedAndInitialized = 3;
    someObject = {
        firstProp: ""1""
        , secondProp: false
        // , undefinedProp not defined
    }
    // var notDefined;

    var tests = [
        'definedButNotInitialized in window',
        'definedAndInitialized in window',
        'someObject.firstProp in window',
        'someObject.secondProp in window',
        'someObject.undefinedProp in window',
        'notDefined in window',

        '""definedButNotInitialized"" in window',
        '""definedAndInitialized"" in window',
        '""someObject.firstProp"" in window',
        '""someObject.secondProp"" in window',
        '""someObject.undefinedProp"" in window',
        '""notDefined"" in window',

        'typeof definedButNotInitialized == ""undefined""',
        'typeof definedButNotInitialized === typeof undefined',
        'definedButNotInitialized === undefined',
        '! definedButNotInitialized',
        '!! definedButNotInitialized',

        'typeof definedAndInitialized == ""undefined""',
        'typeof definedAndInitialized === typeof undefined',
        'definedAndInitialized === undefined',
        '! definedAndInitialized',
        '!! definedAndInitialized',

        'typeof someObject.firstProp == ""undefined""',
        'typeof someObject.firstProp === typeof undefined',
        'someObject.firstProp === undefined',
        '! someObject.firstProp',
        '!! someObject.firstProp',

        'typeof someObject.secondProp == ""undefined""',
        'typeof someObject.secondProp === typeof undefined',
        'someObject.secondProp === undefined',
        '! someObject.secondProp',
        '!! someObject.secondProp',

        'typeof someObject.undefinedProp == ""undefined""',
        'typeof someObject.undefinedProp === typeof undefined',
        'someObject.undefinedProp === undefined',
        '! someObject.undefinedProp',
        '!! someObject.undefinedProp',

        'typeof notDefined == ""undefined""',
        'typeof notDefined === typeof undefined',
        'notDefined === undefined',
        '! notDefined',
        '!! notDefined'
    ];

    var output = document.getElementById('results');
    var result = '';
    for(var t in tests) {
        if( !tests.hasOwnProperty(t) ) continue; // bleh

        try {
            result = eval(tests[t]);
        } catch(ex) {
            result = 'Exception--' + ex;
        }
        console.log(tests[t], result);
        output.innerHTML += ""\n"" + tests[t] + "": "" + result;
    }
})();


And results:

definedButNotInitialized in window: true
definedAndInitialized in window: false
someObject.firstProp in window: false
someObject.secondProp in window: false
someObject.undefinedProp in window: true
notDefined in window: Exception--ReferenceError: notDefined is not defined
""definedButNotInitialized"" in window: false
""definedAndInitialized"" in window: true
""someObject.firstProp"" in window: false
""someObject.secondProp"" in window: false
""someObject.undefinedProp"" in window: false
""notDefined"" in window: false
typeof definedButNotInitialized == ""undefined"": true
typeof definedButNotInitialized === typeof undefined: true
definedButNotInitialized === undefined: true
! definedButNotInitialized: true
!! definedButNotInitialized: false
typeof definedAndInitialized == ""undefined"": false
typeof definedAndInitialized === typeof undefined: false
definedAndInitialized === undefined: false
! definedAndInitialized: false
!! definedAndInitialized: true
typeof someObject.firstProp == ""undefined"": false
typeof someObject.firstProp === typeof undefined: false
someObject.firstProp === undefined: false
! someObject.firstProp: false
!! someObject.firstProp: true
typeof someObject.secondProp == ""undefined"": false
typeof someObject.secondProp === typeof undefined: false
someObject.secondProp === undefined: false
! someObject.secondProp: true
!! someObject.secondProp: false
typeof someObject.undefinedProp == ""undefined"": true
typeof someObject.undefinedProp === typeof undefined: true
someObject.undefinedProp === undefined: true
! someObject.undefinedProp: true
!! someObject.undefinedProp: false
typeof notDefined == ""undefined"": true
typeof notDefined === typeof undefined: true
notDefined === undefined: Exception--ReferenceError: notDefined is not defined
! notDefined: Exception--ReferenceError: notDefined is not defined
!! notDefined: Exception--ReferenceError: notDefined is not defined

    // x has not been defined before
if (typeof x === 'undefined') { // Evaluates to true without errors.
   // These statements execute.
}

if (x === undefined) { // Throws a ReferenceError

}

    On the contrary of @Thomas Eding answer:

If I forget to declare myVar in my code, then I'll get myVar is not defined.

Let's take a real example:

I've a variable name, but I am not sure if it is declared somewhere or not.

Then @Anurag's answer will help:

var myVariableToCheck = 'myVar';
if (window[myVariableToCheck] === undefined)
    console.log(""Not declared or declared, but undefined."");

// Or you can check it directly 
if (window['myVar'] === undefined) 
    console.log(""Not declared or declared, but undefined."");

        var x;
    if (x === undefined) {
        alert (""I am declared, but not defined."")
    };
    if (typeof y === ""undefined"") {
        alert (""I am not even declared."")
    };

    /* One more thing to understand: typeof ==='undefined' also checks 
       for if a variable is declared, but no value is assigned. In other 
       words, the variable is declared, but not defined. */

    // Will repeat above logic of x for typeof === 'undefined'
    if (x === undefined) {
        alert (""I am declared, but not defined."")
    };
    /* So typeof === 'undefined' works for both, but x === undefined 
       only works for a variable which is at least declared. */

    /* Say if I try using typeof === undefined (not in quotes) for 
       a variable which is not even declared, we will get run a 
       time error. */

    if (z === undefined) {
        alert (""I am neither declared nor defined."")
    };
    // I got this error for z ReferenceError: z is not defined 

    I use it as a function parameter and exclude it on function execution that way I get the ""real"" undefined. Although it does require you to put your code inside a function. I found this while reading the jQuery source.

undefined = 2;

(function (undefined) {
   console.log(undefined); // prints out undefined
   // and for comparison:
   if (undeclaredvar === undefined) console.log(""it works!"")
})()


Of course you could just use typeof though. But all my code is usually inside a containing function anyways, so using this method probably saves me a few bytes here and there.
    ","[2840, 3059, 1419, 313, 102, 69, 23, 27, 7, 14, 17, 11, 18, 5, 4, 3, 0]",3171057,636,2010-08-02T17:53:29,2020-07-24 23:25:26Z,javascript 
"What is the difference between #include <filename> and #include ""filename""?","
                
In the C and C++ programming languages, what is the difference between using angle brackets and using quotes in an include statement, as follows?


#include <filename> 
#include ""filename""

    In practice, the difference is in the location where the preprocessor searches for the included file. 

For #include <filename> the preprocessor searches in an implementation dependent manner, normally in search directories pre-designated by the compiler/IDE. This method is normally used to include standard library header files.

For #include ""filename"" the preprocessor searches first in the same directory as the file containing the directive, and then follows the search path used for the #include <filename> form. This method is normally used to include programmer-defined header files.

A more complete description is available in the GCC documentation on search paths.
    The only way to know is to read your implementation's documentation.

In the C standard, section 6.10.2, paragraphs 2 to 4 state:


  
  A preprocessing directive of the form

#include <h-char-sequence> new-line

  
  searches a sequence of implementation-defined places for a header identified uniquely by the specified sequence between the < and > delimiters, and causes the replacement of that directive by the entire contents of the header. How the places are specified or the header identified is implementation-defined.
  A preprocessing directive of the form

#include ""q-char-sequence"" new-line

  
  causes the replacement of that directive by the entire contents of the source file identified by the specified sequence between the "" delimiters. The named source file is searched for in an implementation-defined manner. If this search is not supported, or if the search fails, the directive is reprocessed as if it read

#include <h-char-sequence> new-line

  
  with the identical contained sequence (including > characters, if any) from the original
  directive.
  A preprocessing directive of the form

#include pp-tokens new-line

  
  (that does not match one of the two previous forms) is permitted. The preprocessing tokens after include in the directive are processed just as in normal text. (Each identifier currently defined as a macro name is replaced by its replacement list of preprocessing tokens.) The directive resulting after all replacements shall match one of the two previous forms. The method by which a sequence of preprocessing tokens between a < and a > preprocessing token pair or a pair of "" characters is combined into a single header name preprocessing token is implementation-defined.
  
  
  Definitions:
  
  
  h-char: any member of the source character set except the new-line character and >
  q-char: any member of the source character set except the new-line character and ""
  

    The sequence of characters between < and > uniquely refer to a header, which isn't necessarily a file. Implementations are pretty much free to use the character sequence as they wish. (Mostly, however, just treat it as a file name and do a search in the include path, as the other posts state.)

If the #include ""file"" form is used, the implementation first looks for a file of the given name, if supported. If not (supported), or if the search fails, the implementation behaves as though the other (#include <file>) form was used.

Also, a third form exists and is used when the #include directive doesn't match either of the forms above. In this form, some basic preprocessing (such as macro expansion) is done on the ""operands"" of the #include directive, and the result is expected to match one of the two other forms.
    The exact behavior of the preprocessor varies between compilers. The following answer applies for GCC and several other compilers.
#include <file.h> tells the compiler to search for the header in its ""includes"" directory, e.g. for MinGW the compiler would search for file.h in C:\MinGW\include\ or wherever your compiler is installed.
#include ""file"" tells the compiler to search the current directory (i.e. the directory in which the source file resides) for file.
You can use the -I flag for GCC to tell it that, when it encounters an include with angled brackets, it should also search for headers in the directory after -I. GCC will treat the directory after the flag as if it were the includes directory.
For instance, if you have a file called myheader.h in your own directory, you could say #include <myheader.h> if you called GCC with the flag -I . (indicating that it should search for includes in the current directory.)
Without the -I flag, you will have to use #include ""myheader.h"" to include the file, or move myheader.h to the include directory of your compiler.
    Some good answers here make references to the C standard but forgot the POSIX standard, especially the specific behavior of the c99 (e.g. C compiler) command.

According to The Open Group Base Specifications Issue 7,


  -I  directory
  
  Change the algorithm for searching for headers whose names are not absolute pathnames to look in the directory named by the directory pathname before looking in the usual places. Thus, headers whose names are enclosed in double-quotes ( """" ) shall be searched for first in the directory of the file with the #include line, then in directories named in -I options, and last in the usual places. For headers whose names are enclosed in angle brackets ( ""<>"" ), the header shall be searched for only in directories named in -I options and then in the usual places. Directories named in -I options shall be searched in the order specified. Implementations shall support at least ten instances of this option in a single c99 command invocation.


So, in a POSIX compliant environment, with a POSIX compliant C compiler, #include ""file.h"" is likely going to search for ./file.h first, where . is the directory where is the file with the #include statement, while #include <file.h>, is likely going to search for /usr/include/file.h first, where /usr/include is your system defined usual places for headers (it's seems not defined by POSIX).
    GCC documentation says the following about the difference between the two:


  Both user and system header files are included using the preprocessing     directive #include. It has two variants:
  
  
    #include <file>
    
    This variant is used for system header files. It searches for a file named file in a standard list of system directories. You can prepend directories to this list with the -I option (see Invocation).
    
    #include ""file""
    
    This variant is used for header files of your own program. It searches for a file named file first in the directory containing the current file, then in the quote directories and then the same directories used for <file>. You can prepend directories to the list of quote directories with the -iquote option.
    The argument of #include, whether delimited with quote marks or angle brackets, behaves like a string constant in that comments are not recognized, and macro names are not expanded. Thus, #include <x/*y> specifies inclusion of a system header file named x/*y.
    
    However, if backslashes occur within file, they are considered ordinary text characters, not escape characters. None of the character escape sequences appropriate to string constants in C are processed. Thus,#include ""x\n\\y""specifies a filename containing three backslashes. (Some systems interpret \ as a pathname separator. All of these also interpret / the same way. It is most portable to use only /.)
    
    It is an error if there is anything (other than comments) on the line after the file name.
  

    The <file> include tells the preprocessor to search in -I directories and in predefined directories first, then in the .c file's directory. The ""file"" include tells the preprocessor to search the source file's directory first, and then revert to -I and predefined. All destinations are searched anyway, only the order of search is different.

The 2011 standard mostly discusses the include files in ""16.2 Source file inclusion"".


  2 A preprocessing directive of the form
  
  # include <h-char-sequence> new-line
  
  searches a sequence of implementation-defined places for a header identified uniquely by the
  specified sequence between the < and > delimiters, and causes the
  replacement of that directive by the entire contents of the header.
  How the places are specified or the header identified is
  implementation-defined.
  
  3 A preprocessing directive of the form
  
  # include ""q-char-sequence"" new-line
  
  causes the replacement of that directive by the entire contents of the source file identified by the
  specified sequence between the "" delimiters. The named source file is
  searched for in an implementation-defined manner. If this search is
  not supported, or if the search fails, the directive is reprocessed as
  if it read
  
  # include <h-char-sequence> new-line
  
  with the identical contained sequence (including > characters, if any) from the original directive.


Note that ""xxx"" form degrades to <xxx> form if the file is not found. The rest is implementation-defined.
    It does:

""mypath/myfile"" is short for ./mypath/myfile


with . being either the directory of the file where the #include is contained in, and/or the current working directory of the compiler, and/or the default_include_paths

and

<mypath/myfile> is short for <defaultincludepaths>/mypath/myfile


If ./ is in <default_include_paths>, then it doesn't make a difference.

If mypath/myfile is in another include directory, the behavior is undefined.
    At least for GCC version <= 3.0, the angle-bracket form does not generate a dependency between the included file and the including one.

So if you want to generate dependency rules (using the GCC -M option for exemple), you must use the quoted form for the files that should be included in the dependency tree.

(See http://gcc.gnu.org/onlinedocs/cpp/Invocation.html )
    When you use #include <filename>, the pre-processor looking for the file in directory of C\C++ header files (stdio.h\cstdio, string, vector, etc.). But, when you use #include ""filename"" first, the pre-processor is looking for the file in the current directory, and if it doesn't here - the pre-processor will look for it in the directory of C\C++ header files.
    By the standard - yes, they are different:


  
  A preprocessing directive of the form

#include <h-char-sequence> new-line

  
  searches a sequence of implementation-defined places for a header identified uniquely by the specified sequence between the < and > delimiters, and causes the replacement of that directive by the entire contents of the header. How the places are specified or the header identified is implementation-defined.
  A preprocessing directive of the form

#include ""q-char-sequence"" new-line

  
  causes the replacement of that directive by the entire contents of the source file identified by the specified sequence between the "" delimiters. The named source file is searched for in an implementation-defined manner. If this search is not supported, or if the search fails, the directive is reprocessed as if it read

#include <h-char-sequence> new-line

  
  with the identical contained sequence (including > characters, if any) from the original
  directive.
  A preprocessing directive of the form

#include pp-tokens new-line

  
  (that does not match one of the two previous forms) is permitted. The preprocessing tokens after include in the directive are processed just as in normal text. (Each identifier currently defined as a macro name is replaced by its replacement list of preprocessing tokens.) The directive resulting after all replacements shall match one of the two previous forms. The method by which a sequence of preprocessing tokens between a < and a > preprocessing token pair or a pair of "" characters is combined into a single header name preprocessing token is implementation-defined.
  
  
  Definitions:
  
  
  h-char: any member of the source character set except the new-line character and >
  q-char: any member of the source character set except the new-line character and ""
  


Note that the standard does not tell any relation between the implementation-defined manners. The first form searches in one implementation-defined way, and the other in a (possibly other) implementation-defined way. The standard also specifies that certain include files shall be present (for example, <stdio.h>).

Formally you'd have to read the manual for your compiler, however normally (by tradition) the #include ""..."" form searches the directory of the file in which the #include was found first, and then the directories that the #include <...> form searches (the include path, eg system headers).
    Many of the answers here focus on the paths the compiler will search in order to find the file. While this is what most compilers do, a conforming compiler is allowed to be preprogrammed with the effects of the standard headers, and to treat, say, #include <list> as a switch, and it need not exist as a file at all.

This is not purely hypothetical. There is at least one compiler that work that way. Using #include <xxx> only with standard headers is recommended.
    An #include with angle brackets will search an ""implementation-dependent list of places"" (which is a very complicated way of saying ""system headers"") for the file to be included.

An #include with quotes will just search for a file (and, ""in an implementation-dependent manner"", bleh). Which means, in normal English, it will try to apply the path/filename that you toss at it and will not prepend a system path or tamper with it otherwise.

Also, if #include """" fails, it is re-read as #include <> by the standard.

The gcc documentation has a (compiler specific) description which although being specific to gcc and not the standard, is a lot easier to understand than the attorney-style talk of the ISO standards.
    In general the difference is where the preprocessor searches for the header file:
#include is a preprocessor directive to include header file. Both #include are used to add or include header file in the program, but first is to include system header files and later one for user defined header files.

#include <filename> is used to include the system library header file in the program, means the C/C++ preprocessor will search for the filename where the C library files are stored or predefined system header files are stored.
#include ""filename"" is used to include user defined header file in the program, means the C/C++ preprocessor will search for the filename in the current directory the program is in and then follows the search path used for the #include <filename>

Check the gcc docs gcc include files
    For #include """" a compiler normally searches the folder of the file which contains that include and then the other folders. For #include <> the compiler does not search the current file's folder.
    """" will search ./ first. Then search the default include path.
You can use command like this to print the default include path:
gcc -v -o a a.c

Here are some examples to make thing more clear:
the code a.c works
// a.c
#include ""stdio.h""
int main() {
        int a = 3;
        printf(""a = %d\n"", a);
        return 0;

}

the code of b.c works too
// b.c
#include <stdio.h>
int main() {
        int a = 3;
        printf(""a = %d\n"", a);
        return 0;

}

but when I create a new file named stdio.h in current directory
// stdio.h
inline int foo()
{
        return 10;
}

a.c will generate compile error, but b.c still works
and """", <> can be used together with the same file name. since the search path priority is different.
so d.c also works
// d.c
#include <stdio.h>
#include ""stdio.h""
int main()
{
        int a = 0;

        a = foo();

        printf(""a=%d\n"", a);

        return 0;
}

    Thanks for the great answers, esp. Adam Stelmaszczyk and piCookie, and aib.
Like many programmers, I have used the informal convention of using the ""myApp.hpp"" form for application specific files, and the <libHeader.hpp> form for library and compiler system files, i.e. files specified in /I and the INCLUDE environment variable, for years thinking that was the standard.
However, the C standard states that the search order is implementation specific, which can make portability complicated.  To make matters worse, we use jam, which automagically figures out where the include files are.  You can use relative or absolute paths for your include files. i.e.
#include ""../../MyProgDir/SourceDir1/someFile.hpp""

Older versions of MSVS required double backslashes (\\), but now that's not required. I don't know when it changed. Just use forward slashes for compatibility with 'nix (Windows will accept that).
If you are really worried about it, use ""./myHeader.h"" for an include file in the same directory as the source code (my current, very large project has some duplicate include file names scattered about--really a configuration management problem).
Here's the MSDN explanation copied here for your convenience).

Quoted form
The preprocessor searches for include files in this order:

In the same directory as the file that contains the #include statement.
In the directories of the currently opened include files, in the reverse order in which
they were opened. The search begins in the directory of the parent include file and
continues upward through the directories of any grandparent include files.
Along the path that's specified by each /I compiler option.
Along the paths that are specified by the INCLUDE environment variable.

Angle-bracket form
The preprocessor searches for include files in this order:

Along the path that's specified by each /I compiler option.
When compiling occurs on the command line, along the paths that are specified by the INCLUDE environment variable.


    #include <file> 

Includes a file where the default include directory is.
#include ""file"" 

Includes a file in the current directory in which it was compiled. Double quotes can specify a full file path to a different location as well.
    
#include <> is for predefined header files


If the header file is predefined then you would simply write the header file name in angular brackets, and it would look like this (assuming we have a predefined header file name iostream):

#include <iostream>



#include "" "" is for header files the programmer defines


If you (the programmer) wrote your own header file then you would write the header file name in quotes. So, suppose you wrote a header file called myfile.h, then this is an example of how you would use the include directive to include that file:

#include ""myfile.h""

    #include <filename>  


The preprocessor searches in an implementation-dependent manner. It tells the compiler to search directory where system header files are held. 
This method usually use to find standard header files.


#include ""filename""


This tell compiler to search header files where program is running. If it was failed it behave like #include <filename> and search that header file at where system header files stored.
This method usually used for identify user defined header files(header files which are created by user). There for don't use this if you want to call standard library because it takes more compiling time than #include <filename>.

    #include <abc.h>


is used to include standard library files. So the compiler will check in the locations where standard library headers are residing.

#include ""xyz.h""


will tell the compiler to include user-defined header files. So the compiler will check for these header files in the current folder or -I defined folders.
    #include <filename>
is used when you want to use the header file of the C/C++ system or compiler libraries. These libraries can be stdio.h, string.h, math.h, etc.
#include ""path-to-file/filename""
is used when you want to use your own custom header file which is in your project folder or somewhere else.
For more information about preprocessors and header. Read C - Preprocessors.
    #include ""filename"" // User defined header
#include <filename> // Standard library header.


Example:

The filename here is Seller.h:

#ifndef SELLER_H     // Header guard
#define SELLER_H     // Header guard

#include <string>
#include <iostream>
#include <iomanip>

class Seller
{
    private:
        char name[31];
        double sales_total;

    public:
        Seller();
        Seller(char[], double);
        char*getName();

#endif


In the class implementation (for example, Seller.cpp, and in other files that will use the file Seller.h), the header defined by the user should now be included, as follows:

#include ""Seller.h""

    Form 1 - #include < xxx >
First, looks for the presence of header file in the current directory from where directive is invoked. If not found, then it searches in the preconfigured list of standard system directories.
Form 2 - #include ""xxx""
This looks for the presence of header file in the current directory from where directive is invoked.

The exact search directory list depends on the target system, how GCC is configured, and where it is installed.
You can find the search directory list of your GCC compiler by running it with -v option.
You can add additional directories to the search path by using - Idir, which causes dir to be searched after the current directory (for the quote form of the directive) and ahead of the standard system directories.

Basically, the form ""xxx"" is nothing but search in current directory; if not found falling back the form 
    To see the search order on your system using gcc, based on current configuration , you can execute the following command.  You can find more detail on this command here

cpp -v /dev/null -o /dev/null


  Apple LLVM version 10.0.0 (clang-1000.10.44.2) 
  Target: x86_64-apple-darwin18.0.0 
  Thread model: posix InstalledDir: Library/Developer/CommandLineTools/usr/bin
  ""/Library/Developer/CommandLineTools/usr/bin/clang"" -cc1 -triple
  x86_64-apple-macosx10.14.0 -Wdeprecated-objc-isa-usage
  -Werror=deprecated-objc-isa-usage -E -disable-free -disable-llvm-verifier -discard-value-names -main-file-name null -mrelocation-model pic -pic-level 2 -mthread-model posix -mdisable-fp-elim -fno-strict-return -masm-verbose -munwind-tables -target-cpu penryn -dwarf-column-info -debugger-tuning=lldb -target-linker-version 409.12 -v -resource-dir /Library/Developer/CommandLineTools/usr/lib/clang/10.0.0 -isysroot
  /Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk
  -I/usr/local/include -fdebug-compilation-dir /Users/hogstrom -ferror-limit 19 -fmessage-length 80 -stack-protector 1 -fblocks -fencode-extended-block-signature -fobjc-runtime=macosx-10.14.0 -fmax-type-align=16 -fdiagnostics-show-option -fcolor-diagnostics -traditional-cpp -o - -x c /dev/null 
  clang -cc1 version 10.0.0 (clang-1000.10.44.2) default target x86_64-apple-darwin18.0.0 ignoring
  nonexistent directory ""/Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/local/include""
  ignoring nonexistent directory ""/Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/Library/Frameworks""
  #include ""..."" search starts here:
  #include <...> search starts here:
  /usr/local/include  
  /Library/Developer/CommandLineTools/usr/lib/clang/10.0.0/include 
  /Library/Developer/CommandLineTools/usr/include  
  /Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/include 
  /Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/System/Library/Frameworks (framework directory)
   End of search list.

    In C++, include a file in two ways:

The first one is #include  which tells the preprocessor to look for the file in the predefined default location.
This location is often an INCLUDE environment variable that denotes the path to include files.

And the second type is #include ""filename"" which tells the preprocessor to look for the file in the current directory first, then look for it in the predefined locations user have set up.
    The #include <filename> is used when a system file is being referred to. That is a header file that can be found at system default locations like /usr/include or /usr/local/include. For your own files that needs to be included in another program you have to use the #include ""filename"" syntax.
    The simple general rule is to use angled brackets to include header files that come with the compiler. Use double quotes to include any other header files. Most compilers do it this way.

1.9  Header files explains in more detail about pre-processor directives. If you are a novice programmer, that page should help you understand all that. I learned it from here, and I have been following it at work.
    
  the "" < filename > "" searches in standard C library locations
  
  whereas ""filename"" searches in the current directory as well.


Ideally, you would use <...> for standard C libraries and ""..."" for libraries that you write and are present in the current directory.
    The implementation-defined warnings generated by the compiler can (and will) treat system libraries differently than program libraries.
So
#include <myFilename>
-- which in effect declares that myFilename is in the system library location -- may well (and probably will) hide dead code and unused variable warnings etc, that would show up when you use:
#include ""myFilename""
    ","[2834, 1733, 796, 327, 63, 132, 59, 44, 50, 21, 16, 25, 13, 15, 4, 18, 3, 19, 3, 13, 5, 10, 5, 10, 6, 3, 7, 6, 5, 5, 0]",760625,692,2008-08-22T01:40:06,2022-03-30 19:41:40Z,c c 
How do you split a list into evenly sized chunks?,"
                
How do I split a list of arbitrary length into equal sized chunks?
Related question: What is the most pythonic way to iterate over a list in chunks?
    Here's a generator that yields the chunks you want:

def chunks(lst, n):
    """"""Yield successive n-sized chunks from lst.""""""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]




import pprint
pprint.pprint(list(chunks(range(10, 75), 10)))
[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
 [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
 [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
 [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
 [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
 [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
 [70, 71, 72, 73, 74]]




If you're using Python 2, you should use xrange() instead of range():

def chunks(lst, n):
    """"""Yield successive n-sized chunks from lst.""""""
    for i in xrange(0, len(lst), n):
        yield lst[i:i + n]




Also you can simply use list comprehension instead of writing a function, though it's a good idea to encapsulate operations like this in named functions so that your code is easier to understand. Python 3:

[lst[i:i + n] for i in range(0, len(lst), n)]


Python 2 version:

[lst[i:i + n] for i in xrange(0, len(lst), n)]

    I know this is kind of old but nobody yet mentioned numpy.array_split:
import numpy as np

lst = range(50)
np.array_split(lst, 5)

Result:
[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
 array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),
 array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),
 array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),
 array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])]

    I'm surprised nobody has thought of using iter's two-argument form:

from itertools import islice

def chunk(it, size):
    it = iter(it)
    return iter(lambda: tuple(islice(it, size)), ())


Demo:

>>> list(chunk(range(14), 3))
[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]


This works with any iterable and produces output lazily. It returns tuples rather than iterators, but I think it has a certain elegance nonetheless. It also doesn't pad; if you want padding, a simple variation on the above will suffice:

from itertools import islice, chain, repeat

def chunk_pad(it, size, padval=None):
    it = chain(iter(it), repeat(padval))
    return iter(lambda: tuple(islice(it, size)), (padval,) * size)


Demo:

>>> list(chunk_pad(range(14), 3))
[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]
>>> list(chunk_pad(range(14), 3, 'a'))
[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 'a')]


Like the izip_longest-based solutions, the above always pads. As far as I know, there's no one- or two-line itertools recipe for a function that optionally pads. By combining the above two approaches, this one comes pretty close:

_no_padding = object()

def chunk(it, size, padval=_no_padding):
    if padval == _no_padding:
        it = iter(it)
        sentinel = ()
    else:
        it = chain(iter(it), repeat(padval))
        sentinel = (padval,) * size
    return iter(lambda: tuple(islice(it, size)), sentinel)


Demo:

>>> list(chunk(range(14), 3))
[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]
>>> list(chunk(range(14), 3, None))
[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]
>>> list(chunk(range(14), 3, 'a'))
[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 'a')]


I believe this is the shortest chunker proposed that offers optional padding.

As Tomasz Gandor observed, the two padding chunkers will stop unexpectedly if they encounter a long sequence of pad values. Here's a final variation that works around that problem in a reasonable way:

_no_padding = object()
def chunk(it, size, padval=_no_padding):
    it = iter(it)
    chunker = iter(lambda: tuple(islice(it, size)), ())
    if padval == _no_padding:
        yield from chunker
    else:
        for ch in chunker:
            yield ch if len(ch) == size else ch + (padval,) * (size - len(ch))


Demo:

>>> list(chunk([1, 2, (), (), 5], 2))
[(1, 2), ((), ()), (5,)]
>>> list(chunk([1, 2, None, None, 5], 2, None))
[(1, 2), (None, None), (5, None)]

    If you want something super simple:

def chunks(l, n):
    n = max(1, n)
    return (l[i:i+n] for i in range(0, len(l), n))


Use xrange() instead of range() in the case of Python 2.x
    Directly from the (old) Python documentation (recipes for itertools):

from itertools import izip, chain, repeat

def grouper(n, iterable, padvalue=None):
    ""grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')""
    return izip(*[chain(iterable, repeat(padvalue, n-1))]*n)


The current version, as suggested by J.F.Sebastian:

#from itertools import izip_longest as zip_longest # for Python 2.x
from itertools import zip_longest # for Python 3.x
#from six.moves import zip_longest # for both (uses the six compat library)

def grouper(n, iterable, padvalue=None):
    ""grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')""
    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)


I guess Guido's time machine worksworkedwill workwill have workedwas working again.

These solutions work because [iter(iterable)]*n (or the equivalent in the earlier version) creates one iterator, repeated n times in the list. izip_longest then effectively performs a round-robin of ""each"" iterator; because this is the same iterator, it is advanced by each such call, resulting in each such zip-roundrobin generating one tuple of n items.
    Don't reinvent the wheel.
Given
import itertools as it
import collections as ct

import more_itertools as mit


iterable = range(11)
n = 3

Code
more_itertools+
list(mit.chunked(iterable, n))
# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]

list(mit.sliced(iterable, n))
# [range(0, 3), range(3, 6), range(6, 9), range(9, 11)]

list(mit.grouper(n, iterable))
# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]

list(mit.windowed(iterable, len(iterable)//n, step=n))
# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]

list(mit.chunked_even(iterable, n))
# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]

(or DIY, if you want)
The Standard Library
list(it.zip_longest(*[iter(iterable)] * n))
# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]


d = {}
for i, x in enumerate(iterable):
    d.setdefault(i//n, []).append(x)
    

list(d.values())
# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]


dd = ct.defaultdict(list)
for i, x in enumerate(iterable):
    dd[i//n].append(x)
    

list(dd.values())
# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]

References

more_itertools.chunked (related posted)
more_itertools.sliced
more_itertools.grouper (related post)
more_itertools.windowed (see also stagger, zip_offset)
more_itertools.chunked_even
zip_longest (related post, related post)
setdefault (ordered results requires Python 3.6+)
collections.defaultdict  (ordered results requires Python 3.6+)

+ A third-party library that implements itertools recipes and more. > pip install more_itertools 
    Simple yet elegant
L = range(1, 1000)
print [L[x:x+10] for x in xrange(0, len(L), 10)]

or if you prefer:
def chunks(L, n): return [L[x: x+n] for x in xrange(0, len(L), n)]
chunks(L, 10)

    [AA[i:i+SS] for i in range(len(AA))[::SS]]

Where AA is array, SS is chunk size. For example:
>>> AA=range(10,21);SS=3
>>> [AA[i:i+SS] for i in range(len(AA))[::SS]]
[[10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]
# or [range(10, 13), range(13, 16), range(16, 19), range(19, 21)] in py3

To expand the ranges in py3 do
(py3) >>> [list(AA[i:i+SS]) for i in range(len(AA))[::SS]]
[[10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]

    Here is a generator that work on arbitrary iterables:

def split_seq(iterable, size):
    it = iter(iterable)
    item = list(itertools.islice(it, size))
    while item:
        yield item
        item = list(itertools.islice(it, size))


Example:

>>> import pprint
>>> pprint.pprint(list(split_seq(xrange(75), 10)))
[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
 [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
 [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
 [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
 [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
 [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
 [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
 [70, 71, 72, 73, 74]]

    If you know list size:

def SplitList(mylist, chunk_size):
    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]


If you don't (an iterator):

def IterChunks(sequence, chunk_size):
    res = []
    for item in sequence:
        res.append(item)
        if len(res) >= chunk_size:
            yield res
            res = []
    if res:
        yield res  # yield the last, incomplete, portion


In the latter case, it can be rephrased in a more beautiful way if you can be sure that the sequence always contains a whole number of chunks of given size (i.e. there is no incomplete last chunk).
    How do you split a list into evenly sized chunks?
""Evenly sized chunks"", to me, implies that they are all the same length, or barring that option, at minimal variance in length. E.g. 5 baskets for 21 items could have the following results:
>>> import statistics
>>> statistics.variance([5,5,5,5,1]) 
3.2
>>> statistics.variance([5,4,4,4,4]) 
0.19999999999999998

A practical reason to prefer the latter result: if you were using these functions to distribute work, you've built-in the prospect of one likely finishing well before the others, so it would sit around doing nothing while the others continued working hard.
Critique of other answers here
When I originally wrote this answer, none of the other answers were evenly sized chunks - they all leave a runt chunk at the end, so they're not well balanced, and have a higher than necessary variance of lengths.
For example, the current top answer ends with:
[60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
[70, 71, 72, 73, 74]]

Others, like list(grouper(3, range(7))), and chunk(range(7), 3) both return: [(0, 1, 2), (3, 4, 5), (6, None, None)]. The None's are just padding, and rather inelegant in my opinion. They are NOT evenly chunking the iterables.
Why can't we divide these better?
Cycle Solution
A high-level balanced solution using itertools.cycle, which is the way I might do it today. Here's the setup:
from itertools import cycle
items = range(10, 75)
number_of_baskets = 10

Now we need our lists into which to populate the elements:
baskets = [[] for _ in range(number_of_baskets)]

Finally, we zip the elements we're going to allocate together with a cycle of the baskets until we run out of elements, which, semantically, it exactly what we want:
for element, basket in zip(items, cycle(baskets)):
    basket.append(element)

Here's the result:
>>> from pprint import pprint
>>> pprint(baskets)
[[10, 20, 30, 40, 50, 60, 70],
 [11, 21, 31, 41, 51, 61, 71],
 [12, 22, 32, 42, 52, 62, 72],
 [13, 23, 33, 43, 53, 63, 73],
 [14, 24, 34, 44, 54, 64, 74],
 [15, 25, 35, 45, 55, 65],
 [16, 26, 36, 46, 56, 66],
 [17, 27, 37, 47, 57, 67],
 [18, 28, 38, 48, 58, 68],
 [19, 29, 39, 49, 59, 69]]

To productionize this solution, we write a function, and provide the type annotations:
from itertools import cycle
from typing import List, Any

def cycle_baskets(items: List[Any], maxbaskets: int) -> List[List[Any]]:
    baskets = [[] for _ in range(min(maxbaskets, len(items)))]
    for item, basket in zip(items, cycle(baskets)):
        basket.append(item)
    return baskets

In the above, we take our list of items, and the max number of baskets. We create a list of empty lists, in which to append each element, in a round-robin style.
Slices
Another elegant solution is to use slices - specifically the less-commonly used step argument to slices. i.e.:
start = 0
stop = None
step = number_of_baskets

first_basket = items[start:stop:step]

This is especially elegant in that slices don't care how long the data are - the result, our first basket, is only as long as it needs to be. We'll only need to increment the starting point for each basket.
In fact this could be a one-liner, but we'll go multiline for readability and to avoid an overlong line of code:
from typing import List, Any

def slice_baskets(items: List[Any], maxbaskets: int) -> List[List[Any]]:
    n_baskets = min(maxbaskets, len(items))
    return [items[i::n_baskets] for i in range(n_baskets)]

And islice from the itertools module will provide a lazily iterating approach, like that which was originally asked for in the question.
I don't expect most use-cases to benefit very much, as the original data is already fully materialized in a list, but for large datasets, it could save nearly half the memory usage.
from itertools import islice
from typing import List, Any, Generator
    
def yield_islice_baskets(items: List[Any], maxbaskets: int) -> Generator[List[Any], None, None]:
    n_baskets = min(maxbaskets, len(items))
    for i in range(n_baskets):
        yield islice(items, i, None, n_baskets)

View results with:
from pprint import pprint

items = list(range(10, 75))
pprint(cycle_baskets(items, 10))
pprint(slice_baskets(items, 10))
pprint([list(s) for s in yield_islice_baskets(items, 10)])

Updated prior solutions
Here's another balanced solution, adapted from a function I've used in production in the past, that uses the modulo operator:
def baskets_from(items, maxbaskets=25):
    baskets = [[] for _ in range(maxbaskets)]
    for i, item in enumerate(items):
        baskets[i % maxbaskets].append(item)
    return filter(None, baskets) 

And I created a generator that does the same if you put it into a list:
def iter_baskets_from(items, maxbaskets=3):
    '''generates evenly balanced baskets from indexable iterable'''
    item_count = len(items)
    baskets = min(item_count, maxbaskets)
    for x_i in range(baskets):
        yield [items[y_i] for y_i in range(x_i, item_count, baskets)]
    

And finally, since I see that all of the above functions return elements in a contiguous order (as they were given):
def iter_baskets_contiguous(items, maxbaskets=3, item_count=None):
    '''
    generates balanced baskets from iterable, contiguous contents
    provide item_count if providing a iterator that doesn't support len()
    '''
    item_count = item_count or len(items)
    baskets = min(item_count, maxbaskets)
    items = iter(items)
    floor = item_count // baskets 
    ceiling = floor + 1
    stepdown = item_count % baskets
    for x_i in range(baskets):
        length = ceiling if x_i < stepdown else floor
        yield [items.next() for _ in range(length)]

Output
To test them out:
print(baskets_from(range(6), 8))
print(list(iter_baskets_from(range(6), 8)))
print(list(iter_baskets_contiguous(range(6), 8)))
print(baskets_from(range(22), 8))
print(list(iter_baskets_from(range(22), 8)))
print(list(iter_baskets_contiguous(range(22), 8)))
print(baskets_from('ABCDEFG', 3))
print(list(iter_baskets_from('ABCDEFG', 3)))
print(list(iter_baskets_contiguous('ABCDEFG', 3)))
print(baskets_from(range(26), 5))
print(list(iter_baskets_from(range(26), 5)))
print(list(iter_baskets_contiguous(range(26), 5)))

Which prints out:
[[0], [1], [2], [3], [4], [5]]
[[0], [1], [2], [3], [4], [5]]
[[0], [1], [2], [3], [4], [5]]
[[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]
[[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]
[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19], [20, 21]]
[['A', 'D', 'G'], ['B', 'E'], ['C', 'F']]
[['A', 'D', 'G'], ['B', 'E'], ['C', 'F']]
[['A', 'B', 'C'], ['D', 'E'], ['F', 'G']]
[[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]
[[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]

Notice that the contiguous generator provide chunks in the same length patterns as the other two, but the items are all in order, and they are as evenly divided as one may divide a list of discrete elements.
    I saw the most awesome Python-ish answer in a duplicate of this question:

from itertools import zip_longest

a = range(1, 16)
i = iter(a)
r = list(zip_longest(i, i, i))
>>> print(r)
[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, 15)]


You can create n-tuple for any n. If a = range(1, 15), then the result will be:

[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, None)]


If the list is divided evenly, then you can replace zip_longest with zip, otherwise the triplet (13, 14, None) would be lost. Python 3 is used above. For Python 2, use izip_longest.
    With Assignment Expressions in Python 3.8 it becomes quite nice:

import itertools

def batch(iterable, size):
    it = iter(iterable)
    while item := list(itertools.islice(it, size)):
        yield item


This works on an arbitrary iterable, not just a list.

>>> import pprint
>>> pprint.pprint(list(batch(range(75), 10)))
[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
 [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
 [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
 [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
 [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
 [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
 [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
 [70, 71, 72, 73, 74]]

    def chunk(input, size):
    return map(None, *([iter(input)] * size))

    I was curious about the performance of different approaches and here it is:

Tested on Python 3.5.1

import time
batch_size = 7
arr_len = 298937

#---------slice-------------

print(""\r\nslice"")
start = time.time()
arr = [i for i in range(0, arr_len)]
while True:
    if not arr:
        break

    tmp = arr[0:batch_size]
    arr = arr[batch_size:-1]
print(time.time() - start)

#-----------index-----------

print(""\r\nindex"")
arr = [i for i in range(0, arr_len)]
start = time.time()
for i in range(0, round(len(arr) / batch_size + 1)):
    tmp = arr[batch_size * i : batch_size * (i + 1)]
print(time.time() - start)

#----------batches 1------------

def batch(iterable, n=1):
    l = len(iterable)
    for ndx in range(0, l, n):
        yield iterable[ndx:min(ndx + n, l)]

print(""\r\nbatches 1"")
arr = [i for i in range(0, arr_len)]
start = time.time()
for x in batch(arr, batch_size):
    tmp = x
print(time.time() - start)

#----------batches 2------------

from itertools import islice, chain

def batch(iterable, size):
    sourceiter = iter(iterable)
    while True:
        batchiter = islice(sourceiter, size)
        yield chain([next(batchiter)], batchiter)


print(""\r\nbatches 2"")
arr = [i for i in range(0, arr_len)]
start = time.time()
for x in batch(arr, batch_size):
    tmp = x
print(time.time() - start)

#---------chunks-------------
def chunks(l, n):
    """"""Yield successive n-sized chunks from l.""""""
    for i in range(0, len(l), n):
        yield l[i:i + n]
print(""\r\nchunks"")
arr = [i for i in range(0, arr_len)]
start = time.time()
for x in chunks(arr, batch_size):
    tmp = x
print(time.time() - start)

#-----------grouper-----------

from itertools import zip_longest # for Python 3.x
#from six.moves import zip_longest # for both (uses the six compat library)

def grouper(iterable, n, padvalue=None):
    ""grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')""
    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)

arr = [i for i in range(0, arr_len)]
print(""\r\ngrouper"")
start = time.time()
for x in grouper(arr, batch_size):
    tmp = x
print(time.time() - start)


Results:

slice
31.18285083770752

index
0.02184295654296875

batches 1
0.03503894805908203

batches 2
0.22681021690368652

chunks
0.019841909408569336

grouper
0.006506919860839844

    The toolz library has the partition function for this:

from toolz.itertoolz.core import partition

list(partition(2, [1, 2, 3, 4]))
[(1, 2), (3, 4)]

    If you had a chunk size of 3 for example, you could do:

zip(*[iterable[i::3] for i in range(3)]) 


source:
http://code.activestate.com/recipes/303060-group-a-list-into-sequential-n-tuples/

I would use this when my chunk size is fixed number I can type, e.g. '3', and would never change.
    You may also use get_chunks function of utilspie library as:

>>> from utilspie import iterutils
>>> a = [1, 2, 3, 4, 5, 6, 7, 8, 9]

>>> list(iterutils.get_chunks(a, 5))
[[1, 2, 3, 4, 5], [6, 7, 8, 9]]


You can install utilspie via pip:

sudo pip install utilspie


Disclaimer: I am the creator of utilspie library.
    code:

def split_list(the_list, chunk_size):
    result_list = []
    while the_list:
        result_list.append(the_list[:chunk_size])
        the_list = the_list[chunk_size:]
    return result_list

a_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

print split_list(a_list, 3)


result:

[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]

    heh, one line version

In [48]: chunk = lambda ulist, step:  map(lambda i: ulist[i:i+step],  xrange(0, len(ulist), step))

In [49]: chunk(range(1,100), 10)
Out[49]: 
[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
 [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
 [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],
 [31, 32, 33, 34, 35, 36, 37, 38, 39, 40],
 [41, 42, 43, 44, 45, 46, 47, 48, 49, 50],
 [51, 52, 53, 54, 55, 56, 57, 58, 59, 60],
 [61, 62, 63, 64, 65, 66, 67, 68, 69, 70],
 [71, 72, 73, 74, 75, 76, 77, 78, 79, 80],
 [81, 82, 83, 84, 85, 86, 87, 88, 89, 90],
 [91, 92, 93, 94, 95, 96, 97, 98, 99]]

    def split_seq(seq, num_pieces):
    start = 0
    for i in xrange(num_pieces):
        stop = start + len(seq[i::num_pieces])
        yield seq[start:stop]
        start = stop


usage:

seq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

for seq in split_seq(seq, 3):
    print seq

    I like the Python doc's version proposed by tzot and J.F.Sebastian a lot,
 but it has two shortcomings:


it is not very explicit
I usually don't want a fill value in the last chunk


I'm using this one a lot in my code:

from itertools import islice

def chunks(n, iterable):
    iterable = iter(iterable)
    while True:
        yield tuple(islice(iterable, n)) or iterable.next()


UPDATE: A lazy chunks version:

from itertools import chain, islice

def chunks(n, iterable):
   iterable = iter(iterable)
   while True:
       yield chain([next(iterable)], islice(iterable, n-1))

    See this reference

>>> orange = range(1, 1001)
>>> otuples = list( zip(*[iter(orange)]*10))
>>> print(otuples)
[(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), ... (991, 992, 993, 994, 995, 996, 997, 998, 999, 1000)]
>>> olist = [list(i) for i in otuples]
>>> print(olist)
[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ..., [991, 992, 993, 994, 995, 996, 997, 998, 999, 1000]]
>>> 


Python3
    Without calling len() which is good for large lists:

def splitter(l, n):
    i = 0
    chunk = l[:n]
    while chunk:
        yield chunk
        i += n
        chunk = l[i:i+n]


And this is for iterables:

def isplitter(l, n):
    l = iter(l)
    chunk = list(islice(l, n))
    while chunk:
        yield chunk
        chunk = list(islice(l, n))


The functional flavour of the above:

def isplitter2(l, n):
    return takewhile(bool,
                     (tuple(islice(start, n))
                            for start in repeat(iter(l))))


OR:

def chunks_gen_sentinel(n, seq):
    continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))
    return iter(imap(tuple, continuous_slices).next,())


OR:

def chunks_gen_filter(n, seq):
    continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))
    return takewhile(bool,imap(tuple, continuous_slices))

    def chunks(iterable,n):
    """"""assumes n is an integer>0
    """"""
    iterable=iter(iterable)
    while True:
        result=[]
        for i in range(n):
            try:
                a=next(iterable)
            except StopIteration:
                break
            else:
                result.append(a)
        if result:
            yield result
        else:
            break

g1=(i*i for i in range(10))
g2=chunks(g1,3)
print g2
'<generator object chunks at 0x0337B9B8>'
print list(g2)
'[[0, 1, 4], [9, 16, 25], [36, 49, 64], [81]]'

    Another more explicit version.

def chunkList(initialList, chunkSize):
    """"""
    This function chunks a list into sub lists 
    that have a length equals to chunkSize.

    Example:
    lst = [3, 4, 9, 7, 1, 1, 2, 3]
    print(chunkList(lst, 3)) 
    returns
    [[3, 4, 9], [7, 1, 1], [2, 3]]
    """"""
    finalList = []
    for i in range(0, len(initialList), chunkSize):
        finalList.append(initialList[i:i+chunkSize])
    return finalList

    Since everybody here talking about iterators. boltons has perfect method for that, called iterutils.chunked_iter.

from boltons import iterutils

list(iterutils.chunked_iter(list(range(50)), 11))


Output:

[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
 [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],
 [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],
 [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],
 [44, 45, 46, 47, 48, 49]]


But if you don't want to be mercy on memory, you can use old-way and store the full list in the first place with iterutils.chunked.
    At this point, I think we need a recursive generator, just in case...

In python 2:

def chunks(li, n):
    if li == []:
        return
    yield li[:n]
    for e in chunks(li[n:], n):
        yield e


In python 3:

def chunks(li, n):
    if li == []:
        return
    yield li[:n]
    yield from chunks(li[n:], n)


Also, in case of massive Alien invasion, a decorated recursive generator might become handy:

def dec(gen):
    def new_gen(li, n):
        for e in gen(li, n):
            if e == []:
                return
            yield e
    return new_gen

@dec
def chunks(li, n):
    yield li[:n]
    for e in chunks(li[n:], n):
        yield e

    A simple solution

The OP has requested ""equal sized chunk"". I understand ""equal sized"" as ""balanced"" sizes: we are looking for groups of items of approximately the same sizes if equal sizes are not possible (e.g, 23/5).

Inputs here are:

the list of items: input_list (list of 23 numbers, for instance)
the number of groups to split those items: n_groups (5, for instance)

Input:
input_list = list(range(23))
n_groups = 5

Groups of contiguous elements:
approx_sizes = len(input_list)/n_groups 

groups_cont = [input_list[int(i*approx_sizes):int((i+1)*approx_sizes)] 
               for i in range(n_groups)]

Groups of ""every-Nth"" elements:
groups_leap = [input_list[i::n_groups] 
               for i in range(n_groups)]

Results
print(len(input_list))

print('Contiguous elements lists:')
print(groups_cont)

print('Leap every ""N"" items lists:')
print(groups_leap)


Will output:
23

Contiguous elements lists:
[[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16, 17], [18, 19, 20, 21, 22]]

Leap every ""N"" items lists:
[[0, 5, 10, 15, 20], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18], [4, 9, 14, 19]]


    Consider using matplotlib.cbook pieces

for example:

import matplotlib.cbook as cbook
segments = cbook.pieces(np.arange(20), 3)
for s in segments:
     print s

    ","[2833, 4069, 367, 264, 637, 339, 42, 81, 40, 117, 55, 60, 52, 26, 66, 23, 27, 28, 20, 19, 14, 13, 21, 11, 12, 10, 12, 10, 15, 3, 9]",1388128,770,2008-11-23T12:15:52,2022-04-01 01:59:49Z,python 
How do I modify the URL without reloading the page?,"
                
Is there a way I can modify the URL of the current page without reloading the page?
I would like to access the portion before the # hash if possible.
I only need to change the portion after the domain, so it's not like I'm violating cross-domain policies.
 window.location.href = ""www.mysite.com/page2.php"";  // this reloads

    This can now be done in Chrome, Safari, Firefox 4+, and InternetExplorer10pp4+!

See this question's answer for more information:
Updating address bar with new URL without hash or reloading the page

Example:

 function processAjaxData(response, urlPath){
     document.getElementById(""content"").innerHTML = response.html;
     document.title = response.pageTitle;
     window.history.pushState({""html"":response.html,""pageTitle"":response.pageTitle},"""", urlPath);
 }


You can then use window.onpopstate to detect the back/forward button navigation:

window.onpopstate = function(e){
    if(e.state){
        document.getElementById(""content"").innerHTML = e.state.html;
        document.title = e.state.pageTitle;
    }
};




For a more in-depth look at manipulating browser history, see this MDN article.
    HTML5 introduced the history.pushState() and history.replaceState() methods, which allow you to add and modify history entries, respectively.
window.history.pushState('page2', 'Title', '/page2.php');

Read more about this from here
    Here is my solution (newUrl is your new URL which you want to replace with the current one):

history.pushState({}, null, newUrl);

    You can also use HTML5 replaceState if you want to change the url but don't want to add the entry to the browser history:

if (window.history.replaceState) {
   //prevents browser from storing history with each change:
   window.history.replaceState(statedata, title, url);
}


This would 'break' the back button functionality. This may be required in some instances such as an image gallery (where you want the back button to return back to the gallery index page instead of moving back through each and every image you viewed) whilst giving each image its own unique url.
    In modern browsers and HTML5, there is a method called pushState on window history. That will change the URL and push it to the history without loading the page.
You can use it like this, it will take 3 parameters, 1) state object 2) title and a URL):
window.history.pushState({page: ""another""}, ""another page"", ""example.html"");

This will change the URL, but not reload the page. Also, it doesn't check if the page exists, so if you do some JavaScript code that is reacting to the URL, you can work with them like this.
Also, there is history.replaceState() which does exactly the same thing, except it will modify the current history instead of creating a new one!
Also you can create a function to check if history.pushState exist, then carry on with the rest like this:
function goTo(page, title, url) {
  if (""undefined"" !== typeof history.pushState) {
    history.pushState({page: page}, title, url);
  } else {
    window.location.assign(url);
  }
}

goTo(""another page"", ""example"", 'example.html');

Also, you can change the # for <HTML5 browsers, which won't reload the page. That's the way Angular uses to do SPA according to hashtag...
Changing # is quite easy, doing like:
window.location.hash = ""example"";

And you can detect it like this:
window.onhashchange = function () {
  console.log(""#changed"", window.location.hash);
}

    You can use this beautiful and simple function to do so anywhere on your application.
function changeurl(url, title) {
    var new_url = '/' + url;
    window.history.pushState('data', title, new_url);
    
}

You can not only edit the URL but you can update the title along with it.
    NOTE: If you are working with an HTML5 browser then you should ignore this answer. This is now possible as can be seen in the other answers.

There is no way to modify the URL in the browser without reloading the page. The URL represents what the last loaded page was. If you change it (document.location) then it will reload the page.

One obvious reason being, you write a site on www.mysite.com that looks like a bank login page. Then you change the browser URL bar to say www.mybank.com. The user will be totally unaware that they are really looking at www.mysite.com.
    parent.location.hash = ""hello"";

    Below is the function to change the URL without reloading the page. It is only supported for HTML5.

  function ChangeUrl(page, url) {
        if (typeof (history.pushState) != ""undefined"") {
            var obj = {Page: page, Url: url};
            history.pushState(obj, obj.Page, obj.Url);
        } else {
            window.location.href = ""homePage"";
            // alert(""Browser does not support HTML5."");
        }
    }

  ChangeUrl('Page1', 'homePage');

    Your new url.
let newUrlIS =  window.location.origin + '/user/profile/management';

In a sense, calling pushState() is similar to setting window.location = ""#foo"", in that both will also create and activate another history entry associated with the current document. But pushState() has a few advantages:
history.pushState({}, null, newUrlIS);

You can check out the root: https://developer.mozilla.org/en-US/docs/Web/API/History_API
    Before HTML5 we can use:

parent.location.hash = ""hello"";


and:

window.location.replace(""http:www.example.com"");


This method will reload your page, but HTML5 introduced the history.pushState(page, caption, replace_url) that should not reload your page.
    If what you're trying to do is allow users to bookmark/share pages, and you don't need it to be exactly the right URL, and you're not using hash anchors for anything else, then you can do this in two parts; you use the location. hash discussed above, and then implement a check on the home page, to look for a URL with a hash anchor in it, and redirect you to the subsequent result.
For instance:

User is on www.site.com/section/page/4

User does some action which changes the URL to www.site.com/#/section/page/6 (with the hash). Say you've loaded the correct content for page 6 into the page, so apart from the hash the user is not too disturbed.

User passes this URL on to someone else, or bookmarks it

Someone else, or the same user at a later date, goes to www.site.com/#/section/page/6

Code on www.site.com/ redirects the user to www.site.com/section/page/6, using something like this:
if (window.location.hash.length > 0){
window.location = window.location.hash.substring(1);
}


Hope that makes sense! It's a useful approach for some situations.
    The HTML5 replaceState is the answer, as already mentioned by Vivart and geo1701. However it is not supported in all browsers/versions.
History.js wraps HTML5 state features and provides additional support for HTML4 browsers.
    Use history.pushState() from the HTML 5 History API.

Refer to the HTML5 History API for more details.
    As pointed out by Thomas Stjernegaard Jeppesen, you could use History.js to modify URL parameters whilst the user navigates through your Ajax links and apps. 

Almost an year has passed since that answer, and History.js grew and became more stable and cross-browser. Now it can be used to manage history states in HTML5-compliant as well as in many HTML4-only browsers. In this demo You can see an example of how it works (as well as being able to try its functionalities and limits. 

Should you need any help in how to use and implement this library, i suggest you to take a look at the source code of the demo page: you will see it's very easy to do.

Finally, for a comprehensive explanation of what can be the issues about using hashes (and hashbangs), check out this link by Benjamin Lupton. 
    This code works for me. I used it into my application in ajax.
history.pushState({ foo: 'bar' }, '', '/bank');

Once a page load into an ID using ajax, It does change the browser url automatically without reloading the page.
This is ajax function bellow.
function showData(){
    $.ajax({
      type: ""POST"",
      url: ""Bank.php"", 
      data: {}, 
      success: function(html){          
        $(""#viewpage"").html(html).show();
        $(""#viewpage"").css(""margin-left"",""0px"");
      }
    });
  }

Example: From any page or controller like ""Dashboard"", When I click on the bank, it loads bank list using the ajax code without reloading the page. At this time, browser URL will not be changed.
history.pushState({ foo: 'bar' }, '', '/bank');

But when I use this code into the ajax, it change the browser url without reloading the page.
This is the full ajax code here in the bellow.
function showData(){
        $.ajax({
          type: ""POST"",
          url: ""Bank.php"", 
          data: {}, 
          success: function(html){          
            $(""#viewpage"").html(html).show();
            $(""#viewpage"").css(""margin-left"",""0px"");
            history.pushState({ foo: 'bar' }, '', '/bank');
          }
        });
      }

    Any changes of the loction (either window.location or document.location) will cause a request on that new URL, if youre not just changing the URL fragment. If you change the URL, you change the URL.

Use server-side URL rewrite techniques like Apaches mod_rewrite if you dont like the URLs you are currently using.
    You can add anchor tags. I use this on my site so that I can track with Google Analytics what people are visiting on the page.

I just add an anchor tag and then the part of the page I want to track:

var trackCode = ""/#"" + urlencode($(""myDiv"").text());
window.location.href = ""http://www.piano-chords.net"" + trackCode;
pageTracker._trackPageview(trackCode);

    This is all you will need to navigate without reload
// add setting without reload 
location.hash = ""setting"";

// if url change with hash do somthing
window.addEventListener('hashchange', () => {
    console.log('url hash changed!');
});

// if url change do somthing (dont detect changes with hash)
//window.addEventListener('locationchange', function(){
//    console.log('url changed!');
//})


// remove #setting without reload 

history.back();

    Simply use, it will not reload the page, but just the URL :
$('#form_name').attr('action', '/shop/index.htm').submit();

    ","[2814, 2317, 765, 167, 186, 53, 16, 116, 90, 20, 8, 27, 24, 28, 8, 9, 1, 13, 11, -1, -7]",1786179,1013,2009-05-05T10:54:05,2022-04-29 20:16:48Z,javascript html 
How to modify a specified commit?,"
                
I usually submit a list of commits for review. If I have the following commits:


HEAD
Commit3  
Commit2  
Commit1


...I know that I can modify head commit with git commit --amend. But how can I modify Commit1, given that it is not the HEAD commit?
    You can use git rebase. For example, if you want to modify commit bbc643cd, run

$ git rebase --interactive 'bbc643cd^'


Please note the caret ^ at the end of the command, because you need actually to rebase back to the commit before the one you wish to modify.

In the default editor, modify pick to edit in the line mentioning 'bbc643cd'.

Save the file and exit: git will interpret and automatically execute the commands in the file. You will find yourself in the previous situation in which you just had created commit bbc643cd.

At this point, bbc643cd is your last commit and you can easily amend it: make your changes and then commit them with the command:

$ git commit --all --amend --no-edit


After that, type:

$ git rebase --continue


to return back to the previous HEAD commit.

WARNING: Note that this will change the SHA-1 of that commit as well as all children -- in other words, this rewrites the history from that point forward. You can break repos doing this if you push using the command git push --force
    Use the awesome interactive rebase:
git rebase -i @~9   # Show the last 9 commits in a text editor

Find the commit you want, change pick to e (edit), and save and close the file. Git will rewind to that commit, allowing you to either:

use git commit --amend to make changes, or
use git reset @~ to discard the last commit, but not the changes to the files (i.e. take you to the point you were at when you'd edited the files, but hadn't committed yet).

The latter is useful for doing more complex stuff like splitting into multiple commits.
Then, run git rebase --continue, and Git will replay the subsequent changes on top of your modified commit. You may be asked to fix some merge conflicts.
Note: @ is shorthand for HEAD, and ~ is the commit before the specified commit.
Read more about rewriting history in the Git docs.

Don't be afraid to rebase
ProTip:  Don't be afraid to experiment with ""dangerous"" commands that rewrite history*  Git doesn't delete your commits for 90 days by default; you can find them in the reflog:
$ git reset @~3   # go back 3 commits
$ git reflog
c4f708b HEAD@{0}: reset: moving to @~3
2c52489 HEAD@{1}: commit: more changes
4a5246d HEAD@{2}: commit: make important changes
e8571e4 HEAD@{3}: commit: make some changes
... earlier commits ...
$ git reset 2c52489
... and you're back where you started

* Watch out for options like --hard and --force though  they can discard data.
*  Also, don't rewrite history on any branches you're collaborating on.


On many systems, git rebase -i will open up Vim by default. Vim doesn't work like most modern text editors, so take a look at how to rebase using Vim. If you'd rather use a different editor, change it with git config --global core.editor your-favorite-text-editor.
    Interactive rebase with --autosquash is something I frequently use when I need to fixup previous commits deeper in the history. It essentially speeds up the process that ZelluX's answer illustrates, and is especially handy when you have more than one commit you need to edit.

From the documentation:


  --autosquash
  
  When the commit log message begins with ""squash! "" (or ""fixup! ""), and there is a commit whose title begins with the same , automatically modify the todo list of rebase -i so that the commit marked for squashing comes right after the commit to be modified


Assume you have a history that looks like this:

$ git log --graph --oneline
* b42d293 Commit3
* e8adec4 Commit2
* faaf19f Commit1


and you have changes that you want to amend to Commit2 then commit your changes using

$ git commit -m ""fixup! Commit2""


alternatively you can use the commit-sha instead of the commit message, so ""fixup! e8adec4 or even just a prefix of the commit message.

Then initiate an interactive rebase on the commit before

$ git rebase e8adec4^ -i --autosquash


your editor will open with the commits already correctly ordered

pick e8adec4 Commit2
fixup 54e1a99 fixup! Commit2
pick b42d293 Commit3


all you need to do is save and exit
    Based on Documentation
Amending the message of older or multiple commit messages
git rebase -i HEAD~3 

The above displays a list of the last 3 commits on the current branch, change 3 to something else if you want more. The list will look similar to the following:
pick e499d89 Delete CNAME
pick 0c39034 Better README
pick f7fde4a Change the commit message but push the same commit.

Replace pick with reword before each commit message you want to change. Let say you change the second commit in the list, your file will look like the following:
pick e499d89 Delete CNAME
reword 0c39034 Better README
pick f7fde4a Change the commit message but push the same commit.

Save and close the commit list file, this will pop up a new editor for you to change your commit message, change the commit message and save.
Finally, force-push the amended commits.
git push --force

    Completely non-interactive command(1)
I just thought I'd share an alias that I'm using for this. It's based on non-interactive interactive rebase. To add it to your git, run this command (explanation given below):
git config --global alias.amend-to '!f() { SHA=`git rev-parse ""$1""`; git commit --fixup ""$SHA"" && GIT_SEQUENCE_EDITOR=true git rebase --interactive --autosquash ""$SHA^""; }; f'

Or, a version that can also handle unstaged files (by stashing and then un-stashing them):
git config --global alias.amend-to '!f() { SHA=`git rev-parse ""$1""`; git stash -k && git commit --fixup ""$SHA"" && GIT_SEQUENCE_EDITOR=true git rebase --interactive --autosquash ""$SHA^"" && git stash pop; }; f'

The biggest advantage of this command is the fact that it's no-vim.

(1)given that there are no conflicts during rebase, of course
Usage
git amend-to <REV> # e.g.
git amend-to HEAD~1
git amend-to aaaa1111

The name amend-to seems appropriate IMHO. Compare the flow with --amend:
git add . && git commit --amend --no-edit
# vs
git add . && git amend-to <REV>

Explanation

git config --global alias.<NAME> '!<COMMAND>' - creates a global git alias named <NAME> that will execute non-git command <COMMAND>
f() { <BODY> }; f - an ""anonymous"" bash function.
SHA=`git rev-parse ""$1""`; - converts the argument to git revision, and assigns the result to variable SHA
git commit --fixup ""$SHA"" - fixup-commit for SHA. See git-commit docs
GIT_SEQUENCE_EDITOR=true git rebase --interactive --autosquash ""$SHA^""

git rebase --interactive ""$SHA^"" part has been covered by other answers.
--autosquash is what's used in conjunction with git commit --fixup, see git-rebase docs for more info
GIT_SEQUENCE_EDITOR=true is what makes the whole thing non-interactive. This hack I learned from this blog post.



    Run:

$ git rebase --interactive commit_hash^

each ^ indicates how many commits back you want to edit, if it's only one (the commit hash that you specified), then you just add one ^.

Using Vim you change the words pick to reword for the commits you want to change, save and quit(:wq). Then git will prompt you with each commit that you marked as reword so you can change the commit message.

Each commit message you have to save and quit(:wq) to go to the next commit message

If you want to exit without applying the changes, press :q!

EDIT: to navigate in vim you use j to go up, k to go down, h to go left, and l to go right( all this in NORMAL mode, press ESC to go to NORMAL mode ).
To edit a text, press i so that you enter the INSERT mode, where you insert text. 
Press ESC to go back to NORMAL mode :)

UPDATE: Here's a great link from github listing How to undo (almost) anything with git 
    The best option is to use ""Interactive rebase command"". 


  The git rebase command is incredibly powerful. It allows you to edit
  commit messages, combine commits, reorder them ...etc. 
  
  Every time you rebase a commit a new SHA will be created for each
  commit regardless of the content will be changed or not! You should be
  careful when to use this command cause it may have drastic
  implications especially if you work in collaboration with other
  developers. They may start working with your commit while you're
  rebasing some. After you force to push the commits they will be out of
  sync and you may find out later in a messy situation. So be careful!
  
  It's recommended to create a backup branch before rebasing so
  whenever you find things out of control you can return back to the
  previous state.


Now how to use this command?

git rebase -i <base> 


-i stand for ""interactive"". Note that you can perform a rebase in non-interactive mode. ex:

#interactivly rebase the n commits from the current position, n is a given number(2,3 ...etc)
git rebase -i HEAD~n 


HEAD indicates your current location(can be also branch name or commit SHA). The ~n means ""n before, so HEAD~n will be the list of ""n"" commits before the one you are currently on. 

git rebase has different command like:


p or pick to keep commit as it is.
r or reword: to keep the commit's content but alter the commit message. 
s or squash: to combine this commit's changes into the previous commit(the commit above it in the list).
... etc.

Note: It's better to get Git working with your code editor to make things simpler. Like for example if you use visual code you can add like this git config --global core.editor ""code --wait"". Or you can search in Google how to associate you preferred your code editor with GIT.


Example of git rebase

I wanted to change the last 2 commits I did so I process like this:


Display the current commits: 

#This to show all the commits on one line
$git log --oneline
4f3d0c8 (HEAD -> documentation) docs: Add project description and included files""
4d95e08 docs: Add created date and project title""
eaf7978 (origin/master , origin/HEAD, master) Inital commit
46a5819 Create README.md

Now I use git rebase to change the 2 last commits messages:
$git rebase -i HEAD~2
It opens the code editor and show this:

pick 4d95e08 docs: Add created date and project title
pick 4f3d0c8 docs: Add project description and included files

# Rebase eaf7978..4f3d0c8 onto eaf7978 (2 commands)
#
# Commands:
# p, pick <commit> = use commit
# r, reword <commit> = use commit, but edit the commit message
...


Since I want to change the commit message for this 2 commits. So I will type r or reword in place of pick. Then Save the file and close the tab. 
Note that rebase is executed in a multi-step process so the next step is to update the messages. Note also that the commits are displayed in reverse chronological order so the last commit is displayed in that one and the first commit in the first line and so forth.
Update the messages: 
Update the first message:

docs: Add created date and project title to the documentation ""README.md""

# Please enter the commit message for your changes. Lines starting
# with '#' will be ignored, and an empty message aborts the commit.
...


save and close
Edit the second message

docs: Add project description and included files to the documentation ""README.md""

# Please enter the commit message for your changes. Lines starting
# with '#' will be ignored, and an empty message aborts the commit.
...


save and close.
You will get a message like this by the end of the rebase: Successfully rebased and updated refs/heads/documentation which means that you succeed. You can display the changes:

5dff827 (HEAD -> documentation) docs: Add project description and included files to the documentation ""README.md""
4585c68 docs: Add created date and project title to the documentation ""README.md""
eaf7978 (origin/master, origin/HEAD, master) Inital commit
46a5819 Create README.md


I wish that may help the new users :).

    git stash + rebase automation

For when I need to modify an old commit a lot of times for Gerrit reviews, I've been doing:

git-amend-old() (
  # Stash, apply to past commit, and rebase the current branch on to of the result.
  current_branch=""$(git rev-parse --abbrev-ref HEAD)""
  apply_to=""$1""
  git stash
  git checkout ""$apply_to""
  git stash apply
  git add -u
  git commit --amend --no-edit
  new_sha=""$(git log --format=""%H"" -n 1)""
  git checkout ""$current_branch""
  git rebase --onto ""$new_sha"" ""$apply_to""
)


GitHub upstream.

Usage:


modify source file, no need to git add if already in repo
git-amend-old $old_sha


I like this over --autosquash as it does not squash other unrelated fixups.
    If for some reason you don't like interactive editors, you can use git rebase --onto.

Say you want to modify Commit1. First, branch from before Commit1:

git checkout -b amending [commit before Commit1]


Second, grab Commit1 with cherry-pick:

git cherry-pick Commit1


Now, amend your changes, creating Commit1':

git add ...
git commit --amend -m ""new message for Commit1""


And finally, after having stashed any other changes, transplant the rest of your commits up to master on top of your
new commit:

git rebase --onto amending Commit1 master


Read: ""rebase, onto the branch amending, all commits between Commit1 (non-inclusive) and master (inclusive)"". That is, Commit2 and Commit3, cutting the old Commit1 out entirely. You could just cherry-pick them, but this way is easier.

Remember to clean up your branches!

git branch -d amending

    Automated interactive rebase edit followed by commit revert  ready for a do-over

I found myself fixing a past commit frequently enough that I wrote a script for it. 

Here's the workflow:


git commit-edit <commit-hash>


This will drop you at the commit you want to edit.
Fix and stage the commit as you wish it had been in the first place.

(You may want to use git stash save to keep any files you're not committing)
Redo the commit with --amend, eg:

git commit --amend

Complete the rebase:

git rebase --continue



For the above to work, put the below script into an executable file called git-commit-edit somewhere in your $PATH:



#!/bin/bash

set -euo pipefail

script_name=${0##*/}

warn () { printf '%s: %s\n' ""$script_name"" ""$*"" >&2; }
die () { warn ""$@""; exit 1; }

[[ $# -ge 2 ]] && die ""Expected single commit to edit. Defaults to HEAD~""

# Default to editing the parent of the most recent commit
# The most recent commit can be edited with `git commit --amend`
commit=$(git rev-parse --short ""${1:-HEAD~}"")
message=$(git log -1 --format='%h %s' ""$commit"")

if [[ $OSTYPE =~ ^darwin ]]; then
  sed_inplace=(sed -Ei """")
else
  sed_inplace=(sed -Ei)
fi

export GIT_SEQUENCE_EDITOR=""${sed_inplace[*]} ""' ""s/^pick ('""$commit""' .*)/edit \\1/""'
git rebase --quiet --interactive --autostash --autosquash ""$commit""~
git reset --quiet @~ ""$(git rev-parse --show-toplevel)""  # Reset the cache of the toplevel directory to the previous commit
git commit --quiet --amend --no-edit --allow-empty  #  Commit an empty commit so that that cache diffs are un-reversed

echo
echo ""Editing commit: $message"" >&2
echo

    Changing the Last Commit:
git commit --amend
// or
git commit --amend -m ""an updated commit message""

Dont amend public commits
Amended commits are actually entirely new commits and the previous commit will no longer be on your current branch.
For example, if you want to change the last three commit messages, or any of the commit messages in that group, you supply as an argument to git rebase -i the parent of the last commit you want to edit, which is HEAD~2^ or HEAD~3. It may be easier to remember the ~3 because youre trying to edit the last three commits, but keep in mind that youre actually designating four commits ago, the parent of the last commit you want to edit:
$ git rebase -i HEAD~3

know more
    If you haven't already pushed the commits then you can go back to a previous commit using git reset HEAD^[1,2,3,4...]

For example

git commit <file1> -m ""Updated files 1 and 2""
git commit <file3> -m ""Updated file 3""


Oops, forgot to add file2 to the first commit...

git reset HEAD^1 // because I only need to go back 1 commit

git add <file2>


This will add file2 to the first commit.
    Well, this solution might sound very silly, but can save you in certain conditions.

A friend of mine just ran into accidentally committing very some huge files (four auto-generated files ranging between 3GB to 5GB each) and then made some additional code commits on top of that before realizing the problem that git push wasn't working any longer!  

The files had been listed in .gitignore but after renaming the container folder, they got exposed and committed!  And now there were a few more commits of the code on top of that, but push was running forever (trying to upload GB of data!) and finally would fail due to Github's file size limits.

The problem with interactive rebase or anything similar was that they would deal with poking around these huge files and would take forever to do anything.  Nevertheless, after spending almost an hour in the CLI, we weren't sure if the files (and deltas) are actually removed from the history or simply not included in the current commits.  The push wasn't working either and my friend was really stuck.

So, the solution I came up with was:


Rename current git folder to ~/Project-old.
Clone the git folder again from github (to ~/Project).  
Checkout to the same branch.
Manually cp -r the files from ~/Project-old folder to ~/Project.  
Make sure the massive files, that are not needed to be checked in are mved, and included in .gitignore properly.  
Also make sure you don't overwrite .git folder in the recently-cloned ~/Project by the old one.  That's where the logs of the problematic history lives!
Now review the changes.  It should be the union of all the recent commits, excluding the problematic files.
Finally commit the changes, and it's good to be push'ed.


The biggest problem with this solution is, it deals with manual copying some files, and also it merges all the recent commits into one (obviously with a new commit-hash.)  B

The big benefits are that, it is very clear in every step, it works great for huge files (as well as sensitive ones), and it doesn't leave any trace in history behind!
    I solved this,

1) by creating new commit with changes i want..

r8gs4r commit 0


2) i know which commit i need to merge with it. which is commit 3.

so, git rebase -i HEAD~4 # 4 represents recent 4 commit (here commit 3 is in 4th place)

3) in interactive rebase recent commit will located at bottom. it will looks alike,

pick q6ade6 commit 3
pick vr43de commit 2
pick ac123d commit 1
pick r8gs4r commit 0


4) here we need to rearrange commit if you want to merge with specific one. it should be like,

parent
|_child

pick q6ade6 commit 3
f r8gs4r commit 0
pick vr43de commit 2
pick ac123d commit 1


after rearrange you need to replace p pick with f (fixup will merge without commit message) or s (squash merge with commit message can change in run time)

and then save your tree.

now merge done with existing commit.


  Note: Its not preferable method unless you're maintain on your own. if
  you have big team size its not a acceptable method to rewrite  git
  tree will end up in conflicts which you know other wont. if you want
  to maintain you tree clean with less commits can try this and if its
  small team otherwise its not preferable.....

    To get a non-interactive command, put a script with this content in your PATH:

#!/bin/sh
#
# git-fixup
# Use staged changes to modify a specified commit
set -e
cmt=$(git rev-parse $1)
git commit --fixup=""$cmt""
GIT_EDITOR=true git rebase -i --autosquash ""$cmt~1""


Use it by staging your changes (with git add) and then run git fixup <commit-to-modify>. Of course, it will still be interactive if you get conflicts.
    Came to this approach (and it is probably exactly the same as using interactive rebase) but for me it's kind of straightforward.

Note: I present this approach for the sake of illustration of what you can do rather than an everyday alternative. Since it has many steps (and possibly some caveats.)

Say you want to change commit 0 and you are currently on feature-branch

some-commit---0---1---2---(feature-branch)HEAD


Checkout to this commit and create a quick-branch. You can also clone your feature branch as a recovery point (before starting).

?(git checkout -b feature-branch-backup)
git checkout 0
git checkout -b quick-branch


You will now have something like this:

0(quick-branch)HEAD---1---2---(feature-branch)


Stage changes, stash everything else.

git add ./example.txt
git stash


Commit changes and checkout back to feature-branch

git commit --amend
git checkout feature-branch


You will now have something like this:

some-commit---0---1---2---(feature-branch)HEAD
           \
             ---0'(quick-branch)


Rebase feature-branch onto quick-branch (resolve any conflicts along the way). Apply stash and remove quick-branch.

git rebase quick-branch
git stash pop
git branch -D quick-branch


And you end up with:

some-commit---0'---1'---2'---HEAD(feature-branch)


Git will not duplicate (although I can't really say to what extent) the 0 commit when rebasing.

Note: all commit hashes are changed starting from the commit we originally intended to change.
    For me it was for removing some credentials from a repo.
I tried rebasing and ran into a ton of seemingly unrelated conflicts along the way when trying to rebase --continue.
Don't bother attempting to rebase yourself, use the tool called BFG (brew install bfg) on mac.
    ","[2810, 3775, 624, 112, 43, 27, 51, 13, 14, 21, 10, 2, 1, 1, 5, 6, 7, 2]",1098662,1109,2009-07-27T05:19:07,2021-12-05 21:00:44Z,
"In the shell, what does "" 2>&1 "" mean?","
                
In a Unix shell, if I want to combine stderr and stdout into the stdout stream for further manipulation, I can append the following on the end of my command:

2>&1


So, if I want to use head on the output from g++, I can do something like this:

g++ lots_of_errors 2>&1 | head


so I can see only the first few errors. 

I always have trouble remembering this, and I constantly have to go look it up, and it is mainly because I don't fully understand the syntax of this particular trick.

Can someone break this up and explain character by character what 2>&1  means?
    File descriptor 1 is the standard output (stdout).
File descriptor 2 is the standard error (stderr).
Here is one way to remember this construct (although it is not entirely accurate): at first, 2>1 may look like a good way to redirect stderr to stdout. However, it will actually be interpreted as ""redirect stderr to a file named 1"". & indicates that what follows and precedes is a file descriptor and not a filename. So the construct becomes: 2>&1.
Consider >& as redirect merger operator.
    echo test > afile.txt

redirects stdout to afile.txt. This is the same as doing
echo test 1> afile.txt

To redirect stderr, you do:
echo test 2> afile.txt

So >& is the syntax to redirect a stream to another file descriptor:

0 is stdin
1 is stdout
2 is stderr

You can redirect stdout to stderr by doing:
echo test 1>&2 # or echo test >&2

Or vice versa:
echo test 2>&1

So, in short... 2> redirects stderr to an (unspecified) file, appending &1 redirects stderr to stdout.
    Some tricks about redirection
Some syntax particularity about this may have important behaviours. There is some little samples about redirections, STDERR, STDOUT, and arguments ordering.
1 - Overwriting or appending?
Symbol > means redirection.

> means send to as a whole completed file, overwriting target if exist (see noclobber bash feature at #3 later).
>> means send in addition to would append to target if exist.

In any case, the file would be created if they not exist.
2 - The shell command line is order dependent!!
For testing this, we need a simple command which will send something on both outputs:
$ ls -ld /tmp /tnt
ls: cannot access /tnt: No such file or directory
drwxrwxrwt 118 root root 196608 Jan  7 11:49 /tmp

$ ls -ld /tmp /tnt >/dev/null
ls: cannot access /tnt: No such file or directory

$ ls -ld /tmp /tnt 2>/dev/null
drwxrwxrwt 118 root root 196608 Jan  7 11:49 /tmp

(Expecting you don't have a directory named /tnt, of course ;). Well, we have it!!
So, let's see:
$ ls -ld /tmp /tnt >/dev/null
ls: cannot access /tnt: No such file or directory

$ ls -ld /tmp /tnt >/dev/null 2>&1

$ ls -ld /tmp /tnt 2>&1 >/dev/null
ls: cannot access /tnt: No such file or directory

The last command line dumps STDERR to the console, and it seem not to be the expected behaviour... But...
If you want to make some post filtering about standard output, error output or both:
$ ls -ld /tmp /tnt | sed 's/^.*$/<-- & --->/'
ls: cannot access /tnt: No such file or directory
<-- drwxrwxrwt 118 root root 196608 Jan  7 12:02 /tmp --->

$ ls -ld /tmp /tnt 2>&1 | sed 's/^.*$/<-- & --->/'
<-- ls: cannot access /tnt: No such file or directory --->
<-- drwxrwxrwt 118 root root 196608 Jan  7 12:02 /tmp --->

$ ls -ld /tmp /tnt >/dev/null | sed 's/^.*$/<-- & --->/'
ls: cannot access /tnt: No such file or directory

$ ls -ld /tmp /tnt >/dev/null 2>&1 | sed 's/^.*$/<-- & --->/'

$ ls -ld /tmp /tnt 2>&1 >/dev/null | sed 's/^.*$/<-- & --->/'
<-- ls: cannot access /tnt: No such file or directory --->

Notice that the last command line in this paragraph is exactly same as in previous paragraph, where I wrote seem not to be the expected behaviour (so, this could even be an expected behaviour).
Well, there is a little tricks about redirections, for doing different operation on both outputs:
$ ( ls -ld /tmp /tnt | sed 's/^/O: /' >&9 ) 9>&2  2>&1  | sed 's/^/E: /'
O: drwxrwxrwt 118 root root 196608 Jan  7 12:13 /tmp
E: ls: cannot access /tnt: No such file or directory

Note: &9 descriptor would occur spontaneously because of ) 9>&2.
Addendum: nota! With the new version of bash (>4.0) there is a new feature and more sexy syntax for doing this kind of things:
$ ls -ld /tmp /tnt 2> >(sed 's/^/E: /') > >(sed 's/^/O: /')
O: drwxrwxrwt 17 root root 28672 Nov  5 23:00 /tmp
E: ls: cannot access /tnt: No such file or directory

And finally for such a cascading output formatting:
$ ((ls -ld /tmp /tnt |sed 's/^/O: /' >&9 ) 2>&1 |sed 's/^/E: /') 9>&1| cat -n
     1  O: drwxrwxrwt 118 root root 196608 Jan  7 12:29 /tmp
     2  E: ls: cannot access /tnt: No such file or directory

Addendum: nota! Same new syntax, in both ways:
$ cat -n <(ls -ld /tmp /tnt 2> >(sed 's/^/E: /') > >(sed 's/^/O: /'))
     1  O: drwxrwxrwt 17 root root 28672 Nov  5 23:00 /tmp
     2  E: ls: cannot access /tnt: No such file or directory

Where STDOUT go through a specific filter, STDERR to another and finally both outputs merged go through a third command filter.
2b - Using |& instead
Syntax command |& ... could be used as an alias for command 2>&1 | .... Same rules about command line order applies. More details at What is the meaning of operator |& in bash?
3 - A word about noclobber option and >| syntax
That's about overwriting:
While set -o noclobber instruct bash to not overwrite any existing file, the >| syntax let you pass through this limitation:
$ testfile=$(mktemp /tmp/testNoClobberDate-XXXXXX)

$ date > $testfile ; cat $testfile
Mon Jan  7 13:18:15 CET 2013

$ date > $testfile ; cat $testfile
Mon Jan  7 13:18:19 CET 2013

$ date > $testfile ; cat $testfile
Mon Jan  7 13:18:21 CET 2013

The file is overwritten each time, well now:
$ set -o noclobber

$ date > $testfile ; cat $testfile
bash: /tmp/testNoClobberDate-WW1xi9: cannot overwrite existing file
Mon Jan  7 13:18:21 CET 2013

$ date > $testfile ; cat $testfile
bash: /tmp/testNoClobberDate-WW1xi9: cannot overwrite existing file
Mon Jan  7 13:18:21 CET 2013

Pass through with >|:
$ date >| $testfile ; cat $testfile
Mon Jan  7 13:18:58 CET 2013

$ date >| $testfile ; cat $testfile
Mon Jan  7 13:19:01 CET 2013

Unsetting this option and/or inquiring if already set.
$ set -o | grep noclobber
noclobber           on

$ set +o noclobber

$ set -o | grep noclobber
noclobber           off

$ date > $testfile ; cat $testfile
Mon Jan  7 13:24:27 CET 2013

$ rm $testfile

4 - Last trick and more...
For redirecting both output from a given command, we see that a right syntax could be:
$ ls -ld /tmp /tnt >/dev/null 2>&1

for this special case, there is a shortcut syntax: &> ... or >&
$ ls -ld /tmp /tnt &>/dev/null

$ ls -ld /tmp /tnt >&/dev/null

Nota: if 2>&1 exist, 1>&2 is a correct syntax too:
$ ls -ld /tmp /tnt 2>/dev/null 1>&2

4b- Now, I will let you think about:
$ ls -ld /tmp /tnt 2>&1 1>&2  | sed -e s/^/++/
++/bin/ls: cannot access /tnt: No such file or directory
++drwxrwxrwt 193 root root 196608 Feb  9 11:08 /tmp/

$ ls -ld /tmp /tnt 1>&2 2>&1  | sed -e s/^/++/
/bin/ls: cannot access /tnt: No such file or directory
drwxrwxrwt 193 root root 196608 Feb  9 11:08 /tmp/

4c- If you're interested in more information
You could read the fine manual by hitting:
man -Len -Pless\ +/^REDIRECTION bash

in a bash console ;-)
    I found this brilliant post on redirection: All about redirections

Redirect both standard output and standard error to a file


  $ command &>file


This one-liner uses the &> operator to redirect both output streams - stdout and stderr - from command to file. This is Bash's shortcut for quickly redirecting both streams to the same destination.

Here is how the file descriptor table looks like after Bash has redirected both streams:



As you can see, both stdout and stderr now point to file. So anything written to stdout and stderr gets written to file.

There are several ways to redirect both streams to the same destination. You can redirect each stream one after another:


  $ command >file 2>&1


This is a much more common way to redirect both streams to a file. First stdout is redirected to file, and then stderr is duplicated to be the same as stdout. So both streams end up pointing to file.

When Bash sees several redirections it processes them from left to right. Let's go through the steps and see how that happens. Before running any commands, Bash's file descriptor table looks like this:



Now Bash processes the first redirection >file. We've seen this before and it makes stdout point to file:



Next Bash sees the second redirection 2>&1. We haven't seen this redirection before. This one duplicates file descriptor 2 to be a copy of file descriptor 1 and we get:



Both streams have been redirected to file.

However be careful here! Writing


  command >file 2>&1


is not the same as writing:


  $ command 2>&1 >file


The order of redirects matters in Bash! This command redirects only the standard output to the file. The stderr will still print to the terminal. To understand why that happens, let's go through the steps again. So before running the command, the file descriptor table looks like this:



Now Bash processes redirections left to right. It first sees 2>&1 so it duplicates stderr to stdout. The file descriptor table becomes:



Now Bash sees the second redirect, >file, and it redirects stdout to file:



Do you see what happens here? Stdout now points to file, but the stderr still points to the terminal! Everything that gets written to stderr still gets printed out to the screen! So be very, very careful with the order of redirects!

Also note that in Bash, writing


  $ command &>file


is exactly the same as:


  $ command >&file

    I found this very helpful if you are a beginner read this
Update:
In Linux or Unix System there are two places programs send output to: Standard output (stdout) and Standard Error (stderr).You can redirect these output to any file.
Like if you do this ls -a > output.txt

Nothing will be printed in console all output (stdout) is redirected to output file.
And if you try print the content of any file that does not exits means output will be an error like if you print test.txt that not present in current directory
cat test.txt > error.txt

Output will be
cat: test.txt :No such file or directory

But error.txt file will be empty because we redirecting the stdout to a file not stderr. 
so we need file descriptor( A file descriptor is nothing more than a positive integer that represents an open file. You can say descriptor is unique id of file) to tell shell which type of output we are sending to file .In Unix /Linux system 1 is for stdout and 2 for stderr.

so now if you do this  ls -a 1> output.txt means you are sending Standard output (stdout) to output.txt. 
and if you do this cat test.txt 2> error.txt means you are sending Standard Error (stderr) to error.txt . 
&1 is used to reference the value of the file descriptor 1 (stdout).

Now to the point 2>&1 means Redirect the stderr to the same place we are redirecting the stdout
Now you can do this <br
cat maybefile.txt > output.txt 2>&1 
 both Standard output (stdout) and Standard Error (stderr) will redirected to output.txt.
Thanks to Ondrej K. for pointing out
    The numbers refer to the file descriptors (fd).  


Zero is stdin  
One is stdout 
Two is stderr


2>&1 redirects fd 2 to 1.  

This works for any number of file descriptors if the program uses them.

You can look at /usr/include/unistd.h if you forget them:

/* Standard file descriptors.  */
#define STDIN_FILENO    0   /* Standard input.  */
#define STDOUT_FILENO   1   /* Standard output.  */
#define STDERR_FILENO   2   /* Standard error output.  */


That said I have written C tools that use non-standard file descriptors for custom logging so you don't see it unless you redirect it to a file or something.
    That construct sends the standard error stream (stderr) to the current location of standard output (stdout) - this currency issue appears to have been neglected by the other answers.

You can redirect any output handle to another by using this method but it's most often used to channel stdout and stderr streams into a single stream for processing.

Some examples are:

# Look for ERROR string in both stdout and stderr.
foo 2>&1 | grep ERROR

# Run the less pager without stderr screwing up the output.
foo 2>&1 | less

# Send stdout/err to file (with append) and terminal.
foo 2>&1 |tee /dev/tty >>outfile

# Send stderr to normal location and stdout to file.
foo >outfile1 2>&1 >outfile2


Note that that last one will not direct stderr to outfile2 - it redirects it to what stdout was when the argument was encountered (outfile1) and then redirects stdout to outfile2.

This allows some pretty sophisticated trickery.
    2 is the console standard error.

1 is the console standard output.

This is the standard Unix, and Windows also follows the POSIX.

E.g. when you run

perl test.pl 2>&1


the standard error is redirected to standard output, so you can see both outputs together:

perl test.pl > debug.log 2>&1


After execution, you can see all the output, including errors, in the debug.log.

perl test.pl 1>out.log 2>err.log


Then standard output goes to out.log, and standard error to err.log.

I suggest you to try to understand these.
    2>&1 is a POSIX shell construct. Here is a breakdown, token by token:



2: ""Standard error"" output file descriptor.

>&: Duplicate an Output File Descriptor operator (a variant of Output Redirection operator >). Given [x]>&[y], the file descriptor denoted by x is made to be a copy of the output file descriptor y.

1 ""Standard output"" output file descriptor.

The expression 2>&1 copies file descriptor 1 to location 2, so any output written to 2 (""standard error"") in the execution environment goes to the same file originally described by 1 (""standard output"").



Further explanation:

File Descriptor: ""A per-process unique, non-negative integer used to identify an open file for the purpose of file access.""

Standard output/error: Refer to the following note in the Redirection section of the shell documentation:


  Open files are represented by decimal numbers starting with zero. The largest possible value is implementation-defined; however, all implementations shall support at least 0 to 9, inclusive, for use by the application. These numbers are called ""file descriptors"". The values 0, 1, and 2 have special meaning and conventional uses and are implied by certain redirection operations; they are referred to as standard input, standard output, and standard error, respectively. Programs usually take their input from standard input, and write output on standard output. Error messages are usually written on standard error. The redirection operators can be preceded by one or more digits (with no intervening  characters allowed) to designate the file descriptor number.

    To answer your question: It takes any error output (normally sent to stderr) and writes it to standard output (stdout). 

This is helpful with, for example 'more' when you need paging for all output. Some programs like printing usage information into stderr.

To help you remember


1 = standard output (where programs print normal output)
2 = standard error (where programs print errors)


""2>&1"" simply points everything sent to stderr, to stdout instead.

I also recommend reading this post on error redirecting where this subject is covered in full detail.
    
  Redirecting Input
  
  Redirection of input causes the file whose name
  results from the expansion of word to be opened for reading on file
  descriptor n, or the standard  input (file descriptor 0) if n is
  not specified.
  
  The general format for redirecting input is:

[n]<word

  
  Redirecting Output
  
  Redirection of output causes the file whose
  name results from the expansion of word to be opened for writing on
  file descriptor n, or the standard output (file  descriptor 1) if n
  is not specified. If the file does not exist it is    created; if it
  does exist it is truncated to zero size.
  
  The general format for redirecting output is:

[n]>word

  
  Moving File Descriptors
  
  The redirection operator,

[n]<&digit-

  
  moves the file descriptor digit to file descriptor n, or the
  standard  input (file  descriptor 0) if n is not specified.
  digit is closed after being duplicated to n.
  
  Similarly, the redirection operator

[n]>&digit-

  
  moves the file descriptor digit to file descriptor n, or the
  standard output (file descriptor 1) if n is not specified.


Ref:

man bash


Type /^REDIRECT to locate to the redirection section, and learn more...

An online version is here: 3.6 Redirections

PS:

Lots of the time, man was the powerful tool to learn Linux.
    Provided that /foo does not exist on your system and /tmp does

$ ls -l /tmp /foo


will print the contents of /tmp and print an error message for /foo

$ ls -l /tmp /foo > /dev/null


will send the contents of /tmp to /dev/null and print an error message for /foo

$ ls -l /tmp /foo 1> /dev/null


will do exactly the same (note the 1)

$ ls -l /tmp /foo 2> /dev/null


will print the contents of /tmp and send the error message to /dev/null

$ ls -l /tmp /foo 1> /dev/null 2> /dev/null


will send both the listing as well as the error message to /dev/null

$ ls -l /tmp /foo > /dev/null 2> &1


is shorthand
    unix_commands 2>&1

This is used to print errors to the terminal.

The following illustrates the process


When errors are produced, they are written into the standard error memory address &2 ""buffer"", from which the standard error stream 2 references.
When output is produced, it is written into the standard output memory address &1 ""buffer"", from which the standard output stream 1 references.


So take the unix_commands standard error stream 2, and redirect > the stream (of errors) to the standard output memory address &1, so that they will be streamed to the terminal and printed.
    People, always remember paxdiablo's hint about the current location of the redirection target... It is important.

My personal mnemonic for the 2>&1 operator is this:


Think of & as meaning 'and' or 'add' (the character is an ampers-and, isn't it?)
So it becomes: 'redirect 2 (stderr) to where 1 (stdout) already/currently is and add both streams'.


The same mnemonic works for the other frequently used redirection too, 1>&2:


Think of & meaning and or add... (you get the idea about the ampersand, yes?)
So it becomes:  'redirect 1 (stdout) to where 2 (stderr) already/currently is and add both streams'.


And always remember: you have to read chains of redirections 'from the end', from right to left (not from left to right).
    From a programmer's point of view, it means precisely this:

dup2(1, 2);


See the man page.

Understanding that 2>&1 is a copy also explains why ...

command >file 2>&1


... is not the same as ...

command 2>&1 >file


The first will send both streams to file, whereas the second will send errors to stdout, and ordinary output into file.
    This is just like passing the error to the stdout or the terminal.

That is, cmd is not a command:

$cmd 2>filename
cat filename

command not found


The error is sent to the file like this:

2>&1


Standard error is sent to the terminal.
    0 for input, 1 for stdout and 2 for stderr.

One Tip:
somecmd >1.txt 2>&1 is correct, while somecmd 2>&1 >1.txt is totally wrong with no effect!
    Note that 1>&2 cannot be used interchangeably with 2>&1.
Imagine your command depends on piping, for example:
docker logs 1b3e97c49e39 2>&1 | grep ""some log""
grepping will happen across both stderr and stdout since stderr is basically merged into stdout.
However, if you try:
docker logs 1b3e97c49e39 1>&2 | grep ""some log"",
grepping will not really search anywhere at all because Unix pipe is connecting processes via connecting stdout | stdin, and stdout in the second case was redirected to stderr in which Unix pipe has no interest.
    ","[2807, 3131, 820, 383, 167, 55, 117, 61, 23, 22, 19, 8, 7, 3, 8, 11, 5, 1, 0]",1449531,1136,2009-05-03T22:57:00,2022-02-26 16:08:10Z,bash bash 
Length of a JavaScript object,"
                
I have a JavaScript object. Is there a built-in or accepted best practice way to get the length of this object?
const myObject = new Object();
myObject[""firstname""] = ""Gareth"";
myObject[""lastname""] = ""Simpson"";
myObject[""age""] = 21;

    Updated answer
Here's an update as of 2016 and widespread deployment of ES5 and beyond.  For IE9+ and all other modern ES5+ capable browsers, you can use Object.keys() so the above code just becomes:
var size = Object.keys(myObj).length;

This doesn't have to modify any existing prototype since Object.keys() is now built-in.
Edit: Objects can have symbolic properties that can not be returned via Object.key method. So the answer would be incomplete without mentioning them.
Symbol type was added to the language to create unique identifiers for object properties. The main benefit of the Symbol type is the prevention of overwrites.
Object.keys or Object.getOwnPropertyNames does not work for symbolic properties. To return them you need to use Object.getOwnPropertySymbols.
var person = {
  [Symbol('name')]: 'John Doe',
  [Symbol('age')]: 33,
  ""occupation"": ""Programmer""
};

const propOwn = Object.getOwnPropertyNames(person);
console.log(propOwn.length); // 1

let propSymb = Object.getOwnPropertySymbols(person);
console.log(propSymb.length); // 2

Older answer
The most robust answer (i.e. that captures the intent of what you're trying to do while causing the fewest bugs) would be:
Object.size = function(obj) {
  var size = 0,
    key;
  for (key in obj) {
    if (obj.hasOwnProperty(key)) size++;
  }
  return size;
};

// Get the size of an object
const myObj = {}
var size = Object.size(myObj);

There's a sort of convention in JavaScript that you don't add things to Object.prototype, because it can break enumerations in various libraries. Adding methods to Object is usually safe, though.

    If you know you don't have to worry about hasOwnProperty checks, you can use the Object.keys() method in this way:
Object.keys(myArray).length

    Updated: If you're using Underscore.js (recommended, it's lightweight!), then you can just do

_.size({one : 1, two : 2, three : 3});
=> 3


If not, and you don't want to mess around with Object properties for whatever reason, and are already using jQuery, a plugin is equally accessible:

$.assocArraySize = function(obj) {
    // http://stackoverflow.com/a/6700/11236
    var size = 0, key;
    for (key in obj) {
        if (obj.hasOwnProperty(key)) size++;
    }
    return size;
};

    Simply use this to get the length:

Object.keys(myObject).length

    We can find the length of Object by using:
const myObject = {};
console.log(Object.values(myObject).length);

    Here's the most cross-browser solution.

This is better than the accepted answer because it uses native Object.keys if exists.
Thus, it is the fastest for all modern browsers.

if (!Object.keys) {
    Object.keys = function (obj) {
        var arr = [],
            key;
        for (key in obj) {
            if (obj.hasOwnProperty(key)) {
                arr.push(key);
            }
        }
        return arr;
    };
}

Object.keys(obj).length;

    I'm not a JavaScript expert, but it looks like you would have to loop through the elements and count them since Object doesn't have a length method:

var element_count = 0;
for (e in myArray) {  if (myArray.hasOwnProperty(e)) element_count++; }


@palmsey: In fairness to the OP, the JavaScript documentation actually explicitly refer to using variables of type Object in this manner as ""associative arrays"".
    Use Object.keys(myObject).length to get the length of object/array
var myObject = new Object();
myObject[""firstname""] = ""Gareth"";
myObject[""lastname""] = ""Simpson"";
myObject[""age""] = 21;

console.log(Object.keys(myObject).length); //3

    This method gets all your object's property names in an array, so you can get the length of that array which is equal to your object's keys' length.

Object.getOwnPropertyNames({""hi"":""Hi"",""msg"":""Message""}).length; // => 2

    Here is a completely different solution that will only work in more modern browsers (InternetExplorer9+, Chrome, Firefox 4+, Opera 11.60+, and Safari 5.1+)
See this jsFiddle.
Setup your associative array class
/**
 * @constructor
 */
AssociativeArray = function () {};

// Make the length property work
Object.defineProperty(AssociativeArray.prototype, ""length"", {
    get: function () {
        var count = 0;
        for (var key in this) {
            if (this.hasOwnProperty(key))
                count++;
        }
        return count;
    }
});

Now you can use this code as follows...
var a1 = new AssociativeArray();
a1[""prop1""] = ""test"";
a1[""prop2""] = 1234;
a1[""prop3""] = ""something else"";
alert(""Length of array is "" + a1.length);

    The simplest way is like this:
Object.keys(myobject).length

Where myobject is the object of what you want the length of.
    If you need an associative data structure that exposes its size, better use a map instead of an object.
const myMap = new Map();

myMap.set(""firstname"", ""Gareth"");
myMap.set(""lastname"", ""Simpson"");
myMap.set(""age"", 21);

console.log(myMap.size); // 3

    <script>
myObj = {""key1"" : ""Hello"", ""key2"" : ""Goodbye""};
var size = Object.keys(myObj).length;
console.log(size);
</script>

<p id=""myObj"">The number of <b>keys</b> in <b>myObj</b> are: <script>document.write(size)</script></p>


This works for me:

var size = Object.keys(myObj).length;

    To not mess with the prototype or other code, you could build and extend your own object:

function Hash(){
    var length=0;
    this.add = function(key, val){
         if(this[key] == undefined)
         {
           length++;
         }
         this[key]=val;
    }; 
    this.length = function(){
        return length;
    };
}

myArray = new Hash();
myArray.add(""lastname"", ""Simpson"");
myArray.add(""age"", 21);
alert(myArray.length()); // will alert 2


If you always use the add method, the length property will be correct. If you're worried that you or others forget about using it, you could add the property counter which the others have posted to the length method, too.

Of course, you could always overwrite the methods. But even if you do, your code would probably fail noticeably, making it easy to debug. ;)
    Here's how and don't forget to check that the property is not on the prototype chain:

var element_count = 0;
for(var e in myArray)
    if(myArray.hasOwnProperty(e))
        element_count++;

    const myObject = new Object();
myObject[""firstname""] = ""Gareth"";
myObject[""lastname""] = ""Simpson"";
myObject[""age""] = 21;

console.log(Object.keys(myObject).length)

// o/p 3

    Try: Object.values(theObject).length
const myObject = new Object();
myObject[""firstname""] = ""Gareth"";
myObject[""lastname""] = ""Simpson"";
myObject[""age""] = 21;
console.log(Object.values(myObject).length);

    var myObject = new Object();
myObject[""firstname""] = ""Gareth"";
myObject[""lastname""] = ""Simpson"";
myObject[""age""] = 21;



Object.values(myObject).length
Object.entries(myObject).length
Object.keys(myObject).length

    A variation on some of the above is:

var objLength = function(obj){    
    var key,len=0;
    for(key in obj){
        len += Number( obj.hasOwnProperty(key) );
    }
    return len;
};


It is a bit more elegant way to integrate hasOwnProp.
    

@palmsey: In fairness to the OP, the JavaScript documentation  actually explicitly refer to using variables of type Object in this manner as ""associative arrays"".


And in fairness to @palmsey he was quite correct. They aren't associative arrays; they're definitely objects :) - doing the job of an associative array. But as regards to the wider point, you definitely seem to have the right of it according to this rather fine article I found:
JavaScript Associative Arrays Considered Harmful
But according to all this, the accepted answer itself is bad practice?


Specify a prototype size() function for Object


If anything else has been added to Object .prototype, then the suggested code will fail:
<script type=""text/javascript"">
Object.prototype.size = function () {
  var len = this.length ? --this.length : -1;
    for (var k in this)
      len++;
  return len;
}
Object.prototype.size2 = function () {
  var len = this.length ? --this.length : -1;
    for (var k in this)
      len++;
  return len;
}
var myArray = new Object();
myArray[""firstname""] = ""Gareth"";
myArray[""lastname""] = ""Simpson"";
myArray[""age""] = 21;
alert(""age is "" + myArray[""age""]);
alert(""length is "" + myArray.size());
</script>

I don't think that answer should be the accepted one as it can't be trusted to work if you have any other code running in the same execution context. To do it in a robust fashion, surely you would need to define the size method within myArray and check for the type of the members as you iterate through them.
    Use:

var myArray = new Object();
myArray[""firstname""] = ""Gareth"";
myArray[""lastname""] = ""Simpson"";
myArray[""age""] = 21;
obj = Object.keys(myArray).length;
console.log(obj)

    You can always do Object.getOwnPropertyNames(myObject).length to get the same result as [].length would give for normal array.
    With the ECMAScript6 in-built Reflect object, you can easily count the properties of an object:
Reflect.ownKeys(targetObject).length

It will give you the length of the target object's own properties (important).
Reflect.ownKeys(target)


Returns an array of the target object's own (not inherited) property
keys.

Now, what does that mean? To explain this, let's see this example.
function Person(name, age){
  this.name = name;
  this.age = age;
}

Person.prototype.getIntro= function() {
  return `${this.name} is ${this.age} years old!!`
}

let student = new Person('Anuj', 11);

console.log(Reflect.ownKeys(student).length) // 2
console.log(student.getIntro()) // Anuj is 11 years old!!

You can see here, it returned only its own properties while the object is still inheriting the property from its parent.
For more information, refer this: Reflect API
    For some cases it is better to just store the size in a separate variable. Especially, if you're adding to the array by one element in one place and can easily increment the size. It would obviously work much faster if you need to check the size often.
    If you don't care about supporting Internet Explorer 8 or lower, you can easily get the number of properties in an object by applying the following two steps:


Run either Object.keys() to get an array that contains the names of only those properties that are enumerable or Object.getOwnPropertyNames() if you want to also include the names of properties that are not enumerable.
Get the .length property of that array.




If you need to do this more than once, you could wrap this logic in a function:

function size(obj, enumerablesOnly) {
    return enumerablesOnly === false ?
        Object.getOwnPropertyNames(obj).length :
        Object.keys(obj).length;
}


How to use this particular function:

var myObj = Object.create({}, {
    getFoo: {},
    setFoo: {}
});
myObj.Foo = 12;

var myArr = [1,2,5,4,8,15];

console.log(size(myObj));        // Output : 1
console.log(size(myObj, true));  // Output : 1
console.log(size(myObj, false)); // Output : 3
console.log(size(myArr));        // Output : 6
console.log(size(myArr, true));  // Output : 6
console.log(size(myArr, false)); // Output : 7


See also this Fiddle for a demo.
    What about something like this --

function keyValuePairs() {
    this.length = 0;
    function add(key, value) { this[key] = value; this.length++; }
    function remove(key) { if (this.hasOwnProperty(key)) { delete this[key]; this.length--; }}
}

    If we have the hash 


  hash =  {""a"" : ""b"", ""c"": ""d""};


we can get the length using the length of the keys which is the length of the hash:


  keys(hash).length

    If you are using AngularJS 1.x you can do things the AngularJS way by creating a filter and using the code from any of the other examples such as the following:

// Count the elements in an object
app.filter('lengthOfObject', function() {
  return function( obj ) {
    var size = 0, key;
    for (key in obj) {
      if (obj.hasOwnProperty(key)) size++;
    }
   return size;
 }
})


Usage

In your controller:

$scope.filterResult = $filter('lengthOfObject')($scope.object)


Or in your view:

<any ng-expression=""object | lengthOfObject""></any>

    Here you can give any kind of varible array,object,string
function length2(obj){
    if (typeof obj==='object' && obj!== null){return Object.keys(obj).length;}
   //if (Array.isArray){return obj.length;}
    return obj.length;

}

    Simple one liner:
console.log(Object.values({id:""1"",age:23,role_number:90}).length);

    ","[2804, 3063, 1986, 305, 39, 24, 66, 42, 17, 36, 21, 15, 17, 17, 27, 22, 10, 3, 12, 9, 15, 16, 6, 3, 16, 9, 12, 12, 11, 1, 1]",2490447,546,2008-08-07T19:42:21,2022-04-21 16:10:49Z,javascript 
How can I save an activity state using the save instance state?,"
                
I've been working on the Android SDK platform, and it is a little unclear how to save an application's state. So given this minor re-tooling of the 'Hello, Android' example:
package com.android.hello;

import android.app.Activity;
import android.os.Bundle;
import android.widget.TextView;

public class HelloAndroid extends Activity {

  private TextView mTextView = null;

  /** Called when the activity is first created. */
  @Override
  public void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);

    mTextView = new TextView(this);

    if (savedInstanceState == null) {
       mTextView.setText(""Welcome to HelloAndroid!"");
    } else {
       mTextView.setText(""Welcome back."");
    }

    setContentView(mTextView);
  }
}

I thought it would be enough for the simplest case, but it always responds with the first message, no matter how I navigate away from the app.
I'm sure the solution is as simple as overriding onPause or something like that, but I've been poking away in the documentation for 30 minutes or so and haven't found anything obvious.
    You need to override onSaveInstanceState(Bundle savedInstanceState) and write the application state values you want to change to the Bundle parameter like this:

@Override
public void onSaveInstanceState(Bundle savedInstanceState) {
  super.onSaveInstanceState(savedInstanceState);
  // Save UI state changes to the savedInstanceState.
  // This bundle will be passed to onCreate if the process is
  // killed and restarted.
  savedInstanceState.putBoolean(""MyBoolean"", true);
  savedInstanceState.putDouble(""myDouble"", 1.9);
  savedInstanceState.putInt(""MyInt"", 1);
  savedInstanceState.putString(""MyString"", ""Welcome back to Android"");
  // etc.
}


The Bundle is essentially a way of storing a NVP (""Name-Value Pair"") map, and it will get passed in to onCreate() and also onRestoreInstanceState() where you would then extract the values from activity like this:

@Override
public void onRestoreInstanceState(Bundle savedInstanceState) {
  super.onRestoreInstanceState(savedInstanceState);
  // Restore UI state from the savedInstanceState.
  // This bundle has also been passed to onCreate.
  boolean myBoolean = savedInstanceState.getBoolean(""MyBoolean"");
  double myDouble = savedInstanceState.getDouble(""myDouble"");
  int myInt = savedInstanceState.getInt(""MyInt"");
  String myString = savedInstanceState.getString(""MyString"");
}


Or from a fragment.

@Override
public void onViewStateRestored(@Nullable Bundle savedInstanceState) {
    super.onViewStateRestored(savedInstanceState);
    // Restore UI state from the savedInstanceState.
    // This bundle has also been passed to onCreate.
    boolean myBoolean = savedInstanceState.getBoolean(""MyBoolean"");
    double myDouble = savedInstanceState.getDouble(""myDouble"");
    int myInt = savedInstanceState.getInt(""MyInt"");
    String myString = savedInstanceState.getString(""MyString"");
}


You would usually use this technique to store instance values for your application (selections, unsaved text, etc.).
    Note that it is not safe to use onSaveInstanceState and onRestoreInstanceState for persistent data, according to  the documentation on Activity.
The document states (in the 'Activity Lifecycle' section):

Note that it is important to save
persistent data in onPause() instead
of onSaveInstanceState(Bundle)
because the later is not part of the
lifecycle callbacks, so will not be
called in every situation as described
in its documentation.

In other words, put your save/restore code for persistent data in onPause() and onResume()!
For further clarification, here's the onSaveInstanceState() documentation:

This method is called before an activity may be killed so that when it
comes back some time in the future it can restore its state. For
example, if activity B is launched in front of activity A, and at some
point activity A is killed to reclaim resources, activity A will have
a chance to save the current state of its user interface via this
method so that when the user returns to activity A, the state of the
user interface can be restored via onCreate(Bundle) or
onRestoreInstanceState(Bundle).

    The savedInstanceState is only for saving state associated with a current instance of an Activity, for example current navigation or selection info, so that if Android destroys and recreates an Activity, it can come back as it was before.  See the documentation for onCreate and onSaveInstanceState

For more long lived state, consider using a SQLite database, a file, or preferences.  See Saving Persistent State.
    My colleague wrote an article explaining application state on Android devices, including explanations on activity lifecycle and state information, how to store state information, and saving to state Bundle and SharedPreferences. Take a look at it here.
The article covers three approaches:
Store local variable/UI control data for application lifetime (i.e. temporarily) using an instance state bundle
[Code sample  Store state in state bundle]
@Override
public void onSaveInstanceState(Bundle savedInstanceState)
{
  // Store UI state to the savedInstanceState.
  // This bundle will be passed to onCreate on next call.  EditText txtName = (EditText)findViewById(R.id.txtName);
  String strName = txtName.getText().toString();

  EditText txtEmail = (EditText)findViewById(R.id.txtEmail);
  String strEmail = txtEmail.getText().toString();

  CheckBox chkTandC = (CheckBox)findViewById(R.id.chkTandC);
  boolean blnTandC = chkTandC.isChecked();

  savedInstanceState.putString(Name, strName);
  savedInstanceState.putString(Email, strEmail);
  savedInstanceState.putBoolean(TandC, blnTandC);

  super.onSaveInstanceState(savedInstanceState);
}

Store local variable/UI control data between application instances (i.e. permanently) using shared preferences
[Code sample  store state in SharedPreferences]
@Override
protected void onPause()
{
  super.onPause();

  // Store values between instances here
  SharedPreferences preferences = getPreferences(MODE_PRIVATE);
  SharedPreferences.Editor editor = preferences.edit();  // Put the values from the UI
  EditText txtName = (EditText)findViewById(R.id.txtName);
  String strName = txtName.getText().toString();

  EditText txtEmail = (EditText)findViewById(R.id.txtEmail);
  String strEmail = txtEmail.getText().toString();

  CheckBox chkTandC = (CheckBox)findViewById(R.id.chkTandC);
  boolean blnTandC = chkTandC.isChecked();

  editor.putString(Name, strName); // value to store
  editor.putString(Email, strEmail); // value to store
  editor.putBoolean(TandC, blnTandC); // value to store
  // Commit to storage
  editor.commit();
}

Keeping object instances alive in memory between activities within application lifetime using a retained non-configuration instance
[Code sample  store object instance]
private cMyClassType moInstanceOfAClass; // Store the instance of an object
@Override
public Object onRetainNonConfigurationInstance()
{
  if (moInstanceOfAClass != null) // Check that the object exists
      return(moInstanceOfAClass);
  return super.onRetainNonConfigurationInstance();
}

    onSaveInstanceState is called when the system needs memory and kills an application. It is not called when the user just closes the application. So I think application state should also be saved in onPause.
It should be saved to some persistent storage like Preferences or SQLite.
    This is a classic 'gotcha' of Android development. There are two issues here:

There is a subtle Android Framework bug which greatly complicates application stack management during development, at least on legacy versions (not entirely sure if/when/how it was fixed). I'll discuss this bug below.
The 'normal' or intended way to manage this issue is, itself, rather complicated with the duality of onPause/onResume and onSaveInstanceState/onRestoreInstanceState

Browsing across all these threads, I suspect that much of the time developers are talking about these two different issues simultaneously ... hence all the confusion and reports of ""this doesn't work for me"".
First, to clarify the 'intended' behavior: onSaveInstance and onRestoreInstance are fragile and only for transient state. The intended usage (as far as I can tell) is to handle Activity recreation when the phone is rotated (orientation change). In other words, the intended usage is when your Activity is still logically 'on top', but still must be reinstantiated by the system. The saved Bundle is not persisted outside of the process/memory/GC, so you cannot really rely on this if your activity goes to the background. Yes, perhaps your Activity's memory will survive its trip to the background and escape GC, but this is not reliable (nor is it predictable).
So if you have a scenario where there is meaningful 'user progress' or state that should be persisted between 'launches' of your application, the guidance is to use onPause and onResume. You must choose and prepare a persistent store yourself.
But - there is a very confusing bug which complicates all of this. Details are here:

Activity stack behaves incorrectly during the first run of an app when started from Eclipse (#36907463)

Marketplace / browser app installer allows second instance off app (#36911210)


Basically, if your application is launched with the SingleTask flag, and then later on you launch it from the home screen or launcher menu, then that subsequent invocation will create a NEW task ... you'll effectively have two different instances of your app inhabiting the same stack ... which gets very strange very fast. This seems to happen when you launch your app during development (i.e. from Eclipse or IntelliJ), so developers run into this a lot. But also through some of the app store update mechanisms (so it impacts your users as well).
I battled through these threads for hours before I realized that my main issue was this bug, not the intended framework behavior. A great write-up and workaround (UPDATE: see below) seems to be from user @kaciula in this answer:
Home key press behaviour
UPDATE June 2013: Months later, I have finally found the 'correct' solution. You don't need to manage any stateful startedApp flags yourself. You can detect this from the framework and bail appropriately. I use this near the beginning of my LauncherActivity.onCreate:
if (!isTaskRoot()) {
    Intent intent = getIntent();
    String action = intent.getAction();
    if (intent.hasCategory(Intent.CATEGORY_LAUNCHER) && action != null && action.equals(Intent.ACTION_MAIN)) {
        finish();
        return;
    }
}

    Saving state is a kludge at best as far as I'm concerned. If you need to save persistent data, just use an SQLite database. Android makes it SOOO easy.

Something like this:

import java.util.Date;
import android.content.Context;
import android.database.Cursor;
import android.database.sqlite.SQLiteDatabase;
import android.database.sqlite.SQLiteOpenHelper;

public class dataHelper {

    private static final String DATABASE_NAME = ""autoMate.db"";
    private static final int DATABASE_VERSION = 1;

    private Context context;
    private SQLiteDatabase db;
    private OpenHelper oh ;

    public dataHelper(Context context) {
        this.context = context;
        this.oh = new OpenHelper(this.context);
        this.db = oh.getWritableDatabase();
    }

    public void close() {
        db.close();
        oh.close();
        db = null;
        oh = null;
        SQLiteDatabase.releaseMemory();
    }


    public void setCode(String codeName, Object codeValue, String codeDataType) {
        Cursor codeRow = db.rawQuery(""SELECT * FROM code WHERE codeName = '""+  codeName + ""'"", null);
        String cv = """" ;

        if (codeDataType.toLowerCase().trim().equals(""long"") == true){
            cv = String.valueOf(codeValue);
        }
        else if (codeDataType.toLowerCase().trim().equals(""int"") == true)
        {
            cv = String.valueOf(codeValue);
        }
        else if (codeDataType.toLowerCase().trim().equals(""date"") == true)
        {
            cv = String.valueOf(((Date)codeValue).getTime());
        }
        else if (codeDataType.toLowerCase().trim().equals(""boolean"") == true)
        {
            String.valueOf(codeValue);
        }
        else
        {
            cv = String.valueOf(codeValue);
        }

        if(codeRow.getCount() > 0) //exists-- update
        {
            db.execSQL(""update code set codeValue = '"" + cv +
                ""' where codeName = '"" + codeName + ""'"");
        }
        else // does not exist, insert
        {
            db.execSQL(""INSERT INTO code (codeName, codeValue, codeDataType) VALUES("" +
                    ""'"" + codeName + ""',"" +
                    ""'"" + cv + ""',"" +
                    ""'"" + codeDataType + ""')"" );
        }
    }

    public Object getCode(String codeName, Object defaultValue){

        //Check to see if it already exists
        String codeValue = """";
        String codeDataType = """";
        boolean found = false;
        Cursor codeRow  = db.rawQuery(""SELECT * FROM code WHERE codeName = '""+  codeName + ""'"", null);
        if (codeRow.moveToFirst())
        {
            codeValue = codeRow.getString(codeRow.getColumnIndex(""codeValue""));
            codeDataType = codeRow.getString(codeRow.getColumnIndex(""codeDataType""));
            found = true;
        }

        if (found == false)
        {
            return defaultValue;
        }
        else if (codeDataType.toLowerCase().trim().equals(""long"") == true)
        {
            if (codeValue.equals("""") == true)
            {
                return (long)0;
            }
            return Long.parseLong(codeValue);
        }
        else if (codeDataType.toLowerCase().trim().equals(""int"") == true)
        {
            if (codeValue.equals("""") == true)
            {
                return (int)0;
            }
            return Integer.parseInt(codeValue);
        }
        else if (codeDataType.toLowerCase().trim().equals(""date"") == true)
        {
            if (codeValue.equals("""") == true)
            {
                return null;
            }
            return new Date(Long.parseLong(codeValue));
        }
        else if (codeDataType.toLowerCase().trim().equals(""boolean"") == true)
        {
            if (codeValue.equals("""") == true)
            {
                return false;
            }
            return Boolean.parseBoolean(codeValue);
        }
        else
        {
            return (String)codeValue;
        }
    }


    private static class OpenHelper extends SQLiteOpenHelper {

        OpenHelper(Context context) {
            super(context, DATABASE_NAME, null, DATABASE_VERSION);
        }

        @Override
        public void onCreate(SQLiteDatabase db) {
            db.execSQL(""CREATE TABLE IF  NOT EXISTS code"" +
            ""(id INTEGER PRIMARY KEY, codeName TEXT, codeValue TEXT, codeDataType TEXT)"");
        }

        @Override
        public void onUpgrade(SQLiteDatabase db, int oldVersion, int newVersion) {
        }
    }
}


A simple call after that

dataHelper dh = new dataHelper(getBaseContext());
String status = (String) dh.getCode(""appState"", ""safetyDisabled"");
Date serviceStart = (Date) dh.getCode(""serviceStartTime"", null);
dh.close();
dh = null;

    Both methods are useful and valid and both are best suited for different scenarios:


The user terminates the application and re-opens it at a later date, but the application needs to reload data from the last session  this requires a persistent storage approach such as using SQLite.
The user switches application and then comes back to the original and wants to pick up where they left off -  save and restore bundle data (such as application state data) in onSaveInstanceState() and onRestoreInstanceState() is usually adequate.


If you save the state data in a persistent manner, it can be reloaded in an onResume() or onCreate() (or actually on any lifecycle call). This may or may not be desired behaviour. If you store it in a bundle in an InstanceState, then it is transient and is only suitable for storing data for use in the same user session (I use the term session loosely) but not between sessions.

It is not that one approach is better than the other, like everything, it is just important to understand what behaviour you require and to select the most appropriate approach.
    onSaveInstanceState() for transient data (restored in onCreate()/onRestoreInstanceState()), onPause() for persistent data (restored in onResume()).
From Android technical resources:

onSaveInstanceState() is called by Android if the Activity is being stopped and may be killed before it is resumed! This means it should store any state necessary to re-initialize to the same condition when the Activity is restarted. It is the counterpart to the onCreate() method, and in fact the savedInstanceState Bundle passed in to onCreate() is the same Bundle that you construct as outState in the onSaveInstanceState() method.
onPause() and onResume() are also complimentary methods. onPause() is always called when the Activity ends, even if we instigated that (with a finish() call for example). We will use this to save the current note back to the database. Good practice is to release any resources that can be released during an onPause() as well, to take up less resources when in the passive state.

    Meanwhile I do in general no more use

Bundle savedInstanceState & Co


The life cycle is for most activities too complicated and not necessary.

And Google states itself, it is NOT even reliable.

My way is to save any changes immediately in the preferences:

 SharedPreferences p;
 p.edit().put(..).commit()


In some way SharedPreferences work similar like Bundles.
And naturally and at first such values have to be read from preferences.

In the case of complex data you may use SQLite instead of using preferences.

When applying this concept, the activity just continues to use the last saved state, regardless of whether it was an initial open with reboots in between or a reopen due to the back stack.
    I think I found the answer. Let me tell what I have done in simple words:

Suppose I have two activities, activity1 and activity2 and I am navigating from activity1 to activity2 (I have done some works in activity2) and again back to activity 1 by clicking on a button in activity1. Now at this stage I wanted to go back to activity2 and I want to see my activity2 in the same condition when I last left activity2.

For the above scenario what I have done is that in the manifest I made some changes like this:

<activity android:name="".activity2""
          android:alwaysRetainTaskState=""true""      
          android:launchMode=""singleInstance"">
</activity>


And in the activity1 on the button click event I have done like this:

Intent intent = new Intent();
intent.setFlags(Intent.FLAG_ACTIVITY_REORDER_TO_FRONT);
intent.setClassName(this,""com.mainscreen.activity2"");
startActivity(intent);


And in activity2 on button click event I have done like this:

Intent intent=new Intent();
intent.setClassName(this,""com.mainscreen.activity1"");
startActivity(intent);


Now what will happen is that whatever the changes we have made in the activity2 will not be lost, and we can view activity2 in the same state as we left previously.

I believe this is the answer and this works fine for me. Correct me if I am wrong.
    Really onSaveInstanceState() is called when the Activity goes to background.

Quote from the docs:
""This method is called before an activity may be killed so that when it comes back some time in the future it can restore its state.""
Source
    To help reduce boilerplate I use the following interface and class to read/write to a Bundle for saving instance state.



First, create an interface that will be used to annotate your instance variables:

import java.lang.annotation.Documented;
import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Documented
@Retention(RetentionPolicy.RUNTIME)
@Target({
        ElementType.FIELD
})
public @interface SaveInstance {

}


Then, create a class where reflection will be used to save values to the bundle:

import android.app.Activity;
import android.app.Fragment;
import android.os.Bundle;
import android.os.Parcelable;
import android.util.Log;

import java.io.Serializable;
import java.lang.reflect.Field;

/**
 * Save and load fields to/from a {@link Bundle}. All fields should be annotated with {@link
 * SaveInstance}.</p>
 */
public class Icicle {

    private static final String TAG = ""Icicle"";

    /**
     * Find all fields with the {@link SaveInstance} annotation and add them to the {@link Bundle}.
     *
     * @param outState
     *         The bundle from {@link Activity#onSaveInstanceState(Bundle)} or {@link
     *         Fragment#onSaveInstanceState(Bundle)}
     * @param classInstance
     *         The object to access the fields which have the {@link SaveInstance} annotation.
     * @see #load(Bundle, Object)
     */
    public static void save(Bundle outState, Object classInstance) {
        save(outState, classInstance, classInstance.getClass());
    }

    /**
     * Find all fields with the {@link SaveInstance} annotation and add them to the {@link Bundle}.
     *
     * @param outState
     *         The bundle from {@link Activity#onSaveInstanceState(Bundle)} or {@link
     *         Fragment#onSaveInstanceState(Bundle)}
     * @param classInstance
     *         The object to access the fields which have the {@link SaveInstance} annotation.
     * @param baseClass
     *         Base class, used to get all superclasses of the instance.
     * @see #load(Bundle, Object, Class)
     */
    public static void save(Bundle outState, Object classInstance, Class<?> baseClass) {
        if (outState == null) {
            return;
        }
        Class<?> clazz = classInstance.getClass();
        while (baseClass.isAssignableFrom(clazz)) {
            String className = clazz.getName();
            for (Field field : clazz.getDeclaredFields()) {
                if (field.isAnnotationPresent(SaveInstance.class)) {
                    field.setAccessible(true);
                    String key = className + ""#"" + field.getName();
                    try {
                        Object value = field.get(classInstance);
                        if (value instanceof Parcelable) {
                            outState.putParcelable(key, (Parcelable) value);
                        } else if (value instanceof Serializable) {
                            outState.putSerializable(key, (Serializable) value);
                        }
                    } catch (Throwable t) {
                        Log.d(TAG, ""The field '"" + key + ""' was not added to the bundle"");
                    }
                }
            }
            clazz = clazz.getSuperclass();
        }
    }

    /**
     * Load all saved fields that have the {@link SaveInstance} annotation.
     *
     * @param savedInstanceState
     *         The saved-instance {@link Bundle} from an {@link Activity} or {@link Fragment}.
     * @param classInstance
     *         The object to access the fields which have the {@link SaveInstance} annotation.
     * @see #save(Bundle, Object)
     */
    public static void load(Bundle savedInstanceState, Object classInstance) {
        load(savedInstanceState, classInstance, classInstance.getClass());
    }

    /**
     * Load all saved fields that have the {@link SaveInstance} annotation.
     *
     * @param savedInstanceState
     *         The saved-instance {@link Bundle} from an {@link Activity} or {@link Fragment}.
     * @param classInstance
     *         The object to access the fields which have the {@link SaveInstance} annotation.
     * @param baseClass
     *         Base class, used to get all superclasses of the instance.
     * @see #save(Bundle, Object, Class)
     */
    public static void load(Bundle savedInstanceState, Object classInstance, Class<?> baseClass) {
        if (savedInstanceState == null) {
            return;
        }
        Class<?> clazz = classInstance.getClass();
        while (baseClass.isAssignableFrom(clazz)) {
            String className = clazz.getName();
            for (Field field : clazz.getDeclaredFields()) {
                if (field.isAnnotationPresent(SaveInstance.class)) {
                    String key = className + ""#"" + field.getName();
                    field.setAccessible(true);
                    try {
                        Object fieldVal = savedInstanceState.get(key);
                        if (fieldVal != null) {
                            field.set(classInstance, fieldVal);
                        }
                    } catch (Throwable t) {
                        Log.d(TAG, ""The field '"" + key + ""' was not retrieved from the bundle"");
                    }
                }
            }
            clazz = clazz.getSuperclass();
        }
    }

}




Example usage:

public class MainActivity extends Activity {

    @SaveInstance
    private String foo;

    @SaveInstance
    private int bar;

    @SaveInstance
    private Intent baz;

    @SaveInstance
    private boolean qux;

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        Icicle.load(savedInstanceState, this);
    }

    @Override
    public void onSaveInstanceState(Bundle outState) {
        super.onSaveInstanceState(outState);
        Icicle.save(outState, this);
    }

}




Note: This code was adapted from a library project named AndroidAutowire which is licensed under the MIT license.
    Kotlin

You must override onSaveInstanceState and onRestoreInstanceState to store and retrieve your variables you want to be persistent

Life cycle graph



Store variables

public override fun onSaveInstanceState(savedInstanceState: Bundle) {
    super.onSaveInstanceState(savedInstanceState)

    // prepare variables here
    savedInstanceState.putInt(""kInt"", 10)
    savedInstanceState.putBoolean(""kBool"", true)
    savedInstanceState.putDouble(""kDouble"", 4.5)
    savedInstanceState.putString(""kString"", ""Hello Kotlin"")
}


Retrieve variables

public override fun onRestoreInstanceState(savedInstanceState: Bundle) {
    super.onRestoreInstanceState(savedInstanceState)

    val myInt = savedInstanceState.getInt(""kInt"")
    val myBoolean = savedInstanceState.getBoolean(""kBool"")
    val myDouble = savedInstanceState.getDouble(""kDouble"")
    val myString = savedInstanceState.getString(""kString"")
    // use variables here
}

    To answer the original question directly. savedInstancestate is null because your Activity is never being re-created.

Your Activity will only be re-created with a state bundle when:


Configuration changes such as changing the orientation or phone language which may requires a new activity instance to be created.
You return to the app from the background after the OS has destroyed the activity. 


Android will destroy background activities when under memory pressure or after they've been in the background for an extended period of time.

When testing your hello world example there are a few ways to leave and return to the Activity.


When you press the back button the Activity is finished. Re-launching the app is a brand new instance. You aren't resuming from the background at all.
When you press the home button or use the task switcher the Activity will go into the background. When navigating back to the application onCreate will only be called if the Activity had to be destroyed. 


In most cases if you're just pressing home and then launching the app again the activity won't need to be re-created. It already exists in memory so onCreate() won't be called.

There is an option under Settings -> Developer Options called ""Don't keep activities"". When it's enabled Android will always destroy activities and recreate them when they're backgrounded. This is a great option to leave enabled when developing because it simulates the worst case scenario. ( A low memory device recycling your activities all the time ).

The other answers are valuable in that they teach you the correct ways to store state but I didn't feel they really answered WHY your code wasn't working in the way you expected.
    The onSaveInstanceState(bundle) and onRestoreInstanceState(bundle) methods are useful for data persistence merely while rotating the screen (orientation change).
They are not even good while switching between applications (since the onSaveInstanceState() method is called but onCreate(bundle) and onRestoreInstanceState(bundle) is not invoked again.
For more persistence use shared preferences. read this article  
    Now Android provides ViewModels for saving state, you should try to use that instead of saveInstanceState.
    My problem was that I needed persistence only during the application lifetime (i.e. a single execution including starting other sub-activities within the same app and rotating the device etc). I tried various combinations of the above answers but did not get what I wanted in all situations. In the end what worked for me was to obtain a reference to the savedInstanceState during onCreate:

mySavedInstanceState=savedInstanceState;


and use that to obtain the contents of my variable when I needed it, along the lines of:

if (mySavedInstanceState !=null) {
   boolean myVariable = mySavedInstanceState.getBoolean(""MyVariable"");
}


I use onSaveInstanceStateand onRestoreInstanceState as suggested above but I guess i could also or alternatively use my method to save the variable when it changes (e.g. using putBoolean)
    Kotlin code:

save:

override fun onSaveInstanceState(outState: Bundle) {
    super.onSaveInstanceState(outState.apply {
        putInt(""intKey"", 1)
        putString(""stringKey"", ""String Value"")
        putParcelable(""parcelableKey"", parcelableObject)
    })
}


and then in onCreate() or onRestoreInstanceState() 

    val restoredInt = savedInstanceState?.getInt(""intKey"") ?: 1 //default int
    val restoredString = savedInstanceState?.getString(""stringKey"") ?: ""default string""
    val restoredParcelable = savedInstanceState?.getParcelable<ParcelableClass>(""parcelableKey"") ?: ParcelableClass() //default parcelable


Add default values if you don't want to have Optionals
    using Android ViewModel & SavedStateHandle to persist the serializable data
public class MainActivity extends AppCompatActivity {
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        ActivityMainBinding binding = ActivityMainBinding.inflate(getLayoutInflater());
        binding.setViewModel(new ViewModelProvider(this).get(ViewModel.class));
        binding.setLifecycleOwner(this);
        setContentView(binding.getRoot());
    }

    public static class ViewModel extends AndroidViewModel {

        //This field SURVIVE the background process reclaim/killing & the configuration change
        public final SavedStateHandle savedStateHandle;

        //This field NOT SURVIVE the background process reclaim/killing but SURVIVE the configuration change
        public final MutableLiveData<String> inputText2 = new MutableLiveData<>();


        public ViewModel(@NonNull Application application, SavedStateHandle savedStateHandle) {
            super(application);
            this.savedStateHandle = savedStateHandle;
        }
    }
}

in layout file
<?xml version=""1.0"" encoding=""utf-8""?>
<layout xmlns:android=""http://schemas.android.com/apk/res/android"">

    <data>

        <variable
            name=""viewModel""
            type=""com.xxx.viewmodelsavedstatetest.MainActivity.ViewModel"" />
    </data>

    <LinearLayout xmlns:tools=""http://schemas.android.com/tools""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""
        android:orientation=""vertical""
        tools:context="".MainActivity"">


        <EditText
            android:layout_width=""match_parent""
            android:layout_height=""wrap_content""
            android:autofillHints=""""
            android:hint=""This field SURVIVE the background process reclaim/killing &amp; the configuration change""
            android:text='@={(String)viewModel.savedStateHandle.getLiveData(""activity_main/inputText"", """")}' />

        <SeekBar
            android:layout_width=""match_parent""
            android:layout_height=""wrap_content""
            android:max=""100""
            android:progress='@={(Integer)viewModel.savedStateHandle.getLiveData(""activity_main/progress"", 50)}' />

        <EditText
            android:layout_width=""match_parent""
            android:layout_height=""wrap_content""
            android:hint=""This field SURVIVE the background process reclaim/killing &amp; the configuration change""
            android:text='@={(String)viewModel.savedStateHandle.getLiveData(""activity_main/inputText"", """")}' />

        <SeekBar
            android:layout_width=""match_parent""
            android:layout_height=""wrap_content""
            android:max=""100""
            android:progress='@={(Integer)viewModel.savedStateHandle.getLiveData(""activity_main/progress"", 50)}' />

        <EditText
            android:layout_width=""match_parent""
            android:layout_height=""wrap_content""
            android:hint=""This field NOT SURVIVE the background process reclaim/killing but SURVIVE the configuration change""
            android:text='@={viewModel.inputText2}' />

    </LinearLayout>
</layout>

Test:
1. start the test activity
2. press home key to go home
3. adb shell kill <the test activity process>
4. open recent app list and restart the test activity

    When an activity is created it's  onCreate() method is called.

   @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
    }


savedInstanceState is an object of Bundle class which is null for the first time, but it contains values when it is recreated. To save Activity's state you have to override onSaveInstanceState().

   @Override
    protected void onSaveInstanceState(Bundle outState) {
      outState.putString(""key"",""Welcome Back"")
        super.onSaveInstanceState(outState);       //save state
    }


put your values in ""outState"" Bundle object like outState.putString(""key"",""Welcome Back"") and save by calling super.
When activity will be destroyed it's state get saved in Bundle object and can be restored after recreation in onCreate() or onRestoreInstanceState(). Bundle received in onCreate() and onRestoreInstanceState() are same.

   @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

          //restore activity's state
         if(savedInstanceState!=null){
          String reStoredString=savedInstanceState.getString(""key"");
            }
    }


or

  //restores activity's saved state
 @Override
    protected void onRestoreInstanceState(Bundle savedInstanceState) {
      String restoredMessage=savedInstanceState.getString(""key"");
    }

    Although the accepted answer is correct, there is a faster and easier method to save the Activity state on Android using a library called Icepick. Icepick is an annotation processor that takes care of all the boilerplate code used in saving and restoring state for you. 

Doing something like this with Icepick:

class MainActivity extends Activity {
  @State String username; // These will be automatically saved and restored
  @State String password;
  @State int age;

  @Override public void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    Icepick.restoreInstanceState(this, savedInstanceState);
  }

  @Override public void onSaveInstanceState(Bundle outState) {
    super.onSaveInstanceState(outState);
    Icepick.saveInstanceState(this, outState);
  }
}


Is the same as doing this:

class MainActivity extends Activity {
  String username;
  String password;
  int age;

  @Override
  public void onSaveInstanceState(Bundle savedInstanceState) {
    super.onSaveInstanceState(savedInstanceState);
    savedInstanceState.putString(""MyString"", username);
    savedInstanceState.putString(""MyPassword"", password);
    savedInstanceState.putInt(""MyAge"", age); 
    /* remember you would need to actually initialize these variables before putting it in the
    Bundle */
  }

  @Override
  public void onRestoreInstanceState(Bundle savedInstanceState) {
    super.onRestoreInstanceState(savedInstanceState);
    username = savedInstanceState.getString(""MyString"");
    password = savedInstanceState.getString(""MyPassword"");
    age = savedInstanceState.getInt(""MyAge"");
  }
}


Icepick will work with any object that saves its state with a Bundle.
    What to save and what not to?

Ever wondered why the text in the EditText gets saved automatically while an orientation change? Well, this answer is for you.

When an instance of an Activity gets destroyed and the System recreates a new instance (for example, configuration change). It tries to recreate it using a set of saved data of old Activity State (instance state).

Instance state is a collection of key-value pairs stored in a Bundle object.


  By default System saves the View objects in the Bundle for example.



Text in EditText
Scroll position in a ListView, etc.


If you need another variable to be saved as a part of instance state you should OVERRIDE onSavedInstanceState(Bundle savedinstaneState) method.

For example, int currentScore in a GameActivity

More detail about the onSavedInstanceState(Bundle savedinstaneState) while saving data

@Override
public void onSaveInstanceState(Bundle savedInstanceState) {
    // Save the user's current game state
    savedInstanceState.putInt(STATE_SCORE, mCurrentScore);

    // Always call the superclass so it can save the view hierarchy state
    super.onSaveInstanceState(savedInstanceState);
}



  So by mistake if you forget to call
  super.onSaveInstanceState(savedInstanceState);the default behavior
  will not work ie Text in EditText will not save.


Which to choose for restoring Activity state?

 onCreate(Bundle savedInstanceState)


OR

onRestoreInstanceState(Bundle savedInstanceState)


Both methods get the same Bundle object, so it does not really matter where you write your restoring logic. The only difference is that in onCreate(Bundle savedInstanceState) method you will have to give a null check while it is not needed in the latter case. Other answers have already code snippets. You can refer them.

More detail about the onRestoreInstanceState(Bundle savedinstaneState)

@Override
public void onRestoreInstanceState(Bundle savedInstanceState) {
    // Always call the superclass so it can restore the view hierarchy
    super.onRestoreInstanceState(savedInstanceState);

    // Restore state members from the saved instance
    mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
}



  Always call super.onRestoreInstanceState(savedInstanceState); so that System restore the View hierarchy by default


Bonus

The onSaveInstanceState(Bundle savedInstanceState) is invoked by the system only when the user intends to come back to the Activity. For example, you are using App X and suddenly you get a call. You move to the caller app and come back to the app X. In this case the onSaveInstanceState(Bundle savedInstanceState) method will be invoked.

But consider this if a user presses the back button. It is assumed that the user does not intend to come back to the Activity, hence in this case onSaveInstanceState(Bundle savedInstanceState) will not be invoked by the system.
Point being you should consider all the scenarios while saving data.

Relevant links:

Demo on default behavior
Android Official Documentation.
    There is a way to make Android save the states without implementing any method. Just add this line to your Manifest in Activity declaration:

android:configChanges=""orientation|screenSize""


It should look like this:

<activity
    android:name="".activities.MyActivity""
    android:configChanges=""orientation|screenSize"">
</activity>


Here you can find more information about this property. 

It's recommended to let Android handle this for you than the manually handling.
    you can use the Live Data and View Model For Lifecycle Handel From JetPack. see this Reference :

https://developer.android.com/topic/libraries/architecture/livedata
    Instead of that, you should use ViewModel, which will retain the data until the activity life cycle.
    Kotlin Solution:
For custom class save in onSaveInstanceState you can be converted your class to JSON string and restore it with Gson convertion and for single String, Double, Int, Long value save and restore as following. The following example is for Fragment and Activity:

For Activity:

For put data in saveInstanceState:

override fun onSaveInstanceState(outState: Bundle) {
        super.onSaveInstanceState(outState)

        //for custom class-----
        val gson = Gson()
        val json = gson.toJson(your_custom_class)
        outState.putString(""CUSTOM_CLASS"", json)

        //for single value------
        outState.putString(""MyString"", stringValue)
        outState.putBoolean(""MyBoolean"", true)
        outState.putDouble(""myDouble"", doubleValue)
        outState.putInt(""MyInt"", intValue)
    }


Restore data:

 override fun onRestoreInstanceState(savedInstanceState: Bundle) {
    super.onRestoreInstanceState(savedInstanceState)

    //for custom class restore
    val json = savedInstanceState?.getString(""CUSTOM_CLASS"")
    if (!json!!.isEmpty()) {
        val gson = Gson()
        testBundle = gson.fromJson(json, Session::class.java)
    }

  //for single value restore

   val myBoolean: Boolean = savedInstanceState?.getBoolean(""MyBoolean"")
   val myDouble: Double = savedInstanceState?.getDouble(""myDouble"")
   val myInt: Int = savedInstanceState?.getInt(""MyInt"")
   val myString: String = savedInstanceState?.getString(""MyString"")
 }


You can restore it on Activity onCreate also.

For fragment:

For put class in saveInstanceState:

 override fun onSaveInstanceState(outState: Bundle) {
        super.onSaveInstanceState(outState)
        val gson = Gson()
        val json = gson.toJson(customClass)
        outState.putString(""CUSTOM_CLASS"", json)
    }


Restore data:

 override fun onActivityCreated(savedInstanceState: Bundle?) {
        super.onActivityCreated(savedInstanceState)

        //for custom class restore
        if (savedInstanceState != null) {
            val json = savedInstanceState.getString(""CUSTOM_CLASS"")
            if (!json!!.isEmpty()) {
                val gson = Gson()
                val customClass: CustomClass = gson.fromJson(json, CustomClass::class.java)
            }
        }

      // for single value restore
      val myBoolean: Boolean = savedInstanceState.getBoolean(""MyBoolean"")
      val myDouble: Double = savedInstanceState.getDouble(""myDouble"")
      val myInt: Int = savedInstanceState.getInt(""MyInt"")
      val myString: String = savedInstanceState.getString(""MyString"")
    }

    In 2020 we have some changes :
If you want your Activity to restore its state after the process is killed and started again, you may want to use the saved state functionality. Previously, you needed to override two methods in the Activity: onSaveInstanceState and onRestoreInstanceState. You can also access the restored state in the onCreate method. Similarly, in Fragment, you have onSaveInstanceState method available (and the restored state is available in the onCreate, onCreateView, and onActivityCreated methods).
Starting with AndroidX SavedState 1.0.0, which is the dependency of the AndroidX Activity and the AndroidX Fragment, you get access to the SavedStateRegistry. You can obtain the SavedStateRegistry from the Activity/Fragment and then register your SavedStateProvider :
class MyActivity : AppCompatActivity() {

  companion object {
    private const val MY_SAVED_STATE_KEY = ""MY_SAVED_STATE_KEY ""
    private const val SOME_VALUE_KEY = ""SOME_VALUE_KEY ""
  }
    
  private lateinit var someValue: String
  private val savedStateProvider = SavedStateRegistry.SavedStateProvider {    
    Bundle().apply {
      putString(SOME_VALUE_KEY, someValue)
    }
  }
  
  override fun onCreate(savedInstanceState: Bundle?) {    
    super.onCreate(savedInstanceState)
    savedStateRegistry.registerSavedStateProvider(MY_SAVED_STATE_KEY, savedStateProvider)
    someValue = savedStateRegistry.consumeRestoredStateForKey(MY_SAVED_STATE_KEY)?.getString(SOME_VALUE_KEY) ?: """"
  }
  
}

As you can see, SavedStateRegistry enforces you to use the key for your data. This can prevent your data from being corrupted by another SavedStateProvider attached to the same Activity/Fragment.Also you can extract your SavedStateProvider to another class to make it work with your data by using whatever abstraction you want and in such a way achieve the clean saved state behavior in your application.
    Now a days you can use live data and Life-cycle-Aware Components
https://developer.android.com/topic/libraries/architecture/lifecycle
    Not sure if my solution is frowned upon or not, but I use a bound service to persist ViewModel state. Whether you store it in memory in the service or persist and retrieve it from a SQLite database depends on your requirements. This is what services of any flavor do, they provide services such as maintaining application state and abstract common business logic. 

Because of memory and processing constraints inherent on mobile devices, I treat Android views in a similar way to a web page. The page does not maintain state, it is purely a presentation layer component whose only purpose is to present application state and accept user input. Recent trends in web app architecture employ the use of the age-old Model, View, Controller (MVC) pattern, where the page is the View, domain data is the model, and the controller sits behind a web service. The same pattern can be employed in Android with the View being, well ... the View, the model is your domain data, and the Controller is implemented as an Android bound service. Whenever you want a view to interact with the controller, bind to it on start/resume and unbind on stop/pause.

This approach gives you the added bonus of enforcing the Separation of Concern design principle in that all of you application business logic can be moved into your service which reduces duplicated logic across multiple views and allows the view to enforce another important design principle, Single Responsibility.
    ","[2802, 2691, 443, 454, 223, 101, 158, 79, 80, 51, 41, 66, 46, 41, 9, 35, 32, 5, 23, 12, 3, 18, 21, 3, 4, 2, 2, 2, 2, 2, 8]",868454,1075,2008-09-30T04:41:15,2021-11-30 12:15:52Z,
"Is there a standard function to check for null, undefined, or blank variables in JavaScript?","
                
Is there a universal JavaScript function that checks that a variable has a value and ensures that it's not undefined or null? I've got this code, but I'm not sure if it covers all cases:

function isEmpty(val){
    return (val === undefined || val == null || val.length <= 0) ? true : false;
}

    You can just check if the variable has a truthy value or not. That means
if( value ) {
}

will evaluate to true if value is not:

null
undefined
NaN
empty string ("""")
0
false

The above list represents all possible falsy values in ECMA-/Javascript. Find it in the specification at the ToBoolean section.
Furthermore, if you do not know whether a variable exists (that means, if it was declared) you should check with the typeof operator. For instance
if( typeof foo !== 'undefined' ) {
    // foo could get resolved and it's defined
}

If you can be sure that a variable is declared at least, you should directly check if it has a truthy value like shown above.
    The verbose method to check if value is undefined or null is:

return value === undefined || value === null;


You can also use the == operator but this expects one to know all the rules:

return value == null; // also returns true if value is undefined

    function isEmpty(value){
  return (value == null || value.length === 0);
}


This will return true for

undefined  // Because undefined == null

null

[]

""""


and zero argument functions since a function's length is the number of declared parameters it takes.

To disallow the latter category, you might want to just check for blank strings

function isEmpty(value){
  return (value == null || value === '');
}

    This is the safest check and I haven't seen it posted here exactly like that:

if (typeof value !== 'undefined' && value) {
    //deal with value'
};


It will cover cases where value was never defined, and also any of these:


null
undefined (value of undefined is not the same as a parameter that was never defined)
0
"""" (empty string)
false
NaN


Edited: Changed to strict equality (!==) because it's the norm by now ;)
    
  Take a look at the new ECMAScript Nullish coalescing operator


You can think of this feature - the ?? operator - as a way to fall back to a default value when dealing with null or undefined.

let x = foo ?? bar();




Again, the above code is equivalent to the following.

let x = (foo !== null && foo !== undefined) ? foo : bar();



    Vacuousness
I don't recommend trying to define or use a function which computes whether any value in the whole world is empty. What does it really mean to be ""empty""? If I have let human = { name: 'bob', stomach: 'empty' }, should isEmpty(human) return true? If I have let reg = new RegExp('');, should isEmpty(reg) return true? What about isEmpty([ null, null, null, null ]) - this list only contains emptiness, so is the list itself empty? I want to put forward here some notes on ""vacuousness"" (an intentionally obscure word, to avoid pre-existing associations) in javascript - and I want to argue that ""vacuousness"" in javascript values should never be dealt with generically.

Truthiness/Falsiness
For deciding how to determine the ""vacuousness"" of values, we need to accomodate javascript's inbuilt, inherent sense of whether values are ""truthy"" or ""falsy"". Naturally, null and undefined are both ""falsy"". Less naturally, the number 0 (and no other number except NaN) is also ""falsy"". Least naturally: '' is falsy, but [] and {} (and new Set(), and new Map()) are truthy - although they all seem equally vacuous!

Null vs Undefined
There is also some discussion concerning null vs undefined - do we really need both in order to express vacuousness in our programs? I personally avoid ever having the letters u, n, d, e, f, i, n, e, d appear in my code in that order. I always use null to signify ""vacuousness"". Again, though, we need to accomodate javascript's inherent sense of how null and undefined differ:

Trying to access a non-existent property gives undefined
Omitting a parameter when calling a function results in that parameter receiving undefined:

let f = a => a;
console.log(f('hi'));
console.log(f());


Parameters with default values receive the default only when given undefined, not null:

let f = (v='hello') => v;
console.log(f(null));
console.log(f(undefined));

To me, null is an explicit signifier of vacuousness; ""something that could have been filled in was intentionally left blank"".
Really undefined is a necessary complication that allows some js features to exist, but in my opinion it should always be left behind the scenes; not interacted with directly. We can think of undefined as, for example, javascript's mechanic for implementing default function arguments. If you refrain from supplying an argument to a function it will receive a value of undefined instead. And a default value will be applied to a function argument if that argument was initially set to undefined. In this case undefined is the linchpin of default function arguments, but it stays in the background: we can achieve default argument functionality without ever referring to undefined:
This is a bad implementation of default arguments as it interacts directly with undefined:
let fnWithDefaults = arg => {
  if (arg === undefined) arg = 'default';
  ...
};

This is a good implementation:
let fnWithDefaults = (arg='default') => { ... };

This is a bad way to accept the default argument:
fnWithDefaults(undefined);

Simply do this instead:
fnWithDefaults();


Non-generic Vacuousness
I believe that vacuousness should never be dealt with in a generic fashion. We should instead always have the rigour to get more information about our data before determining if it is vacuous - I mainly do this by checking what type of data I'm dealing with:
let isType = (value, Cls) => {
  // Intentional use of loose comparison operator detects `null`
  // and `undefined`, and nothing else!
  return value != null && Object.getPrototypeOf(value).constructor === Cls;
};

Note that this function ignores inheritance - it expects value to be a direct instance of Cls, and not an instance of a subclass of Cls. I avoid instanceof for two main reasons:

([] instanceof Object) === true   (""An Array is an Object"")
('' instanceof String) === false  (""A String is not a String"")

Note that Object.getPrototypeOf is used to avoid an (obscure) edge-case such as let v = { constructor: String }; The isType function still returns correctly for isType(v, String) (false), and isType(v, Object) (true).
Overall, I recommend using this isType function along with these tips:

Minimize the amount of code processing values of unknown type. E.g., for let v = JSON.parse(someRawValue);, our v variable is now of unknown type. As early as possible, we should limit our possibilities. The best way to do this can be by requiring a particular type: e.g. if (!isType(v, Array)) throw new Error('Expected Array'); - this is a really quick and expressive way to remove the generic nature of v, and ensure it's always an Array. Sometimes, though, we need to allow v to be of multiple types. In those cases, we should create blocks of code where v is no longer generic, as early as possible:

if (isType(v, String)) {
  /* v isn't generic in this block - It's a String! */
} else if (isType(v, Number)) {
  /* v isn't generic in this block - It's a Number! */
} else if (isType(v, Array)) {
  /* v isn't generic in this block - it's an Array! */
} else {
  throw new Error('Expected String, Number, or Array');
}


Always use ""whitelists"" for validation. If you require a value to be, e.g., a String, Number, or Array, check for those 3 ""white"" possibilities, and throw an Error if none of the 3 are satisfied. We should be able to see that checking for ""black"" possibilities isn't very useful: Say we write if (v === null) throw new Error('Null value rejected'); - this is great for ensuring that null values don't make it through, but if a value does make it through, we still know hardly anything about it. A value v which passes this null-check is still VERY generic - it's anything but null! Blacklists hardly dispell generic-ness.
Unless a value is null, never consider ""a vacuous value"". Instead, consider ""an X which is vacuous"". Essentially, never consider doing anything like if (isEmpty(val)) { /* ... */ } - no matter how that isEmpty function is implemented (I don't want to know...), it isn't meaningful! And it's way too generic! Vacuousness should only be calculated with knowledge of val's type. Vacuousness-checks should look like this:

""A string, with no chars"":
if (isType(val, String) && val.length === 0) ...

""An Object, with 0 props"": if (isType(val, Object) && Object.entries(val).length === 0) ...

""A number, equal or less than zero"": if (isType(val, Number) && val <= 0) ...

""An Array, with no items"": if (isType(val, Array) && val.length === 0) ...

The only exception is when null is used to signify certain functionality. In this case it's meaningful to say: ""A vacuous value"": if (val === null) ...




    This condition check

if (!!foo) {
    //foo is defined
}


is all you need.
    A solution I like a lot:

Let's define that a blank variable is null, or undefined, or if it has length, it is zero, or if it is an object, it has no keys:

function isEmpty (value) {
  return (
    // null or undefined
    (value == null) ||

    // has length and it's zero
    (value.hasOwnProperty('length') && value.length === 0) ||

    // is an Object and has no keys
    (value.constructor === Object && Object.keys(value).length === 0)
  )
}


Returns:


true: undefined, null, """", [], {}
false: true, false, 1, 0, -1, ""foo"", [1, 2, 3], { foo: 1 }

    The probably shortest answer is

val==null || val==''


if you change rigth side to val==='' then empty array will give false. Proof 

function isEmpty(val){
    return val==null || val==''
}

// ------------
// TEST
// ------------

var log = (name,val) => console.log(`${name} -> ${isEmpty(val)}`);

log('null', null);
log('undefined', undefined);
log('NaN', NaN);
log('""""', """");
log('{}', {});
log('[]', []);
log('[1]', [1]);
log('[0]', [0]);
log('[[]]', [[]]);
log('true', true);
log('false', false);
log('""true""', ""true"");
log('""false""', ""false"");
log('Infinity', Infinity);
log('-Infinity', -Infinity);
log('1', 1);
log('0', 0);
log('-1', -1);
log('""1""', ""1"");
log('""0""', ""0"");
log('""-1""', ""-1"");

// ""void 0"" case
console.log('---\n""true"" is:', true);
console.log('""void 0"" is:', void 0);
log(void 0,void 0); // ""void 0"" is ""undefined"" - so we should get here TRUE


More details about == (source here)



BONUS: Reason why === is more clear than == 



To write clear and  easy 
understandable  code, use explicite list of accepted values

val===undefined || val===null || val===''|| (Array.isArray(val) && val.length===0)


function isEmpty(val){
    return val===undefined || val===null || val==='' || (Array.isArray(val) && val.length===0)
}

// ------------
// TEST
// ------------

var log = (name,val) => console.log(`${name} -> ${isEmpty(val)}`);

log('null', null);
log('undefined', undefined);
log('NaN', NaN);
log('""""', """");
log('{}', {});
log('[]', []);
log('[1]', [1]);
log('[0]', [0]);
log('[[]]', [[]]);
log('true', true);
log('false', false);
log('""true""', ""true"");
log('""false""', ""false"");
log('Infinity', Infinity);
log('-Infinity', -Infinity);
log('1', 1);
log('0', 0);
log('-1', -1);
log('""1""', ""1"");
log('""0""', ""0"");
log('""-1""', ""-1"");

// ""void 0"" case
console.log('---\n""true"" is:', true);
console.log('""void 0"" is:', void 0);
log(void 0,void 0); // ""void 0"" is ""undefined"" - so we should get here TRUE

    You are a bit overdoing it. To check if a variable is not given a value, you would only need to check against undefined and null.

function isEmpty(value){
    return (typeof value === ""undefined"" || value === null);
}


This is assuming 0, """", and objects(even empty object and array) are valid ""values"".
    You could use the nullish coalescing operator ?? to check for null and undefined values. See the MDN Docs
null ?? 'default string'; // returns ""default string""

0 ?? 42;  // returns 0

(null || undefined) ?? ""foo""; // returns ""foo""

    ! check for empty strings (""""), null, undefined, false and the number 0 and NaN. Say, if a string is empty var name = """" then console.log(!name) returns true.

function isEmpty(val){
  return !val;
}


this function will return true if val is empty, null, undefined, false, the number 0 or NaN.

OR

According to your problem domain you can just use like !val or !!val.
    You may find the following function useful: 

function typeOf(obj) {
  return {}.toString.call(obj).split(' ')[1].slice(0, -1).toLowerCase();
}


Or in ES7 (comment if further improvements)

function typeOf(obj) {
  const { toString } = Object.prototype;
  const stringified = obj::toString();
  const type = stringified.split(' ')[1].slice(0, -1);

  return type.toLowerCase();
}


Results:

typeOf(); //undefined
typeOf(null); //null
typeOf(NaN); //number
typeOf(5); //number
typeOf({}); //object
typeOf([]); //array
typeOf(''); //string
typeOf(function () {}); //function
typeOf(/a/) //regexp
typeOf(new Date()) //date
typeOf(new WeakMap()) //weakmap
typeOf(new Map()) //map


""Note that the bind operator (::) is not part of ES2016 (ES7) nor any later edition of the ECMAScript standard at all. It's currently a stage 0 (strawman) proposal for being introduced to the language.""  Simon Kjellberg. the author wishes to add his support for this beautiful proposal to receive royal ascension.
    The optional chaining operator provides a way to simplify accessing values through connected objects when it's possible that a reference or function may be undefined or null.

let customer = {
  name: ""Carl"",
  details: {
    age: 82,
    location: ""Paradise Falls"" // detailed address is unknown
  }
};
let customerCity = customer.details?.address?.city;


The nullish coalescing operator may be used after optional chaining in order to build a default value when none was found:

let customer = {
  name: ""Carl"",
  details: { age: 82 }
};
const customerCity = customer?.city ?? ""Unknown city"";
console.log(customerCity); // Unknown city

    this is my solution to check if data is empty or not.
const _isEmpty = (data) => {
  return (
    // this way we can also check for undefined values. null==undefined is true
    data == null ||
    data == """" ||
    (Array.isArray(data) && data.length === 0) ||
    // we want {} to be false. we cannot use !! because !!{} turns to be true
    // !!{}=true and !!{name:""yilmaz""}=true. !! does not work ofr objects
    (data.constructor === Object && Object.keys(data).length === 0)

  );
};

    Will return false only for undefined and null:
return value ?? false
    return val || 'Handle empty variable'


is a really nice and clean way to handle it in a lot of places, can also be used to assign variables 

const res = val || 'default value'

    I think this makes your code looks simpler
To check if variable IS undefined or null
var a=undefined, b=null, c='hello world', d;
if(a !== (a ?? {})) { /**/ } // true
if(b !== (b ?? {})) { /**/ } // true
if(c !== (c ?? {})) { /**/ } // false
if(d !== (d ?? {})) { /**/ } // true

To check if variable is NOT undefined or null
var a=undefined, b=null, c='hello world', d;
if(a === (a ?? {})) { /**/ } // false
if(b === (b ?? {})) { /**/ } // false
if(c === (c ?? {})) { /**/ } // true
if(d === (d ?? {})) { /**/ } // false

    For my case I tried with if null,'', !variable, But it did not work.

See my code below to get the text from an html field

var status=$(this).text(); //for example (for my case)


if there was no value(no text) in the status variable ,I was trying to set the value 'novalue' to status variable. 

the following code worked.

if(status == false)
{
   status='novalue';
} 


when there was no text found for satus variable the above code assigned 'novalue' to the status variable
    Here's mine - returns true if value is null, undefined, etc or blank (ie contains only blank spaces):

function stringIsEmpty(value) {

    return value ? value.trim().length == 0 : true;

}

    The first answer with best rating is wrong. If value is undefined it will throw an exception in modern browsers. You have to use: 

if (typeof(value) !== ""undefined"" && value)


or 

if (typeof value  !== ""undefined"" && value)

    Try With Different Logic. You can use bellow code for check all four(4) condition for validation like not null, not blank, not undefined and not zero only use this code (!(!(variable))) in javascript and jquery.

function myFunction() {
    var data;  //The Values can be like as null, blank, undefined, zero you can test

    if(!(!(data)))
    {
        alert(""data ""+data);
    } 
    else 
    {
        alert(""data is ""+data);
    }
}

    function notEmpty(value){
  return (typeof value !== 'undefined' && value.trim().length);
}


it will also check white spaces ('    ') along with following:


null ,undefined ,NaN ,empty ,string ("""") ,0 ,false

    If the variable hasn't been declared, you wont be able to test for undefined using a function because you will get an error. 

if (foo) {}
function (bar) {}(foo)


Both will generate an error if foo has not been declared.

If you want to test if a variable has been declared you can use

typeof foo != ""undefined""


if you want to test if foo has been declared and it has a value you can use

if (typeof foo != ""undefined"" && foo) {
    //code here
}

    Most of the existing answers failed for my use case, most returned empty if a function was assigned to the variable or if NaN was returned. Pascal's answer was good.

Here's my implementation, please test and let me know if you find anything. You can see how I tested this function here.

function isEmpty(value) {
  return (
    // Null or undefined.
    (value == null) ||
    // Check if a Set() or Map() is empty
    (value.size === 0) ||
    // NaN - The only JavaScript value that is unequal to itself.
    (value !== value) ||
    // Length is zero && it's not a function.
    (value.length === 0 && typeof value !== ""function"") ||
    // Is an Object && has no keys.
    (value.constructor === Object && Object.keys(value).length === 0)
  )
}


Returns:


true: undefined, null, """", [], {}, NaN, new Set(), //
false: true, false, 1, 0, -1, ""foo"", [1, 2, 3], { foo: 1 }, function () {}

    function isEmpty(obj) {
    if (typeof obj == 'number') return false;
    else if (typeof obj == 'string') return obj.length == 0;
    else if (Array.isArray(obj)) return obj.length == 0;
    else if (typeof obj == 'object') return obj == null || Object.keys(obj).length == 0;
    else if (typeof obj == 'boolean') return false;
    else return !obj;
}


In ES6 with trim to handle whitespace strings:

const isEmpty = value => {
    if (typeof value === 'number') return false
    else if (typeof value === 'string') return value.trim().length === 0
    else if (Array.isArray(value)) return value.length === 0
    else if (typeof value === 'object') return value == null || Object.keys(value).length === 0
    else if (typeof value === 'boolean') return false
    else return !value
}

    Code on GitHub

const isEmpty = value => (
  (!value && value !== 0 && value !== false)
  || (Array.isArray(value) && value.length === 0)
  || (isObject(value) && Object.keys(value).length === 0)
  || (typeof value.size === 'number' && value.size === 0)

  // `WeekMap.length` is supposed to exist!?
  || (typeof value.length === 'number'
      && typeof value !== 'function' && value.length === 0)
);

// Source: https://levelup.gitconnected.com/javascript-check-if-a-variable-is-an-object-and-nothing-else-not-an-array-a-set-etc-a3987ea08fd7
const isObject = value =>
  Object.prototype.toString.call(value) === '[object Object]';


Poor man's tests 

const test = () => {
  const run = (label, values, expected) => {
    const length = values.length;
    console.group(`${label} (${length} tests)`);
    values.map((v, i) => {
      console.assert(isEmpty(v) === expected, `${i}: ${v}`);
    });
    console.groupEnd();
  };

  const empty = [
    null, undefined, NaN, '', {}, [],
    new Set(), new Set([]), new Map(), new Map([]),
  ];
  const notEmpty = [
    ' ', 'a', 0, 1, -1, false, true, {a: 1}, [0],
    new Set([0]), new Map([['a', 1]]),
    new WeakMap().set({}, 1),
    new Date(), /a/, new RegExp(), () => {},
  ];
  const shouldBeEmpty = [
    {undefined: undefined}, new Map([[]]),
  ];

  run('EMPTY', empty, true);
  run('NOT EMPTY', notEmpty, false);
  run('SHOULD BE EMPTY', shouldBeEmpty, true);
};


Test results:

EMPTY (10 tests)
NOT EMPTY (16 tests)
SHOULD BE EMPTY (2 tests)
  Assertion failed: 0: [object Object]
  Assertion failed: 1: [object Map]

    It may be usefull.

All values in array represent what you want to be (null, undefined or another things) and you search what you want in it.

var variablesWhatILookFor = [null, undefined, ''];
variablesWhatILookFor.indexOf(document.DocumentNumberLabel) > -1

    If you are using TypeScript and don't want to account for ""values those are false"" then this is the solution for you:                       

First:   import { isNullOrUndefined } from 'util';

Then:    isNullOrUndefined(this.yourVariableName)                  

Please Note: As mentioned below this is now deprecated, use value === undefined || value === null instead. ref.
    If you want to avoid getting true if the value is any of the following, according to jAndy's answer:


null
undefined
NaN
empty string ("""")
0
false


One possible solution that might avoid getting truthy values is the following:

function isUsable(valueToCheck) {
    if (valueToCheck === 0     || // Avoid returning false if the value is 0.
        valueToCheck === ''    || // Avoid returning false if the value is an empty string.
        valueToCheck === false || // Avoid returning false if the value is false.
        valueToCheck)             // Returns true if it isn't null, undefined, or NaN.
    {
        return true;
    } else {
        return false;
    }
}


It would be used as follows:

if (isUsable(x)) {
    // It is usable!
}
// Make sure to avoid placing the logical NOT operator before the parameter (isUsable(!x)) and instead, use it before the function, to check the returned value.
if (!isUsable(x)) {
    // It is NOT usable!
}




In addition to those scenarios, you may want to return false if the object or array is empty:


Object: {} (Using ECMA 7+)
Array: [] (Using ECMA 5+)


You would go about it this way:

function isEmptyObject(valueToCheck) {
    if(typeof valueToCheck === 'object' && !Object.keys(valueToCheck).length){
        // Object is empty!
        return true;
    } else {
        // Object is not empty!
        return false;
    }
}

function isEmptyArray(valueToCheck) {
    if(Array.isArray(valueToCheck) && !valueToCheck.length) {
        // Array is empty!
        return true;
    } else {
        // Array is not empty!
        return false;
    }
}


If you wish to check for all whitespace strings ("" ""), you may do the following:

function isAllWhitespace(){
    if (valueToCheck.match(/^ *$/) !== null) {
        // Is all whitespaces!
        return true;
    } else {
        // Is not all whitespaces!
        return false;
    }
}




Note: hasOwnProperty returns true for empty strings, 0, false, NaN, null, and undefined, if the variable was declared as any of them, so it might not be the best to use. The function may be modified to use it to show that it was declared, but is not usable.
    ","[2797, 5319, 292, 109, 64, 23, 14, 26, 25, 9, 19, 5, 20, 31, 3, 2, 2, 7, 1, -2, 11, 27, 4, 2, 6, 1, 5, 2, 5, 4, 2]",2486847,817,2011-04-01T15:14:24,2022-04-08 16:37:15Z,javascript 
Git fetch remote branch,"
                
My colleague and I are working on the same repository. We've branched it into two branches, each technically for different projects, but they have similarities, so we'll sometimes want to commit back to the *master from the branch.

However, I have the branch. How can my colleague pull that branch specifically?

A git clone of the repository does not seem to create the branches locally for him, though I can see them live on unfuddle after a push on my end.

Also, when I originally made the branch, I did -b checkout. Does that make much difference?

$ git branch -r
origin/HEAD -> origin/master
origin/daves_branch
origin/discover
origin/master

$ git fetch origin discover
$ git checkout discover


These are the commands I ran. But it definitely is not working.

I want to be able to check out that branch and then push and commit back just the branches changes from various collaborators or workstations.
    Update: Using Git Switch
All of the information written below was accurate, but a new command, git switch has been added that simplifies the effort.
If daves_branch exists on the remote repository, but not on your local branch, you can simply type:
git switch daves_branch

Since you do not have the branch locally, this will automatically make switch look on the remote repo.  It will then also automatically set up remote branch tracking.
Note that if daves_branch doesn't exist locally you'll need to git fetch first before using switch.

Original Post
You need to create a local branch that tracks a remote branch. The following command will create a local branch named daves_branch, tracking the remote branch origin/daves_branch. When you push your changes the remote branch will be updated.
For most recent versions of Git:
git checkout --track origin/daves_branch

--track is shorthand for git checkout -b [branch] [remotename]/[branch] where [remotename] is origin in this case and [branch] is twice the same, daves_branch in this case.
For Git 1.5.6.5 you needed this:
git checkout --track -b daves_branch origin/daves_branch

For Git 1.7.2.3 and higher, this is enough (it might have started earlier, but this is the earliest confirmation I could find quickly):
git checkout daves_branch

Note that with recent Git versions, this command will not create a local branch and will put you in a 'detached HEAD' state. If you want a local branch, use the --track option.
Full details are here: 3.5 Git Branching - Remote Branches, Tracking Branches
    I have used fetch followed by checkout...

git fetch <remote> <rbranch>:<lbranch>
git checkout <lbranch>


...where <rbranch> is the remote branch or source ref and <lbranch> is the as yet non-existent local branch or destination ref you want to track and which you probably want to name the same as the remote branch or source ref. This is explained under options in the explanation of <refspec>.

Git is so smart it auto completes the first command if I tab after the first few letters of the remote branch. That is, I don't even have to name the local branch, Git automatically copies the name of the remote branch for me. Thanks Git!

Also as the answer in this similar StackOverflow post shows, if you don't name the local branch in fetch, you can still create it when you check it out by using the -b flag. That is, git fetch <remote> <branch> followed by git checkout -b <branch> <remote>/<branch> does exactly the same as my initial answer. And evidently, if your repository has only one remote, then you can just do git checkout <branch> after fetch and it will create a local branch for you. For example, you just cloned a repository and want to check out additional branches from the remote.

I believe that some of the documentation for fetch may have been copied verbatim from pull. In particular the section on <refspec> in options is the same. However, I do not believe that fetch will ever merge, so that if you leave the destination side of the colon empty, fetch should do nothing.

NOTE: git fetch <remote> <refspec> is short for git fetch <remote> <refspec>: which would therefore do nothing, but git fetch <remote> <tag> is the same as git fetch <remote> <tag>:<tag> which should copy the remote <tag> locally.

I guess this is only helpful if you want to copy a remote branch locally, but not necessarily check it out right away. Otherwise, I now would use the accepted answer, which is explained in detail in the first section of the checkout description and later in the options section under the explanation of --track, since it's a one-liner. Well... sort of a one-liner, because you would still have to run git fetch <remote> first.

FYI: The order of the <refspecs> (source:destination) explains the bizarre pre Git1.7 method for deleting remote branches. That is, push nothing into the destination refspec.
    If you are trying to ""checkout"" a new remote branch (that exists only on the remote, but not locally), here's what you'll need:

git fetch origin
git checkout --track origin/<remote_branch_name>


This assumes you want to fetch from origin. If not, replace origin by your remote name.
    fetch the branch locally:
git fetch origin <branchName>

move to that branch:
git checkout <branchName>

    To checkout myBranch that exists remotely and not a locally - This worked for me:

git fetch --all
git checkout myBranch


I got this message:

Branch myBranch set up to track remote branch myBranch from origin
Switched to a new branch 'myBranch'

    Use git branch -a (both local and remote branches) or git branch -r (only remote branches) to see all the remotes and their branches. You can then do a git checkout -t remotes/repo/branch to the remote and create a local branch.

There is also a git-ls-remote command to see all the refs and tags for that remote.
    To fetch a branch that exists on remote, the simplest way is:

git fetch origin branchName
git checkout branchName


You can see if it already exists on remote with: 

git branch -r


This will fetch the remote branch to your local and will automatically track the remote one.
    [Quick Answer]
There are many alternatives, and my favourites are:
- Alternative 1:
git fetch --all
git checkout YourBranch


Using this alternative using a branch that exist remotely, but not in your local.

- Alternative 2:
git checkout -b 'YourBranch' origin/'YourRemote'


Probably, this is the simplest way.

    The title and the question are confused:


Git fetch remote branch
how can my colleague pull that branch specifically.


If the question is, how can I get a remote branch to work with, or how can I Git checkout a remote branch?, a simpler solution is:

With Git (>= 1.6.6) you are able to use:

git checkout <branch_name>


If local <branch_name> is not found, but there does exist a tracking branch in exactly one remote with a matching name, treat it as equivalent to:

git checkout -b <branch_name> --track <remote>/<branch_name>


See documentation for Git checkout

For your friend:

$ git checkout discover
Branch discover set up to track remote branch discover
Switched to a new branch 'discover'

    With this simple command:

git checkout -b 'your_branch' origin/'remote branch'

    Use:

git checkout -b serverfix origin/serverfix


This is a common enough operation that Git provides the --track shorthand:

git checkout --track origin/serverfix


In fact, this is so common that theres even a shortcut for that shortcut. If the branch name youre trying to checkout (a) doesnt exist and (b) exactly matches a name on only one remote, Git will create a tracking branch for you:

git checkout serverfix


To set up a local branch with a different name than the remote branch, you can easily use the first version with a different local branch name:

git checkout -b sf origin/serverfix


Now, your local branch sf will automatically pull from origin/serverfix.

Source: Pro Git, 2nd Edition, written by Scott Chacon and Ben Straub (cut for readability)
    The steps are as follows;


git fetch origin or git fetch --all , this will fetch all the remote branches to your local and then this the second option you can proced with.
git checkout --track origin/<The_remote_branch you want to switch over>


Then work on this branch and you can verify whether you are on that branch or not by  typing 

git branch


It displayes the branch you currently in.
    Simply try:

git pull origin your_branch_name

    If you have a repository that was cloned with --depth 1 then many of the commands that were listed will not work. For example, see here

% git clone --depth 1 https://github.com/repo/code
Cloning into 'code'...
cd code
remote: Counting objects: 1778, done.
remote: Compressing objects: 100% (1105/1105), done.
remote: Total 1778 (delta 87), reused 1390 (delta 58), pack-reused 0
Receiving objects: 100% (1778/1778), 5.54 MiB | 4.33 MiB/s, done.
Resolving deltas: 100% (87/87), done.
Checking connectivity... done.
Checking out files: 100% (1215/1215), done.
% cd code
% git checkout other_branch
error: pathspec 'other_branch' did not match any file(s) known to git.
% git fetch origin other_branch
remote: Counting objects: 47289, done.
remote: Compressing objects: 100% (15906/15906), done.
remote: Total 47289 (delta 30151), reused 46699 (delta 29570), pack-reused 0
Receiving objects: 100% (47289/47289), 31.03 MiB | 5.70 MiB/s, done.
Resolving deltas: 100% (30151/30151), completed with 362 local objects.
From https://github.com/repo/code
 * branch            other_branch-> FETCH_HEAD
% git checkout other_branch
error: pathspec 'other_branch' did not match any file(s) known to git.
%


In this case I would reclone the repository, but perhaps there are other techniques e.g. git shallow clone (clone --depth) misses remote branches
    What helped me was 

1) To view all available remote branches (e.g. 'remote-branch-name')

git branch -r


2) Create a local branch using remote branch name

git fetch && git checkout 'remote-branch-name'

    Check your .git/config file, particularly what tracking is present on fetch for that remote.

[remote ""randomRemote""]
    url = git@github.com:someUser/someRepo.git
    fetch = +refs/heads/*:refs/remotes/randomRemote/*


If it has heads/* pointing to randomRemote/*, when you run git fetch randomRemote, it will fetch all branches.

Then you can just checkout that branch.

Otherwise,


You need to add remote branches to the tracking using this. Check your .git/config after running this. You will understand.

git remote set-branches --add randomRemote randomBranch

Run git fetch randomRemote. This will fetch the remote branch.
Now you can run git checkout randomBranch.

    If you would like to fetch all remote branches, please type just:

git fetch --all

    Let's say that your remote is git@xyz.git and you want its random_branch branch. The process should be as follows:


First check the list of your remotes by


  git remote -v

If you don't have the git@xyz.git remote in the above command's output, you would add it by


  git remote add xyz git@xyz.git

Now you can fetch the contents of that remote by


  git fetch xyz

Now checkout the branch of that remote by


  git checkout -b my_copy_random_branch xyz/random_branch 

Check the branch list by


  git branch -a



The local branch my_copy_random_branch would be tracking the random_branch branch of your remote.
    At times you are asked not to fiddle with the master branch and work only the remote branch (as I was asked to). So all you need is the remote branch.

So to clone the remote branch alone (without the master), do this

git clone url --branch remote_branch_name


where,
remote_branch_name is the name of the remote branch

For example,

git clone git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git --branch v3.15


This will make sure that you clone the remote branch to your local branch with the name of the remote branch.

Now if you commit your code and push, the code will be submitted to that branch alone.
    git fetch

git branch -r

git checkout <branch_name>

    I want to give you one-liner command for fetching all the remote branches to your local and switch to your desired newly created local branch:

git fetch && git checkout discover


After running the above command you will get the below message:

Switched to a new branch 'discover'
Branch discover set up to track remote branch discover from origin.


The first line states that switched to a
new branch - why new? It is already there in remote!

But actually you have to create it locally too. The branch is taken from the remote index and created locally for you.

Here discover is a new branch which were created from your repository's remote branch discover.

But the second line gives more information than the first one which tell us that:

Our branch is set up to track remote branch with the same name.

Although git fetch fetches all branches to local. But if you run git branch after it, you will see only master branch in local. Why?

Because for every branch you have in remote you have to create it locally too, for tracking it as git checkout <branchname> as we have done in the above example.

After running git checkout command you can run git branch, and now you can see both the branch:


master and 2. discover in your local listing.

    I typed

git checkout <branch_name>


and got

Branch <branch_name> set up to track remote branch <branch_name> from origin.
Switched to a new branch '<branch_name>'

    You can fetch and checkout the remote branch in one shot too:

git fetch && git checkout the-branch-name

    git fetch && git checkout <your friend's branch name> should do the trick
    
git fetch --all & git checkout <branch name>

    A simple command, git checkout remote_branch_name will help you to create a local branch that has all the changes in the remote branch.
    If you download the repository with git clone <repo_url> -b <branch> (only cloning certaing branch), you should modify the <repo_name>/.git/config file.
Replace or modify the line that references the fetch target of the [remote ""origin""] section to let the command git fetch --all discover all branches:
[remote ""origin""]
        url = <repo_git_url>
        fetch = +refs/heads/master:refs/remotes/origin/master

Be sure to set the fetch parameter point to /heads/master.
Care with git fetch --all because this will fetch all, so may take a long time.
    git branch <name> --track origin/<name>
    git checkout -b branch_name
git pull remote_name branch_name

    If you already know your remote branch like so...

git remote
=> One
=> Two


and you know the branch name you wish to checkout, for example, br1.2.3.4, then do 

git fetch One
=> returns all meta data of remote, that is, the branch name in question.


All that is left is to checkout the branch

git checkout br.1.2.3.4


Then make any new branches off of it.
    ","[2789, 3780, 1139, 462, 117, 150, 82, 43, 29, 53, 31, 36, 12, 9, 9, 17, 5, 9, 10, 13, 16, 7, 14, 15, 7, 8, 1, 1, 2, -1, 3]",3552886,927,2012-03-02T17:06:56,2022-02-22 13:49:39Z,
How to add images to README.md on GitHub?,"
                
Recently I joined GitHub. I hosted some projects there.

I need to include some images in my README File. I don't know how to do that.

I searched about this, but all I got was some links which tell me to ""host images on web and specify the image path in README.md file"".

Is there any way to do this without hosting the images on any third-party web hosting services?
    Try this markdown:

![alt text](http://url/to/img.png)


I think you can link directly to the raw version of an image if it's stored in your repository. i.e.

![alt text](https://github.com/[username]/[reponame]/blob/[branch]/image.jpg?raw=true)

    You can also use relative paths like
![Alt text](relative/path/to/img.jpg?raw=true ""Title"")

Also try the following with the desired .fileExtention:
![plot](./directory_1/directory_2/.../directory_n/plot.png)

    
You can create a New Issue  
upload(drag & drop) images to it 
Copy the images URL and paste it into your README.md file.


here is a detailed youTube video explained this in detail:

https://www.youtube.com/watch?v=nvPOUdz5PL4
    It's much simpler than that.

Just upload your image to the repository root, and link to the filename without any path, like so:

![Screenshot](screenshot.png)

    You can also insert animated SVG images in the markdown file like any other format.
It can be a good alternative to GIF images.
![image description](relative/path/in/repository/to/image.svg)
OR
<img src=""relative/path/in/repository/to/image.svg"" width=""128""/>

Example (assuming the image is in assets directory in the repository):
![My animated logo](assets/my-logo.svg)

Result:



    You can also add images with simple HTML tags:

<p align=""center"">
  <img src=""your_relative_path_here"" width=""350"" title=""hover text"">
  <img src=""your_relative_path_here_number_2_large_name"" width=""350"" alt=""accessibility text"">
</p>

    Many of the posted solutions are incomplete or not to my taste.


An external CDN like imgur adds another tool to the chain. Meh.
Creating a dummy issue in the issue tracker is a hack. It creates clutter and confuses users. It's a pain to migrate this solution to a fork, or off GitHub.
Using the gh-pages branch makes the URLs brittle. Another person working on the project maintaining the gh-page may not know something external depends on the path to these images. The gh-pages branch has a particular behavior on GitHub which is not necessary for hosting CDN images.
Tracking assets in version control is a good thing. As a project grows and changes it's a more sustainable way to manage and track changes by multiple users.
If an image applies to a specific revision of the software, it may be preferable to link an immutable image. That way, if the image is later updated to reflect changes to the software, anyone reading that revision's readme will find the correct image.


My preferred solution, inspired by this gist, is to use an assets branch with permalinks to specific revisions.

git checkout --orphan assets
git reset --hard
cp /path/to/cat.png .
git add .
git commit -m 'Added cat picture'
git push -u origin assets
git rev-parse HEAD  # Print the SHA, which is optional, you'll see below.


Construct a ""permalink"" to this revision of the image, and wrap it in Markdown. 


  Looking up the commit SHA by hand is inconvenient, however, so as a shortcut press Y to a permalink to a file in a specific commit as this help.github page says.


To always show the latest image on the assets branch, use the blob URL: 

https://github.com/github/{repository}/blob/assets/cat.png 


(From the same GitHub help page File views show the latest version on a branch)
    Very Simple : Can be done using Ctrl + C/V
Most of the answers here directly or indirectly involve uploading the image somewhere else & then providing a link to it.
It can be done very simply by just copying any image and pasting it while editing Readme.md

Copying the image - You can just click on the image file and use Ctrl + C or may copy the screenshot image to your clipboard using the snipping tool
You can then simply do Ctrl + V while editing Readme.md

Guithub will automatically upload it to user-images.githubusercontent.com and a link to it will be inserted there
    You Can use
![A test image](image.png)

Where ![A test image] is your alt text and (image.png) is the link to your image.

You can have the image on a cloud service or other online image hosting platforms
Or you can provide the image link from the repository if it is in the repo

You can also make a specific folder inside your repository dedicated to your readme images
    
  I need to include some images in my README File. I don't know how to
  do that.


I created a small wizard that allows you to create and customize simple image galleries for your GitHub repository's readme: See ReadmeGalleryCreatorForGitHub.

The wizard takes advantage of the fact that GitHub allows img tags to occur in the README.md. Also, the wizard makes use of the popular trick of uploading images to GitHub by drag'n'dropping them in the issue area (as already mentioned in one of the answers in this thread).


    You can now drag and drop the images while editing the readme file.
Github will create a link for you which will be in the format of:
https://user-images.githubusercontent.com/******/********.file_format

Alternatively, at the bottom of the file, it says ""Attach files by dragging & dropping, selecting or pasting them"". If you click on that one, it will give you an option to upload a file directly or you can just paste it!
    Commit your image (image.png) in a folder (myFolder) and add the following line in your README.md: 

![Optional Text](../master/myFolder/image.png)
    Step by step process,
First create a folder ( name your folder ) and add the image/images that you want to upload in Readme.md file. ( you can also add the image/images in any existing folder of your project. )
Now,Click on edit icon of Readme.md file,then

![](relative url where images is located/refrence_image.png)  // refrence_image is the name of image in my case.


After adding image, you can see preview of changes in the, ""Preview Changes"" tab.you will find your image here.
for example like this,
In my case,

![](app/src/main/res/drawable/refrence_image.png)


app folder -> src folder -> main folder -> res folder -> drawable folder -> and inside drawable folder refrence_image.png file is located.
For adding multiple images, you can do it like this,

![](app/src/main/res/drawable/refrence_image1.png)
![](app/src/main/res/drawable/refrence_image2.png)
![](app/src/main/res/drawable/refrence_image3.png)


Note 1 - Make sure your image file name does not contain any spaces. If it contain spaces then you need to add %20 for each space between the file name. It's better to remove the spaces.

Note 2 - you can even resize the image using HTML tags, or there are other ways. you can google it for more.
if you need it.

After this, write your commit changes message, and then commit your Changes.

There are many other hacks of doing it like, create a issue and etc and etc. By far this is the best method that I have came across.
    Basic Syntax
![myimage-alt-tag](url-to-image)

Here:

my-image-alt-tag : text that will be displayed if image is not shown.
url-to-image :  whatever your image resource is. URI of the image

Example:
![stack Overflow](http://lmsotfy.com/so.png)

This will look like the following:

    
Create an issue regarding adding images
Add the image by drag and drop or by file chooser
Then copy image source
Now add ![alt tag](http://url/to/img.png) to your README.md file


Done!

Alternatively you can use some image hosting site like imgur and get it's url and add it in your README.md file or you can use some static file hosting too.

Sample issue
    Just add an <img> tag to your README.md with relative src to your repository. If you're not using relative src, make sure the server supports CORS.

It works because GitHub support inline-html

<img src=""/docs/logo.png"" alt=""My cool logo""/>
# My cool project and above is the logo of it


Observe here
    In new Github UI, this works for me -
Example - Commit your image.png in a folder (myFolder) and add the following line in your README.md:
![Optional Text](../main/myFolder/image.png)

    Use tables to stand out, it will give separate charm to it

Table Syntax is:

Separate each column cell by symbol |

and table header (First row) by 2nd row by  ---

| col 1| col 2|
|------------|-------------|
|    image 1    |   image 2   |

output



Now just put <img src=""url/relativePath""> at image 1 and image 2 if you are using two images

Note: if using multiple images just include more columns, you may use width and height attribute to make it look readable.

Example

| col 1| col 2|
|------------|-------------|
|   <img src=""https://media.wired.com/photos/5926db217034dc5f91becd6b/master/w_582,c_limit/so-logo-s.jpg"" width=""250"">   |   <img src=""https://mk0jobadderjftub56m0.kinstacdn.com/wp-content/uploads/stackoverflow.com-300.jpg"" width=""250""> |


Spacing does not matter

Output image



helped by : adam-p
    JUST THIS WORKS!!

take care about your file name uppercase in tag and put PNG file inroot, and link to the filename without any path:

![Screenshot](screenshot.png)

    You can just do:

git checkout --orphan assets
cp /where/image/currently/located/on/machine/diagram.png .
git add .
git commit -m 'Added diagram'
git push -u origin assets


Then you can just reference it in the README file like so: 

![diagram](diagram.png)
    There are 2 simple way you can do this ,

1) use HTML img tag , 

2) ![](the path where your image is saved/image-name.png)

the path would you can copy from the URL in the browser while you have opened that image.
there might be an issue occur of spacing so make sure if there is any space b/w two words of path or in image name add-> %20. just like browser do.

Both of them will work , if you want to understand more you can check my github -> https://github.com/adityarawat29
    This Answer can also be found at:
https://github.com/YourUserAccount/YourProject/blob/master/DirectoryPath/ReadMe.md

Display images from repo using:

prepend domain: https://raw.githubusercontent.com/

append flag: ?sanitize=true&raw=true

use <img /> tag

Eample url works for svg, png, and jpg using:


Domain: raw.githubusercontent.com/
UserName: YourUserAccount/
Repo: YourProject/
Branch: YourBranch/
Path: DirectoryPath/
Filename: example.png


Works for SVG, PNG, and JPEG

 - `raw.githubusercontent.com/YourUserAccount/YourProject/YourBranch/DirectoryPath/svgdemo1.svg?sanitize=true&raw=true`


Working example code displayed below after used:

**raw.githubusercontent.com**:
<img src=""https://raw.githubusercontent.com/YourUserAccount/YourProject/master/DirectoryPath/Example.png?raw=true"" />

<img src=""https://raw.githubusercontent.com/YourUserAccount/YourProject/master/DirectoryPath/svgdemo1.svg?sanitize=true&raw=true"" />


raw.githubusercontent.com:





Thanks:
   - https://stackoverflow.com/a/48723190/1815624
   - https://github.com/potherca-blog/StackOverflow/edit/master/question.13808020.include-an-svg-hosted-on-github-in-markdown/readme.md
    I have solved this problem. You only need to refer to someone else's readme file.

At first,you should upload an image file to github code library ! Then direct reference to the address of the image file .








    LATEST

Wikis can display PNG, JPEG, or GIF images

Now you can use:
[[https://github.com/username/repository/blob/master/img/octocat.png|alt=octocat]]

-OR-
Follow these steps:

On GitHub, navigate to the main page of the repository.

Under your repository name, click  Wiki.

Using the wiki sidebar, navigate to the page you want to change, and then click Edit.

On the wiki toolbar, click Image.

In the ""Insert Image"" dialog box, type the image URL and the alt text (which is used by search engines and screen readers).

Click OK.


Refer Docs.
    I usually host the image on the site, this can link to any hosted image.   Just toss this in the readme.  Works for .rst files, not sure about .md

.. image:: https://url/path/to/image
   :height: 100px
   :width: 200 px
   :scale: 50 %

    For upload images with size managing option:
<img src=""a.jpg"" alt=""J"" width=""200""/>

    Add image in your repository from upload file option then in README file

![Alt text](""enter repository image URL here"") 

    Consider using a table if adding multiple screenshots and want to align them using tabular data for improved accessibility as shown here:



If your markdown parser supports it you could also add the role=""presentation"" WIA-ARIA attribute to the TABLE element and omit the th tags.
    You can link to images in your project from README.md (or externally) using the alternative github CDN link.

The URL will look like this:

https://cdn.rawgit.com/<USER>/<REPO>/<BRANCH>/<PATH>/<TO>/<FILE>


I have an SVG image in my project, and when I reference it in my Python project documentation, it does not render.

Project link

Here is the project link to the file (does not render as an image):

https://github.com/jongracecox/anybadge/blob/master/examples/awesomeness.svg

Example embedded image: 

Raw link

Here is the RAW link to the file (still does not render as an image):

https://raw.githubusercontent.com/jongracecox/anybadge/master/examples/awesomeness.svg

Example embedded image: 

CDN link

Using the CDN link, I can link to the file using (renders as an image):

https://cdn.rawgit.com/jongracecox/anybadge/master/examples/awesomeness.svg

Example embedded image: 

This is how I am able to use images from my project in both my README.md file, and in my PyPi project reStructredText doucmentation (here)
    I have found another solution but quite different and i'll explain it

Basically, i used the  tag to show the image, but i wanted to go to another page when the image was clicked and here is how i did it.

<a href=""the-url-you-want-to-go-when-image-is-clicked.com"" />
<img src=""image-source-url-location.com"" />


If you put it right next to each other, separated by a new line, i guess when you click the image, it goes to the  tag which has the href to the other site you want to redirect.
    ","[2789, 2851, 761, 360, 227, 40, 118, 75, 19, 17, 33, 17, 34, 15, 22, 23, 18, 11, 13, 9, 10, 7, 7, 8, 4, 8, 1, 2, 3, 8, 4]",1622989,466,2013-01-24T05:44:06,2022-04-14 22:59:36Z,
How do I print colored text to the terminal in Python?,"
                
How can I output colored text to the terminal in Python?
    This somewhat depends on what platform you are on. The most common way to do this is by printing ANSI escape sequences. For a simple example, here's some Python code from the Blender build scripts:
class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

To use code like this, you can do something like:
print(bcolors.WARNING + ""Warning: No active frommets remain. Continue?"" + bcolors.ENDC)

Or, with Python 3.6+:
print(f""{bcolors.WARNING}Warning: No active frommets remain. Continue?{bcolors.ENDC}"")

This will work on unixes including OS X, Linux and Windows (provided you use ANSICON, or in Windows 10 provided you enable VT100 emulation). There are ANSI codes for setting the color, moving the cursor, and more.
If you are going to get complicated with this (and it sounds like you are if you are writing a game), you should look into the ""curses"" module, which handles a lot of the complicated parts of this for you. The Python Curses HowTO is a good introduction.
If you are not using extended ASCII (i.e., not on a PC), you are stuck with the ASCII characters below 127, and '#' or '@' is probably your best bet for a block. If you can ensure your terminal is using a IBM extended ASCII character set, you have many more options. Characters 176, 177, 178 and 219 are the ""block characters"".
Some modern text-based programs, such as ""Dwarf Fortress"", emulate text mode in a graphical mode, and use images of the classic PC font. You can find some of these bitmaps that you can use on the Dwarf Fortress Wiki see (user-made tilesets).
The Text Mode Demo Contest has more resources for doing graphics in text mode.
    There is also the Python termcolor module. Usage is pretty simple:
from termcolor import colored

print colored('hello', 'red'), colored('world', 'green')

Or in Python 3:
print(colored('hello', 'red'), colored('world', 'green'))

It may not be sophisticated enough, however, for game programming and the ""colored blocks"" that you want to do...
To get the ANSI codes working on windows, first run
os.system('color')

    The answer is Colorama for all cross-platform coloring in Python.
It supports Python 3.5+ as well as Python 2.7.
And as of January 2021 it is maintained.
Example screenshot:

    Print a string that starts a color/style, then the string, and then end the color/style change with '\x1b[0m':
print('\x1b[6;30;42m' + 'Success!' + '\x1b[0m')


Get a table of format options for shell text with the following code:
def print_format_table():
    """"""
    prints table of formatted text format options
    """"""
    for style in range(8):
        for fg in range(30,38):
            s1 = ''
            for bg in range(40,48):
                format = ';'.join([str(style), str(fg), str(bg)])
                s1 += '\x1b[%sm %s \x1b[0m' % (format, format)
            print(s1)
        print('\n')

print_format_table()

Light-on-dark example (complete)

Dark-on-light example (partial)

Reference: https://en.wikipedia.org/wiki/ANSI_escape_code#Colors
    Define a string that starts a color and a string that ends the color. Then print your text with the start string at the front and the end string at the end.
CRED = '\033[91m'
CEND = '\033[0m'
print(CRED + ""Error, does not compute!"" + CEND)

This produces the following in Bash, in urxvt with a Zenburn-style color scheme:

Through experimentation, we can get more colors:

Note: \33[5m and \33[6m are blinking.
This way we can create a full color collection:
CEND      = '\33[0m'
CBOLD     = '\33[1m'
CITALIC   = '\33[3m'
CURL      = '\33[4m'
CBLINK    = '\33[5m'
CBLINK2   = '\33[6m'
CSELECTED = '\33[7m'

CBLACK  = '\33[30m'
CRED    = '\33[31m'
CGREEN  = '\33[32m'
CYELLOW = '\33[33m'
CBLUE   = '\33[34m'
CVIOLET = '\33[35m'
CBEIGE  = '\33[36m'
CWHITE  = '\33[37m'

CBLACKBG  = '\33[40m'
CREDBG    = '\33[41m'
CGREENBG  = '\33[42m'
CYELLOWBG = '\33[43m'
CBLUEBG   = '\33[44m'
CVIOLETBG = '\33[45m'
CBEIGEBG  = '\33[46m'
CWHITEBG  = '\33[47m'

CGREY    = '\33[90m'
CRED2    = '\33[91m'
CGREEN2  = '\33[92m'
CYELLOW2 = '\33[93m'
CBLUE2   = '\33[94m'
CVIOLET2 = '\33[95m'
CBEIGE2  = '\33[96m'
CWHITE2  = '\33[97m'

CGREYBG    = '\33[100m'
CREDBG2    = '\33[101m'
CGREENBG2  = '\33[102m'
CYELLOWBG2 = '\33[103m'
CBLUEBG2   = '\33[104m'
CVIOLETBG2 = '\33[105m'
CBEIGEBG2  = '\33[106m'
CWHITEBG2  = '\33[107m'

Here is the code to generate the test:
x = 0
for i in range(24):
  colors = """"
  for j in range(5):
    code = str(x+j)
    colors = colors + ""\33["" + code + ""m\\33["" + code + ""m\033[0m ""
  print(colors)
  x = x + 5

    Here's a solution that works on Windows 10 natively.
Using a system call, such as os.system(""""), allows colours to be printed in Command Prompt and Powershell natively:
import os

# System call
os.system("""")

# Class of different styles
class style():
    BLACK = '\033[30m'
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'
    UNDERLINE = '\033[4m'
    RESET = '\033[0m'

print(style.YELLOW + ""Hello, World!"")

Note: Windows does not fully support ANSI codes, whether through system calls or modules. Not all text decoration is supported, and although the bright colours display, they are identical to the regular colours.
Thanks to @j-l for finding an even shorter method.
tl;dr: Add os.system("""")
    Rich is a relatively new Python library for working with color in the terminal.
There are a few ways of working with color in Rich. The quickest way to get started would be the rich print method which renders a BBCode-like syntax in to ANSI control codes:
from rich import print
print(""[red]Color[/] in the [bold magenta]Terminal[/]!"")

There are other ways of applying color with Rich (regex, syntax) and related formatting features.

    This is, in my opinion, the easiest method. As long as you have the RGB values of the color you want, this should work:
def colored(r, g, b, text):
    return f""\033[38;2;{r};{g};{b}m{text}\033[38;2;255;255;255m""

An example of printing red text:
text = 'Hello, World!'
colored_text = colored(255, 0, 0, text)
print(colored_text)

#or

print(colored(255, 0, 0, 'Hello, World!'))

Multi-colored text
text = colored(255, 0, 0, 'Hello, ') + colored(0, 255, 0, 'World')
print(text)

    sty is similar to colorama, but it's less verbose, supports 8-bit and 24-bit (RGB) colors, supports all effects (bold, underline, etc.), allows you to register your own styles, is fully typed and high performant, supports muting, is not messing with globals such as sys.stdout, is really flexible, well documented and more...
Examples:
from sty import fg, bg, ef, rs

foo = fg.red + 'This is red text!' + fg.rs
bar = bg.blue + 'This has a blue background!' + bg.rs
baz = ef.italic + 'This is italic text' + rs.italic
qux = fg(201) + 'This is pink text using 8bit colors' + fg.rs
qui = fg(255, 10, 10) + 'This is red text using 24bit colors.' + fg.rs

# Add custom colors:

from sty import Style, RgbFg

fg.orange = Style(RgbFg(255, 150, 50))

buf = fg.orange + 'Yay, Im orange.' + fg.rs

print(foo, bar, baz, qux, qui, buf, sep='\n')

prints:

Demo:

    Here is my modern (2021) solution: yachalk
It is one of the few libraries that properly supports nested styles:

Apart from that yachalk is auto-complete-friendly, has 256/truecolor support, comes with terminal-capability detection, and is fully typed.
Here are some design decision you may consider for choosing your solution.
High-level libraries vs low-level libraries / manual style handling?
Many answers to this question demonstrate how to ANSI escape codes directly, or suggest low-level libraries that require manual style enabling/disabling.
These approaches have subtle issues: Inserting on/off styles manually is

more verbose syntactically, because resets have to be specified explicitly,
more error prone, because you can accidentally forget to reset a style,
fails to get edge cases right: For instance in some terminals it is necessary to reset styles before newlines, and re-activate them after the line break. Also, some terminal have problems with simply overriding mutually exclusive styles, and require inserting ""unnecessary"" reset codes. If a developer's local terminal doesn't have these quirks, the developer will not discover these quirks immediately. The issue will only be reported later by others or cause problems e.g. on CI terminals.

Therefore if compatibility with many terminals is a goal, it's best to use a high-level library that offers automatic handling of style resets. This allows the library to take care of all edge cases by inserting the ""spurious"" ANSI escape codes where needed.
Why yet another library?
In JavaScript the de-facto standard library for the task is chalk, and after using it for a while in JS projects, the solutions available in the Python world were lacking in comparison. Not only is the chalk API more convenient to use (fully auto-complete compatible), it also gets all the edge cases right.
The idea of yachalk is to bring the same convenience to the Python ecosystem. If you're interested in a comparison to other libraries I've started feature comparison on the projects page. In addition, here is a long (but still incomplete) list of alternatives that came up during my research -- a lot to choose from :)

colored
ansicolors
termcolor
colorama
sty
blessings
rich
colorit
colorprint
console-color
pyfance
couleur
style (formerly known as clr)
pychalk
simple-chalk
chlk
chalky

    You want to learn about ANSI escape sequences. Here's a brief example:
CSI = ""\x1B[""
print(CSI+""31;40m"" + ""Colored Text"" + CSI + ""0m"")

For more information, see ANSI escape code.
For a block character, try a Unicode character like \u2588:
print(u""\u2588"")

Putting it all together:
print(CSI+""31;40m"" + u""\u2588"" + CSI + ""0m"")

    I have a library called colorit. It is super simple.
Here are some examples:
from colorit import *

# Use this to ensure that ColorIt will be usable by certain command line interfaces
# Note: This clears the terminal
init_colorit()

# Foreground
print(color(""This text is red"", Colors.red))
print(color(""This text is orange"", Colors.orange))
print(color(""This text is yellow"", Colors.yellow))
print(color(""This text is green"", Colors.green))
print(color(""This text is blue"", Colors.blue))
print(color(""This text is purple"", Colors.purple))
print(color(""This text is white"", Colors.white))

# Background
print(background(""This text has a background that is red"", Colors.red))
print(background(""This text has a background that is orange"", Colors.orange))
print(background(""This text has a background that is yellow"", Colors.yellow))
print(background(""This text has a background that is green"", Colors.green))
print(background(""This text has a background that is blue"", Colors.blue))
print(background(""This text has a background that is purple"", Colors.purple))
print(background(""This text has a background that is white"", Colors.white))

# Custom
print(color(""This color has a custom grey text color"", (150, 150, 150)))
print(background(""This color has a custom grey background"", (150, 150, 150)))

# Combination
print(
    background(
        color(""This text is blue with a white background"", Colors.blue), Colors.white
    )
)

# If you are using Windows Command Line, this is so that it doesn't close immediately
input()

This gives you:

It's also worth noting that this is cross platform and has been tested on Mac, Linux, and Windows.
You might want to try it out: https://github.com/SuperMaZingCoder/colorit
colorit is now available to be installed with PyPi! You can install it with pip install color-it on Windows and pip3 install color-it on macOS and Linux.
    # Pure Python 3.x demo, 256 colors
# Works with bash under Linux and MacOS

fg = lambda text, color: ""\33[38;5;"" + str(color) + ""m"" + text + ""\33[0m""
bg = lambda text, color: ""\33[48;5;"" + str(color) + ""m"" + text + ""\33[0m""

def print_six(row, format, end=""\n""):
    for col in range(6):
        color = row*6 + col - 2
        if color>=0:
            text = ""{:3d}"".format(color)
            print (format(text,color), end="" "")
        else:
            print(end=""    "")   # four spaces
    print(end=end)

for row in range(0, 43):
    print_six(row, fg, "" "")
    print_six(row, bg)

# Simple usage: print(fg(""text"", 160))



Try it online
    def black(text):
    print('\033[30m', text, '\033[0m', sep='')

def red(text):
    print('\033[31m', text, '\033[0m', sep='')

def green(text):
    print('\033[32m', text, '\033[0m', sep='')

def yellow(text):
    print('\033[33m', text, '\033[0m', sep='')

def blue(text):
    print('\033[34m', text, '\033[0m', sep='')

def magenta(text):
    print('\033[35m', text, '\033[0m', sep='')

def cyan(text):
    print('\033[36m', text, '\033[0m', sep='')

def gray(text):
    print('\033[90m', text, '\033[0m', sep='')


black(""BLACK"")
red(""RED"")
green(""GREEN"")
yellow(""YELLOW"")
blue(""BLACK"")
magenta(""MAGENTA"")
cyan(""CYAN"")
gray(""GRAY"")


Try online 
    I generated a class with all the colors using a for loop to iterate every combination of color up to 100, and then wrote a class with Python colors. Copy and paste as you will, GPLv2 by me:
class colors:
    '''Colors class:
    Reset all colors with colors.reset
    Two subclasses fg for foreground and bg for background.
    Use as colors.subclass.colorname.
    i.e. colors.fg.red or colors.bg.green
    Also, the generic bold, disable, underline, reverse, strikethrough,
    and invisible work with the main class
    i.e. colors.bold
    '''
    reset='\033[0m'
    bold='\033[01m'
    disable='\033[02m'
    underline='\033[04m'
    reverse='\033[07m'
    strikethrough='\033[09m'
    invisible='\033[08m'
    class fg:
        black='\033[30m'
        red='\033[31m'
        green='\033[32m'
        orange='\033[33m'
        blue='\033[34m'
        purple='\033[35m'
        cyan='\033[36m'
        lightgrey='\033[37m'
        darkgrey='\033[90m'
        lightred='\033[91m'
        lightgreen='\033[92m'
        yellow='\033[93m'
        lightblue='\033[94m'
        pink='\033[95m'
        lightcyan='\033[96m'
    class bg:
        black='\033[40m'
        red='\033[41m'
        green='\033[42m'
        orange='\033[43m'
        blue='\033[44m'
        purple='\033[45m'
        cyan='\033[46m'
        lightgrey='\033[47m'

    Try this simple code
def prRed(prt): print(""\033[91m {}\033[00m"" .format(prt))
def prGreen(prt): print(""\033[92m {}\033[00m"" .format(prt))
def prYellow(prt): print(""\033[93m {}\033[00m"" .format(prt))
def prLightPurple(prt): print(""\033[94m {}\033[00m"" .format(prt))
def prPurple(prt): print(""\033[95m {}\033[00m"" .format(prt))
def prCyan(prt): print(""\033[96m {}\033[00m"" .format(prt))
def prLightGray(prt): print(""\033[97m {}\033[00m"" .format(prt))
def prBlack(prt): print(""\033[98m {}\033[00m"" .format(prt))

prGreen(""Hello, World!"")

    I have wrapped joeld's answer into a module with global functions that I can use anywhere in my code.
File: log.py
def enable():
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = ""\033[1m""

def disable():
    HEADER = ''
    OKBLUE = ''
    OKGREEN = ''
    WARNING = ''
    FAIL = ''
    ENDC = ''

def infog(msg):
    print(OKGREEN + msg + ENDC)

def info(msg):
    print(OKBLUE + msg + ENDC)

def warn(msg):
    print(WARNING + msg + ENDC)

def err(msg):
    print(FAIL + msg + ENDC)

enable()

Use as follows:
import log
log.info(""Hello, World!"")
log.err(""System Error"")

    Emoji
You can use colors for text as others mentioned in their answers to have colorful text with a background or foreground color.
But you can use emojis instead! for example, you can use for warning messages and  for error messages.
Or simply use these notebooks as a color:
: error message
: warning message
: ok status message
: action message
: canceled status message
: Or anything you like and want to recognize immediately by color


 Bonus:
This method also helps you to quickly scan and find logs directly in the source code.
But some operating systems (including some Linux distributions in some version with some window managers) default emoji font is not colorful by default and you may want to make them colorful, first.
    My favorite way is with the Blessings library (full disclosure: I wrote it). For example:

from blessings import Terminal

t = Terminal()
print t.red('This is red.')
print t.bold_bright_red_on_black('Bright red on black')


To print colored bricks, the most reliable way is to print spaces with background colors. I use this technique to draw the progress bar in nose-progressive:

print t.on_green(' ')


You can print in specific locations as well:

with t.location(0, 5):
    print t.on_yellow(' ')


If you have to muck with other terminal capabilities in the course of your game, you can do that as well. You can use Python's standard string formatting to keep it readable:

print '{t.clear_eol}You just cleared a {t.bold}whole{t.normal} line!'.format(t=t)


The nice thing about Blessings is that it does its best to work on all sorts of terminals, not just the (overwhelmingly common) ANSI-color ones. It also keeps unreadable escape sequences out of your code while remaining concise to use. Have fun!
    On Windows you can use module 'win32console' (available in some Python distributions) or module 'ctypes' (Python 2.5 and up) to access the Win32 API.

To see complete code that supports both ways, see the color console reporting code from Testoob.

ctypes example:

import ctypes

# Constants from the Windows API
STD_OUTPUT_HANDLE = -11
FOREGROUND_RED    = 0x0004 # text color contains red.

def get_csbi_attributes(handle):
    # Based on IPython's winconsole.py, written by Alexander Belchenko
    import struct
    csbi = ctypes.create_string_buffer(22)
    res = ctypes.windll.kernel32.GetConsoleScreenBufferInfo(handle, csbi)
    assert res

    (bufx, bufy, curx, cury, wattr,
    left, top, right, bottom, maxx, maxy) = struct.unpack(""hhhhHhhhhhh"", csbi.raw)
    return wattr


handle = ctypes.windll.kernel32.GetStdHandle(STD_OUTPUT_HANDLE)
reset = get_csbi_attributes(handle)

ctypes.windll.kernel32.SetConsoleTextAttribute(handle, FOREGROUND_RED)
print ""Cherry on top""
ctypes.windll.kernel32.SetConsoleTextAttribute(handle, reset)

    If you want to use just built-in packages, follow this structure:
Actually, I enhanced the Mohamed Samy answer which is now responsible for multiple inputs as well as numbers. Also, it supports other print() arguments such as end=. Additionally, I added a .store() method in order to write down logs into a file as well.
You can create a utility to use that anywhere into your codes:
# utility.py

from datetime import datetime

class ColoredPrint:
    def __init__(self):
        self.PINK = '\033[95m'
        self.OKBLUE = '\033[94m'
        self.OKGREEN = '\033[92m'
        self.WARNING = '\033[93m'
        self.FAIL = '\033[91m'
        self.ENDC = '\033[0m'

    def disable(self):
        self.PINK = ''
        self.OKBLUE = ''
        self.OKGREEN = ''
        self.WARNING = ''
        self.FAIL = ''
        self.ENDC = ''

    def store(self):
        date = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
        with open('logfile.log', mode='a') as file_:
            file_.write(f""{self.msg} -- {date}"")
            file_.write(""\n"")

    def success(self, *args, **kwargs):
        self.msg = ' '.join(map(str, args))
        print(self.OKGREEN + self.msg + self.ENDC, **kwargs)
        return self

    def info(self, *args, **kwargs):
        self.msg = ' '.join(map(str, args))
        print(self.OKBLUE + self.msg + self.ENDC, **kwargs)
        return self

    def warn(self, *args, **kwargs):
        self.msg = ' '.join(map(str, args))
        print(self.WARNING + self.msg + self.ENDC, **kwargs)
        return self

    def err(self, *args, **kwargs):
        self.msg = ' '.join(map(str, args))
        print(self.FAIL + self.msg + self.ENDC, **kwargs)
        return self

    def pink(self, *args, **kwargs):
        self.msg = ' '.join(map(str, args))
        print(self.PINK + self.msg + self.ENDC, **kwargs)
        return self

e.g.
from utility import ColoredPrint

log = ColoredPrint()

log.success(""Hello"" , 123, ""Bye"").store()
log.info(""Hello"" , 123, ""Bye"")
log.warn(""Hello"" , 123, ""Bye"")
log.err(""Hello"" , 123, ""Bye"").store()
log.pink(""Hello"" , 123, ""Bye"")

Out:


[UPDATE]:
Now, its PyPI package is available:
pip install python-colored-print

    I ended up doing this, and I felt it was cleanest:
formatters = {
    'RED': '\033[91m',
    'GREEN': '\033[92m',
    'END': '\033[0m',
}

print 'Master is currently {RED}red{END}!'.format(**formatters)
print 'Help make master {GREEN}green{END} again!'.format(**formatters)

    For Windows you cannot print to console with colors unless you're using the Win32 API.
For Linux it's as simple as using print, with the escape sequences outlined here:
Colors
For the character to print like a box, it really depends on what font you are using for the console window. The pound symbol works well, but it depends on the font:
#

    An easier option would be to use the cprint function from the termcolor package.

It also supports %s, %d format of printing:

Results can be terminal dependant, so review the Terminal Properties section of the package documentation.

Windows Command Prompt and Python IDLE don't work




JupyterLab notebook does work


    import click

click.secho('Hello, World!', fg='green')
click.secho('Some more text', bg='blue', fg='white')
click.secho('ATTENTION', blink=True, bold=True)

click (CLI library) has a very convenient way of doing this, and is worth considering if you're writing a command-line tool, anyway.
    Building on joeld's answer, using https://pypi.python.org/pypi/lazyme 
pip install -U lazyme:
from lazyme.string import color_print
>>> color_print('abc')
abc
>>> color_print('abc', color='pink')
abc
>>> color_print('abc', color='red')
abc
>>> color_print('abc', color='yellow')
abc
>>> color_print('abc', color='green')
abc
>>> color_print('abc', color='blue', underline=True)
abc
>>> color_print('abc', color='blue', underline=True, bold=True)
abc
>>> color_print('abc', color='pink', underline=True, bold=True)
abc

Screenshot:


Some updates to the color_print with new formatters, e.g.:
>>> from lazyme.string import palette, highlighter, formatter
>>> from lazyme.string import color_print
>>> palette.keys() # Available colors.
['pink', 'yellow', 'cyan', 'magenta', 'blue', 'gray', 'default', 'black', 'green', 'white', 'red']
>>> highlighter.keys() # Available highlights.
['blue', 'pink', 'gray', 'black', 'yellow', 'cyan', 'green', 'magenta', 'white', 'red']
>>> formatter.keys() # Available formatter,
['hide', 'bold', 'italic', 'default', 'fast_blinking', 'faint', 'strikethrough', 'underline', 'blinking', 'reverse']

Note: italic, fast blinking, and strikethrough may not work on all terminals, and they don't work on Mac and Ubuntu.
E.g.,
>>> color_print('foo bar', color='pink', highlight='white')
foo bar
>>> color_print('foo bar', color='pink', highlight='white', reverse=True)
foo bar
>>> color_print('foo bar', color='pink', highlight='white', bold=True)
foo bar
>>> color_print('foo bar', color='pink', highlight='white', faint=True)
foo bar
>>> color_print('foo bar', color='pink', highlight='white', faint=True, reverse=True)
foo bar
>>> color_print('foo bar', color='pink', highlight='white', underline=True, reverse=True)
foo bar

Screenshot:

    Note how well the with keyword mixes with modifiers like these that need to be reset (using Python 3 and Colorama):
from colorama import Fore, Style
import sys

class Highlight:
  def __init__(self, clazz, color):
    self.color = color
    self.clazz = clazz
  def __enter__(self):
    print(self.color, end="""")
  def __exit__(self, type, value, traceback):
    if self.clazz == Fore:
      print(Fore.RESET, end="""")
    else:
      assert self.clazz == Style
      print(Style.RESET_ALL, end="""")
    sys.stdout.flush()

with Highlight(Fore, Fore.GREEN):
  print(""this is highlighted"")
print(""this is not"")

    In windows 10 you can try this tiny script, which works as a color mixer with values from 0-255 for Red, Green and blue:
import os

os.system('')


def RGB(red=None, green=None, blue=None,bg=False):
    if(bg==False and red!=None and green!=None and blue!=None):
        return f'\u001b[38;2;{red};{green};{blue}m'
    elif(bg==True and red!=None and green!=None and blue!=None):
        return f'\u001b[48;2;{red};{green};{blue}m'
    elif(red==None and green==None and blue==None):
        return '\u001b[0m'

and call the RGB function to make any combination of colors as:
g0 = RGB()
g1 = RGB(0,255,0)
g2 = RGB(0,100,0,True)+""""+RGB(100,255,100)
g3 = RGB(0,255,0,True)+""""+RGB(0,50,0)

print(f""{g1}green1{g0}"")
print(f""{g2}green2{g0}"")
print(f""{g3}green3{g0}"")

RGB() with no parameter will cleanup and set the foreground/background color to default. In case you want black you should call it as RGB(0,0,0) and for white RGB(255,255,255). While RGB(0,255,0) creates absolute green RGB(150,255,150) will produce light green.
This supports background & foreground color, to set the color as background color you must pass it with bg=True which is False by default.
For Example: To set red as the background color it should be called as RGB(255,0,0,True) but to choose red as font color just call it as RGB(255,0,0,False) since bg is by default False this simplifies to just call it as RGB(255,0,0)
    I was moved there by google when I was looking how to color logs so:
coloredlogs
Instalation
pip install coloredlogs

Usage
Minimal usage:
import logging
import coloredlogs

coloredlogs.install()  # install a handler on the root logger

logging.debug('message with level debug')
logging.info('message with level info')
logging.warning('message with level warning')
logging.error('message with level error')
logging.critical('message with level critical')

Results with:

Start from message level debug:
import logging
import coloredlogs

coloredlogs.install(level='DEBUG')  # install a handler on the root logger with level debug

logging.debug('message with level debug')
logging.info('message with level info')
logging.warning('message with level warning')
logging.error('message with level error')
logging.critical('message with level critical')

Results with:

Hide messages from libraries:
import logging
import coloredlogs

logger = logging.getLogger(__name__)  # get a specific logger object
coloredlogs.install(level='DEBUG')  # install a handler on the root logger with level debug
coloredlogs.install(level='DEBUG', logger=logger)  # pass a specific logger object

logging.debug('message with level debug')
logging.info('message with level info')
logging.warning('message with level warning')
logging.error('message with level error')
logging.critical('message with level critical')

Results with:

Format log messages:
import logging
import coloredlogs

logger = logging.getLogger(__name__)  # get a specific logger object
coloredlogs.install(level='DEBUG')  # install a handler on the root logger with level debug
coloredlogs.install(level='DEBUG', logger=logger)  # pass a specific logger object
coloredlogs.install(
    level='DEBUG', logger=logger,
    fmt='%(asctime)s.%(msecs)03d %(filename)s:%(lineno)d %(levelname)s %(message)s'
)

logging.debug('message with level debug')
logging.info('message with level info')
logging.warning('message with level warning')
logging.error('message with level error')
logging.critical('message with level critical')

Results with:

Available format attributes:

%(asctime)s - Time as human-readable string, when logging call was issued
%(created)f - Time as float when logging call was issued
%(filename)s - File name
%(funcName)s - Name of function containing the logging call
%(hostname)s - System hostname
%(levelname)s - Text logging level
%(levelno)s - Integer logging level
%(lineno)d - Line number where the logging call was issued
%(message)s - Message passed to logging call (same as %(msg)s)
%(module)s - File name without extension where the logging call was issued
%(msecs)d - Millisecond part of the time when logging call was issued
%(msg)s - Message passed to logging call (same as %(message)s)
%(name)s - Logger name
%(pathname)s - Full pathname to file containing the logging call
%(process)d - Process ID
%(processName)s - Process name
%(programname)s - System programname
%(relativeCreated)d - Time as integer in milliseconds when logging call was issued, relative to the time when logging module was loaded
%(thread)d -  Thread ID
%(threadName)s - Thread name
%(username)s - System username

Sources:
Coloredlogs package
Logging library
    You could use Clint:
from clint.textui import colored
print colored.red('some warning message')
print colored.green('nicely done!')

    ","[2787, 2472, 1044, 924, 577, 280, 153, 78, 49, 102, 25, 112, 43, 41, 32, 62, 55, 38, 12, 69, 41, 10, 27, 27, 15, 10, 24, 23, 4, 4, 21]",1913207,1011,2008-11-13T18:58:10,2022-04-27 00:21:59Z,python 
What are the correct version numbers for C#?,"
                
What are the correct version numbers for C#? What came out when? Why can't I find any answers about C# 3.5?

This question is primarily to aid those who are searching for an answer using an incorrect version number, e.g. C# 3.5. The hope is that anyone failing to find an answer with the wrong version number will find this question and then search again with the right version number.
    C# language version history:
These are the versions of C# known about at the time of this writing:

C# 1.0 released with .NET 1.0 and VS2002 (January 2002)
C# 1.2 (bizarrely enough); released with .NET 1.1 and VS2003 (April 2003). First version to call Dispose on IEnumerators which implemented IDisposable. A few other small features.
C# 2.0 released with .NET 2.0 and VS2005 (November 2005). Major new features: generics, anonymous methods, nullable types, and iterator blocks
C# 3.0 released with .NET 3.5 and VS2008 (November 2007). Major new features: lambda expressions, extension methods, expression trees, anonymous types, implicit typing (var), and query expressions
C# 4.0 released with .NET 4 and VS2010 (April 2010). Major new features: late binding (dynamic), delegate and interface generic variance, more COM support, named arguments, tuple data type and optional parameters
C# 5.0 released with .NET 4.5 and VS2012 (August 2012). Major features: async programming, and caller info attributes. Breaking change: loop variable closure.
C# 6.0 released with .NET 4.6 and VS2015 (July 2015). Implemented by Roslyn. Features: initializers for automatically implemented properties, using directives to import static members, exception filters, element initializers, await in catch and finally, extension Add methods in collection initializers.
C# 7.0 released with .NET 4.7 and VS2017 (March 2017). Major new features: tuples, ref locals and ref return, pattern matching (including pattern-based switch statements), inline out parameter declarations, local functions, binary literals, digit separators, and arbitrary async returns.
C# 7.1 released with VS2017 v15.3 (August 2017). New features: async main, tuple member name inference, default expression, and pattern matching with generics.
C# 7.2 released with VS2017 v15.5 (November 2017). New features: private protected access modifier, Span<T>, aka interior pointer, aka stackonly struct, and everything else.
C# 7.3 released with VS2017 v15.7 (May 2018). New features: enum, delegate and unmanaged generic type constraints. ref reassignment. Unsafe improvements: stackalloc initialization, unpinned indexed fixed buffers, custom fixed statements. Improved overloading resolution. Expression variables in initializers and queries. == and != defined for tuples. Auto-properties' backing fields can now be targeted by attributes.
C# 8.0 released with .NET Core 3.0 and VS2019 v16.3 (September 2019). Major new features: nullable reference-types, asynchronous streams, indices and ranges, readonly members, using declarations, default interface methods, static local functions, and enhancement of interpolated verbatim strings.
C# 9.0 released with .NET 5.0 and VS2019 v16.8 (November 2020). Major new features: init-only properties, records, with-expressions, data classes, positional records, top-level programs, improved pattern matching (simple type patterns, relational patterns, logical patterns), improved target typing (target-type new expressions, target typed ?? and ?), and covariant returns. Minor features: relax ordering of ref and partial modifiers, parameter null checking, lambda discard parameters, native ints, attributes on local functions, function pointers, static lambdas, extension GetEnumerator, module initializers, and extending partial.
C# 10.0 released with .NET 6.0 (November 2021). Major new features: record structs, struct parameterless constructors, interpolated string handlers, global using directives, file-scoped namespace declarations, extended property patterns, const interpolated strings, mixed assignment and declaration in deconstruction, async method builders (via attributes) for individual methods, the CallerArgumentExpression attribute for parameters, enhanced #line pragmas.

In response to the OP's question:

What are the correct version numbers for C#? What came out when? Why can't I find any answers about C# 3.5?

There is no such thing as C# 3.5 - the cause of confusion here is that the C# 3.0 is present in .NET 3.5. The language and framework are versioned independently, however - as is the CLR, which is at version 2.0 for .NET 2.0 through 3.5, .NET 4 introducing CLR 4.0, service packs notwithstanding. The CLR in .NET 4.5 has various improvements, but the versioning is unclear: in some places it may be referred to as CLR 4.5 (this MSDN page used to refer to it that way, for example), but the Environment.Version property still reports 4.0.xxx.
As of May 3, 2017, the C# Language Team created a history of C# versions and features on their GitHub repository: Features Added in C# Language Versions. There is also a page that tracks upcoming and recently implemented language features.
    This is the same as most answers here, but tabularized for ease, and it has Visual Studio and .NET versions for completeness.




C# version
VS version
.NET version
CLR version
Release date




1.0
2002
1.0
1.0
Feb 2002


1.2
2003
1.1
1.1
Apr 2003


2.0
2005
2.0
2.0
Nov 2005




3.0
2.0
Nov 2006


3.0
2008
3.5
2.0
Nov 2007


4.0
2010
4.0
4
Apr 2010


5.0
2012
4.5
4
Aug 2012


5.0
2013
4.5.1
4
Oct 2013




4.5.2
4
May 2014


6.0
2015
4.6
4
Jul 2015




4.6.1
4
Nov 2015




4.6.2
4
Aug 2016


7.0
2017


Mar 2017




4.7
4
May 2017


7.1
2017 (v15.3)


Aug 2017




4.7.1
4
Oct 2017


7.2
2017 (v15.5)


Dec 2017




4.7.2
4
Apr 2018


7.3
2017 (v15.7)


May 2018


8.0
2019
4.8
4
Apr 2019


9.0
2019 (v16.8)
5.0*
**
Nov 2020



Versions since .NET Core




C# version
VS version
.NET version
Release date
End of Support





2015 Update 3
.NET Core 1.0
Jun 2016
Jun 2019




.NET Core 1.1
Nov 2016
Jun 2019


7.1
2017 (v15.3)
.NET Core 2.0
Aug 2017
Oct 2018


7.3
2017 (v15.7)
.NET Core 2.1
May 2018
Aug 2021




.NET Core 2.2
Dec 2018
Dec 2019



2019 (v16.3)
.NET Core 3.0
Sep 2019
Mar 2020



2019 (v16.4)
.NET Core 3.1
Dec 2019
Dec 2022


9.0
2019 (v16.8)
.NET 5
Nov 2020
Feb 2022


10.0
2022
.NET 6
Nov 2021
Nov 2024




.NET 7
Nov 2022
Feb 2023




.NET 8
Nov 2023
Nov 2026




* - .NET 5.0 is not a newer version of .NET framework but .NET Core 3. Starting from .NET 5.0, there are no newer versions of .NET full framework.
** - There are no separate CLR (CoreCLR) versions for .NET Core. Whatever is the .NET Core version is the CoreCLR version. So not mentioning it.

Note: .NET development is pretty much independent of VS these days, there is no correlation between versions of each. Refer to "".NET Framework versions and dependencies"" and "".NET release cadence"" for more.
    The biggest problem when dealing with C#'s version numbers is the fact that it is not tied to a version of the .NET Framework, which it appears to be due to the synchronized releases between Visual Studio and the .NET Framework.
The version of C# is actually bound to the compiler, not the framework. For instance, in VisualStudio2008 you can write C# 3.0 and target .NET Framework 2.0, 3.0 and 3.5. The C# 3.0 nomenclature describes the version of the code syntax and supported features in the same way that ANSI C89, C90, and C99 describe the code syntax and features for C.
Take a look at Mono, and you will see that Mono 2.0 (mostly implemented version 2.0 of the .NET Framework from the ECMA specifications) supports the C# 3.0 syntax and features.
    
C# 1.0 with Visual Studio .NET

C# 2.0 with VisualStudio2005

C# 3.0 with VisualStudio2008

C# 4.0 with VisualStudio2010

C# 5.0 with VisualStudio2012

C# 6.0 with VisualStudio2015

C# 7.0 with VisualStudio2017

C# 8.0 with VisualStudio2019

C# 9.0 with VisualStudio2019


    



Version
Language specification
Microsoft compiler




C# 1.0/1.2
December 2001?/2003?
January 2002?


C# 2.0
September 2005
November 2005?


C# 3.0
May 2006
November 2006?


C# 4.0
March 2009 (draft)
April 2010?


C# 5.0

Released with .NET 4.5 in August 2012


C# 6.0

Released with .NET  4.6   2015


C# 7.0

Released with .NET  4.7   2017


C# 8.0

Released with .NET  4.8   2019



    I've summarised most of the versions in this table. The only ones missing should be ASP.NET Core versions. I've also added different versions of ASP.NET MVC.

Note that ASP.NET 5 has been rebranded as ASP.NET Core 1.0 and ASP.NET MVC 6 has been rebranded as ASP.NET Core MVC 1.0.0. I believe this change occurred sometime around Jan 2016.

I have included the release date of ASP.NET 5 RC1 in the table, but I've yet to include ASP.NET core 1.0 and other core versions, because I couldn't find the exact release dates. You can read more about the release dates regarding ASP.NET Core here: When is ASP.NET Core 1.0 (ASP.NET 5 / vNext) scheduled for release?


    C# 1.0 - Visual Studio .NET 2002

Classes
Structs
Interfaces
Events
Properties
Delegates
Expressions
Statements
Attributes
Literals

C# 1.2 - Visual Studio .NET 2003

Dispose in foreach
foreach over string specialization
C# 2 - Visual Studio 2005
Generics
Partial types
Anonymous methods
Iterators
Nullable types
Getter/setter separate accessibility
Method group conversions (delegates)
Static classes
Delegate inference

C# 3 - Visual Studio 2008

Implicitly typed local variables
Object and collection initializers
Auto-Implemented properties
Anonymous types
Extension methods
Query expressions
Lambda expression
Expression trees
Partial methods

C# 4 - Visual Studio 2010

Dynamic binding
Named and optional arguments
Co- and Contra-variance for generic delegates and interfaces
Embedded interop types (""NoPIA"")

C# 5 - Visual Studio 2012

Asynchronous methods
Caller info attributes

C# 6 - Visual Studio 2015

Draft Specification online
Compiler-as-a-service (Roslyn)
Import of static type members into namespace
Exception filters
Await in catch/finally blocks
Auto property initializers
Default values for getter-only properties
Expression-bodied members
Null propagator (null-conditional operator, succinct null checking)
String interpolation
nameof operator
Dictionary initializer

C# 7.0 - Visual Studio 2017

Out variables
Pattern matching
Tuples
Deconstruction
Discards
Local Functions
Binary Literals
Digit Separators
Ref returns and locals
Generalized async return types
More expression-bodied members
Throw expressions

C# 7.1 - Visual Studio 2017 version 15.3

Async main
Default expressions
Reference assemblies
Inferred tuple element names
Pattern-matching with generics

C# 7.2 - Visual Studio 2017 version 15.5

Span and ref-like types
In parameters and readonly references
Ref conditional
Non-trailing named arguments
Private protected accessibility
Digit separator after base specifier

C# 7.3 - Visual Studio 2017 version 15.7

System.Enum, System.Delegate and unmanaged constraints.
Ref local re-assignment: Ref locals and ref parameters can now be reassigned with the ref assignment operator (= ref).
Stackalloc initializers: Stack-allocated arrays can now be initialized, e.g. Span x = stackalloc[] { 1, 2, 3 };.
Indexing movable fixed buffers: Fixed buffers can be indexed into without first being pinned.
Custom fixed statement: Types that implement a suitable GetPinnableReference can be used in a fixed statement.
Improved overload candidates: Some overload resolution candidates can be ruled out early, thus reducing ambiguities.
Expression variables in initializers and queries: Expression variables like out var and pattern variables are allowed in field initializers, constructor initializers and LINQ queries.
Tuple comparison: Tuples can now be compared with == and !=.
Attributes on backing fields: Allows [field: ] attributes on an auto-implemented property to target its backing field.

C# 8.0 - .NET Core 3.0 and Visual Studio 2019 version 16.3

Nullable reference types: express nullability intent on reference types with ?, notnull constraint and annotations attributes in APIs, the compiler will use those to try and detect possible null values being dereferenced or passed to unsuitable APIs.
Default interface members: interfaces can now have members with default implementations, as well as static/private/protected/internal members except for state (ie. no fields).
Recursive patterns: positional and property patterns allow testing deeper into an object, and switch expressions allow for testing multiple patterns and producing corresponding results in a compact fashion.
Async streams: await foreach and await using allow for asynchronous enumeration and disposal of IAsyncEnumerable collections and IAsyncDisposable resources, and async-iterator methods allow convenient implementation of such asynchronous streams.
Enhanced using: a using declaration is added with an implicit scope and using statements and declarations allow disposal of ref structs using a pattern.
Ranges and indexes: the i..j syntax allows constructing System.Range instances, the ^k syntax allows constructing System.Index instances, and those can be used to index/slice collections.
Null-coalescing assignment: ??= allows conditionally assigning when the value is null.
Static local functions: local functions modified with static cannot capture this or local variables, and local function parameters now shadow locals in parent scopes.
Unmanaged generic structs: generic struct types that only have unmanaged fields are now considered unmanaged (ie. they satisfy the unmanaged constraint).
Readonly members: individual members can now be marked as readonly to indicate and enforce that they do not modify instance state.
Stackalloc in nested contexts: stackalloc expressions are now allowed in more expression contexts.
Alternative interpolated verbatim strings: @$""..."" strings are recognized as interpolated verbatim strings just like $@""..."".
Obsolete on property accessors: property accessors can now be individually marked as obsolete.
Permit t is null on unconstrained type parameter

[Source]: https://github.com/dotnet/csharplang/blob/master/Language-Version-History.md
    You can check the latest C# versions here

    Version     .NET Framework     Visual Studio     Important Features

C# 1.0     .NET Framework 1.0/1.1     Visual Studio .NET 2002

Basic features

C# 2.0     .NET Framework 2.0     Visual Studio 2005

Generics
Partial types
Anonymous methods
Iterators
Nullable types
Private setters (properties)
Method group conversions (delegates)
Covariance and Contra-variance
Static classes

C# 3.0     .NET Framework 3.0\3.5     Visual Studio 2008

Implicitly typed local variables
Object and collection initializers
Auto-Implemented properties
Anonymous types
Extension methods
Query expressions
Lambda expressions
Expression trees
Partial Methods

C# 4.0     .NET Framework 4.0     Visual Studio 2010

Dynamic binding (late binding)
Named and optional arguments
Generic co- and contravariance
Embedded interop types

C# 5.0     .NET Framework 4.5     Visual Studio 2012/2013

Async features
Caller information

C# 6.0     .NET Framework 4.6     Visual Studio 2013/2015

Expression Bodied Methods
Auto-property initializer
nameof Expression
Primary constructor
Await in catch block
Exception Filter
String Interpolation

C# 7.0     .NET Core 2.0     Visual Studio 2017

out variables
Tuples
Discards
Pattern Matching
Local functions
Generalized async return types
Numeric literal syntax improvements

C# 8.0     .NET Core 3.0     Visual Studio 2019

Readonly members
Default interface methods
Pattern matching enhancements:

Switch expressions
Property patterns
Tuple patterns
Positional patterns


Using declarations
Static local functions
Disposable ref structs
Nullable reference types
Asynchronous streams
Asynchronous disposable
Indices and ranges
Null-coalescing assignment
Unmanaged constructed types
Stackalloc in nested expressions
Enhancement of interpolated verbatim strings

    Comparing the MSDN articles ""What's New in the C# 2.0 Language and Compiler"" and ""What's New in Visual C# 2005"", it is possible to deduce that ""C# major_version.minor_version"" is coined according to the compiler's version numbering.

There is C# 1.2 corresponding to .NET  1.1 and VS 2003 and also named as Visual C# .NET 2003.

But further on Microsoft stopped to increment the minor version (after the dot) numbers or to have them other than zero, 0. Though it should be noted that C# corresponding to .NET 3.5 is named in msdn.microsoft.com as ""Visual C# 2008 Service Pack 1"".

There are two parallel namings: By major .NET/compiler version numbering and by Visual Studio numbering.

C# 2.0 is a synonym for Visual C# 2005

C# 3.0  corresponds (or, more correctly, can target) to:


.NET 2.0 <==> Visual C# 2005
.NET3.0 <==> Visual C# 2008
.NET 3.5 <==> Visual C# 2008 Service Pack 1

    C# 8.0 is the latest version of C#. It is supported only on .NET Core 3.x and newer versions. Many of the newest features require library and runtime features introduced in .NET Core 3.x.
The following table lists the target framework with version and their default C# version.

Source - C# language versioning
    I was looking for a concise history of the .NET, C#, CLR, and VisualStudio versions alongside the key language features.
Since I couldn't find any up-to-date table that contains all the information I needed in one place - I merged details from the Microsoft docs into what I tried to keep a concise table that contains what I was looking for.
Its available here: https://mantinband.github.io/dotnet-shmotnet/
I probably have some mistakes or missing information so please feel free to open an issue or contribute over here: https://github.com/mantinband/dotnet-shmotnet
Sneak peek:

    Preview: C# 11.0 .NET Core 7.0 Visual Studio 2022 Update 1
READ MORE

Allow newlines in the holes of interpolated strings
List patterns
Parameter null-checking
Interaction with Nullable Reference Types
Generic attributes
field keyword
Static abstracts in interfaces
Declarations under or patterns
Records and initialization
Discriminated unions
Params Span of T
Statements as expressions
Expression trees
Type system extensions

    ","[2784, 3089, 408, 319, 214, 95, 61, 16, 55, 8, 39, 4, 1, 0]",388329,663,2008-10-29T17:09:40,2022-03-01 15:09:48Z,c 
Determine installed PowerShell version,"
                
How can I determine what version of PowerShell is installed on a computer, and indeed if it is installed at all?
    Use $PSVersionTable.PSVersion to determine the engine version. If the variable does not exist, it is safe to assume the engine is version 1.0.

Note that $Host.Version and (Get-Host).Version are not reliable - they reflect
the version of the host only, not the engine. PowerGUI,
PowerShellPLUS, etc. are all hosting applications, and
they will set the host's version to reflect their product
version which is entirely correct, but not what you're looking for.

PS C:\> $PSVersionTable.PSVersion

Major  Minor  Build  Revision
-----  -----  -----  --------
4      0      -1     -1

    I would use either Get-Host or $PSVersionTable.  As Andy Schneider points out, $PSVersionTable doesn't work in version 1; it was introduced in version 2.

get-host

Name             : ConsoleHost
Version          : 2.0
InstanceId       : d730016e-2875-4b57-9cd6-d32c8b71e18a
UI               : System.Management.Automation.Internal.Host.InternalHostUserInterface
CurrentCulture   : en-GB
CurrentUICulture : en-US
PrivateData      : Microsoft.PowerShell.ConsoleHost+ConsoleColorProxy
IsRunspacePushed : False
Runspace         : System.Management.Automation.Runspaces.LocalRunspace

$PSVersionTable

Name                           Value
----                           -----
CLRVersion                     2.0.50727.4200
BuildVersion                   6.0.6002.18111
PSVersion                      2.0
WSManStackVersion              2.0
PSCompatibleVersions           {1.0, 2.0}
SerializationVersion           1.1.0.1
PSRemotingProtocolVersion      2.1

    You can directly check the version with one line only by invoking PowerShell externally, such as from Command Prompt

powershell -Command ""$PSVersionTable.PSVersion""


According to @psaul you can actually have one command that is agnostic from where it came (CMD, PowerShell or Pwsh). Thank you for that.

powershell -command ""(Get-Variable PSVersionTable -ValueOnly).PSVersion""


I've tested and it worked flawlessly on both CMD and PowerShell.


    You can verify that Windows PowerShell version installed by completing the following check:


Click Start, click All Programs, click Accessories, click Windows PowerShell, and then click Windows PowerShell.
In the Windows PowerShell console, type the following command at the command prompt and then press ENTER:

Get-Host | Select-Object Version



You will see output that looks like this:

Version
-------
3.0


http://www.myerrorsandmysolutions.com/how-to-verify-the-windows-powershell-version-installed/
    You can look at the built in variable, $psversiontable. If it doesn't exist, you have V1. If it does exist, it will give you all the info you need.

1 >  $psversiontable

Name                           Value                                           
----                           -----                                           
CLRVersion                     2.0.50727.4927                                  
BuildVersion                   6.1.7600.16385                                  
PSVersion                      2.0                                             
WSManStackVersion              2.0                                             
PSCompatibleVersions           {1.0, 2.0}                                      
SerializationVersion           1.1.0.1                                         
PSRemotingProtocolVersion      2.1    

    PowerShell 7
The accepted answer is only appropriate if one version of PowerShell is installed on a computer. With the advent of PowerShell 7, this scenario becomes increasingly unlikely.
Microsoft's documentation states that additional registry keys are created when PowerShell 7 is installed:

Beginning in PowerShell 7.1, the [installer] package creates registry keys
that store the installation location and version of PowerShell. These
values are located in
HKLM\Software\Microsoft\PowerShellCore\InstalledVersions\<GUID>. The
value of <GUID> is unique for each build type (release or preview),
major version, and architecture.

Exploring the registry in the aforementioned location reveals the following registry value: SemanticVersion. This value contains the information we seek.
On my computer it appears like the following:
Path                                                                                           Name              Type Data
----                                                                                           ----              ---- ----
HKLM:\SOFTWARE\Microsoft\PowerShellCore\InstalledVersions\31ab5147-9a97-4452-8443-d9709f0516e1 SemanticVersion String 7.1.3


As you can see, the version of PowerShell 7 installed on my computer is 7.1.3. If PowerShell 7 is not installed on the target computer, the key in its entirety should not exist.
As mentioned in the Microsoft documentation, the registry path will be slightly different dependent on installed PowerShell version.
Part of the key path changing could pose a challenge in some scenarios, but for those interested in a command line-based solution, PowerShell itself can handle this problem easily.
The PowerShell cmdlet used to query the data in this registry value is the Get-ItemPropertyValue cmdlet. Observe its use and output as follows (note the asterisk wildcard character used in place of the part of the key path that is likely to change):
PS> Get-ItemPropertyValue -Path ""HKLM:\SOFTWARE\Microsoft\PowerShellCore\InstalledVersions\*"" -Name ""SemanticVersion""

7.1.3

Just a simple one-liner.
    To determine if PowerShell is installed, you can check the registry for the existence of 

HKEY_LOCAL_MACHINE\Software\Microsoft\PowerShell\1\Install


and

HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\PowerShell\3


and, if it exists, whether the value is 1 (for installed), as detailed in the blog post Check if PowerShell installed and version.

To determine the version of PowerShell that is installed, you can check the registry keys 

HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\PowerShell\1\PowerShellEngine\PowerShellVersion


and

HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\PowerShell\3\PowerShellEngine\PowerShellVersion


To determine the version of PowerShell that is installed from a .ps1 script, you can use the following one-liner, as detailed on PowerShell.com in Which PowerShell Version Am I Running.

$isV2 = test-path variable:\psversiontable


The same site also gives a function to return the version:

function Get-PSVersion {
    if (test-path variable:psversiontable) {$psversiontable.psversion} else {[version]""1.0.0.0""}
}

    I needed to check the version of PowerShell and then run the appropriate code. Some of our servers run v5, and others v4. This means that some functions, like compress, may or may not be available.

This is my solution: 

if ($PSVersionTable.PSVersion.Major -eq 5) {
    #Execute code available in PowerShell 5, like Compress
    Write-Host ""You are running PowerShell version 5""
}
else {
    #Use a different process
    Write-Host ""This is version $PSVersionTable.PSVersion.Major""
}

    The below  cmdlet will return the PowerShell version.

$PSVersionTable.PSVersion.Major

    This is the top search result for ""Batch file get powershell version"", so I'd like to provide a basic example of how to do conditional flow in a batch file depending on the powershell version

Generic example



powershell ""exit $PSVersionTable.PSVersion.Major""
if %errorlevel% GEQ 5 (
    echo Do some fancy stuff that only powershell v5 or higher supports
) else (
    echo Functionality not support by current powershell version.
)


Real world example

powershell ""exit $PSVersionTable.PSVersion.Major""
if %errorlevel% GEQ 5 (
    rem Unzip archive automatically
    powershell Expand-Archive Compressed.zip
) else (
    rem Make the user unzip, because lazy
    echo Please unzip Compressed.zip prior to continuing...
    pause
)

    Microsoft's recommended forward compatible method for checking if PowerShell is installed and determining the installed version is to look at two specific registry keys. I've reproduced the details here in case the link breaks.

According to the linked page:


  Depending on any other registry key(s), or version of PowerShell.exe or the location of PowerShell.exe is not guaranteed to work in the long term.


To check if any version of PowerShell is installed, check for the following value in the registry:


Key Location: HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\PowerShell\1
Value Name: Install
Value Type: REG_DWORD
Value Data: 0x00000001 (1


To check whether version 1.0 or 2.0 of PowerShell is installed, check for the following value in the registry:


Key Location: HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\PowerShell\1\PowerShellEngine
Value Name: PowerShellVersion
Value Type: REG_SZ
Value Data: <1.0 | 2.0>

    The easiest way to forget this page and never return to it is to learn the Get-Variable: 

Get-Variable | where {$_.Name -Like '*version*'} | %{$_[0].Value}


There is no need to remember every variable. Just Get-Variable is enough (and ""There should be something about version""). 
    Since the most helpful answer didn't address the if exists portion, I thought I'd give one take on it via a quick-and-dirty solution. It relies on PowerShell being in the path environment variable which is likely what you want. (Hat tip to the top answer as I didn't know that.) Paste this into a text file and name it


  Test Powershell Version.cmd


or similar.

@echo off
echo Checking powershell version...
del ""%temp%\PSVers.txt"" 2>nul
powershell -command ""[string]$PSVersionTable.PSVersion.Major +'.'+ [string]$PSVersionTable.PSVersion.Minor | Out-File ([string](cat env:\temp) + '\PSVers.txt')"" 2>nul
if errorlevel 1 (
 echo Powershell is not installed. Please install it from download.Microsoft.com; thanks.
) else (
 echo You have installed Powershell version:
 type ""%temp%\PSVers.txt""
 del ""%temp%\PSVers.txt"" 2>nul
)
timeout 15

    Use:

$psVersion = $PSVersionTable.PSVersion
If ($psVersion)
{
    #PowerShell Version Mapping
    $psVersionMappings = @()
    $psVersionMappings += New-Object PSObject -Property @{Name='5.1.14393.0';FriendlyName='Windows PowerShell 5.1 Preview';ApplicableOS='Windows 10 Anniversary Update'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.1.14300.1000';FriendlyName='Windows PowerShell 5.1 Preview';ApplicableOS='Windows Server 2016 Technical Preview 5'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.0.10586.494';FriendlyName='Windows PowerShell 5 RTM';ApplicableOS='Windows 10 1511 + KB3172985 1607'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.0.10586.122';FriendlyName='Windows PowerShell 5 RTM';ApplicableOS='Windows 10 1511 + KB3140743 1603'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.0.10586.117';FriendlyName='Windows PowerShell 5 RTM 1602';ApplicableOS='Windows Server 2012 R2, Windows Server 2012, Windows Server 2008 R2 SP1, Windows 8.1, and Windows 7 SP1'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.0.10586.63';FriendlyName='Windows PowerShell 5 RTM';ApplicableOS='Windows 10 1511 + KB3135173 1602'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.0.10586.51';FriendlyName='Windows PowerShell 5 RTM 1512';ApplicableOS='Windows Server 2012 R2, Windows Server 2012, Windows Server 2008 R2 SP1, Windows 8.1, and Windows 7 SP1'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.0.10514.6';FriendlyName='Windows PowerShell 5 Production Preview 1508';ApplicableOS='Windows Server 2012 R2'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.0.10018.0';FriendlyName='Windows PowerShell 5 Preview 1502';ApplicableOS='Windows Server 2012 R2'}
    $psVersionMappings += New-Object PSObject -Property @{Name='5.0.9883.0';FriendlyName='Windows PowerShell 5 Preview November 2014';ApplicableOS='Windows Server 2012 R2, Windows Server 2012, Windows 8.1'}
    $psVersionMappings += New-Object PSObject -Property @{Name='4.0';FriendlyName='Windows PowerShell 4 RTM';ApplicableOS='Windows Server 2012 R2, Windows Server 2012, Windows Server 2008 R2 SP1, Windows 8.1, and Windows 7 SP1'}
    $psVersionMappings += New-Object PSObject -Property @{Name='3.0';FriendlyName='Windows PowerShell 3 RTM';ApplicableOS='Windows Server 2012, Windows Server 2008 R2 SP1, Windows 8, and Windows 7 SP1'}
    $psVersionMappings += New-Object PSObject -Property @{Name='2.0';FriendlyName='Windows PowerShell 2 RTM';ApplicableOS='Windows Server 2008 R2 SP1 and Windows 7'}
    foreach ($psVersionMapping in $psVersionMappings)
    {
        If ($psVersion -ge $psVersionMapping.Name) {
            @{CurrentVersion=$psVersion;FriendlyName=$psVersionMapping.FriendlyName;ApplicableOS=$psVersionMapping.ApplicableOS}
            Break
        }
    }
}
Else{
    @{CurrentVersion='1.0';FriendlyName='Windows PowerShell 1 RTM';ApplicableOS='Windows Server 2008, Windows Server 2003, Windows Vista, Windows XP'}
}


You can download the detailed script from How to determine installed PowerShell version.
    $host.version is just plain wrong/unreliable. This gives you the version of the hosting executable (powershell.exe, powergui.exe, powershell_ise.exe, powershellplus.exe etc) and not the version of the engine itself.

The engine version is contained in $psversiontable.psversion. For PowerShell 1.0, this variable does not exist, so obviously if this variable is not available it is entirely safe to assume the engine is 1.0, obviously.
    I tried this on version 7.1.0 and it worked:
$PSVersionTable | Select-Object PSVersion

Output
PSVersion
---------
7.1.0

It doesn't work on version 5.1 though, so rather go for this on versions below 7:
$PSVersionTable.PSVersion

Output
Major  Minor  Build  Revision
-----  -----  -----  --------
5      1      18362  1171

    I found the easiest way to check if installed was to:


run a command prompt (Start, Run, cmd, then OK)
type powershell then hit return.  You should then get the PowerShell PS prompt:




C:\Users\MyUser>powershell

Windows PowerShell
Copyright (C) 2009 Microsoft Corporation. All rights reserved.

PS C:\Users\MyUser>


You can then check the version from the PowerShell prompt by typing $PSVersionTable.PSVersion:

PS C:\Users\MyUser> $PSVersionTable.PSVersion

Major  Minor  Build  Revision
-----  -----  -----  --------
2      0      -1     -1

PS C:\Users\MyUser>


Type exit if you want to go back to the command prompt (exit again if you want to also close the command prompt).

To run scripts, see http://ss64.com/ps/syntax-run.html.
    To check if PowerShell is installed use:

HKLM\Software\Microsoft\PowerShell\1 Install ( = 1 )


To check if RC2 or RTM is installed use:

HKLM\Software\Microsoft\PowerShell\1 PID (=89393-100-0001260-00301) -- For RC2
HKLM\Software\Microsoft\PowerShell\1 PID (=89393-100-0001260-04309) -- For RTM


Source: this website.
    You can also call the ""host"" command from the PowerShell commandline. It should give you the value of the $host variable.
    Extending the answer with a select operator:

Get-Host | select {$_.Version}

    I have made a small batch script that can determine PowerShell version:

@echo off
for /f ""tokens=2 delims=:"" %%a in ('powershell -Command Get-Host ^| findstr /c:Version') do (echo %%a)


This simply extracts the version of PowerShell using Get-Host and searches the string Version

When the line with the version is found, it uses the for command to extract the version. In this case we are saying that the delimiter is a colon and search next the first colon, resulting in my case 5.1.18362.752.
    ","[2780, 3744, 449, 59, 44, 103, 6, 96, 6, 5, 3, 22, 8, 7, 9, 9, 0, 11, 6, 0, -2, -2]",3048093,380,2009-12-01T11:30:03,2021-12-06 14:49:44Z,powershell 
How do I pass command line arguments to a Node.js program?,"
                
I have a web server written in Node.js and I would like to launch with a specific folder. I'm not sure how to access arguments in JavaScript. I'm running node like this:

$ node server.js folder


here server.js is my server code. Node.js help says this is possible:

$ node -h
Usage: node [options] script.js [arguments]


How would I access those arguments in JavaScript? Somehow I was not able to find this information on the web.
    Standard Method (no library)

The arguments are stored in process.argv

Here are the node docs on handling command line args:


  process.argv is an array containing the command line arguments. The first element will be 'node', the second element will be the name of the JavaScript file. The next elements will be any additional command line arguments.


// print process.argv
process.argv.forEach(function (val, index, array) {
  console.log(index + ': ' + val);
});


This will generate:

$ node process-2.js one two=three four
0: node
1: /Users/mjr/work/node/process-2.js
2: one
3: two=three
4: four

    To normalize the arguments like a regular javascript function would receive, I do this in my node.js shell scripts:

var args = process.argv.slice(2);


Note that the first arg is usually the path to nodejs, and the second arg is the location of the script you're executing.
    The up-to-date right answer for this it to use the minimist library. We used to use node-optimist but it has since been deprecated. 

Here is an example of how to use it taken straight from the minimist documentation:

var argv = require('minimist')(process.argv.slice(2));
console.dir(argv);


-

$ node example/parse.js -a beep -b boop
{ _: [], a: 'beep', b: 'boop' }


-

$ node example/parse.js -x 3 -y 4 -n5 -abc --beep=boop foo bar baz
{ _: [ 'foo', 'bar', 'baz' ],
  x: 3,
  y: 4,
  n: 5,
  a: true,
  b: true,
  c: true,
  beep: 'boop' }

    2018 answer based on current trends in the wild:



Vanilla javascript argument parsing:

const args = process.argv;
console.log(args);


This returns:

$ node server.js one two=three four
['node', '/home/server.js', 'one', 'two=three', 'four']


Official docs



Most used NPM packages for argument parsing:

Minimist: For minimal argument parsing.

Commander.js: Most adopted module for argument parsing.

Meow: Lighter alternative to Commander.js

Yargs: More sophisticated argument parsing (heavy).

Vorpal.js: Mature / interactive command-line applications with argument parsing.
    No Libs with Flags Formatted into a Simple Object
function getArgs () {
    const args = {};
    process.argv
        .slice(2, process.argv.length)
        .forEach( arg => {
        // long arg
        if (arg.slice(0,2) === '--') {
            const longArg = arg.split('=');
            const longArgFlag = longArg[0].slice(2,longArg[0].length);
            const longArgValue = longArg.length > 1 ? longArg[1] : true;
            args[longArgFlag] = longArgValue;
        }
        // flags
        else if (arg[0] === '-') {
            const flags = arg.slice(1,arg.length).split('');
            flags.forEach(flag => {
            args[flag] = true;
            });
        }
    });
    return args;
}
const args = getArgs();
console.log(args);

Examples
Simple
input
node test.js -D --name=Hello

output
{ D: true, name: 'Hello' }

Real World
input
node config/build.js -lHRs --ip=$HOST --port=$PORT --env=dev

output
{ 
  l: true,
  H: true,
  R: true,
  s: true,
  ip: '127.0.0.1',
  port: '8080',
  env: 'dev'
}

    Simple + ES6 + no-dependency + supports boolean flags
const process = require( 'process' );

const argv = key => {
  // Return true if the key exists and a value is defined
  if ( process.argv.includes( `--${ key }` ) ) return true;

  const value = process.argv.find( element => element.startsWith( `--${ key }=` ) );

  // Return null if the key does not exist and a value is not defined
  if ( !value ) return null;
  
  return value.replace( `--${ key }=` , '' );
}

Output:

If invoked with node app.js then argv('foo') will return null
If invoked with node app.js --foo then argv('foo') will return true
If invoked with node app.js --foo= then argv('foo') will return ''
If invoked with node app.js --foo=bar then argv('foo') will return 'bar'

    Several great answers here, but it all seems very complex. This is very similar to how bash scripts access argument values and it's already provided standard with node.js as MooGoo pointed out.
(Just to make it understandable to somebody that's new to node.js)

Example:

$ node yourscript.js banana monkey

var program_name = process.argv[0]; //value will be ""node""
var script_path = process.argv[1]; //value will be ""yourscript.js""
var first_value = process.argv[2]; //value will be ""banana""
var second_value = process.argv[3]; //value will be ""monkey""

    Stdio Library

The easiest way to parse command-line arguments in NodeJS is using the stdio module. Inspired by UNIX getopt utility, it is as trivial as follows:

var stdio = require('stdio');
var ops = stdio.getopt({
    'check': {key: 'c', args: 2, description: 'What this option means'},
    'map': {key: 'm', description: 'Another description'},
    'kaka': {args: 1, required: true},
    'ooo': {key: 'o'}
});


If you run the previous code with this command:

node <your_script.js> -c 23 45 --map -k 23 file1 file2


Then ops object will be as follows:

{ check: [ '23', '45' ],
  args: [ 'file1', 'file2' ],
  map: true,
  kaka: '23' }


So you can use it as you want. For instance:

if (ops.kaka && ops.check) {
    console.log(ops.kaka + ops.check[0]);
}


Grouped options are also supported, so you can write -om instead of -o -m.

Furthermore, stdio can generate a help/usage output automatically. If you call ops.printHelp() you'll get the following:

USAGE: node something.js [--check <ARG1> <ARG2>] [--kaka] [--ooo] [--map]
  -c, --check <ARG1> <ARG2>   What this option means (mandatory)
  -k, --kaka                  (mandatory)
  --map                       Another description
  -o, --ooo


The previous message is shown also if a mandatory option is not given (preceded by the error message) or if it is mispecified (for instance, if you specify a single arg for an option and it needs 2).

You can install stdio module using NPM:

npm install stdio

    proj.js
for(var i=0;i<process.argv.length;i++){
  console.log(process.argv[i]);
}

Terminal:
nodemon app.js ""arg1"" ""arg2"" ""arg3""

Result:
0 'C:\\Program Files\\nodejs\\node.exe'
1 'C:\\Users\\Nouman\\Desktop\\Node\\camer nodejs\\proj.js'
2 'arg1' your first argument you passed.
3 'arg2' your second argument you passed.
4 'arg3' your third argument you passed.

Explaination:

The directory of node.exe in your machine (C:\Program Files\nodejs\node.exe)
The directory of your project file
(proj.js)
Your first argument to node  (arg1)
Your second argument to node (arg2)
Your third argument to node  (arg3)


your actual arguments start form second index of argv array, that is process.argv[2].
    Parsing argument based on standard input ( --key=value )
const argv = (() => {
    const arguments = {};
    process.argv.slice(2).map( (element) => {
        const matches = element.match( '--([a-zA-Z0-9]+)=(.*)');
        if ( matches ){
            arguments[matches[1]] = matches[2]
                .replace(/^['""]/, '').replace(/['""]$/, '');
        }
    });
    return arguments;
})();

Command example
node app.js --name=stackoverflow --id=10 another-argument --text=""Hello World""

Result of argv: console.log(argv)
{
    name: ""stackoverflow"",
    id: ""10"",
    text: ""Hello World""
}

    Here's my 0-dep solution for named arguments:

const args = process.argv
    .slice(2)
    .map(arg => arg.split('='))
    .reduce((args, [value, key]) => {
        args[value] = key;
        return args;
    }, {});

console.log(args.foo)
console.log(args.fizz)


Example:

$ node test.js foo=bar fizz=buzz
bar
buzz


Note: Naturally this will fail when the argument contains a =. This is only for very simple usage.
    If your script is called myScript.js and you want to pass the first and last name, 'Sean Worthington', as arguments like below:

node myScript.js Sean Worthington


Then within your script you write:

var firstName = process.argv[2]; // Will be set to 'Sean'
var lastName = process.argv[3]; // Will be set to 'Worthington'

    Commander.js

Works great for defining your options, actions, and arguments. It also generates the help pages for you.

Promptly

Works great for getting input from the user, if you like the callback approach.

Co-Prompt

Works great for getting input from the user, if you like the generator approach.
    Optimist (node-optimist)

Check out optimist library, it is much better than parsing command line options by hand.

Update

Optimist is deprecated. Try yargs which is an active fork of optimist.
    whithout librairies: using Array.prototype.reduce()

const args = process.argv.slice(2).reduce((acc, arg) => {

    let [k, v = true] = arg.split('=')
    acc[k] = v
    return acc

}, {})


for this command node index.js count=2 print debug=false msg=hi

console.log(args) // { count: '2', print: true, debug: 'false', msg: 'hi' }


also,

we can change 

    let [k, v = true] = arg.split('=')
    acc[k] = v


by (much longer)

    let [k, v] = arg.split('=')
    acc[k] = v === undefined ? true : /true|false/.test(v) ? v === 'true' : /[\d|\.]+/.test(v) ? Number(v) : v


to auto parse Boolean & Number

console.log(args) // { count: 2, print: true, debug: false, msg: 'hi' }

    command-line-args is worth a look!

You can set options using the main notation standards (learn more). These commands are all equivalent, setting the same values:

$ example --verbose --timeout=1000 --src one.js --src two.js
$ example --verbose --timeout 1000 --src one.js two.js
$ example -vt 1000 --src one.js two.js
$ example -vt 1000 one.js two.js


To access the values, first create a list of option definitions describing the options your application accepts. The type property is a setter function (the value supplied is passed through this), giving you full control over the value received.

const optionDefinitions = [
  { name: 'verbose', alias: 'v', type: Boolean },
  { name: 'src', type: String, multiple: true, defaultOption: true },
  { name: 'timeout', alias: 't', type: Number }
]


Next, parse the options using commandLineArgs():

const commandLineArgs = require('command-line-args')
const options = commandLineArgs(optionDefinitions)


options now looks like this:

{
  src: [
    'one.js',
    'two.js'
  ],
  verbose: true,
  timeout: 1000
}


Advanced usage

Beside the above typical usage, you can configure command-line-args to accept more advanced syntax forms.

Command-based syntax (git style) in the form:

$ executable <command> [options]


For example.

$ git commit --squash -m ""This is my commit message""


Command and sub-command syntax (docker style) in the form:

$ executable <command> [options] <sub-command> [options]


For example.

$ docker run --detached --image centos bash -c yum install -y httpd


Usage guide generation

A usage guide (typically printed when --help is set) can be generated using command-line-usage. See the examples below and read the documentation for instructions how to create them.

A typical usage guide example.



The polymer-cli usage guide is a good real-life example.



Further Reading

There is plenty more to learn, please see the wiki for examples and documentation.
    TypeScript solution with no libraries:

interface IParams {
  [key: string]: string
}

function parseCliParams(): IParams {
  const args: IParams = {};
  const rawArgs = process.argv.slice(2, process.argv.length);
  rawArgs.forEach((arg: string, index) => {
    // Long arguments with '--' flags:
    if (arg.slice(0, 2).includes('--')) {
      const longArgKey = arg.slice(2, arg.length);
      const longArgValue = rawArgs[index + 1]; // Next value, e.g.: --connection connection_name
      args[longArgKey] = longArgValue;
    }
    // Shot arguments with '-' flags:
    else if (arg.slice(0, 1).includes('-')) {
      const longArgKey = arg.slice(1, arg.length);
      const longArgValue = rawArgs[index + 1]; // Next value, e.g.: -c connection_name
      args[longArgKey] = longArgValue;
    }
  });
  return args;
}

const params = parseCliParams();
console.log('params: ', params);


Input: ts-node index.js -p param --parameter parameter

Output: { p: 'param ', parameter: 'parameter' }
    You can get command-line information from process.argv()
And I don't want to limit the problem to node.js. Instead, I want to turn it into how to parse the string as the argument.
console.log(ArgumentParser(`--debug --msg=""Hello World"" --title=""Test"" --desc=demo -open --level=5 --MyFloat=3.14`))

output
{
  ""debug"": true,
  ""msg"": ""Hello World"",
  ""title"": ""Test"",
  ""desc"": ""demo"",
  ""open"": true,
  ""level"": 5,
  ""MyFloat"": 3.14
}

code
Pure javascript, no dependencies needed
//  Below is Test
(() => {
  window.onload = () => {
    const testArray = [
      `--debug --msg=""Hello World"" --title=""Test"" --desc=demo -open --level=5 --MyFloat=3.14`,
    ]
    for (const testData of testArray) {
      try {
        const obj = ArgumentParser(testData)
        console.log(obj)
      } catch (e) {
        console.error(e.message)
      }
    }
  }
})()

//  Script
class ParserError extends Error {

}

function Cursor(str, pos) {
  this.str = str
  this.pos = pos
  this.MoveRight = (step = 1) => {
    this.pos += step
  }

  this.MoveToNextPara = () => {
    const curStr = this.str.substring(this.pos)
    const match = /^(?<all> *--?(?<name>[a-zA-Z_][a-zA-Z0-9_]*)(=(?<value>[^-]*))?)/g.exec(curStr) // https://regex101.com/r/k004Gv/2
    if (match) {
      let {groups: {all, name, value}} = match

      if (value !== undefined) {
        value = value.trim()
        if (value.slice(0, 1) === '""') { // string
          if (value.slice(-1) !== '""') {
            throw new ParserError(`Parsing error: '""' expected`)
          }
          value = value.slice(1, -1)
        } else { // number or string (without '""')
          value = isNaN(Number(value)) ? String(value) : Number(value)
        }
      }

      this.MoveRight(all.length)
      return [name, value ?? true] // If the value is undefined, then set it as ture.
    }
    throw new ParserError(`illegal format detected. ${curStr}`)
  }
}

function ArgumentParser(str) {
  const obj = {}
  const cursor = new Cursor(str, 0)
  while (1) {
    const [name, value] = cursor.MoveToNextPara()
    obj[name] = value
    if (cursor.pos === str.length) {
      return obj
    }
  }
}

    There's an app for that. Well, module. Well, more than one, probably hundreds.

Yargs is one of the fun ones, its docs are cool to read.

Here's an example from the github/npm page:

#!/usr/bin/env node
var argv = require('yargs').argv;
console.log('(%d,%d)', argv.x, argv.y);
console.log(argv._);


Output is here (it reads options with dashes etc, short and long, numeric etc).

$ ./nonopt.js -x 6.82 -y 3.35 rum
(6.82,3.35)
[ 'rum' ] 
$ ./nonopt.js ""me hearties"" -x 0.54 yo -y 1.12 ho
(0.54,1.12)
[ 'me hearties', 'yo', 'ho' ]

    Although Above answers are perfect, and someone has already suggested yargs, using the package is really easy.
This is a nice package which makes passing arguments to command line really easy.

npm i yargs
const yargs = require(""yargs"");
const argv = yargs.argv;
console.log(argv);


Please visit https://yargs.js.org/ for more info.
    Passing arguments is easy, and receiving them is just a matter of reading the process.argv array Node makes accessible from everywhere, basically. But you're sure to want to read them as key/value pairs, so you'll need a piece to script to interpret it.

Joseph Merdrignac posted a beautiful one using reduce, but it relied on a key=value syntax instead of -k value and --key value. I rewrote it much uglier and longer to use that second standard, and I'll post it as an answer because it wouldn't fit as a commentary. But it does get the job done.

   const args = process.argv.slice(2).reduce((acc,arg,cur,arr)=>{
     if(arg.match(/^--/)){
       acc[arg.substring(2)] = true
       acc['_lastkey'] = arg.substring(2)
     } else
     if(arg.match(/^-[^-]/)){
       for(key of arg.substring(1).split('')){
         acc[key] = true
         acc['_lastkey'] = key
       }
     } else
       if(acc['_lastkey']){
         acc[acc['_lastkey']] = arg
         delete acc['_lastkey']
       } else
         acc[arg] = true
     if(cur==arr.length-1)
       delete acc['_lastkey']
     return acc
   },{})


With this code a command node script.js alpha beta -charlie delta --echo foxtrot would give you the following object


args = {
 ""alpha"":true,
 ""beta"":true,
 ""c"":true,
 ""h"":true,
 ""a"":true,
 ""r"":true
 ""l"":true,
 ""i"":true,
 ""e"":""delta"",
 ""echo"":""foxtrot""
}

    ES6-style no-dependencies solution:
const longArgs = arg => {
    const [ key, value ] = arg.split('=');
    return { [key.slice(2)]: value || true }
};

const flags = arg => [...arg.slice(1)].reduce((flagObj, f) => ({ ...flagObj, [f]: true }), {});


const args = () =>
    process.argv
        .slice(2)
        .reduce((args, arg) => ({
            ...args,
            ...((arg.startsWith('--') && longArgs(arg)) || (arg[0] === '-' && flags(arg)))
        }), {});

console.log(args());

    Passing,parsing arguments is an easy process. Node provides you with the process.argv property, which is an array of strings, which are the arguments that were used when Node was invoked. 
The first entry of the array is the Node executable, and the second entry is the name of your script. 

If you run script with below atguments

$ node args.js arg1 arg2


File : args.js

console.log(process.argv)


You will get array like

 ['node','args.js','arg1','arg2']

    The simplest way of retrieving arguments in Node.js is via the process.argv array. This is a global object that you can use without importing any additional libraries to use it. You simply need to pass arguments to a Node.js application, just like we showed earlier, and these arguments can be accessed within the application via the process.argv array.

The first element of the process.argv array will always be a file system path pointing to the node executable. The second element is the name of the JavaScript file that is being executed. And the third element is the first argument that was actually passed by the user.

'use strict';

for (let j = 0; j < process.argv.length; j++) {  
    console.log(j + ' -> ' + (process.argv[j]));
}


All this script does is loop through the process.argv array and prints the indexes, along with the elements stored in those indexes. It's very useful for debugging if you ever question what arguments you're receiving, and in what order.

You can also use libraries like yargs for working with commnadline arguments.
    process.argv is your friend, capturing command line args is natively supported in Node JS. See example below::

process.argv.forEach((val, index) => {
  console.log(`${index}: ${val}`);
})

    npm install ps-grab


If you want to run something like this :

node greeting.js --user Abdennour --website http://abdennoor.com 


--

var grab=require('ps-grab');
grab('--username') // return 'Abdennour'
grab('--action') // return 'http://abdennoor.com'




Or something like : 

node vbox.js -OS redhat -VM template-12332 ;


--

var grab=require('ps-grab');
grab('-OS') // return 'redhat'
grab('-VM') // return 'template-12332'

    I extended the getArgs function just to get also commands, as well as flags (-f, --anotherflag) and named args (--data=blablabla):

The module

/**
 * @module getArgs.js
 * get command line arguments (commands, named arguments, flags)
 *
 * @see https://stackoverflow.com/a/54098693/1786393
 *
 * @return {Object}
 *
 */
function getArgs () {
  const commands = []
  const args = {}
  process.argv
    .slice(2, process.argv.length)
    .forEach( arg => {
      // long arg
      if (arg.slice(0,2) === '--') {
        const longArg = arg.split('=')
        const longArgFlag = longArg[0].slice(2,longArg[0].length)
        const longArgValue = longArg.length > 1 ? longArg[1] : true
        args[longArgFlag] = longArgValue
     }
     // flags
      else if (arg[0] === '-') {
        const flags = arg.slice(1,arg.length).split('')
        flags.forEach(flag => {
          args[flag] = true
        })
      }
     else {
      // commands
      commands.push(arg)
     } 
    })
  return { args, commands }
}


// test
if (require.main === module) {
  // node getArgs test --dir=examples/getUserName --start=getUserName.askName
  console.log( getArgs() )
}

module.exports = { getArgs }



Usage example:

$ node lib/getArgs test --dir=examples/getUserName --start=getUserName.askName
{
  args: { dir: 'examples/getUserName', start: 'getUserName.askName' },
  commands: [ 'test' ]
}

$ node lib/getArgs --dir=examples/getUserName --start=getUserName.askName test tutorial
{
  args: { dir: 'examples/getUserName', start: 'getUserName.askName' },
  commands: [ 'test', 'tutorial' ]
}


    You can reach command line arguments using system.args. And i use the solution below to parse arguments into an object, so i can get which one i want by name.

var system = require('system');

var args = {};
system.args.map(function(x){return x.split(""="")})
    .map(function(y){args[y[0]]=y[1]});


now you don't need to know the index of the argument. use it like args.whatever


  Note: you should use named arguments like file.js x=1 y=2 to use
  this solution.

    Most of the people have given good answers. I would also like to contribute something here. I am providing the answer using lodash library to iterate through all command line arguments we pass while starting the app:

// Lodash library
const _ = require('lodash');

// Function that goes through each CommandLine Arguments and prints it to the console.
const runApp = () => {
    _.map(process.argv, (arg) => {
        console.log(arg);
    });
};

// Calling the function.
runApp();


To run above code just run following commands:

npm install
node index.js xyz abc 123 456


The result will be:

xyz 
abc 
123
456

    as stated in the node docs 
The process.argv property returns an array containing the command line arguments passed when the Node.js process was launched.

For example, assuming the following script for process-args.js:

// print process.argv
process.argv.forEach((val, index) => {
   console.log(`${index}: ${val}`);
});


Launching the Node.js process as:

 $ node process-args.js one two=three four


Would generate the output:

0: /usr/local/bin/node
1: /Users/mjr/work/node/process-args.js
2: one
3: two=three
4: four

    ","[2778, 3400, 828, 447, 375, 86, 26, 118, 76, 23, 19, 31, 55, 88, 130, 17, 32, 6, 3, 26, 6, 7, 3, 13, 5, 3, 10, 2, 8, 1, 1]",1489061,452,2010-12-04T01:56:46,2022-04-17 20:35:22Z,javascript 
How do I get the current date in JavaScript?,"
                    
            
        
            
                    
                        
                    
                
                    
                        Want to improve this post? Provide detailed answers to this question, including citations and an explanation of why your answer is correct. Answers without enough detail may be edited or deleted.
                        
                    
                
            
        

    

How do I get the current date in JavaScript?
    Use new Date() to generate a new Date object containing the current date and time.

var today = new Date();
var dd = String(today.getDate()).padStart(2, '0');
var mm = String(today.getMonth() + 1).padStart(2, '0'); //January is 0!
var yyyy = today.getFullYear();

today = mm + '/' + dd + '/' + yyyy;
document.write(today);


This will give you today's date in the format of mm/dd/yyyy.

Simply change today = mm +'/'+ dd +'/'+ yyyy; to whatever format you wish.
    The shortest possible. 

To get format like ""2018-08-03"":

let today = new Date().toISOString().slice(0, 10)

console.log(today)


To get format like ""8/3/2018"":

let today = new Date().toLocaleDateString()

console.log(today)


Also, you can pass locale as argument, for example toLocaleDateString(""sr""), etc.
    var utc = new Date().toJSON().slice(0,10).replace(/-/g,'/');
document.write(utc);


Use the replace option if you're going to reuse the utc variable, such as new Date(utc), as Firefox and Safari don't recognize a date with dashes.
    
  UPDATED!, Scroll Down


If you want something simple pretty to the end-user ... Also, fixed a small suffix issue in the first version below. Now properly returns suffix.

var objToday = new Date(),
	weekday = new Array('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'),
	dayOfWeek = weekday[objToday.getDay()],
	domEnder = function() { var a = objToday; if (/1/.test(parseInt((a + """").charAt(0)))) return ""th""; a = parseInt((a + """").charAt(1)); return 1 == a ? ""st"" : 2 == a ? ""nd"" : 3 == a ? ""rd"" : ""th"" }(),
	dayOfMonth = today + ( objToday.getDate() < 10) ? '0' + objToday.getDate() + domEnder : objToday.getDate() + domEnder,
	months = new Array('January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'),
	curMonth = months[objToday.getMonth()],
	curYear = objToday.getFullYear(),
	curHour = objToday.getHours() > 12 ? objToday.getHours() - 12 : (objToday.getHours() < 10 ? ""0"" + objToday.getHours() : objToday.getHours()),
	curMinute = objToday.getMinutes() < 10 ? ""0"" + objToday.getMinutes() : objToday.getMinutes(),
	curSeconds = objToday.getSeconds() < 10 ? ""0"" + objToday.getSeconds() : objToday.getSeconds(),
	curMeridiem = objToday.getHours() > 12 ? ""PM"" : ""AM"";
var today = curHour + "":"" + curMinute + ""."" + curSeconds + curMeridiem + "" "" + dayOfWeek + "" "" + dayOfMonth + "" of "" + curMonth + "", "" + curYear;

document.getElementsByTagName('h1')[0].textContent = today;<h1></h1>



  UBBER UPDATE After much procrastination, I've finally GitHubbed and updated this with the final solution I've been using for myself. It's even had some last-minute edits to make it sweeter! If you're looking for the old jsFiddle, please see this.


This update comes in 2 flavors, still relatively small, though not as small as my above, original answer. If you want extremely small, go with that.  Also Note: This is still less bloated than moment.js. While moment.js is nice, imo, it has too many secular methods, which require learning moment as if it were a language. Mine here uses the same common format as PHP: date.

Quick Links


Date.format.min.js 5.08 KB
dateFormat.min.js 4.16 KB



  Flavor 1 new Date().format(String)
  My Personal Fav. I know the taboo but works great on the Date Object. Just be aware of any other mods you may have to the Date Object.


//  use as simple as
new Date().format('m-d-Y h:i:s');   //  07-06-2016 06:38:34



  Flavor 2 dateFormat(Date, String)
  More traditional all-in-one method. Has all the ability of the previous, but is called via the method with Date param.


//  use as simple as
dateFormat(new Date(), 'm-d-Y h:i:s');  //  07-06-2016 06:38:34



  BONUS Flavor (requires jQuery) $.date(Date, String)
  This contains much more than just a simple format option. It extends the base Date object and includes methods such as addDays. For more information, please see the Git.


In this mod, the format characters are inspired by PHP: date. For a complete list, please see my README

This mod also has a much longer list of pre-made formats. To use a premade format, simply enter its key name. dateFormat(new Date(), 'pretty-a');


'compound'


'commonLogFormat' == 'd/M/Y:G:i:s'
'exif' == 'Y:m:d G:i:s'
'isoYearWeek' == 'Y\\WW'
'isoYearWeek2' == 'Y-\\WW'
'isoYearWeekDay' == 'Y\\WWj'
'isoYearWeekDay2' == 'Y-\\WW-j'
'mySQL' == 'Y-m-d h:i:s'
'postgreSQL' == 'Y.z'
'postgreSQL2' == 'Yz'
'soap' == 'Y-m-d\\TH:i:s.u'
'soap2' == 'Y-m-d\\TH:i:s.uP'
'unixTimestamp' == '@U'
'xmlrpc' == 'Ymd\\TG:i:s'
'xmlrpcCompact' == 'Ymd\\tGis'
'wddx' == 'Y-n-j\\TG:i:s'

'constants'


'AMERICAN' == 'F j Y'
'AMERICANSHORT' == 'm/d/Y'
'AMERICANSHORTWTIME' == 'm/d/Y h:i:sA'
'ATOM' == 'Y-m-d\\TH:i:sP'
'COOKIE' == 'l d-M-Y H:i:s T'
'EUROPEAN' == 'j F Y'
'EUROPEANSHORT' == 'd.m.Y'
'EUROPEANSHORTWTIME' == 'd.m.Y H:i:s'
'ISO8601' == 'Y-m-d\\TH:i:sO'
'LEGAL' == 'j F Y'
'RFC822' == 'D d M y H:i:s O'
'RFC850' == 'l d-M-y H:i:s T'
'RFC1036' == 'D d M y H:i:s O'
'RFC1123' == 'D d M Y H:i:s O'
'RFC2822' == 'D d M Y H:i:s O'
'RFC3339' == 'Y-m-d\\TH:i:sP'
'RSS' == 'D d M Y H:i:s O'
'W3C' == 'Y-m-d\\TH:i:sP'

'pretty'


'pretty-a' == 'g:i.sA l jS \\o\\f F Y'
'pretty-b' == 'g:iA l jS \\o\\f F Y'
'pretty-c' == 'n/d/Y g:iA'
'pretty-d' == 'n/d/Y'
'pretty-e' == 'F jS - g:ia'
'pretty-f' == 'g:iA'



As you may notice, you can use double \ to escape a character.


    If you just want a date without time info, use:

var today = new Date();
    today.setHours(0, 0, 0, 0);

document.write(today);

    Most of the other answers are providing the date with time.
If you only need date.
new Date().toISOString().split(""T"")[0]

Output
[ '2021-02-08', '06:07:44.629Z' ]

If you want it in / format use replaceAll.
new Date().toISOString().split(""T"")[0].replaceAll(""-"", ""/"")

If you want other formats then best to use momentjs.
    Cleaner, simpler version:
new Date().toLocaleString();

Result varies according to the user's locale:

2/27/2017, 9:15:41 AM

    Using JS built in Date.prototype.toLocaleDateString() (more options here MDN docs)  :
const options = { 
  month: '2-digit', 
  day: '2-digit',
  year: 'numeric', 
};

console.log(new Date().toLocaleDateString('en-US', options)); // mm/dd/yyyy

Update: We can get similar behavior using Intl.DateTimeFormat which has decent browser support. Similar to toLocaleDateString(), we can pass an object with options:
const date = new Date('Dec 2, 2021') // Thu Dec 16 2021 15:49:39 GMT-0600
const options = {
  day: '2-digit',
  month: '2-digit',
  year: 'numeric',
}
new Intl.DateTimeFormat('en-US', options).format(date) // '12/02/2021'

    var date = new Date().toLocaleDateString(""en-US"");

Also, you can call method toLocaleDateString with two parameters:
var date = new Date().toLocaleDateString(""en-US"", {
    ""year"": ""numeric"",
    ""month"": ""numeric""
});

More about this method on MDN.
    Try this:

var currentDate = new Date()
var day = currentDate.getDate()
var month = currentDate.getMonth() + 1
var year = currentDate.getFullYear()
document.write(""<b>"" + day + ""/"" + month + ""/"" + year + ""</b>"")


The result will be like

15/2/2012

    Try

`${Date()}`.slice(4,15)


console.log( `${Date()}`.slice(4,15) )


We use here standard JS functionalities: template literals, Date object which is cast to string, and slice. This is probably shortest solution which meet OP requirements (no time, only date)
    If you are happy with YYYY-MM-DD format, this will do the job as well.

new Date().toISOString().split('T')[0]

2018-03-10
    You can use moment.js: http://momentjs.com/

var m = moment().format(""DD/MM/YYYY"");

document.write(m);<script src=""https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.14.1/moment.min.js""></script>

    var d = (new Date()).toString().split(' ').splice(1,3).join(' ');

document.write(d)


To break it down into steps:


(new Date()).toString() gives ""Fri Jun 28 2013 15:30:18 GMT-0700 (PDT)""
(new Date()).toString().split(' ') divides the above string on each space and returns an array as follows: [""Fri"", ""Jun"", ""28"", ""2013"", ""15:31:14"", ""GMT-0700"", ""(PDT)""]
(new Date()).toString().split(' ').splice(1,3).join(' ') takes the second, third and fourth values from the above array, joins them with spaces, and returns a string ""Jun 28 2013""

    // Try this simple way

const today = new Date();
let date = today.getFullYear()+'-'+(today.getMonth()+1)+'-'+today.getDate();
console.log(date);

    This works every time:

    var now = new Date();
    var day = (""0"" + now.getDate()).slice(-2);
    var month = (""0"" + (now.getMonth() + 1)).slice(-2);
    var today = now.getFullYear() + ""-"" + (month) + ""-"" + (day);
    
    console.log(today);

    new Date().toISOString().slice(0,10); 


would work too
    You can checkout this
var today = new Date();
today = parseInt(today.getMonth()+1)+'/'+today.getDate()+'/'+today.getFullYear()+""\nTime : ""+today.getHours()+"":""+today.getMinutes()+"":""+today.getSeconds();
document.write(today);

And see the documentation for Date() constructor.
link
Get Current Date Month Year in React js
    As toISOString() will only return current UTC time , not local time. We have to make a date by using '.toString()' function to get date in yyyy-MM-dd format like

document.write(new Date(new Date().toString().split('GMT')[0]+' UTC').toISOString().split('T')[0]);


To get date and time into in yyyy-MM-ddTHH:mm:ss format

document.write(new Date(new Date().toString().split('GMT')[0]+' UTC').toISOString().split('.')[0]);


To get date and time into in yyyy-MM-dd HH:mm:ss format

document.write(new Date(new Date().toString().split('GMT')[0]+' UTC').toISOString().split('.')[0].replace('T',' '));

    new Date().toDateString();


Result:


  ""Wed Feb 03 2016""

    If you're looking for a lot more granular control over the date formats, I thoroughly recommend  checking out momentjs. Terrific library - and only 5KB.
http://momentjs.com/
    A straighforward way to pull that off (whilst considering your current time zone it taking advantage of the ISO yyyy-mm-dd format) is:

let d = new Date().toISOString().substring(0,19).replace(""T"","" "") // ""2020-02-18 16:41:58""


Usually, this is a pretty all-purpose compatible date format and you can convert it to pure date value if needed:

Date.parse(d); // 1582044297000

    If by ""current date"" you are thinking about ""today"", then this trick may work for you:

> new Date(3600000*Math.floor(Date.now()/3600000))
2020-05-07T07:00:00.000Z


This way you are getting today Date instance with time 0:00:00.

The principle of operation is very simple: we take the current timestamp and divide it for 1 day expressed in milliseconds. We will get a fraction. By using Math.floor, we get rid of the fraction, so we get an integer. Now if we multiply it back by one day (again - in milliseconds), we get a date timestamp with the time exactly at the beginning of the day.

> now = Date.now()
1588837459929
> daysInMs = now/3600000
441343.73886916664
> justDays = Math.floor(daysInMs)
441343
> today = justDays*3600000
1588834800000
> new Date(today)
2020-05-07T07:00:00.000Z


Clean and simple.
    So many complicated answers...
Just use new Date() and if you need it as a string, simply use new Date().toISOString()
Enjoy!
    LATEST EDIT: 8/23/19
The date-fns library works much like moment.js but has a WAY smaller footprint. It lets you cherry pick which functions you want to include in your project so you don't have to compile the whole library to format today's date. If a minimal 3rd party lib isn't an option for your project, I endorse the accepted solution by Samuel Meddows up top.

Preserving history below because it helped a few people. But for the record it's pretty hacky and liable to break without warning, as are most of the solutions on this post

EDIT 2/7/2017
A one-line JS solution: 


  tl;dr
  var todaysDate =new Date(Date.now()).toLocaleString().slice(0,3).match(/[0-9]/i) ? new Date(Date.now()).toLocaleString().split(' ')[0].split(',')[0] : new Date(Date.now()).toLocaleString().split(' ')[1] + "" "" + new Date(Date.now()).toLocaleString().split(' ')[2] + "" "" + new Date(Date.now()).toLocaleString().split(' ')[3];
  edge, ff latest, & chrome return todaysDate = ""2/7/2017"" ""works""* in IE10+


Explanation

I found out that IE10 and IE Edge do things a bit differently.. go figure.
with new Date(Date.now()).toLocaleString() as input,

IE10 returns: 

""Tuesday, February 07, 2017 2:58:25 PM""


I could write a big long function and FTFY. But you really ought to use moment.js for this stuff. My script merely cleans this up and gives you the expanded traditional US notation: > todaysDate = ""March 06, 2017""

IE EDGE returns:

""2/7/2017 2:59:27 PM""


Of course it couldn't be that easy. Edge's date string has invisible """" characters between each visible one. So not only will we now be checking if the first character is a number, but the first 3 characters, since it turns out that any single character in the whole date rangewill eventually be a dot or a slash at some point. So to keep things simple, just .slice() the first three chars (tiny buffer against future shenanigans) and then check for numbers. It should probably be noted that these invisible dots could potentially persist in your code. I'd maybe dig into that if you've got bigger plans than just printing this string to your view.

 updated one-liner:

var todaysDate =new Date(Date.now()).toLocaleString().slice(0,3).match(/[0-9]/i) ? new Date(Date.now()).toLocaleString().split(' ')[0].split(',')[0] : new Date(Date.now()).toLocaleString().split(' ')[1] + "" "" + new Date(Date.now()).toLocaleString().split(' ')[2] + "" "" + new Date(Date.now()).toLocaleString().split(' ')[3];


That sucks to read. How about:

var dateString =new Date(Date.now()).toLocaleString();
var todaysDate =dateString.slice(0,3).match(/[0-9]/i) ? dateString.split(' ')[0].split(',')[0] : dateString.split(' ')[1] + "" "" + dateString.split(' ')[2] + "" "" + dateString.split(' ')[3];


ORIGINAL ANSWER

I've got a one-liner for you:

new Date(Date.now()).toLocaleString().split(', ')[0];


and [1] will give you the time of day.
    The Shortest Answer is: new Date().toJSON().slice(0,10)
    You can use Date.js library which extens Date object, thus you can have .today() method.
    Varun's answer does not account for TimezoneOffset. Here is a version that does:

var d = new Date()
new Date(d.getTime() - d.getTimezoneOffset() * 60000).toJSON().slice(0, 10) // 2015-08-11


The TimezoneOffset is minutes, while the Date constructor takes milliseconds, thus the multiplication by 60000.
    This does a lot;
    var today = new Date();
    var date = today.getFullYear()+'/'+(today.getMonth()+1)+'/'+today.getDate();
    document.write(date);

Where today.getFullYear() gets current year,
today.getMonth()+1 gets current month
and today.getDate() gets today's date.
All of this is concatinated with '/'.
    You can get the current date call the static method now like this:

var now = Date.now()


reference:

https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Date/now
    ","[2778, 3224, 369, 507, 265, 199, 40, 43, 16, 57, 114, 13, 34, 59, 56, 6, 50, 14, 9, 15, 15, 68, 8, 8, 5, 10, 16, 25, 18, 6, 20]",3622349,437,2009-10-07T11:39:02,2022-03-14 15:12:53Z,javascript 
How do I detect a click outside an element?,"
                
I have some HTML menus, which I show completely when a user clicks on the head of these menus. I would like to hide these elements when the user clicks outside the menus' area.

Is something like this possible with jQuery?

$(""#menuscontainer"").clickOutsideThisElement(function() {
    // Hide the menus
});

    You can listen for a click event on document and then make sure #menucontainer is not an ancestor or the target of the clicked element by using  .closest().
If it is not, then the clicked element is outside of the #menucontainer and you can safely hide it.
$(document).click(function(event) { 
  var $target = $(event.target);
  if(!$target.closest('#menucontainer').length && 
  $('#menucontainer').is("":visible"")) {
    $('#menucontainer').hide();
  }        
});

Edit  2017-06-23
You can also clean up after the event listener if you plan to dismiss the menu and want to stop listening for events. This function will clean up only the newly created listener, preserving any other click listeners on document. With ES2015 syntax:
export function hideOnClickOutside(selector) {
  const outsideClickListener = (event) => {
    const $target = $(event.target);
    if (!$target.closest(selector).length && $(selector).is(':visible')) {
        $(selector).hide();
        removeClickListener();
    }
  }

  const removeClickListener = () => {
    document.removeEventListener('click', outsideClickListener);
  }

  document.addEventListener('click', outsideClickListener);
}

Edit  2018-03-11
For those who don't want to use jQuery. Here's the above code in plain vanillaJS (ECMAScript6).
function hideOnClickOutside(element) {
    const outsideClickListener = event => {
        if (!element.contains(event.target) && isVisible(element)) { // or use: event.target.closest(selector) === null
          element.style.display = 'none';
          removeClickListener();
        }
    }

    const removeClickListener = () => {
        document.removeEventListener('click', outsideClickListener);
    }

    document.addEventListener('click', outsideClickListener);
}

const isVisible = elem => !!elem && !!( elem.offsetWidth || elem.offsetHeight || elem.getClientRects().length ); // source (2018-03-11): https://github.com/jquery/jquery/blob/master/src/css/hiddenVisibleSelectors.js 

NOTE:
This is based on Alex comment to just use !element.contains(event.target) instead of the jQuery part.
But element.closest() is now also available in all major browsers (the W3C version differs a bit from the jQuery one).
Polyfills can be found here: Element.closest()
Edit  2020-05-21
In the case where you want the user to be able to click-and-drag inside the element, then release the mouse outside the element, without closing the element:
      ...
      let lastMouseDownX = 0;
      let lastMouseDownY = 0;
      let lastMouseDownWasOutside = false;

      const mouseDownListener = (event: MouseEvent) => {
        lastMouseDownX = event.offsetX;
        lastMouseDownY = event.offsetY;
        lastMouseDownWasOutside = !$(event.target).closest(element).length;
      }
      document.addEventListener('mousedown', mouseDownListener);

And in outsideClickListener:
const outsideClickListener = event => {
        const deltaX = event.offsetX - lastMouseDownX;
        const deltaY = event.offsetY - lastMouseDownY;
        const distSq = (deltaX * deltaX) + (deltaY * deltaY);
        const isDrag = distSq > 3;
        const isDragException = isDrag && !lastMouseDownWasOutside;

        if (!element.contains(event.target) && isVisible(element) && !isDragException) { // or use: event.target.closest(selector) === null
          element.style.display = 'none';
          removeClickListener();
          document.removeEventListener('mousedown', mouseDownListener); // Or add this line to removeClickListener()
        }
    }

    
Note: Using stopPropagation is something that should be avoided as it breaks normal event flow in the DOM. See this CSS Tricks article for more information. Consider using this method instead.

Attach a click event to the document body which closes the window. Attach a separate click event to the container which stops propagation to the document body.
$(window).click(function() {
  //Hide the menus if visible
});

$('#menucontainer').click(function(event){
  event.stopPropagation();
});

    
How to detect a click outside an element?

The reason that this question is so popular and has so many answers is that it is deceptively complex. After almost eight years and dozens of answers, I am genuinely surprised to see how little care has been given to accessibility.

I would like to hide these elements when the user clicks outside the menus' area.

This is a noble cause and is the actual issue. The title of the questionwhich is what most answers appear to attempt to addresscontains an unfortunate red herring.
Hint: it's the word ""click""!
You don't actually want to bind click handlers.
If you're binding click handlers to close the dialog, you've already failed. The reason you've failed is that not everyone triggers click events. Users not using a mouse will be able to escape your dialog (and your pop-up menu is arguably a type of dialog) by pressing Tab, and they then won't be able to read the content behind the dialog without subsequently triggering a click event.
So let's rephrase the question.

How does one close a dialog when a user is finished with it?

This is the goal. Unfortunately, now we need to bind the userisfinishedwiththedialog event, and that binding isn't so straightforward.
So how can we detect that a user has finished using a dialog?
focusout event
A good start is to determine if focus has left the dialog.
Hint: be careful with the blur event, blur doesn't propagate if the event was bound to the bubbling phase!
jQuery's focusout will do just fine. If you can't use jQuery, then you can use blur during the capturing phase:
element.addEventListener('blur', ..., true);
//                       use capture: ^^^^

Also, for many dialogs you'll need to allow the container to gain focus. Add tabindex=""-1"" to allow the dialog to receive focus dynamically without otherwise interrupting the tabbing flow.
$('a').on('click', function () {
  $(this.hash).toggleClass('active').focus();
});

$('div').on('focusout', function () {
  $(this).removeClass('active');
});div {
  display: none;
}
.active {
  display: block;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<a href=""#example"">Example</a>
<div id=""example"" tabindex=""-1"">
  Lorem ipsum <a href=""http://example.com"">dolor</a> sit amet.
</div>


If you play with that demo for more than a minute you should quickly start seeing issues.
The first is that the link in the dialog isn't clickable. Attempting to click on it or tab to it will lead to the dialog closing before the interaction takes place. This is because focusing the inner element triggers a focusout event before triggering a focusin event again.
The fix is to queue the state change on the event loop. This can be done by using setImmediate(...), or setTimeout(..., 0) for browsers that don't support setImmediate. Once queued it can be cancelled by a subsequent focusin:
$('.submenu').on({
  focusout: function (e) {
    $(this).data('submenuTimer', setTimeout(function () {
      $(this).removeClass('submenu--active');
    }.bind(this), 0));
  },
  focusin: function (e) {
    clearTimeout($(this).data('submenuTimer'));
  }
});

$('a').on('click', function () {
  $(this.hash).toggleClass('active').focus();
});

$('div').on({
  focusout: function () {
    $(this).data('timer', setTimeout(function () {
      $(this).removeClass('active');
    }.bind(this), 0));
  },
  focusin: function () {
    clearTimeout($(this).data('timer'));
  }
});div {
  display: none;
}
.active {
  display: block;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<a href=""#example"">Example</a>
<div id=""example"" tabindex=""-1"">
  Lorem ipsum <a href=""http://example.com"">dolor</a> sit amet.
</div>

The second issue is that the dialog won't close when the link is pressed again. This is because the dialog loses focus, triggering the close behavior, after which the link click triggers the dialog to reopen.
Similar to the previous issue, the focus state needs to be managed. Given that the state change has already been queued, it's just a matter of handling focus events on the dialog triggers:
This should look familiar
$('a').on({
  focusout: function () {
    $(this.hash).data('timer', setTimeout(function () {
      $(this.hash).removeClass('active');
    }.bind(this), 0));
  },
  focusin: function () {
    clearTimeout($(this.hash).data('timer'));  
  }
});

$('a').on('click', function () {
  $(this.hash).toggleClass('active').focus();
});

$('div').on({
  focusout: function () {
    $(this).data('timer', setTimeout(function () {
      $(this).removeClass('active');
    }.bind(this), 0));
  },
  focusin: function () {
    clearTimeout($(this).data('timer'));
  }
});

$('a').on({
  focusout: function () {
    $(this.hash).data('timer', setTimeout(function () {
      $(this.hash).removeClass('active');
    }.bind(this), 0));
  },
  focusin: function () {
    clearTimeout($(this.hash).data('timer'));  
  }
});div {
  display: none;
}
.active {
  display: block;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<a href=""#example"">Example</a>
<div id=""example"" tabindex=""-1"">
  Lorem ipsum <a href=""http://example.com"">dolor</a> sit amet.
</div>


Esc key
If you thought you were done by handling the focus states, there's more you can do to simplify the user experience.
This is often a ""nice to have"" feature, but it's common that when you have a modal or popup of any sort that the Esc key will close it out.
keydown: function (e) {
  if (e.which === 27) {
    $(this).removeClass('active');
    e.preventDefault();
  }
}

$('a').on('click', function () {
  $(this.hash).toggleClass('active').focus();
});

$('div').on({
  focusout: function () {
    $(this).data('timer', setTimeout(function () {
      $(this).removeClass('active');
    }.bind(this), 0));
  },
  focusin: function () {
    clearTimeout($(this).data('timer'));
  },
  keydown: function (e) {
    if (e.which === 27) {
      $(this).removeClass('active');
      e.preventDefault();
    }
  }
});

$('a').on({
  focusout: function () {
    $(this.hash).data('timer', setTimeout(function () {
      $(this.hash).removeClass('active');
    }.bind(this), 0));
  },
  focusin: function () {
    clearTimeout($(this.hash).data('timer'));  
  }
});div {
  display: none;
}
.active {
  display: block;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<a href=""#example"">Example</a>
<div id=""example"" tabindex=""-1"">
  Lorem ipsum <a href=""http://example.com"">dolor</a> sit amet.
</div>


If you know you have focusable elements within the dialog, you won't need to focus the dialog directly. If you're building a menu, you could focus the first menu item instead.
click: function (e) {
  $(this.hash)
    .toggleClass('submenu--active')
    .find('a:first')
    .focus();
  e.preventDefault();
}

$('.menu__link').on({
  click: function (e) {
    $(this.hash)
      .toggleClass('submenu--active')
      .find('a:first')
      .focus();
    e.preventDefault();
  },
  focusout: function () {
    $(this.hash).data('submenuTimer', setTimeout(function () {
      $(this.hash).removeClass('submenu--active');
    }.bind(this), 0));
  },
  focusin: function () {
    clearTimeout($(this.hash).data('submenuTimer'));  
  }
});

$('.submenu').on({
  focusout: function () {
    $(this).data('submenuTimer', setTimeout(function () {
      $(this).removeClass('submenu--active');
    }.bind(this), 0));
  },
  focusin: function () {
    clearTimeout($(this).data('submenuTimer'));
  },
  keydown: function (e) {
    if (e.which === 27) {
      $(this).removeClass('submenu--active');
      e.preventDefault();
    }
  }
});.menu {
  list-style: none;
  margin: 0;
  padding: 0;
}
.menu:after {
  clear: both;
  content: '';
  display: table;
}
.menu__item {
  float: left;
  position: relative;
}

.menu__link {
  background-color: lightblue;
  color: black;
  display: block;
  padding: 0.5em 1em;
  text-decoration: none;
}
.menu__link:hover,
.menu__link:focus {
  background-color: black;
  color: lightblue;
}

.submenu {
  border: 1px solid black;
  display: none;
  left: 0;
  list-style: none;
  margin: 0;
  padding: 0;
  position: absolute;
  top: 100%;
}
.submenu--active {
  display: block;
}

.submenu__item {
  width: 150px;
}

.submenu__link {
  background-color: lightblue;
  color: black;
  display: block;
  padding: 0.5em 1em;
  text-decoration: none;
}

.submenu__link:hover,
.submenu__link:focus {
  background-color: black;
  color: lightblue;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<ul class=""menu"">
  <li class=""menu__item"">
    <a class=""menu__link"" href=""#menu-1"">Menu 1</a>
    <ul class=""submenu"" id=""menu-1"" tabindex=""-1"">
      <li class=""submenu__item""><a class=""submenu__link"" href=""http://example.com/#1"">Example 1</a></li>
      <li class=""submenu__item""><a class=""submenu__link"" href=""http://example.com/#2"">Example 2</a></li>
      <li class=""submenu__item""><a class=""submenu__link"" href=""http://example.com/#3"">Example 3</a></li>
      <li class=""submenu__item""><a class=""submenu__link"" href=""http://example.com/#4"">Example 4</a></li>
    </ul>
  </li>
  <li class=""menu__item"">
    <a  class=""menu__link"" href=""#menu-2"">Menu 2</a>
    <ul class=""submenu"" id=""menu-2"" tabindex=""-1"">
      <li class=""submenu__item""><a class=""submenu__link"" href=""http://example.com/#1"">Example 1</a></li>
      <li class=""submenu__item""><a class=""submenu__link"" href=""http://example.com/#2"">Example 2</a></li>
      <li class=""submenu__item""><a class=""submenu__link"" href=""http://example.com/#3"">Example 3</a></li>
      <li class=""submenu__item""><a class=""submenu__link"" href=""http://example.com/#4"">Example 4</a></li>
    </ul>
  </li>
</ul>
lorem ipsum <a href=""http://example.com/"">dolor</a> sit amet.


WAI-ARIA Roles and Other Accessibility Support
This answer hopefully covers the basics of accessible keyboard and mouse support for this feature, but as it's already quite sizable I'm going to avoid any discussion of WAI-ARIA roles and attributes, however I highly recommend that implementers refer to the spec for details on what roles they should use and any other appropriate attributes.
    It's 2020 and you can use event.composedPath()
From: https://developer.mozilla.org/en-US/docs/Web/API/Event/composedPath

The composedPath() method of the Event interface returns the events path, which is an array of the objects on which listeners will be invoked.

const target = document.querySelector('#myTarget')

document.addEventListener('click', (event) => {
  const withinBoundaries = event.composedPath().includes(target)

  if (withinBoundaries) {
    target.innerText = 'Click happened inside element'
  } else {
    target.innerText = 'Click happened **OUTSIDE** element'
  } 
})/* just to make it good looking. you don't need this */
#myTarget {
  margin: 50px auto;
  width: 500px;
  height: 500px;
  background: gray;
  border: 10px solid black;
}<div id=""myTarget"">
  click me (or not!)
</div>

    The other solutions here didn't work for me so I had to use:
if(!$(event.target).is('#foo'))
{
    // hide menu
}

Edit: Plain Javascript variant (2021-03-31)
I used this method to handle closing a drop down menu when clicking outside of it.
First, I created a custom class name for all the elements of the component. This class name will be added to all elements that make up the menu widget.
const className = `dropdown-${Date.now()}-${Math.random() * 100}`;

I create a function to check for clicks and the class name of the clicked element. If clicked element does not contain the custom class name I generated above, it should set the show flag to false and the menu will close.
const onClickOutside = (e) => {
  if (!e.target.className.includes(className)) {
    show = false;
  }
};

Then I attached the click handler to the window object.
// add when widget loads
window.addEventListener(""click"", onClickOutside);

... and finally some housekeeping
// remove listener when destroying the widget
window.removeEventListener(""click"", onClickOutside);

    Use focusout for accessability
There is one answer here that says (quite correctly) that focusing on click events is an accessibility problem since we want to cater for keyboard users. The focusout event is the correct thing to use here, but it can be done much more simply than in the other answer (and in pure javascript too):
A simpler way of doing it:
The 'problem' with using focusout is that if an element inside your dialog/modal/menu loses focus, to something also 'inside' the event will still get fired. We can check that this isn't the case by looking at event.relatedTarget (which tells us what element will have gained focus).
dialog = document.getElementById(""dialogElement"")

dialog.addEventListener(""focusout"", function (event) {
    if (
        // we are still inside the dialog so don't close
        dialog.contains(event.relatedTarget) ||
        // we have switched to another tab so probably don't want to close 
        !document.hasFocus()  
    ) {
        return;
    }
    dialog.close();  // or whatever logic you want to use to close
});

There is one slight gotcha to the above, which is that relatedTarget may be null. This is fine if the user is clicking outside the dialog, but will be a problem if unless the user clicks inside the dialog and the dialog happens to not be focusable. To fix this you have to make sure to set tabIndex=0 so your dialog is focusable.
    After research I have found three working solutions (I forgot the page links for reference)

First solution

<script>
    //The good thing about this solution is it doesn't stop event propagation.

    var clickFlag = 0;
    $('body').on('click', function () {
        if(clickFlag == 0) {
            console.log('hide element here');
            /* Hide element here */
        }
        else {
            clickFlag=0;
        }
    });
    $('body').on('click','#testDiv', function (event) {
        clickFlag = 1;
        console.log('showed the element');
        /* Show the element */
    });
</script>


Second solution

<script>
    $('body').on('click', function(e) {
        if($(e.target).closest('#testDiv').length == 0) {
           /* Hide dropdown here */
        }
    });
</script>


Third solution

<script>
    var specifiedElement = document.getElementById('testDiv');
    document.addEventListener('click', function(event) {
        var isClickInside = specifiedElement.contains(event.target);
        if (isClickInside) {
          console.log('You clicked inside')
        }
        else {
          console.log('You clicked outside')
        }
    });
</script>

    I am surprised nobody actually acknowledged focusout event:
var button = document.getElementById('button');
button.addEventListener('click', function(e){
  e.target.style.backgroundColor = 'green';
});
button.addEventListener('focusout', function(e){
  e.target.style.backgroundColor = '';
});<!DOCTYPE html>
<html>
<head>
  <meta charset=""utf-8"">
</head>
<body>
  <button id=""button"">Click</button>
</body>
</html>

    I have an application that works similarly to Eran's example, except I attach the click event to the body when I open the menu... Kinda like this:

$('#menucontainer').click(function(event) {
  $('html').one('click',function() {
    // Hide the menus
  });

  event.stopPropagation();
});


More information on jQuery's one() function
    A simple solution for the situation is:

$(document).mouseup(function (e)
{
    var container = $(""YOUR SELECTOR""); // Give you class or ID

    if (!container.is(e.target) &&            // If the target of the click is not the desired div or section
        container.has(e.target).length === 0) // ... nor a descendant-child of the container
    {
        container.hide();
    }
});


The above script will hide the div if outside of the div click event is triggered.

You can see the following blog for more information : http://www.codecanal.com/detect-click-outside-div-using-javascript/
    2020 solution using native JS API closest method.
document.addEventListener('click', ({ target }) => {
  if (!target.closest('.el1, .el2, #el3')) {
    alert('click outside')
  }
})


    $(""#menuscontainer"").click(function() {
    $(this).focus();
});
$(""#menuscontainer"").blur(function(){
    $(this).hide();
});


Works for me just fine.
    This is the simplest answer I have found to this question:
window.addEventListener('click', close_window = function () {
  if(event.target !== windowEl){
    windowEl.style.display = ""none"";
    window.removeEventListener('click', close_window, false);
  }
});

And you will see I named the function ""close_window"" so that I could remove the event listener when the window closes.
    A way to write in pure JavaScript
let menu = document.getElementById(""menu"");

document.addEventListener(""click"", function(){
    // Hide the menus
    menu.style.display = ""none"";
}, false);

document.getElementById(""menuscontainer"").addEventListener(""click"", function(e){
    // Show the menus
    menu.style.display = ""block"";
    e.stopPropagation();
}, false);

    The event has a property called event.path of the element which is a ""static ordered list of all its ancestors in tree order"". To check if an event originated from a specific DOM element or one of its children, just check the path for that specific DOM element. It can also be used to check multiple elements by logically ORing the element check in the some function.

$(""body"").click(function() {
  target = document.getElementById(""main"");
  flag = event.path.some(function(el, i, arr) {
    return (el == target)
  })
  if (flag) {
    console.log(""Inside"")
  } else {
    console.log(""Outside"")
  }
});#main {
  display: inline-block;
  background:yellow;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>
<div id=""main"">
  <ul>
    <li>Test-Main</li>
    <li>Test-Main</li>
    <li>Test-Main</li>
    <li>Test-Main</li>
    <li>Test-Main</li>
  </ul>
</div>
<div id=""main2"">
  Outside Main
</div>


So for your case It should be 

$(""body"").click(function() {
  target = $(""#menuscontainer"")[0];
  flag = event.path.some(function(el, i, arr) {
    return (el == target)
  });
  if (!flag) {
    // Hide the menus
  }
});

    If someone curious here is javascript solution(es6):

window.addEventListener('mouseup', e => {
        if (e.target != yourDiv && e.target.parentNode != yourDiv) {
            yourDiv.classList.remove('show-menu');
            //or yourDiv.style.display = 'none';
        }
    })


and es5, just in case:

window.addEventListener('mouseup', function (e) {
if (e.target != yourDiv && e.target.parentNode != yourDiv) {
    yourDiv.classList.remove('show-menu'); 
    //or yourDiv.style.display = 'none';
}


});
    Check the window click event target (it should propagate to the window, as long as it's not captured anywhere else), and ensure that it's not any of the menu elements.  If it's not, then you're outside your menu.

Or check the position of the click, and see if it's contained within the menu area.
    All of these answers solve the problem, but I would like to contribute with a moders es6 solution that does exactly what is needed. I just hope to make someone happy with this runnable demo.
window.clickOutSide = (element, clickOutside, clickInside) => {
  document.addEventListener('click', (event) => {
    if (!element.contains(event.target)) {
      if (typeof clickInside === 'function') {
        clickOutside();
      }
    } else {
      if (typeof clickInside === 'function') {
        clickInside();
      }
    }
  });
};

window.clickOutSide(document.querySelector('.block'), () => alert('clicked outside'), () => alert('clicked inside'));.block {
  width: 400px;
  height: 400px;
  background-color: red;
}<div class=""block""></div>

    I've had success with something like this:

var $menuscontainer = ...;

$('#trigger').click(function() {
  $menuscontainer.show();

  $('body').click(function(event) {
    var $target = $(event.target);

    if ($target.parents('#menuscontainer').length == 0) {
      $menuscontainer.hide();
    }
  });
});


The logic is: when #menuscontainer is shown, bind a click handler to the body that hides #menuscontainer only if the target (of the click) isn't a child of it.
    Here is the vanilla JavaScript solution for future viewers.

Upon clicking any element within the document, if the clicked element's id is toggled, or the hidden element is not hidden and the hidden element does not contain the clicked element, toggle the element.

(function () {
    ""use strict"";
    var hidden = document.getElementById('hidden');
    document.addEventListener('click', function (e) {
        if (e.target.id == 'toggle' || (hidden.style.display != 'none' && !hidden.contains(e.target))) hidden.style.display = hidden.style.display == 'none' ? 'block' : 'none';
    }, false);
})();


(function () {
    ""use strict"";
    var hidden = document.getElementById('hidden');
    document.addEventListener('click', function (e) {
        if (e.target.id == 'toggle' || (hidden.style.display != 'none' && !hidden.contains(e.target))) hidden.style.display = hidden.style.display == 'none' ? 'block' : 'none';
    }, false);
})();<a href=""javascript:void(0)"" id=""toggle"">Toggle Hidden Div</a>
<div id=""hidden"" style=""display: none;"">This content is normally hidden. click anywhere other than this content to make me disappear</div>


If you are going to have multiple toggles on the same page you can use something like this:


Add the class name hidden to the collapsible item.
Upon document click, close all hidden elements which do not contain the clicked element and are not hidden
If the clicked element is a toggle, toggle the specified element.


(function () {
    ""use strict"";
    var hiddenItems = document.getElementsByClassName('hidden'), hidden;
    document.addEventListener('click', function (e) {
        for (var i = 0; hidden = hiddenItems[i]; i++) {
            if (!hidden.contains(e.target) && hidden.style.display != 'none')
                hidden.style.display = 'none';
        }
        if (e.target.getAttribute('data-toggle')) {
            var toggle = document.querySelector(e.target.getAttribute('data-toggle'));
            toggle.style.display = toggle.style.display == 'none' ? 'block' : 'none';
        }
    }, false);
})();<a href=""javascript:void(0)"" data-toggle=""#hidden1"">Toggle Hidden Div</a>
<div class=""hidden"" id=""hidden1"" style=""display: none;"" data-hidden=""true"">This content is normally hidden</div>
<a href=""javascript:void(0)"" data-toggle=""#hidden2"">Toggle Hidden Div</a>
<div class=""hidden"" id=""hidden2"" style=""display: none;"" data-hidden=""true"">This content is normally hidden</div>
<a href=""javascript:void(0)"" data-toggle=""#hidden3"">Toggle Hidden Div</a>
<div class=""hidden"" id=""hidden3"" style=""display: none;"" data-hidden=""true"">This content is normally hidden</div>

    As another poster said there are a lot of gotchas, especially if the element you are displaying (in this case a menu) has interactive elements.
I've found the following method to be fairly robust:

$('#menuscontainer').click(function(event) {
    //your code that shows the menus fully

    //now set up an event listener so that clicking anywhere outside will close the menu
    $('html').click(function(event) {
        //check up the tree of the click target to check whether user has clicked outside of menu
        if ($(event.target).parents('#menuscontainer').length==0) {
            // your code to hide menu

            //this event listener has done its job so we can unbind it.
            $(this).unbind(event);
        }

    })
});

    This worked for me perfectly!!

$('html').click(function (e) {
    if (e.target.id == 'YOUR-DIV-ID') {
        //do something
    } else {
        //do something
    }
});

    Now there is a plugin for that: outside events (blog post)

The following happens when a clickoutside handler (WLOG) is bound to an element:


the element is added to an array which holds all elements with clickoutside handlers
a (namespaced) click handler is bound to the document (if not already there)
on any click in the document, the clickoutside event is triggered for those elements in that array that are not equal to or a parent of the click-events target
additionally, the event.target for the clickoutside event is set to the element the user clicked on (so you even know what the user clicked, not just that he clicked outside)


So no events are stopped from propagation and additional click handlers may be used ""above"" the element with the outside-handler.
    Solution1

Instead of using event.stopPropagation() which can have some side affects, just define a simple flag variable and add one if condition. I tested this and worked properly without any side affects of stopPropagation:

var flag = ""1"";
$('#menucontainer').click(function(event){
    flag = ""0""; // flag 0 means click happened in the area where we should not do any action
});

$('html').click(function() {
    if(flag != ""0""){
        // Hide the menus if visible
    }
    else {
        flag = ""1"";
    }
});


Solution2

With just a simple if condition:

$(document).on('click', function(event){
    var container = $(""#menucontainer"");
    if (!container.is(event.target) &&            // If the target of the click isn't the container...
        container.has(event.target).length === 0) // ... nor a descendant of the container
    {
        // Do whatever you want to do when click is outside the element
    }
});

    I have used below script and done with jQuery.

jQuery(document).click(function(e) {
    var target = e.target; //target div recorded
    if (!jQuery(target).is('#tobehide') ) {
        jQuery(this).fadeOut(); //if the click element is not the above id will hide
    }
})


Below find the HTML code

<div class=""main-container"">
<div> Hello I am the title</div>
<div class=""tobehide"">I will hide when you click outside of me</div>
</div>


You can read the tutorial here 
    Here is a simple solution by pure javascript. It is up-to-date with ES6:

var isMenuClick = false;
var menu = document.getElementById('menuscontainer');
document.addEventListener('click',()=>{
    if(!isMenuClick){
       //Hide the menu here
    }
    //Reset isMenuClick 
    isMenuClick = false;
})
menu.addEventListener('click',()=>{
    isMenuClick = true;
})

    Still looking for that perfect solution for detecting clicking outside? Look no further! Introducing Clickout-Event, a package that provides universal support for clickout and other similar events, and it works in all scenarios: plain HTML onclickout attributes, .addEventListener('clickout') of vanilla JavaScript, .on('clickout') of jQuery, v-on:clickout directives of Vue.js, you name it. As long as a front-end framework internally uses addEventListener to handle events, Clickout-Event works for it. Just add the script tag anywhere in your page, and it simply works like magic.
HTML attribute
<div onclickout=""console.log('clickout detected')"">...</div>

Vanilla JavaScript
document.getElementById('myId').addEventListener('clickout', myListener);

jQuery
$('#myId').on('clickout', myListener);

Vue.js
<div v-on:clickout=""open=false"">...</div>

Angular
<div (clickout)=""close()"">...</div>

    I don't think what you really need is to close the menu when the user clicks outside; what you need is for the menu to close when the user clicks anywhere at all on the page. If you click on the menu, or off the menu it should close right? 

Finding no satisfactory answers above prompted me to write this blog post the other day. For the more pedantic, there are a number of gotchas to take note of: 


If you attach a click event handler to the body element at click time be sure to wait for the 2nd click before closing the menu, and unbinding the event. Otherwise the click event that opened the menu will bubble up to the listener that has to close the menu.
If you use event.stopPropogation() on a click event, no other elements in your page can have a click-anywhere-to-close feature.
Attaching a click event handler to the body element indefinitely is not a performant solution
Comparing the target of the event, and its parents to the handler's creator assumes that what you want is to close the menu when you click off it, when what you really want is to close it when you click anywhere on the page.
Listening for events on the body element will make your code more brittle. Styling as innocent as this would break it: body { margin-left:auto; margin-right: auto; width:960px;}

    Let's say the div you want to detect if the user clicked outside or inside has an id, for example: ""my-special-widget"".

Listen to body click events:

document.body.addEventListener('click', (e) => {
    if (isInsideMySpecialWidget(e.target, ""my-special-widget"")) {
        console.log(""user clicked INSIDE the widget"");
    }
    console.log(""user clicked OUTSIDE the widget"");
});

function isInsideMySpecialWidget(elem, mySpecialWidgetId){
    while (elem.parentElement) {
        if (elem.id === mySpecialWidgetId) {
            return true;
        }
        elem = elem.parentElement;
    }
    return false;
}


In this case, you won't break the normal flow of click on some element in your page, since you are not using the ""stopPropagation"" method.
    I just want to make @Pistos answer more apparent since it's hidden in the comments.

This solution worked perfectly for me. Plain JS:

var elementToToggle = $('.some-element');
$(document).click( function(event) {
  if( $(event.target).closest(elementToToggle).length === 0 ) {
    elementToToggle.hide();
  }
});


in CoffeeScript:

elementToToggle = $('.some-element')
$(document).click (event) ->
  if $(event.target).closest(elementToToggle).length == 0
    elementToToggle.hide()

    ","[2774, 1549, 1958, 386, 84, 165, 14, 53, 21, 133, 29, 7, 43, 3, 3, 17, 8, 22, 3, 19, 13, 26, 33, 38, 21, 7, 7, 2, 26, 4, 4]",1533104,729,2008-09-30T13:17:12,2022-04-24 15:02:36Z,javascript 
Flash CS4 refuses to let go,"
                
I have a Flash project, and it has many source files. I have a fairly heavily-used class, call it Jenine. I recently (and, perhaps, callously) relocated Jenine from one namespace to another. I thought we were ready - I thought it was time. The new Jenine was better in every way - she had lost some code bloat, she had decoupled herself from a few vestigial class relationships, and she had finally come home to the namespace that she had always secretly known in her heart was the one she truly belonged to. She was among her own kind.

Unfortunately, Flash would have none of that. Perhaps it had formed an attachment. Perhaps it didn't want Jenine to be decoupled. Either way, it clung to the old, perfect version of Jenine in its memory. It refused to move on. It ignored her (function) calls. It tried to forget her new, public interfaces. Instead, every instance of Jenine that it constructed was always a copy of the old version, down to its classpath:

var jenineInstance:Jenine = new Jenine();
trace( getQualifiedClassName(jenineInstance));
// Should print: com.newnamespace.subspace::Jenine
// Prints: com.oldnamespace.subspace::Jenine
// Ah, young love!


We fought. I'm not proud of some of the things I said or did. In the end, in a towering fit of rage, I deleted all references of Jenine completely. She was utterly, completely erased from the system. My cursor fell upon the ""Empty Trash"" menu option like the cold lid of a casket.

I don't think Flash ever recovered. To this day it still clings to the memory of Jenine. Her old, imperfect definitions still float through my project like abandoned ghosts. Whenever I force Flash to compile, it still lovingly inserts her into my movie, nestling her definition in amongst the other, living classes, like a small shrine. I wonder if they can see her.

Flash and I don't really talk anymore. I write my code, it compiles it. There's a new girl in town named Summer who looks almost identical to Jenine, as if someone had just copied her source-code wholesale into a new class, but Flash hasn't shown any interest. Most days it just mopes around and writes bad poetry in my comments when it thinks I'm not looking.

I hope no one else has had a similar experience, that this is just a singular, painful ripple in the horrifying dark lagoon that is the Flash code-base. Does anyone have any idea how to erase whatever cache the compiler is using?
    Flash still has the ASO file, which is the compiled byte code for your classes. On Windows, you can see the ASO files here:

C:\Documents and Settings\username\Local Settings\Application Data\Adobe\Flash CS4\en\Configuration\Classes\aso


On a Mac, the directory structure is similar in /Users/username/Library/Application Support/ 



You can remove those files by hand, or in Flash you can select Control->Delete ASO files to remove them.
    I have found one related behaviour that may help (sounds like your specific problem runs deeper though):

Flash checks whether a source file needs recompiling by looking at timestamps. If its compiled version is older than the source file, it will recompile. But it doesn't check whether the compiled version was generated from the same source file or not.

Specifically, if you have your actionscript files under version control, and you Revert a change, the reverted file will usually have an older timestamp, and Flash will ignore it.
    Also, to use your new namespaced class you can also do

var jenine:com.newnamespace.subspace.Jenine = com.newnamespace.subspace.Jenine()

    What if you compile it using another machine? A fresh installed one would be lovely. I hope your machine is not jealous.
    Try deleting your ASO files.

ASO files are cached compiled versions of your class files. Although the IDE is a lot better at letting go of old caches when changes are made, sometimes you have to manually delete them. To delete ASO files: Control>Delete ASO Files.

This is also the cause of the ""I-am-not-seeing-my-changes-so-let-me-add-a-trace-now-everything-works"" bug that was introduced in CS3.
    Do you have several swf-files? If your class is imported in one of the swf's, other swf's will also use the same version of the class. One old import with * in one swf will do it. Recompile everything and see if it works. 
    Use a grep analog to find the strings oldnamespace and Jenine inside the files in your whole project folder. Then you'd know what step to do next.
    ","[2774, 707, 126, 104, 164, 198, 96, 11]",112289,496,2010-02-03T17:32:15,2019-06-03 17:02:02Z,
How can I prevent SQL injection in PHP?,"
                    
            
        
            
                    
                        
                    
                
                    
                        This question's answers are a community effort. Edit existing answers to improve this post. It is not currently accepting new answers or interactions.
                        
                    
                
            
        

    

If user input is inserted without modification into an SQL query, then the application becomes vulnerable to SQL injection, like in the following example:

$unsafe_variable = $_POST['user_input']; 

mysql_query(""INSERT INTO `table` (`column`) VALUES ('$unsafe_variable')"");


That's because the user can input something like value'); DROP TABLE table;--, and the query becomes:

INSERT INTO `table` (`column`) VALUES('value'); DROP TABLE table;--')


What can be done to prevent this from happening?
    The correct way to avoid SQL injection attacks, no matter which database you use, is to separate the data from SQL, so that data stays data and will never be interpreted as commands by the SQL parser. It is possible to create an SQL statement with correctly formatted data parts, but if you don't fully understand the details, you should always use prepared statements and parameterized queries. These are SQL statements that are sent to and parsed by the database server separately from any parameters. This way it is impossible for an attacker to inject malicious SQL.
You basically have two options to achieve this:

Using PDO (for any supported database driver):
 $stmt = $pdo->prepare('SELECT * FROM employees WHERE name = :name');
$stmt->execute([ 'name' => $name ]);

foreach ($stmt as $row) {
    // Do something with $row
}


Using MySQLi (for MySQL):
 $stmt = $dbConnection->prepare('SELECT * FROM employees WHERE name = ?');
$stmt->bind_param('s', $name); // 's' specifies the variable type => 'string'
$stmt->execute();

$result = $stmt->get_result();
while ($row = $result->fetch_assoc()) {
    // Do something with $row
}



If you're connecting to a database other than MySQL, there is a driver-specific second option that you can refer to (for example, pg_prepare() and pg_execute() for PostgreSQL). PDO is the universal option.

Correctly setting up the connection
PDO
Note that when using PDO to access a MySQL database real prepared statements are not used by default. To fix this you have to disable the emulation of prepared statements. An example of creating a connection using PDO is:
$dbConnection = new PDO('mysql:dbname=dbtest;host=127.0.0.1;charset=utf8mb4', 'user', 'password');

$dbConnection->setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
$dbConnection->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);

In the above example, the error mode isn't strictly necessary, but it is advised to add it. This way PDO will inform you of all MySQL errors by means of throwing the PDOException.
What is mandatory, however, is the first setAttribute() line, which tells PDO to disable emulated prepared statements and use real prepared statements. This makes sure the statement and the values aren't parsed by PHP before sending it to the MySQL server (giving a possible attacker no chance to inject malicious SQL).
Although you can set the charset in the options of the constructor, it's important to note that 'older' versions of PHP (before 5.3.6) silently ignored the charset parameter in the DSN.
Mysqli
For mysqli we have to follow the same routine:
mysqli_report(MYSQLI_REPORT_ERROR | MYSQLI_REPORT_STRICT); // error reporting
$dbConnection = new mysqli('127.0.0.1', 'username', 'password', 'test');
$dbConnection->set_charset('utf8mb4'); // charset


Explanation
The SQL statement you pass to prepare is parsed and compiled by the database server. By specifying parameters (either a ? or a named parameter like :name in the example above) you tell the database engine where you want to filter on. Then when you call execute, the prepared statement is combined with the parameter values you specify.
The important thing here is that the parameter values are combined with the compiled statement, not an SQL string. SQL injection works by tricking the script into including malicious strings when it creates SQL to send to the database. So by sending the actual SQL separately from the parameters, you limit the risk of ending up with something you didn't intend.
Any parameters you send when using a prepared statement will just be treated as strings (although the database engine may do some optimization so parameters may end up as numbers too, of course). In the example above, if the $name variable contains 'Sarah'; DELETE FROM employees the result would simply be a search for the string ""'Sarah'; DELETE FROM employees"", and you will not end up with an empty table.
Another benefit of using prepared statements is that if you execute the same statement many times in the same session it will only be parsed and compiled once, giving you some speed gains.
Oh, and since you asked about how to do it for an insert, here's an example (using PDO):
$preparedStatement = $db->prepare('INSERT INTO table (column) VALUES (:column)');

$preparedStatement->execute([ 'column' => $unsafeValue ]);


Can prepared statements be used for dynamic queries?
While you can still use prepared statements for the query parameters, the structure of the dynamic query itself cannot be parametrized and certain query features cannot be parametrized.
For these specific scenarios, the best thing to do is use a whitelist filter that restricts the possible values.
// Value whitelist
// $dir can only be 'DESC', otherwise it will be 'ASC'
if (empty($dir) || $dir !== 'DESC') {
   $dir = 'ASC';
}

    To use the parameterized query, you need to use either Mysqli or PDO. To rewrite your example with mysqli, we would need something like the following.
<?php
mysqli_report(MYSQLI_REPORT_ERROR | MYSQLI_REPORT_STRICT);
$mysqli = new mysqli(""server"", ""username"", ""password"", ""database_name"");

$variable = $_POST[""user-input""];
$stmt = $mysqli->prepare(""INSERT INTO table (column) VALUES (?)"");
// ""s"" means the database expects a string
$stmt->bind_param(""s"", $variable);
$stmt->execute();

The key function you'll want to read up on there would be mysqli::prepare.
Also, as others have suggested, you may find it useful/easier to step up a layer of abstraction with something like PDO.
Please note that the case you asked about is a fairly simple one and that more complex cases may require more complex approaches. In particular:

If you want to alter the structure of the SQL based on user input, parameterized queries are not going to help, and the escaping required is not covered by mysql_real_escape_string. In this kind of case, you would be better off passing the user's input through a whitelist to ensure only 'safe' values are allowed through.

    Every answer here covers only part of the problem.
In fact, there are four different query parts which we can add to SQL dynamically: -

a string
a number
an identifier
a syntax keyword

And prepared statements cover only two of them.
But sometimes we have to make our query even more dynamic, adding operators or identifiers as well.
So, we will need different protection techniques.
In general, such a protection approach is based on whitelisting.
In this case, every dynamic parameter should be hardcoded in your script and chosen from that set.
For example, to do dynamic ordering:
$orders  = array(""name"", ""price"", ""qty""); // Field names
$key = array_search($_GET['sort'], $orders)); // if we have such a name
$orderby = $orders[$key]; // If not, first one will be set automatically. 
$query = ""SELECT * FROM `table` ORDER BY $orderby""; // Value is safe

To ease the process I wrote a whitelist helper function that does all the job in one line:
$orderby = white_list($_GET['orderby'], ""name"", [""name"",""price"",""qty""], ""Invalid field name"");
$query  = ""SELECT * FROM `table` ORDER BY `$orderby`""; // sound and safe

There is another way to secure identifiers - escaping but I rather stick to whitelisting as a more robust and explicit approach. Yet as long as you have an identifier quoted, you can escape the quote character to make it safe. For example, by default for mysql you have to double the quote character to escape it. For other other DBMS escaping rules would be different.
Still, there is an issue with SQL syntax keywords (such as AND, DESC and such), but white-listing seems the only approach in this case.
So, a general recommendation may be phrased as


Any variable that represents an SQL data literal, (or, to put it simply - an SQL string, or a number) must be added through a prepared statement. No Exceptions.
Any other query part, such as an SQL keyword, a table or a field name, or an operator - must be filtered through a white list.


Update
Although there is a general agreement on the best practices regarding SQL injection protection, there are still many bad practices as well. And some of them too deeply rooted in the minds of PHP users. For instance, on this very page there are (although invisible to most visitors) more than 80 deleted answers - all removed by the community due to bad quality or promoting bad and outdated practices. Worse yet, some of the bad answers aren't deleted, but rather prospering.
For example, there(1) are(2) still(3) many(4) answers(5), including the second most upvoted answer suggesting you manual string escaping - an outdated approach that is proven to be insecure.
Or there is a slightly better answer that suggests just another method of string formatting and even boasts it as the ultimate panacea. While of course, it is not. This method is no better than regular string formatting, yet it keeps all its drawbacks: it is applicable to strings only and, like any other manual formatting, it's essentially optional, non-obligatory measure, prone to human error of any sort.
I think that all this because of one very old superstition, supported by such authorities like OWASP or the PHP manual, which proclaims equality between whatever ""escaping"" and protection from SQL injections.
Regardless of what PHP manual said for ages, *_escape_string by no means makes data safe and never has been intended to. Besides being useless for any SQL part other than string, manual escaping is wrong, because it is manual as opposite to automated.
And OWASP makes it even worse, stressing on escaping user input which is an utter nonsense: there should be no such words in the context of injection protection. Every variable is potentially dangerous - no matter the source! Or, in other words - every variable has to be properly formatted to be put into a query - no matter the source again. It's the destination that matters. The moment a developer starts to separate the sheep from the goats (thinking whether some particular variable is ""safe"" or not) he/she takes his/her first step towards disaster. Not to mention that even the wording suggests bulk escaping at the entry point, resembling the very magic quotes feature - already despised, deprecated and removed.
So, unlike whatever ""escaping"", prepared statements is the measure that indeed protects from SQL injection (when applicable).
    I'd recommend using PDO (PHP Data Objects) to run parameterized SQL queries. 

Not only does this protect against SQL injection, but it also speeds up queries. 

And by using PDO rather than mysql_, mysqli_, and pgsql_ functions, you make your application a little more abstracted from the database, in the rare occurrence that you have to switch database providers.
    Use PDO and prepared queries.
($conn is a PDO object)
$stmt = $conn->prepare(""INSERT INTO tbl VALUES(:id, :name)"");
$stmt->bindValue(':id', $id);
$stmt->bindValue(':name', $name);
$stmt->execute();

    As you can see, people suggest you use prepared statements at the most. It's not wrong, but when your query is executed just once per process, there would be a slight performance penalty. 

I was facing this issue, but I think I solved it in very sophisticated way - the way hackers use to avoid using quotes. I used this in conjunction with emulated prepared statements. I use it to prevent all kinds of possible SQL injection attacks.

My approach:


If you expect input to be integer make sure it's really integer. In a variable-type language like PHP it is this very important. You can use for example this very simple but powerful solution: sprintf(""SELECT 1,2,3 FROM table WHERE 4 = %u"", $input);  
If you expect anything else from integer hex it. If you hex it, you will perfectly escape all input. In C/C++ there's a function called mysql_hex_string(), in PHP you can use bin2hex().

Don't worry about that the escaped string will have a 2x size of its original length because even if you use mysql_real_escape_string, PHP has to allocate same capacity ((2*input_length)+1), which is the same.
This hex method is often used when you transfer binary data, but I see no reason why not use it on all data to prevent SQL injection attacks. Note that you have to prepend data with 0x or use the MySQL function UNHEX instead.


So, for example, the query:

SELECT password FROM users WHERE name = 'root';


Will become:

SELECT password FROM users WHERE name = 0x726f6f74;


or

SELECT password FROM users WHERE name = UNHEX('726f6f74');


Hex is the perfect escape. No way to inject.

Difference between UNHEX function and 0x prefix

There was some discussion in comments, so I finally want to make it clear. These two approaches are very similar, but they are a little different in some ways:

The 0x prefix can only be used for data columns such as char, varchar, text, block, binary, etc.
Also, its use is a little complicated if you are about to insert an empty string. You'll have to entirely replace it with '', or you'll get an error.

UNHEX() works on any column; you do not have to worry about the empty string.



Hex methods are often used as attacks

Note that this hex method is often used as an SQL injection attack where integers are just like strings and escaped just with mysql_real_escape_string. Then you can avoid the use of quotes.

For example, if you just do something like this:

""SELECT title FROM article WHERE id = "" . mysql_real_escape_string($_GET[""id""])


an attack can inject you very easily. Consider the following injected code returned from your script:

SELECT ... WHERE id = -1 UNION ALL SELECT table_name FROM information_schema.tables;


and now just extract table structure:

SELECT ... WHERE id = -1 UNION ALL SELECT column_name FROM information_schema.column WHERE table_name = __0x61727469636c65__;


And then just select whatever data ones want. Isn't it cool?

But if the coder of an injectable site would hex it, no injection would be possible because the query would look like this:

SELECT ... WHERE id = UNHEX('2d312075...3635');

    You could do something basic like this:

$safe_variable = mysqli_real_escape_string($_POST[""user-input""], $dbConnection);
mysqli_query($dbConnection, ""INSERT INTO table (column) VALUES ('"" . $safe_variable . ""')"");


This won't solve every problem, but it's a very good stepping stone. I left out obvious items such as checking the variable's existence, format (numbers, letters, etc.).
    
  Deprecated Warning:
  This answer's sample code (like the question's sample code) uses PHP's MySQL extension, which was deprecated in PHP 5.5.0 and removed entirely in PHP 7.0.0.
  
  Security Warning: This answer is not in line with security best practices. Escaping is inadequate to prevent SQL injection, use prepared statements instead. Use the strategy outlined below at your own risk. (Also, mysql_real_escape_string() was removed in PHP 7.)
  
  IMPORTANT
  
  The best way to prevent SQL Injection is to use Prepared Statements instead of escaping, as the accepted answer demonstrates. 
  
  There are libraries such as Aura.Sql and EasyDB that allow developers to use prepared statements easier. To learn more about why prepared statements are better at stopping SQL injection, refer to this mysql_real_escape_string() bypass and recently fixed Unicode SQL Injection vulnerabilities in WordPress.


Injection prevention - mysql_real_escape_string()

PHP has a specially-made function to prevent these attacks. All you need to do is use the mouthful of a function, mysql_real_escape_string.

mysql_real_escape_string takes a string that is going to be used in a MySQL query and return the same string with all SQL injection attempts safely escaped. Basically, it will replace those troublesome quotes(') a user might enter with a MySQL-safe substitute, an escaped quote \'.

NOTE: you must be connected to the database to use this function!

// Connect to MySQL

$name_bad = ""' OR 1'""; 

$name_bad = mysql_real_escape_string($name_bad);

$query_bad = ""SELECT * FROM customers WHERE username = '$name_bad'"";
echo ""Escaped Bad Injection: <br />"" . $query_bad . ""<br />"";


$name_evil = ""'; DELETE FROM customers WHERE 1 or username = '""; 

$name_evil = mysql_real_escape_string($name_evil);

$query_evil = ""SELECT * FROM customers WHERE username = '$name_evil'"";
echo ""Escaped Evil Injection: <br />"" . $query_evil;


You can find more details in MySQL - SQL Injection Prevention.
    Whatever you do end up using, make sure that you check your input hasn't already been mangled by magic_quotes or some other well-meaning rubbish, and if necessary, run it through stripslashes or whatever to sanitize it.
    There are many ways of preventing SQL injections and other SQL hacks. You can easily find it on the Internet (Google Search). Of course PDO is one of the good solutions. But I would like to suggest you some good links prevention from SQL injection.

What is SQL injection and how to prevent

PHP manual for SQL injection

Microsoft explanation of SQL injection and prevention in PHP

And some other like Preventing SQL injection with MySQL and PHP.

Now, why you do you need to prevent your query from SQL injection?

I would like to let you know: Why do we try for preventing SQL injection with a short example below:

Query for login authentication match:

$query=""select * from users where email='"".$_POST['email'].""' and password='"".$_POST['password'].""' "";


Now, if someone (a hacker) puts

$_POST['email']= admin@emali.com' OR '1=1


and password anything....

The query will be parsed into the system only up to:

$query=""select * from users where email='admin@emali.com' OR '1=1';


The other part will be discarded. So, what will happen? A non-authorized user (hacker) will be able to log in as administrator without having his/her password. Now, he/she can do anything that the administrator/email person can do. See, it's very dangerous if SQL injection is not prevented.
    
  Deprecated Warning:
  This answer's sample code (like the question's sample code) uses PHP's MySQL extension, which was deprecated in PHP 5.5.0 and removed entirely in PHP 7.0.0.
  
  Security Warning: This answer is not in line with security best practices. Escaping is inadequate to prevent SQL injection, use prepared statements instead. Use the strategy outlined below at your own risk. (Also, mysql_real_escape_string() was removed in PHP 7.)


Parameterized query AND input validation is the way to go. There are many scenarios under which SQL injection may occur, even though mysql_real_escape_string() has been used.

Those examples are vulnerable to SQL injection:

$offset = isset($_GET['o']) ? $_GET['o'] : 0;
$offset = mysql_real_escape_string($offset);
RunQuery(""SELECT userid, username FROM sql_injection_test LIMIT $offset, 10"");


or

$order = isset($_GET['o']) ? $_GET['o'] : 'userid';
$order = mysql_real_escape_string($order);
RunQuery(""SELECT userid, username FROM sql_injection_test ORDER BY `$order`"");


In both cases, you can't use ' to protect the encapsulation.

Source: The Unexpected SQL Injection (When Escaping Is Not Enough)
    I favor stored procedures (MySQL has had stored procedures support since 5.0) from a security point of view - the advantages are -


Most databases (including MySQL) enable user access to be restricted to executing stored procedures. The fine-grained security access control is useful to prevent escalation of privileges attacks. This prevents compromised applications from being able to run SQL directly against the database.
They abstract the raw SQL query from the application so less information of the database structure is available to the application. This makes it harder for people to understand the underlying structure of the database and design suitable attacks.
They accept only parameters, so the advantages of parameterized queries are there. Of course - IMO you still need to sanitize your input - especially if you are using dynamic SQL inside the stored procedure.


The disadvantages are -


They (stored procedures) are tough to maintain and tend to multiply very quickly. This makes managing them an issue.
They are not very suitable for dynamic queries - if they are built to accept dynamic code as parameters then a lot of the advantages are negated.

    If possible, cast the types of your parameters. But it's only working on simple types like int, bool, and float.

$unsafe_variable = $_POST['user_id'];

$safe_variable = (int)$unsafe_variable ;

mysqli_query($conn, ""INSERT INTO table (column) VALUES ('"" . $safe_variable . ""')"");

    A few guidelines for escaping special characters in SQL statements.

Don't use MySQL. This extension is deprecated. Use MySQLi or PDO instead.

MySQLi

For manually escaping special characters in a string you can use the mysqli_real_escape_string function. The function will not work properly unless the correct character set is set with mysqli_set_charset.

Example:

$mysqli = new mysqli('host', 'user', 'password', 'database');
$mysqli->set_charset('charset');

$string = $mysqli->real_escape_string($string);
$mysqli->query(""INSERT INTO table (column) VALUES ('$string')"");


For automatic escaping of values with prepared statements, use mysqli_prepare, and mysqli_stmt_bind_param where types for the corresponding bind variables must be provided for an appropriate conversion:

Example:

$stmt = $mysqli->prepare(""INSERT INTO table (column1, column2) VALUES (?,?)"");

$stmt->bind_param(""is"", $integer, $string);

$stmt->execute();


No matter if you use prepared statements or mysqli_real_escape_string, you always have to know the type of input data you're working with.

So if you use a prepared statement, you must specify the types of the variables for mysqli_stmt_bind_param function.

And the use of mysqli_real_escape_string is for, as the name says, escaping special characters in a string, so it will not make integers safe. The purpose of this function is to prevent breaking the strings in SQL statements, and the damage to the database that it could cause. mysqli_real_escape_string is a useful function when used properly, especially when combined with sprintf.

Example:

$string = ""x' OR name LIKE '%John%"";
$integer = '5 OR id != 0';

$query = sprintf( ""SELECT id, email, pass, name FROM members WHERE email ='%s' AND id = %d"", $mysqli->real_escape_string($string), $integer);

echo $query;
// SELECT id, email, pass, name FROM members WHERE email ='x\' OR name LIKE \'%John%' AND id = 5

$integer = '99999999999999999999';
$query = sprintf(""SELECT id, email, pass, name FROM members WHERE email ='%s' AND id = %d"", $mysqli->real_escape_string($string), $integer);

echo $query;
// SELECT id, email, pass, name FROM members WHERE email ='x\' OR name LIKE \'%John%' AND id = 2147483647

    In my opinion, the best way to generally prevent SQL injection in your PHP application (or any web application, for that matter) is to think about your application's architecture. If the only way to protect against SQL injection is to remember to use a special method or function that does The Right Thing every time you talk to the database, you are doing it wrong. That way, it's just a matter of time until you forget to correctly format your query at some point in your code.

Adopting the MVC pattern and a framework like CakePHP or CodeIgniter is probably the right way to go: Common tasks like creating secure database queries have been solved and centrally implemented in such frameworks. They help you to organize your web application in a sensible way and make you think more about loading and saving objects than about securely constructing single SQL queries. 
    I think if someone wants to use PHP and MySQL or some other dataBase server:


Think about learning PDO (PHP Data Objects)  it is a database access layer providing a uniform method of access to multiple databases.
Think about learning MySQLi
Use native PHP functions like: strip_tags, mysql_real_escape_string or if variable numeric, just (int)$foo. Read more about type of variables in PHP here. If you're using libraries such as PDO or MySQLi, always use PDO::quote() and mysqli_real_escape_string().




Libraries examples:

---- PDO


  ----- No placeholders - ripe for SQL injection! It's bad


$request = $pdoConnection->(""INSERT INTO parents (name, addr, city) values ($name, $addr, $city)"");



  ----- Unnamed placeholders


$request = $pdoConnection->(""INSERT INTO parents (name, addr, city) values (?, ?, ?);



  ----- Named placeholders


$request = $pdoConnection->(""INSERT INTO parents (name, addr, city) value (:name, :addr, :city)"");


--- MySQLi

$request = $mysqliConnection->prepare('
       SELECT * FROM trainers
       WHERE name = ?
       AND email = ?
       AND last_login > ?');

    $query->bind_param('first_param', 'second_param', $mail, time() - 3600);
    $query->execute();




P.S:

PDO wins this battle with ease. With support for twelve
different database drivers and named parameters, we can ignore the
small performance loss, and get used to its API. From a security
standpoint, both of them are safe as long as the developer uses them
the way they are supposed to be used

But while both PDO and MySQLi are quite fast, MySQLi performs
insignificantly faster in benchmarks  ~2.5% for non-prepared
statements, and ~6.5% for prepared ones.

And please test every query to your database - it's a better way to prevent injection.
    For those unsure of how to use PDO (coming from the mysql_ functions), I made a very, very simple PDO wrapper that is a single file. It exists to show how easy it is to do all the common things applications need to be done. Works with PostgreSQL, MySQL, and SQLite.
Basically, read it while you read the manual to see how to put the PDO functions to use in real life to make it simple to store and retrieve values in the format you want.

I want a single column
$count = DB::column('SELECT COUNT(*) FROM `user`');

I want an array(key => value) results (i.e. for making a selectbox)
$pairs = DB::pairs('SELECT `id`, `username` FROM `user`');

I want a single row result
$user = DB::row('SELECT * FROM `user` WHERE `id` = ?', array($user_id));

I want an array of results
$banned_users = DB::fetch('SELECT * FROM `user` WHERE `banned` = ?', array('TRUE'));


    If you want to take advantage of cache engines, like Redis or Memcached, maybe DALMP could be a choice. It uses pure MySQLi. Check this: DALMP Database Abstraction Layer for MySQL using PHP. 

Also, you can 'prepare' your arguments before preparing your query so that you can build dynamic queries and at the end have a fully prepared statements query. DALMP Database Abstraction Layer for MySQL using PHP. 
    Regarding many useful answers, I hope to add some value to this thread.

SQL injection is an attack that can be done through user inputs (inputs that filled by a user and then used inside queries). The SQL injection patterns are correct query syntax while we can call it: bad queries for bad reasons, and we assume that there might be a bad person that try to get secret information (bypassing access control) that affect the three principles of security (confidentiality, integrity, and availability).

Now, our point is to prevent security threats such as SQL injection attacks, the question asking (how to prevent an SQL injection attack using PHP), be more realistic, data filtering or clearing input data is the case when using user-input data inside such query, using PHP or any other programming language is not the case, or as recommended by more people to use modern technology such as prepared statement or any other tools that currently supporting SQL injection prevention, consider that these tools not available anymore? How do you secure your application?

My approach against SQL injection is: clearing user-input data before sending it to the database (before using it inside any query).

Data filtering for (converting unsafe data to safe data)

Consider that PDO and MySQLi are not available. How can you secure your application? Do you force me to use them? What about other languages other than PHP? I prefer to provide general ideas as it can be used for wider border, not just for a specific language.


SQL user (limiting user privilege): most common SQL operations are (SELECT, UPDATE, INSERT), then, why give the UPDATE privilege to a user that does not require it? For example, login, and search pages are only using SELECT, then, why use DB users in these pages with high privileges?


RULE: do not create one database user for all privileges. For all SQL operations, you can create your scheme like (deluser, selectuser, updateuser) as usernames for easy usage.

See principle of least privilege.


Data filtering: before building any query user input, it should be validated and filtered. For programmers, it's important to define some properties for each user-input variables:
data type, data pattern, and data length. A field that is a number between (x and y) must be exactly validated using the exact rule, and for a field that is a string (text): pattern is the case, for example, a username must contain only some characters, lets say [a-zA-Z0-9_-.]. The length varies between (x and n) where x and n (integers, x <=n).
Rule: creating exact filters and validation rules are best practices for me.
Use other tools: Here, I will also agree with you that a prepared statement (parametrized query) and stored procedures. The disadvantages here is these ways require advanced skills which do not exist for most users. The basic idea here is to distinguish between the SQL query and the data that is used inside. Both approaches can be used even with unsafe data, because the user-input data here does not add anything to the original query, such as (any or x=x).


For more information, please read OWASP SQL Injection Prevention Cheat Sheet.

Now, if you are an advanced user, start using this defense as you like, but, for beginners, if they can't quickly implement a stored procedure and prepared the statement, it's better to filter input data as much they can.

Finally, let's consider that a user sends this text below instead of entering his/her username:

[1] UNION SELECT IF(SUBSTRING(Password,1,1)='2',BENCHMARK(100000,SHA1(1)),0) User,Password FROM mysql.user WHERE User = 'root'


This input can be checked early without any prepared statement and stored procedures, but to be on the safe side, using them starts after user-data filtering and validation.

The last point is detecting unexpected behavior which requires more effort and complexity; it's not recommended for normal web applications.

Unexpected behavior in the above user input is SELECT, UNION, IF, SUBSTRING, BENCHMARK, SHA, and root. Once these words detected, you can avoid the input.

UPDATE 1:

A user commented that this post is useless, OK! Here is what OWASP.ORG provided:


  Primary defenses: 
   
      Option #1: Use of Prepared Statements (Parameterized Queries) 
      Option #2: Use of Stored Procedures 
      Option #3: Escaping all User Supplied Input  
   
  Additional defenses: 
   
      Also Enforce: Least Privilege 
      Also Perform: White List Input Validation 


As you may know, claiming an article should be supported by a valid argument, at least by one reference! Otherwise, it's considered as an attack and a bad claim!

Update 2:

From the PHP manual, PHP: Prepared Statements - Manual:


  Escaping and SQL injection 
  
  Bound variables will be escaped automatically by the server. The
  server inserts their escaped values at the appropriate places into the
  statement template before execution. A hint must be provided to the
  server for the type of bound variable, to create an appropriate
  conversion. See the mysqli_stmt_bind_param() function for more
  information. 
  
  The automatic escaping of values within the server is sometimes
  considered a security feature to prevent SQL injection. The same
  degree of security can be achieved with non-prepared statements if
  input values are escaped correctly. 


Update 3:

I created test cases for knowing how PDO and MySQLi send the query to the MySQL server when using a prepared statement:

PDO:

$user = ""''1''""; // Malicious keyword
$sql = 'SELECT * FROM awa_user WHERE userame =:username';
$sth = $dbh->prepare($sql, array(PDO::ATTR_CURSOR => PDO::CURSOR_FWDONLY));
$sth->execute(array(':username' => $user));


Query Log:


    189 Query SELECT * FROM awa_user WHERE userame ='\'\'1\'\''
    189 Quit



MySQLi:

$stmt = $mysqli->prepare(""SELECT * FROM awa_user WHERE username =?"")) {
$stmt->bind_param(""s"", $user);
$user = ""''1''"";
$stmt->execute();


Query Log:


    188 Prepare   SELECT * FROM awa_user WHERE username =?
    188 Execute   SELECT * FROM awa_user WHERE username ='\'\'1\'\''
    188 Quit



It's clear that a prepared statement is also escaping the data, nothing else.

As also mentioned in the above statement,


  The automatic escaping of values within the server is sometimes considered a security feature to prevent SQL injection. The same degree of security can be achieved with non-prepared statements, if input values are escaped correctly


Therefore, this proves that data validation such as intval() is a good idea for integer values before sending any query. In addition, preventing malicious user data before sending the query is a correct and valid approach.

Please see this question for more detail: PDO sends raw query to MySQL while Mysqli sends prepared query, both produce the same result

References:


SQL Injection Cheat Sheet
SQL Injection
Information security
Security Principles
Data validation

    Using this PHP function mysql_escape_string() you can get a good prevention in a fast way.

For example: 

SELECT * FROM users WHERE name = '"".mysql_escape_string($name_from_html_form).""'


mysql_escape_string  Escapes a string for use in a mysql_query

For more prevention, you can add at the end ... 

wHERE 1=1   or  LIMIT 1


Finally you get:

SELECT * FROM users WHERE name = '"".mysql_escape_string($name_from_html_form).""' LIMIT 1

    A good idea is to use an object-relational mapper like Idiorm:

$user = ORM::for_table('user')
->where_equal('username', 'j4mie')
->find_one();

$user->first_name = 'Jamie';
$user->save();

$tweets = ORM::for_table('tweet')
    ->select('tweet.*')
    ->join('user', array(
        'user.id', '=', 'tweet.user_id'
    ))
    ->where_equal('user.username', 'j4mie')
    ->find_many();

foreach ($tweets as $tweet) {
    echo $tweet->text;
}


It not only saves you from SQL injections, but from syntax errors too! It also supports collections of models with method chaining to filter or apply actions to multiple results at once and multiple connections.
    Warning: the approach described in this answer only applies to very specific scenarios and isn't secure since SQL injection attacks do not only rely on being able to inject X=Y.

If the attackers are trying to hack into the form via PHP's $_GET variable or with the URL's query string, you would be able to catch them if they're not secure.

RewriteCond %{QUERY_STRING} ([0-9]+)=([0-9]+)
RewriteRule ^(.*) ^/track.php


Because 1=1, 2=2, 1=2, 2=1, 1+1=2, etc... are the common questions to an SQL database of an attacker. Maybe also it's used by many hacking applications.

But you must be careful, that you must not rewrite a safe query from your site. The code above is giving you a tip, to rewrite or redirect (it depends on you) that hacking-specific dynamic query string into a page that will store the attacker's IP address, or EVEN THEIR COOKIES, history, browser, or any other sensitive information, so you can deal with them later by banning their account or contacting authorities.
    The simple alternative to this problem could be solved by granting appropriate permissions in the database itself.
For example: if you are using a MySQL database then enter into the database through terminal or the UI provided and just follow this command:

 GRANT SELECT, INSERT, DELETE ON database TO username@'localhost' IDENTIFIED BY 'password';


This will restrict the user to only get confined with the specified query's only. Remove the delete permission and so the data would never get deleted from the query fired from the PHP page.
The second thing to do is to flush the privileges so that the MySQL refreshes the permissions and updates.

FLUSH PRIVILEGES; 


more information about flush.

To see the current privileges for the user fire the following query.

select * from mysql.user where User='username';


Learn more about GRANT.
    A simple way would be to use a PHP framework like CodeIgniter or Laravel which have inbuilt features like filtering and active-record so that you don't have to worry about these nuances.
    
  Security Warning: This answer is not in line with security best practices. Escaping is inadequate to prevent SQL injection, use prepared statements instead. Use the strategy outlined below at your own risk. (Also, mysql_real_escape_string() was removed in PHP 7.)
  
  Deprecated Warning: The mysql extension is deprecated at this time. we recommend using the PDO extension


I use three different ways to prevent my web application from being vulnerable to SQL injection.


Use of mysql_real_escape_string(), which is a pre-defined function in PHP, and this code add backslashes to the following characters: \x00, \n, \r, \, ', "" and \x1a. Pass the input values as parameters to minimize the chance of SQL injection.
The most advanced way is to use PDOs.


I hope this will help you.

Consider the following query:

$iId = mysql_real_escape_string(""1 OR 1=1"");
 $sSql = ""SELECT * FROM table WHERE id = $iId"";

mysql_real_escape_string() will not protect here. If you use single quotes (' ') around your variables inside your query is what protects you against this. Here is an solution below for this:

$iId = (int) mysql_real_escape_string(""1 OR 1=1"");
 $sSql = ""SELECT * FROM table WHERE id = $iId"";

This question has some good answers about this.

I suggest, using PDO is the best option.

Edit:

mysql_real_escape_string() is deprecated as of PHP 5.5.0. Use either mysqli or PDO.

An alternative to mysql_real_escape_string() is 

string mysqli_real_escape_string ( mysqli $link , string $escapestr )


Example:

$iId = $mysqli->real_escape_string(""1 OR 1=1"");
$mysqli->query(""SELECT * FROM table WHERE id = $iId"");

    
  Deprecated Warning:
  This answer's sample code (like the question's sample code) uses PHP's MySQL extension, which was deprecated in PHP 5.5.0 and removed entirely in PHP 7.0.0.
  
  Security Warning: This answer is not in line with security best practices. Escaping is inadequate to prevent SQL injection, use prepared statements instead. Use the strategy outlined below at your own risk. (Also, mysql_real_escape_string() was removed in PHP 7.)


Using PDO and MYSQLi is a good practice to prevent SQL injections, but if you really want to work with MySQL functions and queries, it would be better to use

mysql_real_escape_string

$unsafe_variable = mysql_real_escape_string($_POST['user_input']);


There are more abilities to prevent this: like identify - if the input is a string, number, char or array, there are so many inbuilt functions to detect this. Also, it would be better to use these functions to check input data.

is_string

$unsafe_variable = (is_string($_POST['user_input']) ? $_POST['user_input'] : '');


is_numeric

$unsafe_variable = (is_numeric($_POST['user_input']) ? $_POST['user_input'] : '');


And it is so much better to use those functions to check input data with mysql_real_escape_string.
    I've written this little function several years ago:

function sqlvprintf($query, $args)
{
    global $DB_LINK;
    $ctr = 0;
    ensureConnection(); // Connect to database if not connected already.
    $values = array();
    foreach ($args as $value)
    {
        if (is_string($value))
        {
            $value = ""'"" . mysqli_real_escape_string($DB_LINK, $value) . ""'"";
        }
        else if (is_null($value))
        {
            $value = 'NULL';
        }
        else if (!is_int($value) && !is_float($value))
        {
            die('Only numeric, string, array and NULL arguments allowed in a query. Argument '.($ctr+1).' is not a basic type, it\'s type is '. gettype($value). '.');
        }
        $values[] = $value;
        $ctr++;
    }
    $query = preg_replace_callback(
        '/{(\\d+)}/', 
        function($match) use ($values)
        {
            if (isset($values[$match[1]]))
            {
                return $values[$match[1]];
            }
            else
            {
                return $match[0];
            }
        },
        $query
    );
    return $query;
}

function runEscapedQuery($preparedQuery /*, ...*/)
{
    $params = array_slice(func_get_args(), 1);
    $results = runQuery(sqlvprintf($preparedQuery, $params)); // Run query and fetch results.   
    return $results;
}


This allows running statements in an one-liner C#-ish String.Format like:

runEscapedQuery(""INSERT INTO Whatever (id, foo, bar) VALUES ({0}, {1}, {2})"", $numericVar, $stringVar1, $stringVar2);


It escapes considering the variable type. If you try to parameterize table, column names, it would fail as it puts every string in quotes which is an invalid syntax.

SECURITY UPDATE: The previous str_replace version allowed injections by adding {#} tokens into user data. This preg_replace_callback version doesn't cause problems if the replacement contains these tokens.
    There are so many answers for PHP and MySQL, but here is code for PHP and Oracle for preventing SQL injection as well as regular use of oci8 drivers:

$conn = oci_connect($username, $password, $connection_string);
$stmt = oci_parse($conn, 'UPDATE table SET field = :xx WHERE ID = 123');
oci_bind_by_name($stmt, ':xx', $fieldval);
oci_execute($stmt);

    ","[2773, 9433, 1716, 1132, 884, 655, 578, 481, 514, 394, 312, 376, 312, 268, 228, 321, 275, 232, 239, 182, 227, 135, 153, 186, 172, 178, 127, 90, 131]",2019377,3778,2008-09-12T23:55:00,2022-03-27 09:08:47Z,php sql 
I need an unordered list without any bullets,"
                
I have created an unordered list. I feel the bullets in the unordered list are bothersome, so I want to remove them. 

Is it possible to have a list without bullets?
    You can remove bullets by setting the list-style-type to none on the CSS for the parent element (typically a <ul>), for example:

ul {
  list-style-type: none;
}


You might also want to add padding: 0 and margin: 0 to that if you want to remove indentation as well.

See Listutorial for a great walkthrough of list formatting techniques.
    If you're using Bootstrap, it has an ""unstyled"" class:

Remove the default list-style and left padding on list items (immediate children only).

Bootstrap 2:
<ul class=""unstyled"">
   <li>...</li>
</ul>

http://twitter.github.io/bootstrap/base-css.html#typography
Bootstrap 3 and 4:
<ul class=""list-unstyled"">
   <li>...</li>
</ul>

Bootstrap 3: http://getbootstrap.com/css/#type-lists
Bootstrap 4: https://getbootstrap.com/docs/4.3/content/typography/#unstyled
Bootstrap 5: https://getbootstrap.com/docs/5.0/content/typography/#unstyled
    You need to use list-style: none;

<ul style=""list-style: none;"">
    <li>...</li>
</ul>

    Small refinement to the previous answers: To make longer lines more readable if they spill over to additional screen lines:
ul, li {list-style-type: none;}

li {padding-left: 2em; text-indent: -2em;}

    If you wanted to accomplish this with pure HTML alone, this solution will work across all major browsers:
Description Lists
Simply using the following HTML:
    <dl>
      <dt>List Item 1</dt>
        <dd>Sub-Item 1.1</dd>
      <dt>List Item 2</dt>
        <dd>Sub-Item 2.1</dd>
        <dd>Sub-Item 2.2</dd>
        <dd>Sub-Item 2.3</dd>
      <dt>List Item 3</dt>
        <dd>Sub-Item 3.1</dd>
    </dl>

Example here: https://jsfiddle.net/zumcmvma/2/
Reference here: https://www.w3schools.com/tags/tag_dl.asp
    If you're unable to make it work at the <ul> level, you might need to place the list-style-type: none; at the <li> level:

<ul>
    <li style=""list-style-type: none;"">Item 1</li>
    <li style=""list-style-type: none;"">Item 2</li>
</ul>


You can create a CSS class to avoid this repetition:

<style>
ul.no-bullets li
{
    list-style-type: none;
}
</style>

<ul class=""no-bullets"">
    <li>Item 1</li>
    <li>Item 2</li>
</ul>


When necessary, use !important:

<style>
ul.no-bullets li
{
    list-style-type: none !important;
}
</style>

    To completely remove the ul default style:

    list-style-type: none;

    margin: 0;
    margin-block-start: 0;
    margin-block-end: 0;
    margin-inline-start: 0;
    margin-inline-end: 0;
    padding-inline-start: 0;

    I used list-style on both the ul and the li to remove the bullets. I wanted to replace the bullets with a custom character, in this case a 'dash'. That gives a nicely indented effect that works fine when the text wraps.
ul.dashed-list {
    list-style: none outside none;
}

ul.dashed-list li:before {
    content: ""\2014"";
    float: left;
    margin: 0 0 0 -27px;
    padding: 0;
}

ul.dashed-list li {
    list-style-type: none;
}<ul class=""dashed-list"">
  <li>text</li>
  <li>text</li>
</ul>

    ul{list-style-type:none;}

Just set the style of unordered list is none.
    This orders a list vertically without bullet points. In just one line!
li {
    display: block;
}

    If you are developing an existing theme, it's possible that the theme has a custom list style.
So if you cant't change the list style using list-style: none; in ul or li tags, first check with !important, because maybe some other line of style is overwriting your style. If !important fixed it, you should find a more specific selector and clear out the !important.
li {
    list-style: none !important;
}

If it's not the case, then check the li:before. If it contains the content, then do:
li:before {
    display: none;
}

     <div class=""custom-control custom-checkbox left"">
    <ul class=""list-unstyled"">
        <li>
         <label class=""btn btn-secondary text-left"" style=""width:100%;text-align:left;padding:2px;"">
           <input type=""checkbox"" style=""zoom:1.7;vertical-align:bottom;"" asp-for=""@Model[i].IsChecked"" class=""custom-control-input"" /> @Model[i].Title
         </label>
        </li>
     </ul>
</div>

    I tried and observed:

header ul {
   margin: 0;
   padding: 0;
}

    In case you want to keep things simple without resorting to CSS, I just put a &nbsp; in my code lines. I.e., <table></table>.

Yeah, it leaves a few spaces, but that's not a bad thing.
    You can hide them using ::marker pseudo-element.

Transparent ::marker


ul li::marker {
  color: transparent;
}

ul li::marker {
  color: transparent;
}

ul {
  padding-inline-start: 10px; /* Just to reset the browser initial padding */
}<ul>
  <li> Bullets are bothersome </li>
  <li> I want to remove them. </li>
  <li> Hey! ::marker to the rescue </li>
</ul>


::marker empty content


ul li::marker {
  content: """";
}

ul li::marker {
   content: """";
}<ul>
  <li> Bullets are bothersome </li>
  <li> I want to remove them </li>
  <li> Hey! ::marker to the rescue </li>
</ul>

It is better when you need to remove bullets from a specific list item.
ul li:nth-child(n)::marker { /* Replace n with the list item's position*/
   content: """";
}

ul li:not(:nth-child(2))::marker {
   content: """";
}<ul>
  <li> Bullets are bothersome </li>
  <li> But I can live with it using ::marker </li>
  <li> Not again though </li>
</ul>

    li{
list-style: none;
}

It removes bullets from all list items.
    It is possible. In a <style> element or a CSS file, making an unordered list without bullets looks like this:
ul {
  list-style-type: none;
}

Of course, you can also add ids and classes as well.
    ","[2761, 4012, 668, 224, 44, 6, 18, 5, 14, 1, 3, 1, -1, -1, -8, 0, 0, 0]",2270244,253,2009-06-22T13:57:53,2021-06-08 08:25:05Z,html css 
How does database indexing work? [closed],"
                    
            
        
            
                
                    
                        Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    
                
            
        
            
        
                
                    
                
            
                
                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                
                    Closed last month.
            The community reviewed whether to reopen this question last month and left it closed:
            
                    Original close reason(s) were not resolved
            

            
        
            
                    
                        Improve this question
                    
            

    

Given that indexing is so important as your data set increases in size, can someone explain how indexing works at a database-agnostic level?

For information on queries to index a field, check out How do I index a database column.
    Why is it needed?
When data is stored on disk-based storage devices, it is stored as blocks of data. These blocks are accessed in their entirety, making them the atomic disk access operation. Disk blocks are structured in much the same way as linked lists; both contain a section for data, a pointer to the location of the next node (or block), and both need not be stored contiguously.
Due to the fact that a number of records can only be sorted on one field, we can state that searching on a field that isnt sorted requires a Linear Search which requires (N+1)/2 block accesses (on average), where N is the number of blocks that the table spans. If that field is a non-key field (i.e. doesnt contain unique entries) then the entire tablespace must be searched at N block accesses.
Whereas with a sorted field, a Binary Search may be used, which has log2 N block accesses. Also since the data is sorted given a non-key field, the rest of the table doesnt need to be searched for duplicate values, once a higher value is found. Thus the performance increase is substantial.
What is indexing?
Indexing is a way of sorting a number of records on multiple fields. Creating an index on a field in a table creates another data structure which holds the field value, and a pointer to the record it relates to. This index structure is then sorted, allowing Binary Searches to be performed on it.
The downside to indexing is that these indices require additional space on the disk since the indices are stored together in a table using the MyISAM engine, this file can quickly reach the size limits of the underlying file system if many fields within the same table are indexed.
How does it work?
Firstly, lets outline a sample database table schema;

Field name       Data type      Size on disk
id (Primary key) Unsigned INT   4 bytes
firstName        Char(50)       50 bytes
lastName         Char(50)       50 bytes
emailAddress     Char(100)      100 bytes

Note: char was used in place of varchar to allow for an accurate size on disk value.
This sample database contains five million rows and is unindexed. The performance of several queries will now be analyzed. These are a query using the id (a sorted key field) and one using the firstName (a non-key unsorted field).
Example 1 - sorted vs unsorted fields
Given our sample database of r = 5,000,000 records of a fixed size giving a record length of R = 204 bytes and they are stored in a table using the MyISAM engine which is using the default block size B = 1,024 bytes. The blocking factor of the table would be bfr = (B/R) = 1024/204 = 5 records per disk block. The total number of blocks required to hold the table is N = (r/bfr) = 5000000/5 = 1,000,000 blocks.
A linear search on the id field would require an average of N/2 = 500,000 block accesses to find a value, given that the id field is a key field. But since the id field is also sorted, a binary search can be conducted requiring an average of log2 1000000 = 19.93 = 20 block accesses. Instantly we can see this is a drastic improvement.
Now the firstName field is neither sorted nor a key field, so a binary search is impossible, nor are the values unique, and thus the table will require searching to the end for an exact N = 1,000,000 block accesses. It is this situation that indexing aims to correct.
Given that an index record contains only the indexed field and a pointer to the original record, it stands to reason that it will be smaller than the multi-field record that it points to. So the index itself requires fewer disk blocks than the original table, which therefore requires fewer block accesses to iterate through. The schema for an index on the firstName field is outlined below;

Field name       Data type      Size on disk
firstName        Char(50)       50 bytes
(record pointer) Special        4 bytes

Note: Pointers in MySQL are 2, 3, 4 or 5 bytes in length depending on the size of the table.
Example 2  - indexing
Given our sample database of r = 5,000,000 records with an index record length of R = 54 bytes and using the default block size B = 1,024 bytes. The blocking factor of the index would be bfr = (B/R) = 1024/54 = 18 records per disk block. The total number of blocks required to hold the index is N = (r/bfr) = 5000000/18 = 277,778 blocks.
Now a search using the firstName field can utilize the index to increase performance. This allows for a binary search of the index with an average of log2 277778 = 18.08 = 19 block accesses. To find the address of the actual record, which requires a further block access to read, bringing the total to 19 + 1 = 20 block accesses, a far cry from the 1,000,000 block accesses required to find a firstName match in the non-indexed table.
When should it be used?
Given that creating an index requires additional disk space (277,778 blocks extra from the above example, a ~28% increase), and that too many indices can cause issues arising from the file systems size limits, careful thought must be used to select the correct fields to index.
Since indices are only used to speed up the searching for a matching field within the records, it stands to reason that indexing fields used only for output would be simply a waste of disk space and processing time when doing an insert or delete operation, and thus should be avoided. Also given the nature of a binary search, the cardinality or uniqueness of the data is important. Indexing on a field with a cardinality of 2 would split the data in half, whereas a cardinality of 1,000 would return approximately 1,000 records. With such a low cardinality the effectiveness is reduced to a linear sort, and the query optimizer will avoid using the index if the cardinality is less than 30% of the record number, effectively making the index a waste of space.
    Classic example ""Index in Books""

Consider a ""Book"" of 1000 pages, divided by 10 Chapters, each section with 100 pages.

Simple, huh?

Now, imagine you want to find a particular Chapter that contains a word ""Alchemist"". Without an index page, you have no other option than scanning through the entire book/Chapters. i.e: 1000 pages. 

This analogy is known as ""Full Table Scan"" in database world.



But with an index page, you know where to go! And more, to lookup any particular Chapter that matters, you just need to look over the index page, again and again, every time. After finding the matching index you can efficiently jump to that chapter by skipping the rest.

But then, in addition to actual 1000 pages, you will need another ~10 pages to show the indices, so totally 1010 pages.


  Thus, the index is a separate section that stores values of indexed
  column + pointer to the indexed row in a sorted order for efficient
  look-ups.


Things are simple in schools, isn't it? :P
    An index is just a data structure that makes the searching faster for a specific column in a database. This structure is usually a b-tree or a hash table but it can be any other logic structure.
    Now, lets say that we want to run a query to find all the details of any employees who are named Abc?

SELECT * FROM Employee 
WHERE Employee_Name = 'Abc'


What would happen without an index?

Database software would literally have to look at every single row in the Employee table to see if the Employee_Name for that row is Abc. And, because we want every row with the name Abc inside it, we can not just stop looking once we find just one row with the name Abc, because there could be other rows with the name Abc. So, every row up until the last row must be searched  which means thousands of rows in this scenario will have to be examined by the database to find the rows with the name Abc. This is what is called a full table scan

How a database index can help performance

The whole point of having an index is to speed up search queries by essentially cutting down the number of records/rows in a table that need to be examined. An index is a data structure (most commonly a B- tree) that stores the values for a specific column in a table. 

How does B-trees index work?

The reason B- trees are the most popular data structure for indexes is due to the fact that they are time efficient  because look-ups, deletions, and insertions can all be done in logarithmic time. And, another major reason B- trees are more commonly used is because the data that is stored inside the B- tree can be sorted. The RDBMS typically determines which data structure is actually used for an index. But, in some scenarios with certain RDBMSs, you can actually specify which data structure you want your database to use when you create the index itself.

How does a hash table index work?

The reason hash indexes are used is because hash tables are extremely efficient when it comes to just looking up values. So, queries that compare for equality to a string can retrieve values very fast if they use a hash index. 

For instance, the query we discussed earlier could benefit from a hash index created on the Employee_Name column. The way a hash index would work is that the column value will be the key into the hash table and the actual value mapped to that key would just be a pointer to the row data in the table. Since a hash table is basically an associative array, a typical entry would look something like Abc => 0x28939, where 0x28939 is a reference to the table row where Abc is stored in memory. Looking up a value like Abc in a hash table index and getting back a reference to the row in memory is obviously a lot faster than scanning the table to find all the rows with a value of Abc in the Employee_Name column.

The disadvantages of a hash index

Hash tables are not sorted data structures, and there are many types of queries which hash indexes can not even help with. For instance, suppose you want to find out all of the employees who are less than 40 years old. How could you do that with a hash table index? Well, its not possible because a hash table is only good for looking up key value pairs  which means queries that check for equality 

What exactly is inside a database index?
So, now you know that a database index is created on a column in a table, and that the index stores the values in that specific column. But, it is important to understand that a database index does not store the values in the other columns of the same table. For example, if we create an index on the Employee_Name column, this means that the Employee_Age and Employee_Address column values are not also stored in the index. If we did just store all the other columns in the index, then it would be just like creating another copy of the entire table  which would take up way too much space and would be very inefficient.

How does a database know when to use an index?
When a query like SELECT * FROM Employee WHERE Employee_Name = Abc  is run, the database will check to see if there is an index on the column(s) being queried. Assuming the Employee_Name column does have an index created on it, the database will have to decide whether it actually makes sense to use the index to find the values being searched  because there are some scenarios where it is actually less efficient to use the database index, and more efficient just to scan the entire table.

What is the cost of having a database index?

It takes up space  and the larger your table, the larger your index. Another performance hit with indexes is the fact that whenever you add, delete, or update rows in the corresponding table, the same operations will have to be done to your index. Remember that an index needs to contain the same up to the minute data as whatever is in the table column(s) that the index covers.

As a general rule, an index should only be created on a table if the data in the indexed column will be queried frequently.

See also 


What columns generally make good indexes?
How do database indexes work

    Simple Description!

The index is nothing but a data structure that stores the values for a specific column in a table. An index is created on a column of a table. 

Example: We have a database table called User with three columns  Name, Age and Address. Assume that the User table has thousands of rows.

Now, lets say that we want to run a query to find all the details of any users who are named 'John'. 
If we run the following query: 

SELECT * FROM User 
WHERE Name = 'John'


The database software would literally have to look at every single row in the User table to see if the Name for that row is John. This will take a long time.

This is where index helps us: index is used to speed up search queries by essentially cutting down the number of records/rows in a table that needs to be examined.  

How to create an index:

CREATE INDEX name_index
ON User (Name)


An index consists of column values(Eg: John) from one table, and those values are stored in a data structure.  


  So now the database will use the index to find employees named John
  because the index will presumably be sorted alphabetically by the
  Users name. And, because it is sorted, it means searching for a name
  is a lot faster because all names starting with a J will be right
  next to each other in the index!

    The first time I read this it was very helpful to me. Thank you.

Since then I gained some insight about the downside of creating indexes:
if you write into a table (UPDATE or INSERT) with one index, you have actually two writing operations in the file system. One for the table data and another one for the index data (and the resorting of it (and - if clustered - the resorting of the table data)). If table and index are located on the same hard disk this costs more time. Thus a table without an index (a heap) , would allow for quicker write operations. (if you had two indexes you would end up with three write operations, and so on)

However, defining two different locations on two different hard disks for index data and table data can decrease/eliminate the problem of increased cost of time. This requires definition of additional file groups with according files on the desired hard disks and definition of table/index location as desired.

Another problem with indexes is their fragmentation over time as data is inserted. REORGANIZE helps, you must write routines to have it done.

In certain scenarios a heap is more helpful than a table with indexes, 

e.g:- If you have lots of rivalling writes but only one nightly read outside business hours for reporting.

Also, a differentiation between clustered and non-clustered indexes is rather important. 

Helped me:- What do Clustered and Non clustered index actually mean?
    Just think of Database Index as Index of a book.

If you have a book about dogs and you want to find an information about let's say, German Shepherds, you could of course flip through all the pages of the book and find what you are looking for  - but this of course is time consuming and not very fast. 

Another option is that, you could just go to the Index section of the book and then find what you are looking for by using the Name of the entity you are looking ( in this instance, German Shepherds) and also looking at the page number to quickly find what you are looking for. 

In Database, the page number is referred to as a pointer which directs the database to the address on the disk where entity is located. Using the same German Shepherd analogy, we could have something like this (German Shepherd, 0x77129) where 0x77129 is the address on the disk where the row data for German Shepherd is stored. 

In short, an index is a data structure that stores the values for a specific column in a table so as to speed up query search.
    ","[2758, 3986, 551, 317, 193, 131, 260, 39]",1028925,1729,2008-08-04T10:07:12,2022-03-08 15:42:31Z,sql 
What is the --save option for npm install?,"
                
I saw some tutorial where the command was:
npm install --save

What does the --save option mean?
    Update npm 5:

As of npm 5.0.0, installed modules are added as a dependency by default, so the --save option is no longer needed. The other save options still exist and are listed in the documentation for npm install.

Original answer:

Before version 5, NPM simply installed a package under node_modules by default. When you were trying to install dependencies for your app/module, you would need to first install them, and then add them (along with the appropriate version number) to the dependencies section of your package.json.

The --save option instructed NPM to include the package inside of the dependencies section of your package.json automatically, thus saving you an additional step.

In addition, there are the complementary options --save-dev and --save-optional which save the package under devDependencies and optionalDependencies, respectively. This is useful when installing development-only packages, like grunt or your testing library.
    Update as of npm 5:
As of npm 5.0.0 (released in May 2017), installed
modules are added as a dependency by default, so the --save option
is no longer needed.
The other save options still exist and are listed in the documentation
for npm install.

Original Answer:
To add package in dependencies:
npm install my_dep --save

or
npm install my_dep -S

or
npm i my_dep -S

To add package in devDependencies
npm install my_test_framework --save-dev

or
npm install my_test_framework -D

or
npm i my_test_framework -D

package.json

    Update as of npm 5:
As of npm 5.0.0, installed modules are added as a dependency by default, so the --save option is no longer needed. The other save options still exist and are listed in the documentation for npm install.

Original answer:
It won't do anything if you don't have a package.json file.  Start by running npm init to create one.  Then calls to npm install --save or npm install --save-dev or npm install --save-optional will update the package.json to list your dependencies.
    according to NPM Doc



So it seems that by running npm install package_name, the package dependency should be automatically added to package.json right? 
    npm v6.x update
Now you can be using one of npm i or npm i -S or npm i -P to install and save module as a dependency.

npm i is the alias of npm install


npm i is equal to npm install, means default save module as a
dependency;
npm i -S is equal to npm install --save (npm v5-)
npm i -P is equal to npm install --save-prod (npm v5+)

check out your npm version
$ npm -v
6.14.4


get npm cli help info
$ npm -h

Usage: npm <command>

where <command> is one of:
    access, adduser, audit, bin, bugs, c, cache, ci, cit,
    clean-install, clean-install-test, completion, config,
    create, ddp, dedupe, deprecate, dist-tag, docs, doctor,
    edit, explore, fund, get, help, help-search, hook, i, init,
    install, install-ci-test, install-test, it, link, list, ln,
    login, logout, ls, org, outdated, owner, pack, ping, prefix,
    profile, prune, publish, rb, rebuild, repo, restart, root,
    run, run-script, s, se, search, set, shrinkwrap, star,
    stars, start, stop, t, team, test, token, tst, un,
    uninstall, unpublish, unstar, up, update, v, version, view,
    whoami

npm <command> -h  quick help on <command>
npm -l            display full usage info
npm help <term>   search for help on <term>
npm help npm      involved overview

Specify configs in the ini-formatted file:
    /Users/xgqfrms-mbp/.npmrc
or on the command line via: npm <command> --key value
Config info can be viewed via: npm help config

npm@6.14.4 /Users/xgqfrms-mbp/.nvm/versions/node/v12.18.0/lib/node_modules/npm

get npm install help

npm help install alias npm -h i

$ npm help install
# OR, alias
# $ npm -h i

npm install (with no args, in package dir)
npm install [<@scope>/]<pkg>
npm install [<@scope>/]<pkg>@<tag>
npm install [<@scope>/]<pkg>@<version>
npm install [<@scope>/]<pkg>@<version range>
npm install <alias>@npm:<name>
npm install <folder>
npm install <tarball file>
npm install <tarball url>
npm install <git:// url>
npm install <github username>/<github project>

aliases: i, isntall, add
common options: [--save-prod|--save-dev|--save-optional] [--save-exact] [--no-save]
  ~ 


refs
https://docs.npmjs.com/cli/install

    npm install --save or -S: When the following command is used with npm install this will save all your installed core packages into the dependency section in the package.json file. Core dependencies are those packages without which your application will not give the desired results. But as mentioned earlier, it is an unnecessary feature in the npm 5.0.0 version onwards.
npm install --save

    You can also use -S, -D or -P which are equivalent of saving the package to an app dependency, a dev dependency or prod dependency. See more NPM shortcuts below:

-v: --version
-h, -?, --help, -H: --usage
-s, --silent: --loglevel silent
-q, --quiet: --loglevel warn
-d: --loglevel info
-dd, --verbose: --loglevel verbose
-ddd: --loglevel silly
-g: --global
-C: --prefix
-l: --long
-m: --message
-p, --porcelain: --parseable
-reg: --registry
-f: --force
-desc: --description
-S: --save
-P: --save-prod
-D: --save-dev
-O: --save-optional
-B: --save-bundle
-E: --save-exact
-y: --yes
-n: --yes false
ll and la commands: ls --long


This list of shortcuts can be obtained by running the following command:

$ npm help 7 config

    As of npm 5, it is more favorable to use --save-prod (or -P) than --save but doing the same thing, as is stated in npm install. So far, --save still works if provided.
    npm install package_x --save

The given package (package_x) will be saved in package.json inside dependencies.
if you add 

npm install <<package_x>> --save-dev

then it will be saved inside devDependencies.
    npm i (Package name) --save 

Simplily, using above command we ll not need to write package name in your package.json file it ll auto add its name and dependency with version that you ll need at time when you go for production or setup another time.

npm help install 

Above command ll help find out more option and correct def.shown in pic

    npm install --save or npm install --save-dev why we choose 1 options between this two while installing package in our project. 

things is clear from the above answers that npm install --save will add entry in the dependency field in pacakage.json file and other one in dev-dependency.

So question arises why we need entry of our installing module in pacakge.json file because whenever we check-in code in git or giving our code to some one we always give it or check it without node-modules because it is very large in size and also available at common place so to avoid this we do that.

so then how other person will get all the modules that is specifically or needed for that project so answers is from the package.json file that have the entry of all the required packages for running or developing that project.

so after getting the code we simply need to run the npm install command it will read the package.json file and install the necessary required packages.
    As of npm 5, npm  will now save by default.
In case,if you would like npm to work in a similar old fashion (no autosave) to how it was working in previous versions, you can update the config option to enable autosave as below.

npm config set save false


To get the current setting, you can execute the following command:

npm config get save


Source:https://blog.pusher.com/what-you-need-know-npm-5/
    The easier (and more awesome) way to add dependencies to your package.json is to do so from the command line, flagging the npm install command with either --save or --save-dev, depending on how you'd like to use that dependency.
    When you are using --save in the npm command to install a package ,
this means that your project will install that dependencies in the production enviroment, for example if you install a library to manage dates.
npm install moment --save 
npm i moment -S (same result)


(this is for production enviroment)

npm install moment --save--dev 
npm i moment -D (same result)


(this is for development enviroment)

    ","[2756, 3219, 312, 174, 37, 16, 7, 18, 11, 10, 6, 6, 9, 5, 0]",927197,443,2013-10-24T23:54:11,2022-03-31 12:17:54Z,
How do I access environment variables in Python?,"
                
How do I get the value of an environment variable in Python?
    Environment variables are accessed through os.environ
import os
print(os.environ['HOME'])

Or you can see a list of all the environment variables using:
os.environ

As sometimes you might need to see a complete list!
# using get will return `None` if a key is not present rather than raise a `KeyError`
print(os.environ.get('KEY_THAT_MIGHT_EXIST'))

# os.getenv is equivalent, and can also give a default value instead of `None`
print(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))

The Python default installation location on Windows is C:\Python. If you want to find out while running python you can do:
import sys
print(sys.prefix)

    To check if the key exists (returns True or False)

'HOME' in os.environ


You can also use get() when printing the key; useful if you want to use a default.

print(os.environ.get('HOME', '/home/username/'))


where /home/username/ is the default
    Actually it can be done this way:
import os

for item, value in os.environ.items():
    print('{}: {}'.format(item, value))

Or simply:
for i, j in os.environ.items():
    print(i, j)

For viewing the value in the parameter:
print(os.environ['HOME'])

Or:
print(os.environ.get('HOME'))

To set the value:
os.environ['HOME'] = '/new/value'

    Import the os module:
import os

To get an environment variable:
os.environ.get('Env_var')

To set an environment variable:
# Set environment variables
os.environ['Env_var'] = 'Some Value'

    The original question (first part) was ""how to check environment variables in Python."" 

Here's how to check if $FOO is set: 

try:  
   os.environ[""FOO""]
except KeyError: 
   print ""Please set the environment variable FOO""
   sys.exit(1)

    You can access the environment variables using
import os
print os.environ

Try to see the content of the PYTHONPATH or PYTHONHOME environment variables. Maybe this will be helpful for your second question.
    As for the environment variables:
import os
print os.environ[""HOME""]

    import os
for a in os.environ:
    print('Var: ', a, 'Value: ', os.getenv(a))
print(""all done"")


That will print all of the environment variables along with their values.
    If you are planning to use the code in a production web application code, using any web framework like Django and Flask, use projects like envparse. Using it, you can read the value as your defined type.
from envparse import env
# will read WHITE_LIST=hello,world,hi to white_list = [""hello"", ""world"", ""hi""]
white_list = env.list(""WHITE_LIST"", default=[])
# Perfect for reading boolean
DEBUG = env.bool(""DEBUG"", default=False)

NOTE: kennethreitz's autoenv is a recommended tool for making project-specific environment variables. For those who are using autoenv, please note to keep the .env file private (inaccessible to public).
    There are also a number of great libraries. Envs, for example, will allow you to parse objects out of your environment variables, which is rad. For example:
from envs import env
env('SECRET_KEY') # 'your_secret_key_here'
env('SERVER_NAMES',var_type='list') #['your', 'list', 'here']

    Edited - October 2021
Following @Peter's comment, here's how you can test it:
main.py
#!/usr/bin/env python


from os import environ

# Initialize variables
num_of_vars = 50
for i in range(1, num_of_vars):
    environ[f""_BENCHMARK_{i}""] = f""BENCHMARK VALUE {i}""  

def stopwatch(repeat=1, autorun=True):
    """"""
    Source: https://stackoverflow.com/a/68660080/5285732
    stopwatch decorator to calculate the total time of a function
    """"""
    import timeit
    import functools
    
    def outer_func(func):
        @functools.wraps(func)
        def time_func(*args, **kwargs):
            t1 = timeit.default_timer()
            for _ in range(repeat):
                r = func(*args, **kwargs)
            t2 = timeit.default_timer()
            print(f""Function={func.__name__}, Time={t2 - t1}"")
            return r
        
        if autorun:
            try:
                time_func()
            except TypeError:
                raise Exception(f""{time_func.__name__}: autorun only works with no parameters, you may want to use @stopwatch(autorun=False)"") from None
        
        return time_func
    
    if callable(repeat):
        func = repeat
        repeat = 1
        return outer_func(func)
    
    return outer_func

@stopwatch(repeat=10000)
def using_environ():
    for item in environ:
        pass

@stopwatch
def using_dict(repeat=10000):
    env_vars_dict = dict(environ)
    for item in env_vars_dict:
        pass

python ""main.py""

# Output
Function=using_environ, Time=0.216224731
Function=using_dict, Time=0.00014206099999999888

If this is true ... It's 1500x faster to use a dict() instead of accessing environ directly.

A performance-driven approach - calling environ is expensive, so it's better to call it once and save it to a dictionary. Full example:
from os import environ


# Slower
print(environ[""USER""], environ[""NAME""])

# Faster
env_dict = dict(environ)
print(env_dict[""USER""], env_dict[""NAME""])

P.S- if you worry about exposing private environment variables, then sanitize env_dict after the assignment.
    You can also try this:
First, install python-decouple
pip install python-decouple

Import it in your file
from decouple import config

Then get the environment variable
SECRET_KEY=config('SECRET_KEY')

Read more about the Python library here.
    For Django, see Django-environ.
$ pip install django-environ

import environ

env = environ.Env(
    # set casting, default value
    DEBUG=(bool, False)
)
# reading .env file
environ.Env.read_env()

# False if not in os.environ
DEBUG = env('DEBUG')

# Raises Django's ImproperlyConfigured exception if SECRET_KEY not in os.environ
SECRET_KEY = env('SECRET_KEY')

    The tricky part of using nested for-loops in one-liners is that you have to use list comprehension. So in order to print all your environment variables, without having to import a foreign library, you can use:
python -c ""import os;L=[f'{k}={v}' for k,v in os.environ.items()]; print('\n'.join(L))""

    You should first import os using
import os

and then actually print the environment variable value
print(os.environ['yourvariable'])

of course, replace yourvariable as the variable you want to access.
    ","[2750, 4003, 311, 69, 24, 74, 57, 34, 30, 23, 14, 6, 7, 4, 1, 1]",2436458,362,2011-02-05T13:03:35,2022-04-01 12:14:16Z,python 
How can I check if a program exists from a Bash script?,"
                
How would I validate that a program exists, in a way that will either return an error and exit, or continue with the script?

It seems like it should be easy, but it's been stumping me.
    Answer
POSIX compatible:
command -v <the_command>

Example use:
if ! command -v <the_command> &> /dev/null
then
    echo ""<the_command> could not be found""
    exit
fi

For Bash specific environments:
hash <the_command> # For regular commands. Or...
type <the_command> # To check built-ins and keywords

Explanation
Avoid which. Not only is it an external process you're launching for doing very little (meaning builtins like hash, type or command are way cheaper), you can also rely on the builtins to actually do what you want, while the effects of external commands can easily vary from system to system.
Why care?

Many operating systems have a which that doesn't even set an exit status, meaning the if which foo won't even work there and will always report that foo exists, even if it doesn't (note that some POSIX shells appear to do this for hash too).
Many operating systems make which do custom and evil stuff like change the output or even hook into the package manager.

So, don't use which. Instead use one of these:
$ command -v foo >/dev/null 2>&1 || { echo >&2 ""I require foo but it's not installed.  Aborting.""; exit 1; }
$ type foo >/dev/null 2>&1 || { echo >&2 ""I require foo but it's not installed.  Aborting.""; exit 1; }
$ hash foo 2>/dev/null || { echo >&2 ""I require foo but it's not installed.  Aborting.""; exit 1; }

(Minor side-note: some will suggest 2>&- is the same 2>/dev/null but shorter  this is untrue.  2>&- closes FD 2 which causes an error in the program when it tries to write to stderr, which is very different from successfully writing to it and discarding the output (and dangerous!))
If your hash bang is /bin/sh then you should care about what POSIX says. type and hash's exit codes aren't terribly well defined by POSIX, and hash is seen to exit successfully when the command doesn't exist (haven't seen this with type yet).  command's exit status is well defined by POSIX, so that one is probably the safest to use.
If your script uses bash though, POSIX rules don't really matter anymore and both type and hash become perfectly safe to use. type now has a -P to search just the PATH and hash has the side-effect that the command's location will be hashed (for faster lookup next time you use it), which is usually a good thing since you probably check for its existence in order to actually use it.
As a simple example, here's a function that runs gdate if it exists, otherwise date:
gnudate() {
    if hash gdate 2>/dev/null; then
        gdate ""$@""
    else
        date ""$@""
    fi
}

Alternative with a complete feature set
You can use scripts-common to reach your need.
To check if something is installed, you can do:
checkBin <the_command> || errorMessage ""This tool requires <the_command>. Install it please, and then run this tool again.""

    The following is a portable way to check whether a command exists in $PATH and is executable:

[ -x ""$(command -v foo)"" ]


Example:

if ! [ -x ""$(command -v git)"" ]; then
  echo 'Error: git is not installed.' >&2
  exit 1
fi


The executable check is needed because bash returns a non-executable file if no executable file with that name is found in $PATH.

Also note that if a non-executable file with the same name as the executable exists earlier in $PATH, dash returns the former, even though the latter would be executed. This is a bug and is in violation of the POSIX standard. [Bug report] [Standard]

In addition, this will fail if the command you are looking for has been defined as an alias.
    It depends on whether you want to know whether it exists in one of the directories in the $PATH variable or whether you know the absolute location of it. If you want to know if it is in the $PATH variable, use

if which programname >/dev/null; then
    echo exists
else
    echo does not exist
fi


otherwise use

if [ -x /path/to/programname ]; then
    echo exists
else
    echo does not exist
fi


The redirection to /dev/null/ in the first example suppresses the output of the which program.
    I agree with lhunath to discourage use of which, and his solution is perfectly valid for Bash users. However, to be more portable, command -v shall be used instead:

$ command -v foo >/dev/null 2>&1 || { echo ""I require foo but it's not installed.  Aborting."" >&2; exit 1; }


Command command is POSIX compliant. See here for its specification: command - execute a simple command

Note: type is POSIX compliant, but type -P is not.
    Expanding on @lhunath's and @GregV's answers, here's the code for the people who want to easily put that check inside an if statement:

exists()
{
  command -v ""$1"" >/dev/null 2>&1
}


Here's how to use it:

if exists bash; then
  echo 'Bash exists!'
else
  echo 'Your system does not have Bash'
fi

    There are a ton of options here, but I was surprised no quick one-liners. This is what I used at the beginning of my scripts:

[[ ""$(command -v mvn)"" ]] || { echo ""mvn is not installed"" 1>&2 ; exit 1; }
[[ ""$(command -v java)"" ]] || { echo ""java is not installed"" 1>&2 ; exit 1; }


This is based on the selected answer here and another source.
    I have a function defined in my .bashrc that makes this easier.

command_exists () {
    type ""$1"" &> /dev/null ;
}


Here's an example of how it's used (from my .bash_profile.)

if command_exists mvim ; then
    export VISUAL=""mvim --nofork""
fi

    Command -v works fine if the POSIX_BUILTINS option is set for the <command> to test for, but it can fail if not. (It has worked for me for years, but I recently ran into one where it didn't work.)
I find the following to be more failproof:
test -x ""$(which <command>)""

Since it tests for three things: path, existence and execution permission.
    The which command might be useful. man which

It returns 0 if the executable is found and returns 1 if it's not found or not executable:

NAME

       which - locate a command

SYNOPSIS

       which [-a] filename ...

DESCRIPTION

       which returns the pathnames of the files which would
       be executed in the current environment, had its
       arguments been given as commands in a strictly
       POSIX-conformant shell. It does this by searching
       the PATH for executable files matching the names
       of the arguments.

OPTIONS

       -a     print all matching pathnames of each argument

EXIT STATUS

       0      if all specified commands are 
              found and executable

       1      if one or more specified commands is nonexistent
              or not executable

       2      if an invalid option is specified


The nice thing about which is that it figures out if the executable is available in the environment that which is run in - it saves a few problems...
    I wanted the same question answered but to run within a Makefile.

install:
    @if [[ ! -x ""$(shell command -v ghead)"" ]]; then \
        echo 'ghead does not exist. Please install it.'; \
        exit -1; \
    fi

    It could be simpler, just:

#!/usr/bin/env bash                                                                
set -x                                                                             

# if local program 'foo' returns 1 (doesn't exist) then...                                                                               
if ! type -P foo; then                                                             
    echo 'crap, no foo'                                                            
else                                                                               
    echo 'sweet, we have foo!'                                                    
fi                                                                                 


Change foo to vi to get the other condition to fire.
    Try using:

test -x filename


or

[ -x filename ]


From the Bash manpage under Conditional Expressions:


 -x file
          True if file exists and is executable.


    zsh only, but very useful for zsh scripting (e.g. when writing completion scripts):
The zsh/parameter module gives access to, among other things, the internal commands hash table. From man zshmodules:
THE ZSH/PARAMETER MODULE
       The zsh/parameter module gives access to some of the internal hash  ta
       bles used by the shell by defining some special parameters.


[...]

       commands
              This  array gives access to the command hash table. The keys are
              the names of external commands, the values are the pathnames  of
              the  files  that would be executed when the command would be in
              voked. Setting a key in this array defines a new entry  in  this
              table  in the same way as with the hash builtin. Unsetting a key
              as in `unset ""commands[foo]""' removes the entry  for  the  given
              key from the command hash table.


Although it is a loadable module, it seems to be loaded by default, as long as zsh is not used with --emulate.
example:
martin@martin ~ % echo $commands[zsh]
/usr/bin/zsh

To quickly check whether a certain command is available, just check if the key exists in the hash:
if (( ${+commands[zsh]} ))
then
  echo ""zsh is available""
fi

Note though that the hash will contain any files in $PATH folders, regardless of whether they are executable or not. To be absolutely sure, you have to spend a stat call on that:
if (( ${+commands[zsh]} )) && [[ -x $commands[zsh] ]]
then
  echo ""zsh is available""
fi

    If you check for program existence, you are probably going to run it later anyway. Why not try to run it in the first place?

if foo --version >/dev/null 2>&1; then
    echo Found
else
    echo Not found
fi


It's a more trustworthy check that the program runs than merely looking at PATH directories and file permissions.

Plus you can get some useful result from your program, such as its version.

Of course the drawbacks are that some programs can be heavy to start and some don't have a --version option to immediately (and successfully) exit.
    This will tell according to the location if the program exist or not:

    if [ -x /usr/bin/yum ]; then
        echo ""This is Centos""
    fi

    To use hash, as @lhunath suggests, in a Bash script:

hash foo &> /dev/null
if [ $? -eq 1 ]; then
    echo >&2 ""foo not found.""
fi


This script runs hash and then checks if the exit code of the most recent command, the value stored in $?, is equal to 1. If hash doesn't find foo, the exit code will be 1. If foo is present, the exit code will be 0.

&> /dev/null redirects standard error and standard output from hash so that it doesn't appear onscreen and echo >&2 writes the message to standard error.
    Check for multiple dependencies and inform status to end users

for cmd in latex pandoc; do
  printf '%-10s' ""$cmd""
  if hash ""$cmd"" 2>/dev/null; then
    echo OK
  else
    echo missing
  fi
done


Sample output:

latex     OK
pandoc    missing


Adjust the 10 to the maximum command length. It is not automatic, because I don't see a non-verbose POSIX way to do it:
How can I align the columns of a space separated table in Bash?

Check if some apt packages are installed with dpkg -s and install them otherwise.

See: Check if an apt-get package is installed and then install it if it's not on Linux

It was previously mentioned at: How can I check if a program exists from a Bash script?
    Use Bash builtins if you can:

which programname


...

type -P programname

    For those interested, none of the methodologies in previous answers work if you wish to detect an installed library. I imagine you are left either with physically checking the path (potentially for header files and such), or something like this (if you are on a Debian-based distribution):

dpkg --status libdb-dev | grep -q not-installed

if [ $? -eq 0 ]; then
    apt-get install libdb-dev
fi


As you can see from the above, a ""0"" answer from the query means the package is not installed. This is a function of ""grep"" - a ""0"" means a match was found, a ""1"" means no match was found.
    I had to check if Git was installed as part of deploying our CI server. My final Bash script was as follows (Ubuntu server):

if ! builtin type -p git &>/dev/null; then
  sudo apt-get -y install git-core
fi

    I'd say there isn't any portable and 100% reliable way due to dangling aliases. For example:

alias john='ls --color'
alias paul='george -F'
alias george='ls -h'
alias ringo=/


Of course, only the last one is problematic (no offence to Ringo!). But all of them are valid aliases from the point of view of command -v.

In order to reject dangling ones like ringo, we have to parse the output of the shell built-in alias command and recurse into them (command -v isn't a superior to alias here.) There isn't any portable solution for it, and even a Bash-specific solution is rather tedious.

Note that a solution like this will unconditionally reject alias ls='ls -F':

test() { command -v $1 | grep -qv alias }

    hash foo 2>/dev/null: works with Z shell (Zsh), Bash, Dash and ash.

type -p foo: it appears to work with Z shell, Bash and ash (BusyBox), but not Dash (it interprets -p as an argument).

command -v foo: works with Z shell, Bash, Dash, but not ash (BusyBox) (-ash: command: not found).

Also note that builtin is not available with ash and Dash.
    #!/bin/bash
a=${apt-cache show program}
if [[ $a == 0 ]]
then
echo ""the program doesn't exist""
else
echo ""the program exists""
fi

#program is not literal, you can change it to the program's name you want to check
    The hash-variant has one pitfall: On the command line you can for example type in

one_folder/process


to have process executed. For this the parent folder of one_folder must be in $PATH. But when you try to hash this command, it will always succeed:

hash one_folder/process; echo $? # will always output '0'

    I second the use of ""command -v"". E.g. like this:

md=$(command -v mkdirhier) ; alias md=${md:=mkdir}  # bash

emacs=""$(command -v emacs) -nw"" || emacs=nano
alias e=$emacs
[[ -z $(command -v jed) ]] && alias jed=$emacs

    To mimic Bash's type -P cmd, we can use the POSIX compliant env -i type cmd 1>/dev/null 2>&1.

man env
# ""The option '-i' causes env to completely ignore the environment it inherits.""
# In other words, there are no aliases or functions to be looked up by the type command.

ls() { echo 'Hello, world!'; }

ls
type ls
env -i type ls

cmd=ls
cmd=lsx
env -i type $cmd 1>/dev/null 2>&1 || { echo ""$cmd not found""; exit 1; }

    If there isn't any external type command available (as taken for granted here), we can use POSIX compliant env -i sh -c 'type cmd 1>/dev/null 2>&1':

# Portable version of Bash's type -P cmd (without output on stdout)
typep() {
   command -p env -i PATH=""$PATH"" sh -c '
      export LC_ALL=C LANG=C
      cmd=""$1""
      cmd=""`type ""$cmd"" 2>/dev/null || { echo ""error: command $cmd not found; exiting ..."" 1>&2; exit 1; }`""
      [ $? != 0 ] && exit 1
      case ""$cmd"" in
        *\ /*) exit 0;;
            *) printf ""%s\n"" ""error: $cmd"" 1>&2; exit 1;;
      esac
   ' _ ""$1"" || exit 1
}

# Get your standard $PATH value
#PATH=""$(command -p getconf PATH)""
typep ls
typep builtin
typep ls-temp


At least on Mac OS X v10.6.8 (SnowLeopard) using Bash 4.2.24(2) command -v ls does not match a moved /bin/ls-temp.
    Script

#!/bin/bash

# Commands found in the hash table are checked for existence before being
# executed and non-existence forces a normal PATH search.
shopt -s checkhash

function exists() {
 local mycomm=$1; shift || return 1

 hash $mycomm 2>/dev/null || \
 printf ""\xe2\x9c\x98 [ABRT]: $mycomm: command does not exist\n""; return 1;
}
readonly -f exists

exists notacmd
exists bash
hash
bash -c 'printf ""Fin.\n""'


Result

 [ABRT]: notacmd: command does not exist
hits    command
   0    /usr/bin/bash
Fin.

    Assuming you are already following safe shell practices:

set -eu -o pipefail
shopt -s failglob

./dummy --version 2>&1 >/dev/null


This assumes the command can be invoked in such a way that it does (almost) nothing, like reporting its version or showing help.

If the dummy command is not found, Bash exits with the following error...

./my-script: line 8: dummy: command not found


This is more useful and less verbose than the other command -v (and similar) answers because the error message is auto generated and also contains a relevant line number.
    GIT=/usr/bin/git                     # STORE THE RELATIVE PATH
# GIT=$(which git)                   # USE THIS COMMAND TO SEARCH FOR THE RELATIVE PATH

if [[ ! -e $GIT ]]; then             # CHECK IF THE FILE EXISTS
    echo ""PROGRAM DOES NOT EXIST.""
    exit 1                           # EXIT THE PROGRAM IF IT DOES NOT
fi

# DO SOMETHING ...

exit 0                               # EXIT THE PROGRAM IF IT DOES

    ","[2744, 3885, 725, 100, 225, 45, 13, 98, 13, 5, 5, 5, 26, 3, 10, 3, 18, 9, 5, 4, 0, 2, 6, 0, 0, 0, 0, 0, -1, -1, -2]",946321,973,2009-02-26T21:52:49,2022-03-05 16:17:49Z,bash 
How can I see the differences between two branches?,"
                
I have two branches: branch_1 and branch_2.
How can I see the differences between them?
    You want to use git diff.
git diff [<options>] <commit>..<commit> [--] [<path>]

Where <commit> is your branch name, the hash of a commit or a shorthand symbolic reference
For instance git diff abc123..def567 or git diff HEAD..origin/master
That will produce the diff between the tips of the two branches. If you'd prefer to find the diff from their common ancestor to test, you can use three dots instead of two:
git diff <commit>...<commit>

And if you just want to check which files differ, not how the content differs, use --name-only:
git diff --name-only <commit>..<commit>

Note that in the <commit>..<commit> (two dot) syntax, the dots are optional; the following is synonymous:
git diff commit1 commit2

    It's very simple. You just go to one branch (e.g. main is your branch).
Run the command
git checkout main
git diff branch2

    Code is simply git diff master..develop
Options:

You can add --name-only to only see the names of the files.
If you want to see the changes of specific files or folders. Then add -- folderOrFileName at the end.
If you want to compare the local branch with the remote one, then fetch --all to fetch all remote branches, and run git diff --name-only [branchName]..origin/[branchName]. For example git diff --name-only develop..origin/develop

    There are many different ways to compare branches, and it's depend on the specific use case you need.
Many times you want to compare because something broken and you want to see what has been changes, then fix it, and see again what changed before commiting.
Personally when I want to see the diff what I like to do:
git checkout branch_1 # checkout the oldest branch
git checkout -b compare-branch # create a new branch
git merge --no-commit --squash branch_2 # put files from the new branch in the working folder
git status # see file names that changes
git diff # see the content that changed.

Using this solution you will see the diff, you can also see only the file names using git status, and the most important part you will be able to execute branch_2 while seeing the diff (branch_2 is on the working tree). If something had broken you can editing the files and fix it. Anytime, you can type again git status or git diff to see the diff from the new edit to branch_a.
    You can simply show difference by-
git diff b1...b2
Or you can show commit difference using-
git log b1..b2
You can see commit difference in a nice graphical way using -
git log --oneline --graph --decorate --abbrev-commit b1..b2
    In Eclipse(J2EE version) ,  open ""Window --> Show view --> Git Repository"".
if you have checked out 2 local git branches for examples then you will have bunch of branches in Local section. select any 2 git local branches and do "" right click and select ""Compare with each other in Tree menu"".
Open view ""Git Tree Compare"" and u will be able to see side by side diff for all files.
    ","[2743, 3631, 63, 12, 7, 7, 0]",1829031,492,2012-03-23T05:47:48,2022-04-11 08:12:27Z,
How do you clone a Git repository into a specific folder?,"
                
Executing the command git clone git@github.com:whatever creates a directory in my current folder named whatever, and drops the contents of the Git repository into that folder:

/httpdocs/whatever/public


My problem is that I need the contents of the Git repository cloned into my current directory so that they appear in the proper location for the web server:

/httpdocs/public


I know how to move the files after I've cloned the repository, but this seems to break Git, and I'd like to be able to update just by calling git pull. How can I do this?
    Option A:
git clone git@github.com:whatever folder-name

Ergo, for right here use:
git clone git@github.com:whatever .

Option B:
Move the .git folder, too. Note that the .git folder is hidden in most graphical file explorers, so be sure to show hidden files.
mv /where/it/is/right/now/* /where/I/want/it/
mv /where/it/is/right/now/.* /where/I/want/it/

The first line grabs all normal files, the second line grabs dot-files. It is also possibe to do it in one line by enabling dotglob (i.e. shopt -s dotglob) but that is probably a bad solution if you are asking the question this answer answers.
Better yet:
Keep your working copy somewhere else, and create a symbolic link. Like this:
ln -s /where/it/is/right/now /the/path/I/want/to/use

For your case this would be something like:
ln -sfn /opt/projectA/prod/public /httpdocs/public

Which easily could be changed to test if you wanted it, i.e.:
ln -sfn /opt/projectA/test/public /httpdocs/public

without moving files around. Added -fn in case someone is copying these lines (-f is force,  -n avoid some often unwanted interactions with already and non-existing links).
If you just want it to work, use Option A, if someone else is going to look at what you have done, use Option C.
    The example I think a lot of people asking this question are after is this. If you are in the directory you want the contents of the git repository dumped to, run:

git clone git@github.com:whatever .


The ""."" at the end specifies the current folder as the checkout folder.
    You can use following git command to clone with custom directory name
git clone <git_repo_url> <your_custom_directory_name>

Note: You don't need to create your custom directory because it will create automatically
    Go into the folder.. If the folder is empty, then:

git clone git@github.com:whatever .


else

git init
git remote add origin PATH/TO/REPO
git fetch
git checkout -t origin/master

    To clone git repository into a specific folder, you can use -C <path> parameter, e.g.

git -C /httpdocs clone git@github.com:whatever


Although it'll still create a whatever folder on top of it, so to clone the content of the repository into current directory, use the following syntax:

cd /httpdocs
git clone git@github.com:whatever .


Note that cloning into an existing directory is only allowed when the directory is empty.

Since you're cloning into folder that is accessible for public, consider separating your Git repository from your working tree by using --separate-git-dir=<git dir> or exclude .git folder in your web server configuration (e.g. in .htaccess file).
    Basic Git Repository Cloning

You clone a repository with

git clone [url]


For example, if you want to clone the Stanford University Drupal Open Framework Git library called open_framework, you can do so like this:

$ git clone git://github.com/SU-SWS/open_framework.git


That creates a directory named open_framework (at your current local file system location), initializes a .git directory inside it, pulls down all the data for that repository, and checks out a working copy of the latest version. If you go into the newly created open_framework directory, youll see the project files in there, ready to be worked on or used.

Cloning a Repository Into a Specific Local Folder

If you want to clone the repository into a directory named something other than open_framework, you can specify that as the next command-line option:

$ git clone git:github.com/SU-SWS/open_framework.git mynewtheme


That command does the same thing as the previous one, but the target directory is called mynewtheme.

Git has a number of different transfer protocols you can use. The previous example uses the git:// protocol, but you may also see http(s):// or user@server:/path.git, which uses the SSH transfer protocol.
    To clone to Present Working Directory:

git clone https://github.com/link.git


To clone to Another Directory:

git clone https://github.com/link.git ./Folder1/Folder2


Hope it Helps :)
    Usage

git clone <repository>


Clone the repository located at the <repository> onto the local machine. The original repository can be located on the local filesystem or on a remote machine accessible via HTTP or SSH.

git clone <repo> <directory>


Clone the repository located at <repository> into the folder called <directory> on the local machine.

Source: Setting up a repository
    If you are in the directory you want the contents of the git repository dumped to, run:
git clone git@github.com:origin .

The ""."" at the end specifies the current folder as the checkout folder.
    If you want to clone into the current folder, you should try this:

git clone https://github.com/example/example.git ./

    When you move the files to where you want them, are you also moving the .git directory? Depending on your OS and configuration, this directory may be hidden.  

It contains the repo and the supporting files, while the project files that are in your /public directory are only the versions in the currently check-out commit (master branch by default).
    Clone:

git clone git@jittre.unfuddle.com:jittre/name.git


Clone the ""specific branch"":

git clone -b [branch-name] git@jittre.unfuddle.com:jittre/name.git

    From some reason this syntax is not standing out:
git clone repo-url [folder]
Here folder is an optional path to the local folder (which will be a local repository).
Git clone will also pull code from remote repository into the local repository.
In fact it is true:
git clone repo-url  =  git init + git remote add origin repo-url + git pull

    Make sure you remove the .git repository if you are trying to check thing out into the current directory.

rm -rf .git then git clone https://github.com/symfony/symfony-sandbox.git
    Here's how I would do it, but I have made an alias to do it for me.

$ cd ~Downloads/git; git clone https:git.foo/poo.git


There is probably a more elegant way of doing this, however I found this to be easiest for myself.

Here's the alias I created to speed things along.  I made it for zsh, but it should work just fine for bash or any other shell like fish, xyzsh, fizsh, and so on.

Edit ~/.zshrc, /.bashrc, etc. with your favorite editor (mine is Leafpad, so I would write $ leafpad ~/.zshrc).

My personal preference, however, is to make a zsh plugin to keep track of all my aliases. You can create a personal plugin for oh-my-zsh by running these commands:

$ cd ~/.oh-my-zsh/
$ cd plugins/
$ mkdir your-aliases-folder-name; cd your-aliases-folder-name
     # In my case '~/.oh-my-zsh/plugins/ev-aliases/ev-aliases'
$ leafpad your-zsh-aliases.plugin.zsh
     # Again, in my case 'ev-aliases.plugin.zsh'


Afterwards, add these lines to your newly created blank alises.plugin file:

# Git aliases
alias gc=""cd ~/Downloads/git; git clone ""


(From here, replace your name with mine.)

Then, in order to get the aliases to work, they (along with zsh) have to be sourced-in (or whatever it's called). To do so, inside your custom plugin document add this:

## Ev's Aliases

#### Remember to re-source zsh after making any changes with these commands:

#### These commands should also work, assuming ev-aliases have already been sourced before:

allsource=""source $ZSH/oh-my-zsh.sh ; source /home/ev/.oh-my-zsh/plugins/ev-aliases/ev-aliases.plugin.zsh; clear""
sourceall=""source $ZSH/oh-my-zsh.sh ; source /home/ev/.oh-my-zsh/plugins/ev-aliases/ev-aliases.plugin.zsh""
#### 

####################################

# git aliases

alias gc=""cd ~/Downloads/git; git clone ""
# alias gc=""git clone ""
# alias gc=""cd /your/git/folder/or/whatever; git clone ""

####################################


Save your oh-my-zsh plugin, and run allsource. If that does not seem to work, simply run source $ZSH/oh-my-zsh.sh; source /home/ev/.oh-my-zsh/plugins/ev-aliases/ev-aliases.plugin.zsh. That will load the plugin source which will allow you to use allsource from now on.



I'm in the process of making a Git repository with all of my aliases. Please feel free to check them out here: Ev's dot-files. Please feel free to fork and improve upon them to suit your needs.
    If you are using ssh for git cloning you can use the following command.

git -C path clone git@github.com:path_to_repo.git

eg:
git -C /home/ubuntu/ clone git@github.com:kennethreitz/requests.git would pull the git repository for requests to your /home/ubuntu/ path.
    For Windows user 

1> Open command prompt.
2> Change the directory to destination folder (Where you want to store your project in local machine.)
3> Now go to project setting online(From where you want to clone)
4> Click on clone, and copy the clone command.
5> Now enter the same on cmd .

It will start cloning saving on the selected folder you given .

    Regarding this line from the original post:


  ""I know how to move the files after I've cloned the repo, but this
  seems to break git""


I am able to do that and I don't see any issues so far with my add, commit, push, pull operations.

This approach is stated above, but just not broken down into steps.
Here's the steps that work for me:


clone the repo into any fresh temporary folder
cd into that root folder you just cloned locally
copy the entire contents of the folder, including the /.git directory - into any existing folder you like; (say an eclipse project that you want to merge with your repo)


The existing folder you just copied the files into , is now ready to interact with git.
    Although all of the answers above are good, I would like to propose a new method instead of using the symbolic link method in public html directory as proposed BEST in the accepted answer. You need to have access to your server virtual host configurations.

It is about configuring virtual host of your web server directly pointing to the repository directory. In Apache you can do it like:


  DocumentRoot /var/www/html/website/your-git-repo


Here is an example of a virtual host file:

<VirtualHost *:443>
    ServerName example.com

    DocumentRoot /path/to/your-git-repo
    ...
    ...
    ...
    ...
</VirtualHost>

    ","[2740, 3861, 627, 34, 251, 54, 108, 33, 17, 5, 19, 18, 16, 7, 13, 6, 4, -1, -5, 0]",2425974,473,2009-03-16T15:56:54,2021-08-11 12:55:34Z,
What is the difference between the 'COPY' and 'ADD' commands in a Dockerfile?,"
                
What is the difference between the COPY and ADD commands in a Dockerfile, and when would I use one over the other?

COPY <src> <dest>



  The COPY instruction will copy new files from <src> and add them to the container's filesystem at path <dest>


ADD <src> <dest>



  The ADD instruction will copy new files from <src> and add them to the container's filesystem at path <dest>.

    You should check the ADD and COPY documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that ADD can do more than COPY:

ADD allows <src> to be a URL
Referring to comments below, the ADD documentation states that:


If  is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.

Note that the Best practices for writing Dockerfiles suggests using COPY where the magic of ADD is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy keep_this_archive_intact.tar.gz into your container, but instead, you spray the contents onto your filesystem.
    COPY is

Same as 'ADD', but without the tar and remote URL handling.

Reference straight from the source code.
    When creating a Dockerfile, there are two commands that you can use to copy files/directories into it  ADD and COPY. Although there are slight differences in the scope of their function, they essentially perform the same task.
So, why do we have two commands, and how do we know when to use one or the other?
DOCKER ADD COMMAND
===
Lets start by noting that the ADD command is older than COPY. Since the launch of the Docker platform, the ADD instruction has been part of its list of commands.
The command copies files/directories to a file system of the specified container.
The basic syntax for the ADD command is:
ADD <src>  <dest>

It includes the source you want to copy (<src>) followed by the destination where you want to store it (<dest>). If the source is a directory, ADD copies everything inside of it (including file system metadata).
For instance, if the file is locally available and you want to add it to the directory of an image, you type:
ADD /source/file/path  /destination/path

ADD can also copy files from a URL. It can download an external file and copy it to the wanted destination. For example:
ADD http://source.file/url  /destination/path

An additional feature is that it copies compressed files, automatically extracting the content in the given destination. This feature only applies to locally stored compressed files/directories.
ADD source.file.tar.gz /temp

Bear in mind that you cannot download and extract a compressed file/directory from a URL. The command does not unpack external packages when copying them to the local filesystem.
DOCKER COPY COMMAND
===
Due to some functionality issues, Docker had to introduce an additional command for duplicating content  COPY.
Unlike its closely related ADD command, COPY only has only one assigned function. Its role is to duplicate files/directories in a specified location in their existing format. This means that it doesnt deal with extracting a compressed file, but rather copies it as-is.
The instruction can be used only for locally stored files. Therefore, you cannot use it with URLs to copy external files to your container.
To use the COPY instruction, follow the basic command format:
Type in the source and where you want the command to extract the content as follows:
COPY <src>  <dest> 

For example:
COPY /source/file/path  /destination/path 

Which command to use?(Best Practice)
Considering the circumstances in which the COPY command was introduced, it is evident that keeping ADD was a matter of necessity. Docker released an official document outlining best practices for writing Dockerfiles, which explicitly advises against using the ADD command.
Dockers official documentation notes that COPY should always be the go-to instruction as it is more transparent than ADD.
If you need to copy from the local build context into a container, stick to using COPY.
The Docker team also strongly discourages using ADD to download and copy a package from a URL. Instead, its safer and more efficient to use wget or curl within a RUN command. By doing so, you avoid creating an additional image layer and save space.
    From Docker docs:

ADD or COPY
Although ADD and COPY are functionally similar, generally speaking, COPY is preferred. Thats because its more transparent than ADD. COPY only supports the basic copying of local files into the container, while ADD has some features (like local-only tar extraction and remote URL support) that are not immediately obvious. Consequently, the best use for ADD is local tar file auto-extraction into the image, as in ADD rootfs.tar.xz /.

More: Best practices for writing Dockerfiles
    There is some official documentation on that point: Best Practices for Writing Dockerfiles 


  Because image size matters, using ADD to fetch packages from remote URLs is strongly discouraged; you should use curl or wget instead. That way you can delete the files you no longer need after they've been extracted and you won't have to add another layer in your image.


RUN mkdir -p /usr/src/things \
  && curl -SL http://example.com/big.tar.gz \
    | tar -xJC /usr/src/things \
  && make -C /usr/src/things all



  For other items (files, directories) that do not require ADDs tar auto-extraction capability, you should always use COPY.

    COPY copies a file/directory from your host to your image.

ADD copies a file/directory from your host to your image, but can also fetch remote URLs, extract TAR files, etc... 

Use COPY for simply copying files and/or directories into the build context. 

Use ADD for downloading remote resources, extracting TAR files, etc..
    If you want to add a xx.tar.gz to a /usr/local in container, unzip it, and then remove the useless compressed package.

For COPY:

COPY resources/jdk-7u79-linux-x64.tar.gz /tmp/
RUN tar -zxvf /tmp/jdk-7u79-linux-x64.tar.gz -C /usr/local
RUN rm /tmp/jdk-7u79-linux-x64.tar.gz


For ADD:

ADD resources/jdk-7u79-linux-x64.tar.gz /usr/local/


ADD supports local-only tar extraction. Besides it, COPY will use three layers, but ADD only uses one layer.
    
COPY doesn't support <src> with URL scheme.
COPY doesn't unpack compression file.
For instruction <src> <dest>, if <src> is a tar compression file and <dest>doesn't end with a trailing slash:
ADD consider <dest> as a directory and unpack <src> to it.
COPY consider <dest> as a file and write <src> to it.
COPY support to overwrite build context by --from arg.

    Source: https://nickjanetakis.com/blog/docker-tip-2-the-difference-between-copy-and-add-in-a-dockerile:

COPY and ADD are both Dockerfile instructions that serve similar purposes. They let you copy files from a specific location into a Docker image.
COPY takes in a src and destination. It only lets you copy in a local file or directory from your host (the machine building the Docker image) into the Docker image itself.
ADD lets you do that too, but it also supports 2 other sources. First, you can use a URL instead of a local file / directory. Secondly, you can extract a tar file from the source directly into the destination
A valid use case for ADD is when you want to extract a local tar file into a specific directory in your Docker image.
If youre copying in local files to your Docker image, always use COPY because its more explicit.

    From Docker docs:
https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#add-or-copy

""Although ADD and COPY are functionally similar, generally speaking, COPY is preferred. Thats because its more transparent than ADD. COPY only supports the basic copying of local files into the container, while ADD has some features (like local-only tar extraction and remote URL support) that are not immediately obvious. Consequently, the best use for ADD is local tar file auto-extraction into the image, as in ADD rootfs.tar.xz /.

If you have multiple Dockerfile steps that use different files from your context, COPY them individually, rather than all at once. This will ensure that each steps build cache is only invalidated (forcing the step to be re-run) if the specifically required files change.

For example:

 COPY requirements.txt /tmp/
 RUN pip install --requirement /tmp/requirements.txt
 COPY . /tmp/


Results in fewer cache invalidations for the RUN step, than if you put the COPY . /tmp/ before it.

Because image size matters, using ADD to fetch packages from remote URLs is strongly discouraged; you should use curl or wget instead. That way you can delete the files you no longer need after theyve been extracted and you wont have to add another layer in your image. For example, you should avoid doing things like:

 ADD http://example.com/big.tar.xz /usr/src/things/
 RUN tar -xJf /usr/src/things/big.tar.xz -C /usr/src/things
 RUN make -C /usr/src/things all


And instead, do something like:

 RUN mkdir -p /usr/src/things \
     && curl -SL htt,p://example.com/big.tar.xz \
     | tar -xJC /usr/src/things \
     && make -C /usr/src/things all


For other items (files, directories) that do not require ADDs tar auto-extraction capability, you should always use COPY.""
    Since Docker 17.05 COPY is used with the --from flag in multi-stage builds to copy artifacts from previous build stages to the current build stage.

from the documentation


  Optionally COPY accepts a flag --from=<name|index> that can be used to set the source location to a previous build stage (created with FROM .. AS ) that will be used instead of a build context sent by the user. 

    ADD instruction copies files or folders from a local or remote source and adds them to the container's file system. It used to copy local files, those must be in the working directory. ADD instruction unpacks local .tar files to the destination image directory.
Example
ADD http://someserver.com/filename.pdf /var/www/html

COPY copies files from the working directory and adds them to the container's file system. It is not possible to copy a remote file using its URL with this Dockerfile instruction.
Example
COPY Gemfile Gemfile.lock ./
COPY ./src/ /var/www/html/

    If you have a foo.tar.gz file, comparing the following command.
The ADD command creates less layers than the COPY command, and saves a lot of net traffic when pushing docker image.
ADD foo.tar.gz /

COPY foo.tar.gz /
RUN tar -zxvf foo.tar.gz
RUN rm -rf foo.tar.gz

    Let's say you have a tar file and you want to uncompress it after placing it in your container, remove it, you can use the COPY command to do this. Butt he various commands would be 1) Copy the tar file to the destination, 2). Uncompress it, 3) Remove the tar file. If you did this in 3 steps then there will be a new image created after each step. You can do this in one step using & but it becomes a hassle.
But you used ADD, then Docker will take care of everything for you and only one intermediate image will be created.
    ADD and COPY both have same functionality of copying files and directories from source to destination but ADD has extra of file extraction and URL file extraction functionality. The best practice is to use COPY in only copy operation only avoid ADD is many areas. The link will explain it with some simple examples difference between COPY and ADD in dockerfile
    docker build -t {image name} -v {host directory}:{temp build directory} .


This is another way to copy files into an image. The -v option temporarily creates a volume that us used during the build process.  

This is different that other volumes because it mounts a host directory for the build only.  Files can be copied using a standard cp command.  

Also, like the curl and wget, it can be run in a command stack (runs in a single container) and not multiply the image size.  ADD and COPY are not stackable because they run in a standalone container and subsequent commands on those files that execute in additional containers will multiply the image size:

With the options set thus:

-v /opt/mysql-staging:/tvol


The following will execute in one container:

RUN cp -r /tvol/mysql-5.7.15-linux-glibc2.5-x86_64 /u1 && \
    mv /u1/mysql-5.7.15-linux-glibc2.5-x86_64 /u1/mysql && \

    mkdir /u1/mysql/mysql-files && \
    mkdir /u1/mysql/innodb && \
    mkdir /u1/mysql/innodb/libdata && \
    mkdir /u1/mysql/innodb/innologs && \
    mkdir /u1/mysql/tmp && \

    chmod 750 /u1/mysql/mysql-files && \
    chown -R mysql /u1/mysql && \
    chgrp -R mysql /u1/mysql

    ","[2739, 2664, 652, 40, 139, 158, 45, 59, 9, 15, 22, 9, 5, 2, 2, 1, -1]",900855,388,2014-07-25T14:31:20,2022-03-19 15:05:23Z,
How to change the author and committer name and e-mail of multiple commits in Git?,"
                
I was writing a simple script on the school computer, and committing the changes to Git (in a repo that was in my pen drive, cloned from my computer at home). After several commits, I realized I was committing stuff as the root user.
Is there any way to change the author of these commits to my name?
    
This answer uses git-filter-branch, for which the docs now give this warning:
git filter-branch has a plethora of pitfalls that can produce non-obvious manglings of the intended history rewrite (and can leave you with little time to investigate such problems since it has such abysmal performance). These safety and performance issues cannot be backward compatibly fixed and as such, its use is not recommended. Please use an alternative history filtering tool such as git filter-repo. If you still need to use git filter-branch, please carefully read SAFETY (and PERFORMANCE) to learn about the land mines of filter-branch, and then vigilantly avoid as many of the hazards listed there as reasonably possible.

Changing the author (or committer) would require rewriting all of the history.  If you're okay with that and think it's worth it then you should check out git filter-branch.  The manual page includes several examples to get you started.  Also note that you can use environment variables to change the name of the author, committer, dates, etc. -- see the ""Environment Variables"" section of the git manual page.
Specifically, you can fix all the wrong author names and emails for all branches and tags with this command (source: GitHub help):
#!/bin/sh

git filter-branch --env-filter '
OLD_EMAIL=""your-old-email@example.com""
CORRECT_NAME=""Your Correct Name""
CORRECT_EMAIL=""your-correct-email@example.com""
if [ ""$GIT_COMMITTER_EMAIL"" = ""$OLD_EMAIL"" ]
then
    export GIT_COMMITTER_NAME=""$CORRECT_NAME""
    export GIT_COMMITTER_EMAIL=""$CORRECT_EMAIL""
fi
if [ ""$GIT_AUTHOR_EMAIL"" = ""$OLD_EMAIL"" ]
then
    export GIT_AUTHOR_NAME=""$CORRECT_NAME""
    export GIT_AUTHOR_EMAIL=""$CORRECT_EMAIL""
fi
' --tag-name-filter cat -- --branches --tags

For using alternative history filtering tool git filter-repo, you can first install it and construct a git-mailmap according to the format of gitmailmap.
Proper Name <proper@email.xx> Commit Name <commit@email.xx>

And then run filter-repo with the created mailmap:
git filter-repo --mailmap git-mailmap

    NOTE: This answer changes SHA1s, so take care when using it on a branch that has already been pushed. If you only want to fix the spelling of a name or update an old email, git lets you do this without rewriting history using .mailmap. See my other answer.
Using Rebase
First, if you haven't already done so, you will likely want to fix your name in git-config:
git config --global user.name ""New Author Name""
git config --global user.email ""<email@address.com>""

This is optional, but it will also make sure to reset the committer name, too, assuming that's what you need.
To rewrite metadata for a range of commits using a rebase, do
git rebase -r <some commit before all of your bad commits> \
    --exec 'git commit --amend --no-edit --reset-author'

--exec will run the git commit step after each commit is rewritten (as if you ran git commit && git rebase --continue repeatedly).
If you also want to change your first commit (also called the 'root' commit), you will have to add --root to the rebase call.
This will change both the committer and the author to your user.name/user.email configuration.  If you did not want to change that config, you can use --author ""New Author Name <email@address.com>"" instead of --reset-author.  Note that doing so will not update the committer -- just the author.
Single Commit
If you just want to change the most recent commit, a rebase is not necessary. Just amend the commit:
 git commit --amend --no-edit --reset-author

For older Git clients (pre-July 2020)
-r,--rebase-merges may not exist for you.  As a replacement, you can use -p.  Note that -p has serious issues and is now deprecated.
    One liner, but be careful if you have a multi-user repository - this will change all commits to have the same (new) author and committer.

git filter-branch -f --env-filter ""GIT_AUTHOR_NAME='Newname'; GIT_AUTHOR_EMAIL='new@email'; GIT_COMMITTER_NAME='Newname'; GIT_COMMITTER_EMAIL='new@email';"" HEAD


With linebreaks in the string (which is possible in bash):

git filter-branch -f --env-filter ""
    GIT_AUTHOR_NAME='Newname'
    GIT_AUTHOR_EMAIL='new@email'
    GIT_COMMITTER_NAME='Newname'
    GIT_COMMITTER_EMAIL='new@email'
  "" HEAD

    A single command to change the author for the last N commits:
git rebase -i HEAD~N -x ""git commit --amend --author 'Author Name <author.name@mail.com>' --no-edit""

NOTES

replace HEAD~N with the reference until where you want to rewrite your commits. This can be a hash, HEAD~4, a branch name, ...
the --no-edit flag makes sure the git commit --amend doesn't ask an extra confirmation
when you use git rebase -i, you can manually select the commits where to change the author,

the file you edit will look like this:
pick 897fe9e simplify code a little
exec git commit --amend --author 'Author Name <author.name@mail.com>' --no-edit
pick abb60f9 add new feature
exec git commit --amend --author 'Author Name <author.name@mail.com>' --no-edit
pick dc18f70 bugfix
exec git commit --amend --author 'Author Name <author.name@mail.com>' --no-edit

You can then still modify some lines to see where you want to change the author. This gives you a nice middle ground between automation and control: you see the steps that will run, and once you save everything will be applied at once.
Note that if you already fixed the author information with git config user.name <your_name> and git config user.email <your_email>, you can also use this command:
git rebase -i HEAD~N -x ""git commit --amend --reset-author --no-edit""

    It happens when you do not have a $HOME/.gitconfig initialized. You may fix this as:
git config --global user.name ""you name""
git config --global user.email you@domain.com
git commit --amend --reset-author

Tested with git version 1.7.5.4.
Note that this fixes only the last commit.
    A safer alternative to git's filter-branch is filter-repo tool as suggested by git docs here.
git filter-repo --commit-callback '
  old_email = b""your-old-email@example.com""
  correct_name = b""Your Correct Name""
  correct_email = b""your-correct-email@example.com""
  
  if commit.committer_email == old_email :
    commit.committer_name = correct_name
    commit.committer_email = correct_email

  if commit.author_email == old_email : 
    commit.author_name = correct_name
    commit.author_email = correct_email
  '

The above command mirrors the logic used in this script but uses filter-repo instead of filter-branch.
The code body after commit-callback option is basically python code used for processing commits. You can write your own logic in python here. See more about commit object and its attributes here.
Since filter-repo tool is not bundled with git you need to install it separately.
See Prerequisties and Installation Guide
If you have a python env >= 3.5, you can use pip to install it.
pip3 install git-filter-repo

Note: It is strongly recommended to try filter-repo tool on a fresh clone. Also remotes are removed once the operation is done. Read more on why remotes are removed here. Also read the limitations of this tool under INTERNALS section.
    In the case where just the top few commits have bad authors, you can do this all inside git rebase -i using the exec command and the --amend commit, as follows:

git rebase -i HEAD~6 # as required


which presents you with the editable list of commits:

pick abcd Someone else's commit
pick defg my bad commit 1
pick 1234 my bad commit 2


Then add exec ... --author=""..."" lines after all lines with bad authors:

pick abcd Someone else's commit
pick defg my bad commit 1
exec git commit --amend --author=""New Author Name <email@address.com>"" -C HEAD
pick 1234 my bad commit 2
exec git commit --amend --author=""New Author Name <email@address.com>"" -C HEAD


save and exit editor (to run).

This solution may be longer to type than some others, but it's highly controllable - I know exactly what commits it hits.

Thanks to @asmeurer for the inspiration.
    I should point out that if the only problem is that the author/email is different from your usual, this is not a problem.  The correct fix is to create a file called .mailmap at the base of the directory with lines like
Name you want <email you want> Name you don't want <email you don't want>

And from then on, commands like git shortlog will consider those two names to be the same (unless you specifically tell them not to).  See https://schacon.github.io/git/git-shortlog.html for more information.
This has the advantage of all the other solutions here in that you don't have to rewrite history, which can cause problems if you have an upstream, and is always a good way to accidentally lose data.
Of course, if you committed something as yourself and it should really be someone else, and you don't mind rewriting history at this point, changing the commit author is probably a good idea for attribution purposes (in which case I direct you to my other answer here).
    You can also do:

git filter-branch --commit-filter '
        if [ ""$GIT_COMMITTER_NAME"" = ""<Old Name>"" ];
        then
                GIT_COMMITTER_NAME=""<New Name>"";
                GIT_AUTHOR_NAME=""<New Name>"";
                GIT_COMMITTER_EMAIL=""<New Email>"";
                GIT_AUTHOR_EMAIL=""<New Email>"";
                git commit-tree ""$@"";
        else
                git commit-tree ""$@"";
        fi' HEAD


Note, if you are using this command in the Windows command prompt, then you need to use "" instead of ':

git filter-branch --commit-filter ""
        if [ ""$GIT_COMMITTER_NAME"" = ""<Old Name>"" ];
        then
                GIT_COMMITTER_NAME=""<New Name>"";
                GIT_AUTHOR_NAME=""<New Name>"";
                GIT_COMMITTER_EMAIL=""<New Email>"";
                GIT_AUTHOR_EMAIL=""<New Email>"";
                git commit-tree ""$@"";
        else
                git commit-tree ""$@"";
        fi"" HEAD

    Github originally had a nice solution (broken link), which was the following shell script:
#!/bin/sh

git filter-branch --env-filter '

an=""$GIT_AUTHOR_NAME""
am=""$GIT_AUTHOR_EMAIL""
cn=""$GIT_COMMITTER_NAME""
cm=""$GIT_COMMITTER_EMAIL""

if [ ""$GIT_COMMITTER_EMAIL"" = ""your@email.to.match"" ]
then
    cn=""Your New Committer Name""
    cm=""Your New Committer Email""
fi
if [ ""$GIT_AUTHOR_EMAIL"" = ""your@email.to.match"" ]
then
    an=""Your New Author Name""
    am=""Your New Author Email""
fi

export GIT_AUTHOR_NAME=""$an""
export GIT_AUTHOR_EMAIL=""$am""
export GIT_COMMITTER_NAME=""$cn""
export GIT_COMMITTER_EMAIL=""$cm""
'

    For reset ALL commits (including first commit) to current user and current timestamp:
git rebase --root --exec ""git commit --amend --no-edit --date 'now' --reset-author""

    The fastest, easiest way to do this is to use the --exec argument of git rebase:

git rebase -i -p --exec 'git commit --amend --reset-author --no-edit'


This will create a todo-list that looks like this:

pick ef11092 Blah blah blah
exec git commit --amend --reset-author --no-edit
pick 52d6391 Blah bloh bloo
exec git commit --amend --reset-author --no-edit
pick 30ebbfe Blah bluh bleh
exec git commit --amend --reset-author --no-edit
...


and this will work all automatically, which works when you have hundreds of commits.
    You can use this as a alias so you can do:

git change-commits GIT_AUTHOR_NAME ""old name"" ""new name""


or for the last 10 commits:

git change-commits GIT_AUTHOR_EMAIL ""old@email.com"" ""new@email.com"" HEAD~10..HEAD


Add to ~/.gitconfig:

[alias]
    change-commits = ""!f() { VAR=$1; OLD=$2; NEW=$3; shift 3; git filter-branch --env-filter \""if [[ \\\""$`echo $VAR`\\\"" = '$OLD' ]]; then export $VAR='$NEW'; fi\"" $@; }; f ""


Source: https://github.com/brauliobo/gitconfig/blob/master/configs/.gitconfig

Hope it is useful.
    For a single commit:

git commit --amend --author=""Author Name <email@address.com>""


(extracted from asmeurer's answer)
    As docgnome mentioned, rewriting history is dangerous and will break other people's repositories.

But if you really want to do that and you are in a bash environment (no problem in Linux, on Windows, you can use git bash, that is provided with the installation of git), use git filter-branch:

git filter-branch --env-filter '
  if [ $GIT_AUTHOR_EMAIL = bad@email ];
    then GIT_AUTHOR_EMAIL=correct@email;
  fi;
export GIT_AUTHOR_EMAIL'


To speed things up, you can specify a range of revisions you want to rewrite:

git filter-branch --env-filter '
  if [ $GIT_AUTHOR_EMAIL = bad@email ];
    then GIT_AUTHOR_EMAIL=correct@email;
  fi;
export GIT_AUTHOR_EMAIL' HEAD~20..HEAD

    This is a more elaborated version of @Brian's version:

To change the author and committer, you can do this (with linebreaks in the string which is possible in bash):

git filter-branch --env-filter '
    if [ ""$GIT_COMMITTER_NAME"" = ""<Old name>"" ];
    then
        GIT_COMMITTER_NAME=""<New name>"";
        GIT_COMMITTER_EMAIL=""<New email>"";
        GIT_AUTHOR_NAME=""<New name>"";
        GIT_AUTHOR_EMAIL=""<New email>"";
    fi' -- --all


You might get one of these errors:


The temporary directory exists already
Refs starting with refs/original exists already
(this means another filter-branch has been run previously on the repository and the then original branch reference is backed up at refs/original)


If you want to force the run in spite of these errors, add the --force flag:

git filter-branch --force --env-filter '
    if [ ""$GIT_COMMITTER_NAME"" = ""<Old name>"" ];
    then
        GIT_COMMITTER_NAME=""<New name>"";
        GIT_COMMITTER_EMAIL=""<New email>"";
        GIT_AUTHOR_NAME=""<New name>"";
        GIT_AUTHOR_EMAIL=""<New email>"";
    fi' -- --all


A little explanation of the -- --all option might be needed: It makes the filter-branch work on all revisions on all refs (which includes all branches). This means, for example, that tags are also rewritten and is visible on the rewritten branches.

A common ""mistake"" is to use HEAD instead, which means filtering all revisions on just the current branch. And then no tags (or other refs) would exist in the rewritten branch.
    If you want to (easily) change the author for the current branch I would use something like this:

# update author for everything since origin/master
git rebase \
  -i origin/master \
  --exec 'git commit --amend --no-edit --author=""Author Name <author.name@email.co.uk>""'

    Note that git stores two different e-mail addresses, one for the committer (the person who committed the change) and another one for the author (the person who wrote the change).

The committer information isn't displayed in most places, but you can see it with git log -1 --format=%cn,%ce (or use show instead of log to specify a particular commit).

While changing the author of your last commit is as simple as git commit --amend --author ""Author Name <email@example.com>"", there is no one-liner or argument to do the same to the committer information.

The solution is to (temporarily, or not) change your user information, then amend the commit, which will update the committer to your current information:

git config user.email my_other_email@example.com 
git commit --amend

    For all the commits, my solution:
git rebase -i --root -x ""git commit --amend --author 'bedorlan <bedorlan@gmail.com>' --no-edit""

    When taking over an unmerged commit from another author, there is an easy way to handle this.

git commit --amend --reset-author
    I use the following to rewrite the author for an entire repository, including tags and all branches:

git filter-branch --tag-name-filter cat --env-filter ""
  export GIT_AUTHOR_NAME='New name';
  export GIT_AUTHOR_EMAIL='New email'
"" -- --all


Then, as described in the MAN page of filter-branch, remove all original refs backed up by filter-branch (this is destructive, backup first):

git for-each-ref --format=""%(refname)"" refs/original/ | \
xargs -n 1 git update-ref -d

    I have tried the scripts above it did not work for me, this fixed my issue:

use Git's ""filter-branch"" command. It allows you to batch-process a (potentially large) number of commits with a script.
You can run the below sample script in your repository (filling in real values for the old and new email and name):

git filter-branch --env-filter '
WRONG_EMAIL=""wrong@example.com""
NEW_NAME=""New Name Value""
NEW_EMAIL=""correct@example.com""

if [ ""$GIT_COMMITTER_EMAIL"" = ""$WRONG_EMAIL"" ]
then
    export GIT_COMMITTER_NAME=""$NEW_NAME""
    export GIT_COMMITTER_EMAIL=""$NEW_EMAIL""
fi
if [ ""$GIT_AUTHOR_EMAIL"" = ""$WRONG_EMAIL"" ]
then
    export GIT_AUTHOR_NAME=""$NEW_NAME""
    export GIT_AUTHOR_EMAIL=""$NEW_EMAIL""
fi
' --tag-name-filter cat -- --branches --tags

See more details here
    
Change commit author name & email by Amend, then replacing old-commit with new-one: 

$ git checkout <commit-hash>                            # checkout to the commit need to modify  
$ git commit --amend --author ""name <author@email.com>"" # change the author name and email

$ git replace <old-commit-hash> <new-commit-hash>      # replace the old commit by new one
$ git filter-branch -- --all                           # rewrite all futures commits based on the replacement                   

$ git replace -d <old-commit-hash>     # remove the replacement for cleanliness 
$ git push -f origin HEAD              # force push 

Another way Rebasing:

$ git rebase -i <good-commit-hash>      # back to last good commit

# Editor would open, replace 'pick' with 'edit' before the commit want to change author

$ git commit --amend --author=""author name <author@email.com>""  # change the author name & email

# Save changes and exit the editor

$ git rebase --continue                # finish the rebase


    I would like to contribute with a modification of @Rognon answer. This answer is just another alternative in case the selected answer or others don't work for you (in my particular issue that was the case):

Objective: You will fix one or more authors with a correct one in the ALL the history, and you will get a clean history without duplicates. This method works by replacing 'master' branch with a 'clean' branch (its not using merge/rebase)

NOTE: Anyone using the ""master"" repository may need to checkout it again (after performing these steps) before pushing, as merge may fail.

We will use a new branch named ""clean"" to perform the operations (assuming you want to fix ""master""):

git checkout -b clean

(be sure you are in the ""clean"" branch: git branch)

Modify the following script (replacing the email addresses and names). Note that this script expects two wrong emails/authors (as example), so if you only need to fix a single author, you can remove the second part of the condition or leave it like that (as it will be ignored as it won't match).

Execute the script.

#/bin/bash

git filter-branch --force --commit-filter '
        if [ ""$GIT_COMMITTER_EMAIL"" = ""wrong1@example.com"" -o ""$GIT_COMMITTER_EMAIL"" = ""wrong2@example.com"" ];
        then
                export GIT_COMMITTER_NAME=""John Doe"";
                export GIT_AUTHOR_NAME=""John Doe"";
                export GIT_COMMITTER_EMAIL=""correct@example.com"";
                export GIT_AUTHOR_EMAIL=""correct@example.com"";
        fi;
        git commit-tree ""$@""
' --tag-name-filter cat -- --all


It has to report: Ref 'refs/heads/clean' was rewritten. If it reports ""unchanged"", maybe the email(s) entered in the script is wrong.

Confirm the history has been corrected with: git log

If you are using github/gitlab (recommended = safe):


create the ""clean"" branch in remote: 


git push --set-upstream origin clean


set ""clean"" branch as default branch
remove ""master"" (be sure everything is as expected before doing this). 
Create a new branch ""master"" based in ""clean"" branch. 
After confirming all is good, you can remove ""clean"" branch now (alternative you can just rename it).


If are not using github/gitlab or you prefer doing it by command:


Delete the master branch from local: 


git branch -d master


Rename the branch:


git branch -m clean master


Push it (be sure you ""master"" is unprotected)


git push --force origin master
    
run git rebase -i <sha1 or ref of starting point>

mark all commits that you want to change with edit (or e)

loop the following two commands until you have processed all the commits:
git commit --amend --reuse-message=HEAD --author=""New Author <new@author.email>""
;
git rebase --continue


This will keep all the other commit information (including the dates).
The --reuse-message=HEAD option prevents the message editor from launching.
    I want to add my Example too. 
I want to create a bash_function with given parameter.

this works in mint-linux-17.3

# $1 => email to change, $2 => new_name, $3 => new E-Mail

function git_change_user_config_for_commit {

 # defaults
 WRONG_EMAIL=${1:-""you_wrong_mail@hello.world""}
 NEW_NAME=${2:-""your name""}
 NEW_EMAIL=${3:-""new_mail@hello.world""}

 git filter-branch -f --env-filter ""
  if [ \$GIT_COMMITTER_EMAIL = '$WRONG_EMAIL' ]; then
    export GIT_COMMITTER_NAME='$NEW_NAME'
    export GIT_COMMITTER_EMAIL='$NEW_EMAIL'
  fi
  if [ \$GIT_AUTHOR_EMAIL = '$WRONG_EMAIL' ]; then
    export GIT_AUTHOR_NAME='$NEW_NAME'
    export GIT_AUTHOR_EMAIL='$NEW_EMAIL'
  fi
 "" --tag-name-filter cat -- --branches --tags;
}

    Try this out. It will do the same as above mentioned, but interactively.

bash <(curl -s  https://raw.githubusercontent.com/majdarbash/git-author-change-script/master/run.sh)


Reference: https://github.com/majdarbash/git-author-change-script
    I adapted this solution which works by ingesting a simple author-conv-file (format is the same as one for git-cvsimport). It works by changing all users as defined in the author-conv-file across all branches. 

We used this in conjunction with cvs2git to migrate our repository from cvs to git.

i.e. Sample author-conv-file

john=John Doe <john.doe@hotmail.com>
jill=Jill Doe <jill.doe@hotmail.com>


The script:

 #!/bin/bash

 export $authors_file=author-conv-file

 git filter-branch -f --env-filter '

 get_name () {
     grep ""^$1="" ""$authors_file"" |
     sed ""s/^.*=\(.*\) <.*>$/\1/""
 }

 get_email () {
     grep ""^$1="" ""$authors_file"" |
     sed ""s/^.*=.* <\(.*\)>$/\1/""
 }

 GIT_AUTHOR_NAME=$(get_name $GIT_COMMITTER_NAME) &&
     GIT_AUTHOR_EMAIL=$(get_email $GIT_COMMITTER_NAME) &&
     GIT_COMMITTER_NAME=$GIT_AUTHOR_NAME &&
     GIT_COMMITTER_EMAIL=$GIT_AUTHOR_EMAIL &&
     export GIT_AUTHOR_NAME GIT_AUTHOR_EMAIL &&
     export GIT_COMMITTER_NAME GIT_COMMITTER_EMAIL
 ' -- --all

    This isn't an answer to your question, but rather a script you can use to avoid this in the future.  It utilizes global hooks available since Git version 2.9 to check your email configuration based on the directory your in:

#!/bin/sh
PWD=`pwd`
if [[ $PWD == *""Ippon""* ]] # 1)
then
  EMAIL=$(git config user.email)
  if [[ $EMAIL == *""Work""* ]] # 2)
  then
    echo """";
  else
    echo ""Email not configured to your Work email in the Work directory."";
    git config user.email ""youremail@youremail.com""
    echo ""Git email configuration has now been changed to \""$(git config user$
    echo ""\nPlease run your command again...""
    echo ''
    exit 1
  fi;
elif [[ $PWD == *""Personal""* ]]
then
  EMAIL=$(git config user.email)
  if [[ $EMAIL == ""youremail@youremail.com"" ]]
  then
    echo """";
  else
    echo ""Email is not configured to your personal account in the Personal di$
    git config user.email ""youremail@youremail.com""
    echo ""Git email configuration has now been changed to \""$(git config user$
    echo ""\nPlease run your command again...""
    echo ''
    exit 1;
  fi;
fi; 


It checks your current working directory, then verifies your git is configured to the correct email.  If not, it changes it automatically.  See the full details here.
    All the answers above rewrite the history of the repository.
As long as the name to change has not been used by multiple authors and especially if the repository has been shared and the commit is old I'd prefer to use .mailmap, documented at https://git-scm.com/docs/git-shortlog.
It allows mapping incorrect names/emails to the correct one without modifying the repo history. You can use lines like:

Proper Name <proper@email.xx> <root@localhost>

    ","[2731, 1466, 1773, 696, 84, 244, 25, 198, 39, 604, 117, 6, 17, 50, 189, 84, 42, 4, 8, 2, 49, 24, 1, 17, 2, 25, 3, 3, 21, -1, 1]",998077,1163,2009-04-15T03:09:24,2022-03-20 00:27:13Z,
How can I create an executable JAR with dependencies using Maven?,"
                
I want to package my project in a single executable JAR for distribution.

How can I make a Maven project package all dependency JARs into my output JAR?
    

<build>
  <plugins>
    <plugin>
      <artifactId>maven-assembly-plugin</artifactId>
      <configuration>
        <archive>
          <manifest>
            <mainClass>fully.qualified.MainClass</mainClass>
          </manifest>
        </archive>
        <descriptorRefs>
          <descriptorRef>jar-with-dependencies</descriptorRef>
        </descriptorRefs>
      </configuration>
    </plugin>
  </plugins>
</build>

and you run it with
mvn clean compile assembly:single

Compile goal should be added before assembly:single or otherwise the code on your own project is not included.
See more details in comments.

Commonly this goal is tied to a build phase to execute automatically. This ensures the JAR is built when executing mvn install or performing a deployment/release.

<build>
  <plugins>
    <plugin>
      <artifactId>maven-assembly-plugin</artifactId>
      <configuration>
        <archive>
          <manifest>
            <mainClass>fully.qualified.MainClass</mainClass>
          </manifest>
        </archive>
        <descriptorRefs>
          <descriptorRef>jar-with-dependencies</descriptorRef>
        </descriptorRefs>
      </configuration>
      <executions>
        <execution>
          <id>make-assembly</id> <!-- this is used for inheritance merges -->
          <phase>package</phase> <!-- bind to the packaging phase -->
          <goals>
            <goal>single</goal>
          </goals>
        </execution>
      </executions>
    </plugin>
  </plugins>
</build>

    See executable-jar-with-maven-example (GitHub)
Notes
Those pros and cons are provided by Stephan.

For Manual Deployment

Pros
Cons

Dependencies are out of the final jar.



Copy Dependencies to a specific directory
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-dependency-plugin</artifactId>
  <executions>
    <execution>
      <id>copy-dependencies</id>
      <phase>prepare-package</phase>
      <goals>
        <goal>copy-dependencies</goal>
      </goals>
      <configuration>
        <outputDirectory>${project.build.directory}/${project.build.finalName}.lib</outputDirectory>
      </configuration>
    </execution>
  </executions>
</plugin>

Make the Jar Executable and Classpath Aware
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-jar-plugin</artifactId>
  <configuration>
    <archive>
      <manifest>
        <addClasspath>true</addClasspath>
        <classpathPrefix>${project.build.finalName}.lib/</classpathPrefix>
        <mainClass>${fully.qualified.main.class}</mainClass>
      </manifest>
    </archive>
  </configuration>
</plugin>

At this point the jar is actually executable with external classpath elements.
$ java -jar target/${project.build.finalName}.jar

Make Deployable Archives
The jar file is only executable with the sibling ...lib/ directory. We need to make archives to deploy with the directory and its content.
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-antrun-plugin</artifactId>
  <executions>
    <execution>
      <id>antrun-archive</id>
      <phase>package</phase>
      <goals>
        <goal>run</goal>
      </goals>
      <configuration>
        <target>
          <property name=""final.name"" value=""${project.build.directory}/${project.build.finalName}""/>
          <property name=""archive.includes"" value=""${project.build.finalName}.${project.packaging} ${project.build.finalName}.lib/*""/>
          <property name=""tar.destfile"" value=""${final.name}.tar""/>
          <zip basedir=""${project.build.directory}"" destfile=""${final.name}.zip"" includes=""${archive.includes}"" />
          <tar basedir=""${project.build.directory}"" destfile=""${tar.destfile}"" includes=""${archive.includes}"" />
          <gzip src=""${tar.destfile}"" destfile=""${tar.destfile}.gz"" />
          <bzip2 src=""${tar.destfile}"" destfile=""${tar.destfile}.bz2"" />
        </target>
      </configuration>
    </execution>
  </executions>
</plugin>

Now you have target/${project.build.finalName}.(zip|tar|tar.bz2|tar.gz) which each contains the jar and lib/*.

Apache Maven Assembly Plugin

Pros
Cons

No class relocation support (use maven-shade-plugin if class relocation is needed).



<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-assembly-plugin</artifactId>
  <executions>
    <execution>
      <phase>package</phase>
      <goals>
        <goal>single</goal>
      </goals>
      <configuration>
        <archive>
          <manifest>
            <mainClass>${fully.qualified.main.class}</mainClass>
          </manifest>
        </archive>
        <descriptorRefs>
          <descriptorRef>jar-with-dependencies</descriptorRef>
        </descriptorRefs>
      </configuration>
    </execution>
  </executions>
</plugin>

You have target/${project.bulid.finalName}-jar-with-dependencies.jar.

Apache Maven Shade Plugin

Pros
Cons

<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-shade-plugin</artifactId>
  <executions>
    <execution>
      <goals>
        <goal>shade</goal>
      </goals>
      <configuration>
        <shadedArtifactAttached>true</shadedArtifactAttached>
        <transformers>
          <transformer implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"">
            <mainClass>${fully.qualified.main.class}</mainClass>
          </transformer>
        </transformers>
      </configuration>
    </execution>
  </executions>
</plugin>

You have target/${project.build.finalName}-shaded.jar.

onejar-maven-plugin

Pros
Cons

Not actively supported since 2012.



<plugin>
  <!--groupId>org.dstovall</groupId--> <!-- not available on the central -->
  <groupId>com.jolira</groupId>
  <artifactId>onejar-maven-plugin</artifactId>
  <executions>
    <execution>
      <configuration>
        <mainClass>${fully.qualified.main.class}</mainClass>
        <attachToBuild>true</attachToBuild>
        <!-- https://code.google.com/p/onejar-maven-plugin/issues/detail?id=8 -->
        <!--classifier>onejar</classifier-->
        <filename>${project.build.finalName}-onejar.${project.packaging}</filename>
      </configuration>
      <goals>
        <goal>one-jar</goal>
      </goals>
    </execution>
  </executions>
</plugin>


Spring Boot Maven Plugin

Pros
Cons

Add potential unecessary Spring and Spring Boot related classes.



<plugin>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-maven-plugin</artifactId>
  <executions>
    <execution>
      <goals>
        <goal>repackage</goal>
      </goals>
      <configuration>
        <classifier>spring-boot</classifier>
        <mainClass>${fully.qualified.main.class}</mainClass>
      </configuration>
    </execution>
  </executions>
</plugin>

You have target/${project.bulid.finalName}-spring-boot.jar.
    You can use the dependency-plugin to generate all dependencies in a separate directory before the package phase and then include that in the classpath of the manifest:

<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-dependency-plugin</artifactId>
    <executions>
        <execution>
            <id>copy-dependencies</id>
            <phase>prepare-package</phase>
            <goals>
                <goal>copy-dependencies</goal>
            </goals>
            <configuration>
                <outputDirectory>${project.build.directory}/lib</outputDirectory>
                <overWriteReleases>false</overWriteReleases>
                <overWriteSnapshots>false</overWriteSnapshots>
                <overWriteIfNewer>true</overWriteIfNewer>
            </configuration>
        </execution>
    </executions>
</plugin>
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-jar-plugin</artifactId>
    <configuration>
        <archive>
            <manifest>
                <addClasspath>true</addClasspath>
                <classpathPrefix>lib/</classpathPrefix>
                <mainClass>theMainClass</mainClass>
            </manifest>
        </archive>
    </configuration>
</plugin>


Alternatively use ${project.build.directory}/classes/lib as OutputDirectory to integrate all jar-files into the main jar, but then you will need to add custom classloading code to load the jars.
    Taking Unanswered's answer and reformatting it, we have:

<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-jar-plugin</artifactId>
            <configuration>
                <archive>
                    <manifest>
                        <addClasspath>true</addClasspath>
                        <mainClass>fully.qualified.MainClass</mainClass>
                    </manifest>
                </archive>
            </configuration>
        </plugin>
        <plugin>
            <artifactId>maven-assembly-plugin</artifactId>
            <configuration>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
            </configuration>
        </plugin>
    </plugins>
</build>


Next, I would recommend making this a natural part of your build, rather than something to call explicitly.  To make this a integral part of your build, add this plugin to your pom.xml and bind it to the package lifecycle event.  However, a gotcha is that you need to call the assembly:single goal if putting this in your pom.xml, while you would call 'assembly:assembly' if executing it manually from the command line.

<project>
  [...]
  <build>
      <plugins>
          <plugin>
              <artifactId>maven-assembly-plugin</artifactId>
              <configuration>
                  <archive>
                      <manifest>
                          <addClasspath>true</addClasspath>
                          <mainClass>fully.qualified.MainClass</mainClass>
                      </manifest>
                  </archive>
                  <descriptorRefs>
                      <descriptorRef>jar-with-dependencies</descriptorRef>
                  </descriptorRefs>
              </configuration>
              <executions>
                  <execution>
                      <id>make-my-jar-with-dependencies</id>
                      <phase>package</phase>
                      <goals>
                          <goal>single</goal>
                      </goals>
                  </execution>
              </executions>
          </plugin>
      [...]
      </plugins>
    [...]
  </build>
</project>

    You can use maven-shade plugin to build a uber jar like below

<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
        </execution>
    </executions>
</plugin>

    I went through every one of these responses looking to make a fat executable jar containing all dependencies and none of them worked right.  The answer is the shade plugin, its very easy and straightforward.

    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-shade-plugin</artifactId>
      <version>2.3</version>
      <executions>
         <!-- Run shade goal on package phase -->
        <execution>
        <phase>package</phase>
        <goals>
            <goal>shade</goal>
        </goals>
        <configuration>
          <transformers>
             <transformer implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"">
                <mainClass>path.to.MainClass</mainClass>
             </transformer>
          </transformers>
        </configuration>
          </execution>
      </executions>
    </plugin>


Be aware that your dependencies need to have a scope of compile or runtime for this to work properly.

This example came from mkyong.com
    Use the maven-shade-plugin to package all dependencies into one uber-jar. It can also be used to build an executable jar by specifying the main class. After trying to use maven-assembly and maven-jar , I found that this plugin best suited my needs. 

I found this plugin particularly useful as it merges content of specific files instead of overwriting them. This is needed when there are resource files that are have the same name across the jars and the plugin tries to package all the resource files

See example below 

      <plugins>
    <!-- This plugin provides the capability to package the artifact in an uber-jar, including its dependencies and to shade - i.e. rename - the packages of some of the dependencies. -->
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <artifactSet>
                        <!-- signed jars-->
                            <excludes>
                                <exclude>bouncycastle:bcprov-jdk15</exclude>
                            </excludes>
                        </artifactSet>

                         <transformers>
                            <transformer
                                implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"">
                                <!-- Main class -->
                                <mainClass>com.main.MyMainClass</mainClass>
                            </transformer>
                            <!-- Use resource transformers to prevent file overwrites -->
                            <transformer 
                                 implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer"">
                                <resource>properties.properties</resource>
                            </transformer>
                            <transformer
                                implementation=""org.apache.maven.plugins.shade.resource.XmlAppendingTransformer"">
                                <resource>applicationContext.xml</resource>
                            </transformer>
                            <transformer
                                implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer"">
                                <resource>META-INF/cxf/cxf.extension</resource>
                            </transformer>
                            <transformer
                                implementation=""org.apache.maven.plugins.shade.resource.XmlAppendingTransformer"">
                                <resource>META-INF/cxf/bus-extensions.xml</resource>
                            </transformer>
                     </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>

    </plugins>

    You can add the following to your pom.xml:

<build>
<defaultGoal>install</defaultGoal>
<plugins>
  <plugin>
    <artifactId>maven-compiler-plugin</artifactId>
    <version>2.3.2</version>
    <configuration>
      <source>1.6</source>
      <target>1.6</target>
    </configuration>
  </plugin>
  <plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-jar-plugin</artifactId>
    <version>2.3.1</version>
    <configuration>
      <archive>
        <manifest>
          <addClasspath>true</addClasspath>
          <mainClass>com.mycompany.package.MainClass</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
      <descriptorRefs>
        <descriptorRef>jar-with-dependencies</descriptorRef>
      </descriptorRefs>
      <archive>
        <manifest>
          <mainClass>com.mycompany.package.MainClass</mainClass>
        </manifest>
      </archive>
    </configuration>
    <executions>
      <execution>
        <id>make-my-jar-with-dependencies</id>
        <phase>package</phase>
        <goals>
          <goal>single</goal>
        </goals>
      </execution>
    </executions>
  </plugin>
</plugins>
</build>


Afterwards you have to switch via the console to the directory, where the pom.xml is located. Then you have to execute mvn assembly:single and then your executable JAR file with dependencies will be hopefully build. You can check it when switching to the output (target) directory with cd ./target and starting your jar with a command similiar to java -jar mavenproject1-1.0-SNAPSHOT-jar-with-dependencies.jar. 

I tested this with Apache Maven 3.0.3.
    You could combine the maven-shade-plugin and maven-jar-plugin.


The maven-shade-plugin packs your classes and all dependencies in a single jar file.
Configure the maven-jar-plugin to specify the main class of your executable jar (see Set Up The Classpath, chapter ""Make The Jar Executable"").


Example POM configuration for maven-jar-plugin:

        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-jar-plugin</artifactId>
            <version>2.3.2</version>
            <configuration>
                <archive>
                    <manifest>
                        <addClasspath>true</addClasspath>
                        <mainClass>com.example.MyMainClass</mainClass>
                    </manifest>
                </archive>
            </configuration>
        </plugin>


Finally create the executable jar by invoking:

mvn clean package shade:shade

    It will work like:
<plugin>
    <artifactId>maven-dependency-plugin</artifactId>
    <executions>
        <execution>
            <id>unpack-dependencies</id>
            <phase>generate-resources</phase>
            <goals>
                <goal>unpack-dependencies</goal>
            </goals>
        </execution>
    </executions>
</plugin>

Unpacking has to be in the generate-resources phase or it will not be included as resources.
    Long used the maven assembly plugin, but I could not find a solution to the problem with ""already added, skipping"". Now, I'm using another plugin - onejar-maven-plugin. Example below (mvn package build jar):

<plugin>
    <groupId>org.dstovall</groupId>
    <artifactId>onejar-maven-plugin</artifactId>
    <version>1.3.0</version>
    <executions>
        <execution>
            <configuration>
                <mainClass>com.company.MainClass</mainClass>
            </configuration>
            <goals>
                <goal>one-jar</goal>
            </goals>
        </execution>
    </executions>
</plugin>


You need to add repository for that plugin:

<pluginRepositories>
    <pluginRepository>
        <id>onejar-maven-plugin.googlecode.com</id>
        <url>http://onejar-maven-plugin.googlecode.com/svn/mavenrepo</url>
    </pluginRepository>
</pluginRepositories>

    You can use maven-dependency-plugin, but the question was how to create an executable JAR. To do that requires the following alteration to Matthew Franglen's response (btw, using the dependency plugin takes longer to build when starting from a clean target):

<build>
    <plugins>
        <plugin>
            <artifactId>maven-jar-plugin</artifactId>
            <configuration>
                <archive>
                    <manifest>
                        <mainClass>fully.qualified.MainClass</mainClass>
                    </manifest>
                </archive>
            </configuration>
        </plugin>
        <plugin>
            <artifactId>maven-dependency-plugin</artifactId>
            <executions>
                <execution>
                    <id>unpack-dependencies</id>
                    <phase>package</phase>
                    <goals>
                        <goal>unpack-dependencies</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
    <resources>
        <resource>
            <directory>${basedir}/target/dependency</directory>
        </resource>
    </resources>
</build>

    For anyone looking for options to exclude specific dependencies from the uber-jar, this is a solution that worked for me:

<project...>
<dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>1.6.1</version>
            <scope>provided</scope> <=============
        </dependency>
</dependencies>
<build>
        <plugins>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                    <archive>
                        <manifest>
                            <mainClass>...</mainClass>
                        </manifest>
                    </archive>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>


So it's not a configuration of the mvn-assembly-plugin but a property of the dependency.
    There are millions of answers already, I wanted to add you don't need <mainClass> if you don't need to add entryPoint to your application. For example APIs may not have necessarily have main method.

maven plugin config

  <build>
    <finalName>log-enrichment</finalName>
    <plugins>
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
      </plugin>
    </plugins>
  </build>


build

mvn clean compile assembly:single


verify

ll target/
total 35100
drwxrwx--- 1 root vboxsf     4096 Sep 29 16:25 ./
drwxrwx--- 1 root vboxsf     4096 Sep 29 16:25 ../
drwxrwx--- 1 root vboxsf        0 Sep 29 16:08 archive-tmp/
drwxrwx--- 1 root vboxsf        0 Sep 29 16:25 classes/
drwxrwx--- 1 root vboxsf        0 Sep 29 16:25 generated-sources/
drwxrwx--- 1 root vboxsf        0 Sep 29 16:25 generated-test-sources/
-rwxrwx--- 1 root vboxsf 35929841 Sep 29 16:10 log-enrichment-jar-with-dependencies.jar*
drwxrwx--- 1 root vboxsf        0 Sep 29 16:08 maven-status/

    tried multiple solutions but the one that worked perfectly in the scenario where we wanted to create a non executable fat jar with all internal dependencies for external systems having no previous relevance. prod scenario tested.
include this in the pom.xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<build>
   <sourceDirectory>src</sourceDirectory>
   <plugins>
      <plugin>
         <artifactId>maven-compiler-plugin</artifactId>
         <version>3.8.1</version>
         <configuration>
            <source>1.8</source>
            <target>1.8</target>
         </configuration>
      </plugin>
      <plugin>
         <artifactId>maven-assembly-plugin</artifactId>
         <configuration>
            <descriptorRefs>
               <descriptorRef>jar-with-dependencies</descriptorRef>
            </descriptorRefs>
         </configuration>
      </plugin>
   </plugins>
</build>



Command to run to build the fat jar->> mvn assembly:assembly
    To resolve this issue we will use Maven Assembly Plugin that will create the JAR together with its dependency JARs into a single executable JAR file. Just add below plugin configuration in your pom.xml file.

<build>
   <pluginManagement>
      <plugins>
         <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-assembly-plugin</artifactId>
            <configuration>
               <archive>
                  <manifest>
                     <addClasspath>true</addClasspath>
                     <mainClass>com.your.package.MainClass</mainClass>
                  </manifest>
               </archive>
               <descriptorRefs>
                  <descriptorRef>jar-with-dependencies</descriptorRef>
               </descriptorRefs>
            </configuration>
            <executions>
               <execution>
                  <id>make-my-jar-with-dependencies</id>
                  <phase>package</phase>
                  <goals>
                     <goal>single</goal>
                  </goals>
               </execution>
            </executions>
         </plugin>
      </plugins>
   </pluginManagement>
</build>


After doing this dont forget to run MAVEN tool with this command mvn clean compile assembly:single

http://jkoder.com/maven-creating-a-jar-together-with-its-dependency-jars-into-a-single-executable-jar-file/
    Another option if you really want to repackage the other JARs contents inside your single resultant JAR is the Maven Assembly plugin.  It unpacks and then repacks everything into a directory via <unpack>true</unpack>. Then you'd have a second pass that built it into one massive JAR.

Another option is the OneJar plugin.  This performs the above repackaging actions all in one step.
    Ken Liu has it right in my opinion. The maven dependency plugin allows you to expand all the dependencies, which you can then treat as resources. This allows you to include them in the main artifact. The use of the assembly plugin creates a secondary artifact which can be difficult to modify - in my case I wanted to add custom manifest entries. My pom ended up as:

<project>
 ...
 <build>
  <plugins>
   <plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-dependency-plugin</artifactId>
    <executions>
     <execution>
      <id>unpack-dependencies</id>
      <phase>package</phase>
      <goals>
       <goal>unpack-dependencies</goal>
      </goals>
     </execution>
    </executions>
   </plugin>
  </plugins>
  ...
  <resources>
   <resource>
    <directory>${basedir}/target/dependency</directory>
    <targetPath>/</targetPath>
   </resource>
  </resources>
 </build>
 ...
</project>

    Problem with locating shared assembly file with maven-assembly-plugin-2.2.1?

Try using descriptorId configuration parameter instead of descriptors/descriptor or descriptorRefs/descriptorRef parameters.

Neither of them do what you need: look for the file on classpath.
Of course you need adding the package where the shared assembly resides on maven-assembly-plugin's classpath (see below).
If you're using Maven 2.x (not Maven 3.x), you may need adding this dependency in top-most parent pom.xml in pluginManagement section.

See this for more details.

Class: org.apache.maven.plugin.assembly.io.DefaultAssemblyReader

Example:

        <!-- Use the assembly plugin to create a zip file of all our dependencies. -->
        <plugin>
            <artifactId>maven-assembly-plugin</artifactId>
            <version>2.2.1</version>
            <executions>
                <execution>
                    <id>make-assembly</id>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                    <configuration>
                        <descriptorId>assembly-zip-for-wid</descriptorId>
                    </configuration>
                </execution>
            </executions>
            <dependencies>
                <dependency>
                    <groupId>cz.ness.ct.ip.assemblies</groupId>
                    <artifactId>TEST_SharedAssemblyDescriptor</artifactId>
                    <version>1.0.0-SNAPSHOT</version>
                </dependency>
            </dependencies>
        </plugin>

    I hope my experience can help somebody. I wanted to migrate my app Spring (using CAS client) to Spring Boot 1.5. I ran into many problems, like:

no main manifest attribute, in target/cas-client-web.jar

I tried to make one unique jar with all dependencies. After searching on the Internet, I was able to do it with these lines:
         <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
            <configuration>
                <fork>true</fork>
                <mainClass>${start-class}</mainClass>
            </configuration>
            <executions>
                <execution>
                    <goals>
                        <goal>repackage</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
        <plugin>
            <artifactId>maven-assembly-plugin</artifactId>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                </execution>
            </executions>
            <configuration>
                <archive>
                    <manifest>
                        <addClasspath>true</addClasspath>
                        <mainClass>${start-class}</mainClass>
                    </manifest>
                </archive>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
            </configuration>
        </plugin>

start-class is my main class:
<properties>
    <java.version>1.8</java.version>
    <start-class>com.test.Application</start-class>
</properties>

And my Application is:
package com.test;

import java.util.Arrays;

import com.test.TestProperties;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.ApplicationContext;
import org.springframework.context.annotation.Bean;


@SpringBootApplication
@EnableAutoConfiguration
@EnableConfigurationProperties({TestProperties.class})
public class Application {

public static void main(String[] args) {
    SpringApplication.run(Application.class, args);
}

@Bean
public CommandLineRunner commandLineRunner(ApplicationContext ctx) {
    return args -> {

        System.out.println(""Let's inspect the beans provided by Spring Boot:"");

        String[] beanNames = ctx.getBeanDefinitionNames();
        Arrays.sort(beanNames);
        for (String beanName : beanNames) {
            System.out.println(beanName);
        }

    };
}

}

    Something that have worked for me was:

  <plugin>
    <artifactId>maven-dependency-plugin</artifactId>
    <executions>
      <execution>
        <id>unpack-dependencies</id>
        <phase>prepare-package</phase>
        <goals>
          <goal>unpack-dependencies</goal>
        </goals>
        <configuration>
          <outputDirectory>${project.build.directory}/classes</outputDirectory>
        </configuration>
      </execution>

    </executions>
  </plugin>


  <plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-jar-plugin</artifactId>
    <executions>
      <execution>
        <id>unpack-dependencies</id>
        <phase>package</phase>
      </execution>
    </executions>
    <configuration>
      <archive>
        <manifest>
          <addClasspath>true</addClasspath>
          <classpathPrefix>lib/</classpathPrefix>
          <mainClass>SimpleKeyLogger</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>


I had extraordinary case because my dependency was system one:

<dependency>
  ..
  <scope>system</scope>
  <systemPath>${project.basedir}/lib/myjar.jar</systemPath>
</dependency>


I have changed the code provided by @user189057 with changes:
1) maven-dependency-plugin is executed in ""prepare-package"" phase
2) I am extracting unpacked classess directly to ""target/classes""
    I compared the tree plugins mentioned in this post. I generated 2 jars and a directory with all the jars. I compared the results and definitely the maven-shade-plugin is the best. My challenge was that I have multiple spring resources that needed to be merged, as well as jax-rs, and JDBC services. They were all merged properly by the shade plugin in comparison with the maven-assembly-plugin. In which case the spring will fail unless you copy them to your own resources folder and merge them manually one time. Both plugins output the correct dependency tree. I had multiple scopes like test,provide, compile, etc the test and provided were skipped by both plugins. They both produced the same manifest but I was able to consolidate licenses with the shade plugin using their transformer.
With the maven-dependency-plugin of course you don't have those problems because the jars are not extracted. But like some other have pointed you need to carry one extra file(s) to work properly.
Here is a snip of the pom.xml

            <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-dependency-plugin</artifactId>
            <executions>
                <execution>
                    <id>copy-dependencies</id>
                    <phase>prepare-package</phase>
                    <goals>
                        <goal>copy-dependencies</goal>
                    </goals>
                    <configuration>
                        <outputDirectory>${project.build.directory}/lib</outputDirectory>
                        <includeScope>compile</includeScope>
                        <excludeTransitive>true</excludeTransitive>
                        <overWriteReleases>false</overWriteReleases>
                        <overWriteSnapshots>false</overWriteSnapshots>
                        <overWriteIfNewer>true</overWriteIfNewer>
                    </configuration>
                </execution>
            </executions>
        </plugin>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-assembly-plugin</artifactId>
            <version>2.6</version>
            <configuration>
                <archive>
                    <manifest>
                        <addClasspath>true</addClasspath>
                        <mainClass>com.rbccm.itf.cdd.poller.landingzone.LandingZonePoller</mainClass>
                    </manifest>
                </archive>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
            </configuration>
            <executions>
                <execution>
                    <id>make-my-jar-with-dependencies</id>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>2.4.3</version>
            <configuration>
                <shadedArtifactAttached>false</shadedArtifactAttached>
                <keepDependenciesWithProvidedScope>false</keepDependenciesWithProvidedScope>
                <transformers>
                    <transformer implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer"">
                        <resource>META-INF/services/javax.ws.rs.ext.Providers</resource>
                    </transformer>
                    <transformer implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer"">
                        <resource>META-INF/spring.factories</resource>
                    </transformer>
                    <transformer implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer"">
                        <resource>META-INF/spring.handlers</resource>
                    </transformer>
                    <transformer implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer"">
                        <resource>META-INF/spring.schemas</resource>
                    </transformer>
                    <transformer implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer"">
                        <resource>META-INF/spring.tooling</resource>
                    </transformer>
                    <transformer implementation=""org.apache.maven.plugins.shade.resource.ServicesResourceTransformer""/>
                    <transformer implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer""/>
                    <transformer implementation=""org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer"">
                    </transformer>
                </transformers>
            </configuration>
            <executions>
                <execution>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>

    This is the best way i found:

  <plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-jar-plugin</artifactId>
    <version>2.4</version>
    <configuration>
      <archive>
        <manifest>
        <addClasspath>true</addClasspath>
        <mainClass>com.myDomain.etc.MainClassName</mainClass>
        <classpathPrefix>dependency-jars/</classpathPrefix>
        </manifest>
      </archive>
    </configuration>
  </plugin>
  <plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-dependency-plugin</artifactId>
    <version>2.5.1</version>
    <executions>
      <execution>
        <id>copy-dependencies</id>
        <phase>package</phase>
        <goals>
            <goal>copy-dependencies</goal>
        </goals>
        <configuration>
            <outputDirectory>
               ${project.build.directory}/dependency-jars/
            </outputDirectory>
        </configuration>
      </execution>
    </executions>
  </plugin>


With this configuration, all dependencies will be located in /dependency-jars. My application has no Main class, just context ones, but one of my dependencies do have a Main class (com.myDomain.etc.MainClassName) that starts the JMX server, and receives a start or a stop parameter. So with this i was able to start my application like this:

java -jar ./lib/TestApp-1.0-SNAPSHOT.jar start


I wait it be useful for you all.
    This could also be an option,You will be able to build your jar file

<build>
    <plugins>
        <plugin>
            <!-- Build an executable JAR -->
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-jar-plugin</artifactId>
            <version>2.4</version>
            <configuration>
                <archive>
                    <manifest>
                        <addClasspath>true</addClasspath>
                        <classpathPrefix>lib/</classpathPrefix>
                        <mainClass>WordListDriver</mainClass>
                    </manifest>
                </archive>
            </configuration>
        </plugin>
    </plugins>
</build>

    I won't answer directly the question as other have already done that before, but I really wonder if it's a good idea to embed all the dependencies in the project's jar itself.

I see the point (ease of deployment / usage) but it depends of the use case of your poject (and there may be alternatives (see below)).

If you use it fully standalone, why not.

But if you use your project in other contexts (like in a webapp, or dropped in a folder where other jars are sitting), you may have jar duplicates in your classpath (the ones in the folder, the one in the jars). Maybe not a bid deal but i usually avoid this.

A good alternative :


deploy your application as a .zip / .war : the archive contains your project's jar and all dependent jars ;
use a dynamic classloader mechanism (see Spring, or you can easily do this yourself) to have a single entry point of your project (a single class to start - see the Manifest mechanism on another answer), which will add (dynamically) to the current classpath all the other needed jars.


Like this, with in the end just a manifest and a ""special dynamic classloader main"", you can start your project with :

java -jar ProjectMainJar.jar com.stackoverflow.projectName.MainDynamicClassLoaderClass

    I tried the most up-voted answer here, and was able to get the jar runnable. But the program didn't run correctly. I do not know what the reason was. When I try to run from Eclipse, I get a different result but when I run the jar from command-line I get a different result (it crashes with a program-specific runtime error).

I had a similar requirement as the OP just that I had too many (Maven) dependencies for my project. Fortunately, the only solution that worked for me was that using Eclipse. Very simple and very straightforward. This is not a solution to the OP but is a solution for someone who has a similar requirement but with many Maven dependencies,

1) Just right-click on your project folder (in Eclipse) and select Export

2) Then select Java -> Runnable Jar

3) You will be asked to choose the location of the jar file

4) Finally, select the class that has the Main method that you want to run and choose Package dependencies with the Jar file and click Finish
    This blog post shows another approach with combining the maven-jar and maven-assembly plugins. With the assembly configuration xml from the blog post it can also be controlled if dependencies will be expanded or just be collected in a folder and referenced by a classpath entry in the manifest:


  The ideal solution is to include the jars in a lib folder and the manifest.mf file of the main jar include all the jars in classpath.


And exactly that one is described here: https://caffebig.wordpress.com/2013/04/05/executable-jar-file-with-dependent-jars-using-maven/
    Add to pom.xml:

  <dependency>
            <groupId>com.jolira</groupId>
            <artifactId>onejar-maven-plugin</artifactId>
            <version>1.4.4</version>
  </dependency>


and

<plugin>
       <groupId>com.jolira</groupId>
       <artifactId>onejar-maven-plugin</artifactId>
       <version>1.4.4</version>
       <executions>
              <execution>
                     <goals>
                         <goal>one-jar</goal>
                     </goals>
              </execution>
       </executions>
</plugin>


Thats it. Next mvn package will also create one fat jar additionally, including all dependency jars.
    To create an executable JAR from command line itself just run the below command from the project path:

mvn assembly:assembly

    Okay, so this is my solution. I know it's not using the pom.xml file. But I had the problem my programmme compiling and running on Netbeans but it failing when I tried Java -jar MyJarFile.jar. Now, I don't fully understand Maven and I think this why was having trouble getting Netbeans 8.0.2 to include my jar file in a library to put them into a jar file. I was thinking about how I used to use jar files with no Maven in Eclipse. 

It's Maven that can compile all the dependanices and plugins. Not Netbeans. (If you can get Netbeans and be able to use java .jar to do this please tell me how (^.^)v )

[Solved - for Linux] by opening a terminal. 

Then

cd /MyRootDirectoryForMyProject


Next

mvn org.apache.maven.plugins:maven-compiler-plugin:compile


Next

mvn install


This will create jar file in the target directory.

MyJarFile-1.0-jar-with-dependencies.jar


Now

cd target


(You may need to run: chmod +x MyJarFile-1.0-jar-with-dependencies.jar)

And finally

java -jar MyJarFile-1.0-jar-with-dependencies.jar


Please see 

https://cwiki.apache.org/confluence/display/MAVEN/LifecyclePhaseNotFoundException

I'll post this solution in on a couple of other pages with a similar problem. Hopefully I can save somebody from a week of frustration.
    ","[2731, 2672, 296, 386, 153, 25, 17, 101, 18, 15, 12, 22, 20, 4, 4, 2, 8, 17, 11, 9, 2, 4, 4, 4, 3, 6, 3, 1, 2, 4, -2]",1666748,965,2009-02-22T08:43:45,2022-04-19 03:02:50Z,java 
Find the current directory and file's directory [duplicate],"
                    
            
        
            
                
                    
                        This question already has answers here:
                        
                    
                
            
                    
                        How do you properly determine the current script directory?
                            
                                (16 answers)
                            
                    
                    
                        How to know/change current directory in Python shell?
                            
                                (7 answers)
                            
                    
                Closed 4 years ago.
        

    

In Python, what commands can I use to find:

the current directory (where I was in the terminal when I ran the Python script), and
where the file I am executing is?

    To get the full path to the directory a Python file is contained in, write this in that file:

import os 
dir_path = os.path.dirname(os.path.realpath(__file__))


(Note that the incantation above won't work if you've already used os.chdir() to change your current working directory, since the value of the __file__ constant is relative to the current working directory and is not changed by an os.chdir() call.)



To get the current working directory use 

import os
cwd = os.getcwd()




Documentation references for the modules, constants and functions used above:


The os and os.path modules.
The __file__ constant
os.path.realpath(path) (returns ""the canonical path of the specified filename, eliminating any symbolic links encountered in the path"")
os.path.dirname(path) (returns ""the directory name of pathname path"")
os.getcwd() (returns ""a string representing the current working directory"")
os.chdir(path) (""change the current working directory to path"")

    The pathlib module, introduced in Python 3.4 (PEP 428  The pathlib module  object-oriented filesystem paths), makes the path-related experience much much better.
pwd

/home/skovorodkin/stack

tree

.
 scripts
     1.py
     2.py

In order to get the current working directory, use Path.cwd():
from pathlib import Path

print(Path.cwd())  # /home/skovorodkin/stack

To get an absolute path to your script file, use the Path.resolve() method:
print(Path(__file__).resolve())  # /home/skovorodkin/stack/scripts/1.py

And to get the path of a directory where your script is located, access .parent (it is recommended to call .resolve() before .parent):
print(Path(__file__).resolve().parent)  # /home/skovorodkin/stack/scripts

Remember that __file__ is not reliable in some situations: How do I get the path of the current executed file in Python?.

Please note, that Path.cwd(), Path.resolve() and other Path methods return path objects (PosixPath in my case), not strings. In Python 3.4 and 3.5 that caused some pain, because open built-in function could only work with string or bytes objects, and did not support Path objects, so you had to convert Path objects to strings or use the Path.open() method, but the latter option required you to change old code:
File scripts/2.py
from pathlib import Path

p = Path(__file__).resolve()

with p.open() as f: pass
with open(str(p)) as f: pass
with open(p) as f: pass

print('OK')

Output
python3.5 scripts/2.py

Traceback (most recent call last):
  File ""scripts/2.py"", line 11, in <module>
    with open(p) as f:
TypeError: invalid file: PosixPath('/home/skovorodkin/stack/scripts/2.py')

As you can see, open(p) does not work with Python 3.5.
PEP 519  Adding a file system path protocol, implemented in Python 3.6, adds support of PathLike objects to the open function, so now you can pass Path objects to the open function directly:
python3.6 scripts/2.py

OK

    You may find this useful as a reference:

import os

print(""Path at terminal when executing this file"")
print(os.getcwd() + ""\n"")

print(""This file path, relative to os.getcwd()"")
print(__file__ + ""\n"")

print(""This file full path (following symlinks)"")
full_path = os.path.realpath(__file__)
print(full_path + ""\n"")

print(""This file directory and name"")
path, filename = os.path.split(full_path)
print(path + ' --> ' + filename + ""\n"")

print(""This file directory only"")
print(os.path.dirname(full_path))

    Current working directory:  os.getcwd()
And the __file__ attribute can help you find out where the file you are executing is located. This StackOverflow post explains everything:  How do I get the path of the current executed file in Python?
    Pathlib can be used this way to get the directory containing the current script:
import pathlib
filepath = pathlib.Path(__file__).resolve().parent

    
To get the current directory full path
>>import os
>>print os.getcwd()

Output: ""C :\Users\admin\myfolder""

To get the current directory folder name alone
>>import os
>>str1=os.getcwd()
>>str2=str1.split('\\')
>>n=len(str2)
>>print str2[n-1]

Output: ""myfolder""


    To get the current directory full path:

os.path.realpath('.')

    If you're using Python 3.4, there is the brand new higher-level pathlib module which allows you to conveniently call pathlib.Path.cwd() to get a Path object representing your current working directory, along with many other new features.

More info on this new API can be found here.
    If you are trying to find the current directory of the file you are currently in:

OS agnostic way:

dirname, filename = os.path.split(os.path.abspath(__file__))

    Answer to #1:

If you want the current directory, do this:

import os
os.getcwd()


If you want just any folder name and you have the path to that folder, do this:

def get_folder_name(folder):
    '''
    Returns the folder name, given a full folder path
    '''
    return folder.split(os.sep)[-1]


Answer to #2:

import os
print os.path.abspath(__file__)

    I think the most succinct way to find just the name of your current execution context would be:
current_folder_path, current_folder_name = os.path.split(os.getcwd())

    For question 1, use os.getcwd() # Get working directory and os.chdir(r'D:\Steam\steamapps\common') # Set working directory

I recommend using sys.argv[0] for question 2 because sys.argv is immutable and therefore always returns the current file (module object path) and not affected by os.chdir(). Also you can do like this:
import os
this_py_file = os.path.realpath(__file__)

# vvv Below comes your code vvv #

But that snippet and sys.argv[0] will not work or will work weird when compiled by PyInstaller, because magic properties are not set in __main__ level and sys.argv[0] is the way your executable was called (it means that it becomes affected by the working directory).
    If you're searching for the location of the currently executed script, you can use sys.argv[0] to get the full path.
    ","[2720, 4278, 219, 332, 357, 46, 71, 35, 38, 42, 31, 30, 11, 17]",4028361,579,2011-02-28T01:51:21,2021-09-06 20:32:45Z,python 
Encode URL in JavaScript?,"
                
How do you safely encode a URL using JavaScript such that it can be put into a GET string?

var myUrl = ""http://example.com/index.html?param=1&anotherParam=2"";
var myOtherUrl = ""http://example.com/index.html?url="" + myUrl;


I assume that you need to encode the myUrl variable on that second line?
    Check out the built-in function encodeURIComponent(str) and encodeURI(str).
In your case, this should work:
var myOtherUrl = 
       ""http://example.com/index.html?url="" + encodeURIComponent(myUrl);

    You have three options:

escape() will not encode: @*/+

encodeURI() will not encode: ~!@#$&*()=:/,;?+'

encodeURIComponent() will not encode: ~!*()'


But in your case, if you want to pass a URL into a GET parameter of other page, you should use escape or encodeURIComponent, but not encodeURI.
See Stack Overflow question Best practice: escape, or encodeURI / encodeURIComponent for further discussion.
    Stick with encodeURIComponent(). The function encodeURI() does not bother to encode many characters that have semantic importance in URLs (e.g. ""#"", ""?"", and ""&""). escape() is deprecated, and does not bother to encode ""+"" characters, which will be interpreted as encoded spaces on the server (and, as pointed out by others here, does not properly URL-encode non-ASCII characters).

There is a nice explanation of the difference between encodeURI() and encodeURIComponent() elsewhere. If you want to encode something so that it can safely be included as a component of a URI (e.g. as a query string parameter), you want to use encodeURIComponent().
    Modern solution (2021)
Since the other answers were written, the URLSearchParams API has been introduced. It can be used like this:
const queryParams = { param1: 'value1', param2: 'value2' }
const queryString = new URLSearchParams(queryParams).toString()
// 'param1=value1&param2=value2'

It also encodes non-URL characters.
For your specific example, you would use it like this:

const myUrl = ""http://example.com/index.html?param=1&anotherParam=2"";
const myOtherUrl = new URL(""http://example.com/index.html"");
myOtherUrl.search = new URLSearchParams({url: myUrl});
console.log(myOtherUrl.toString());


This solution is also mentioned here and here.
    The best answer is to use encodeURIComponent on values in the query string (and nowhere else).

However, I find that many APIs want to replace "" "" with ""+"" so I've had to use the following:

const value = encodeURIComponent(value).replace('%20','+');
const url = 'http://example.com?lang=en&key=' + value


escape is implemented differently in different browsers and encodeURI doesn't encode many characters (like # and even /) -- it's made to be used on a full URI/URL without breaking it  which isn't super helpful or secure.

And as @Jochem points out below, you may want to use encodeURIComponent() on a (each) folder name, but for whatever reason these APIs don't seem to want + in folder names so plain old encodeURIComponent works great.

Example:

const escapedValue = encodeURIComponent(value).replace('%20','+');
const escapedFolder = encodeURIComponent('My Folder'); // no replace
const url = `http://example.com/${escapedFolder}/?myKey=${escapedValue}`;

    I think now in 2021 to be really safe, you should always consider constructing your URLs using URL() interface. It'll do most of the job for you. So coming to your code,
const baseURL = 'http://example.com/index.html';

const myUrl = new URL(baseURL);
myUrl.searchParams.append('param', '1');
myUrl.searchParams.append('anotherParam', '2');

const myOtherUrl = new URL(baseURL);
myOtherUrl.searchParams.append('url', myUrl.href);

console.log(myUrl.href);
// Outputs: http://example.com/index.html?param=1&anotherParam=2
console.log(myOtherUrl.href);
// Outputs: http://example.com/index.html?url=http%3A%2F%2Fexample.com%2Findex.html%3Fparam%3D1%26anotherParam%3D2
console.log(myOtherUrl.searchParams.get('url'));
// Outputs: http://example.com/index.html?param=1&anotherParam=2

Or..
const params = new URLSearchParams(myOtherUrl.search);

console.log(params.get('url'));
// Outputs: http://example.com/index.html?param=1&anotherParam=2

Something like this is assured not to fail.
    I would suggest to use qs npm package
qs.stringify({a:""1=2"", b:""Test 1""}); // gets a=1%3D2&b=Test+1

it is easier to use with JS object and it gives you proper URL encoding for all parameters
If you are using jQuery I would go for $.param method. It URL encodes an object mapping fields to values, which is easier to read than calling an escape method on each value.
$.param({a:""1=2"", b:""Test 1""}) // gets a=1%3D2&b=Test+1

    You should not use encodeURIComponent() directly.
Take a look at RFC3986: Uniform Resource Identifier (URI): Generic Syntax

sub-delims  = ""!"" / ""$"" / ""&"" / ""'"" / ""("" / "")""
/ ""*"" / ""+"" / "","" / "";"" / ""=""
The purpose of reserved characters is to provide a set of delimiting characters that are distinguishable from other data within a URI.

These reserved characters from the URI definition in RFC3986 ARE NOT escaped by encodeURIComponent().
MDN Web Docs: encodeURIComponent()

To be more stringent in adhering to RFC 3986 (which reserves !, ', (, ), and *), even though these characters have no formalized URI delimiting uses, the following can be safely used:

Use the MDN Web Docs function...
function fixedEncodeURIComponent(str) {
  return encodeURIComponent(str).replace(/[!'()*]/g, function(c) {
    return '%' + c.charCodeAt(0).toString(16);
  });
}

    encodeURIComponent() is the way to go.

var myOtherUrl = ""http://example.com/index.html?url="" + encodeURIComponent(myUrl);


BUT you should keep in mind that there are small differences from php version urlencode() and as @CMS mentioned, it will not encode every char. Guys at http://phpjs.org/functions/urlencode/ made js equivalent to phpencode():

function urlencode(str) {
  str = (str + '').toString();

  // Tilde should be allowed unescaped in future versions of PHP (as reflected below), but if you want to reflect current
  // PHP behavior, you would need to add "".replace(/~/g, '%7E');"" to the following.
  return encodeURIComponent(str)
    .replace('!', '%21')
    .replace('\'', '%27')
    .replace('(', '%28')
    .replace(')', '%29')
    .replace('*', '%2A')
    .replace('%20', '+');
}

    To prevent double encoding it's a good idea to decode the url before encoding (if you are dealing with user entered urls for example, which might be already encoded).

Lets say we have abc%20xyz 123 as input (one space is already encoded):

encodeURI(""abc%20xyz 123"")            //   wrong: ""abc%2520xyz%20123""
encodeURI(decodeURI(""abc%20xyz 123"")) // correct: ""abc%20xyz%20123""

    Performance

Today (2020.06.12) I perform speed test for chosen solutions on MacOs HighSierra 10.13.6 on browsers Chrome 83.0, Safari 13.1, Firefox 77.0. This results can be useful for massive urls encoding.

Conclusions


encodeURI (B) seems to be fastest but it is not recommended for url-s  
escape (A) is fast cross-browser solution
solution F recommended by MDN is medium fast
solution D is slowest




Details

For solutions 
A
B
C
D
E
F
I perform two tests


for short url - 50 char - you can run it HERE
for long url - 1M char - you can run it HERE


function A(url) {
	return escape(url);
}

function B(url) {
	return encodeURI(url);
}

function C(url) {
	return encodeURIComponent(url);
}

function D(url) {
	return new URLSearchParams({url}).toString();
}

function E(url){
     return encodeURIComponent(url).replace(/[!'()]/g, escape).replace(/\*/g, ""%2A"");
}

function F(url) {
  return encodeURIComponent(url).replace(/[!'()*]/g, function(c) {
    return '%' + c.charCodeAt(0).toString(16);
  });
}



// ----------
// TEST
// ----------

var myUrl = ""http://example.com/index.html?param=1&anotherParam=2"";

[A,B,C,D,E,F]
  .forEach(f=> console.log(`${f.name} ?url=${f(myUrl).replace(/^url=/,'')}`));This snippet only presents code of choosen solutions


Example results for Chrome


    What is URL encoding:

A URL should be encoded when there are special characters located inside the URL. For example:

console.log(encodeURIComponent('?notEncoded=&+'));


We can observe in this example that all characters except the string notEncoded are encoded with % signs. URL encoding is also known as percentage encoding because it escapes all special characters with a %. Then after this % sign every special character has a unique code

Why do we need URL encoding:

Certain characters have a special value in a URL string. For example, the ? character denotes the beginning of a query string. In order to succesfully locate a resource on the web, it is necesarry to distinguish between when a character is meant as a part of string or part of the url structure.

How can we achieve URL encoding in JS:

JS offers a bunch of build in utility function which we can use to easily encode URL's. These are two convenient options:


encodeURIComponent(): Takes a component of a URI as an argument and returns the encoded URI string.
encodeURI():  Takes a  URI as an argument and returns the encoded URI string.


Example and caveats:

Be aware of not passing in the whole URL (including scheme, e.g https://) into encodeURIComponent(). This can actually transform it into a not functional URL. For example:

// for a whole URI don't use encodeURIComponent it will transform
// the / characters and the URL won't fucntion properly
console.log(encodeURIComponent(""http://www.random.com/specials&char.html""));

// instead use encodeURI for whole URL's
console.log(encodeURI(""http://www.random.com/specials&char.html""));


We can observe f we put the whole URL in encodeURIComponent that the foward slashes (/) are also converted to special characters. This will cause the URL to not function properly anymore. 

Therefore (as the name implies) use:


encodeURIComponent on a certain part of a URL which you want to encode.
encodeURI on a whole URL which you want to encode.

    To encode a URL, as has been said before, you have two functions:

encodeURI()


and 

encodeURIComponent()


The reason both exist is that the first preserves the URL with the risk of leaving too many things unescaped, while the second encodes everything needed.

With the first, you could copy the newly escaped URL into address bar (for example) and it would work.  However your unescaped '&'s would interfere with field delimiters, the '='s would interfere with field names and values, and the '+'s would look like spaces.  But for simple data when you want to preserve the URL nature of what you are escaping, this works.

The second is everything you need to do to make sure nothing in your string interfers with a URL.  It leaves various unimportant characters unescaped so that the URL remains as human readable as possible without interference.  A URL encoded this way will no longer work as a URL without unescaping it.

So if you can take the time, you always want to use encodeURIComponent() -- before adding on name/value pairs encode both the name and the value using this function before adding it to the query string.

I'm having a tough time coming up with reasons to use the encodeURI() -- I'll leave that to the smarter people.
    Here is a LIVE DEMO of encodeURIComponent() and decodeURIComponent() JS built in functions:

<!DOCTYPE html>
<html>
  <head>
    <style>
      textarea{
        width:30%;
        height:100px;
      }
    </style>
    <script>
      // encode string to base64
      function encode()
      {
        var txt = document.getElementById(""txt1"").value;
        var result = btoa(txt);
        document.getElementById(""txt2"").value = result;
      }
      // decode base64 back to original string
      function decode()
      {
        var txt = document.getElementById(""txt3"").value;
        var result = atob(txt);
        document.getElementById(""txt4"").value = result;
      }
    </script>
  </head>
  <body>
    <div>
      <textarea id=""txt1"">Some text to decode
      </textarea>
    </div>
    <div>
      <input type=""button"" id=""btnencode"" value=""Encode"" onClick=""encode()""/>
    </div>
    <div>
      <textarea id=""txt2"">
      </textarea>
    </div>
    <br/>
    <div>
      <textarea id=""txt3"">U29tZSB0ZXh0IHRvIGRlY29kZQ==
      </textarea>
    </div>
    <div>
      <input type=""button"" id=""btndecode"" value=""Decode"" onClick=""decode()""/>
    </div>
    <div>
      <textarea id=""txt4"">
      </textarea>
    </div>
  </body>
</html>

    Use fixedEncodeURIComponent function to strictly comply with RFC 3986:
function fixedEncodeURIComponent(str) {
  return encodeURIComponent(str).replace(/[!'()*]/g, function(c) {
    return '%' + c.charCodeAt(0).toString(16);
  });
}

    Encode URL String
    
    var url = $(location).attr('href'); //get current url
    //OR
    var url = 'folder/index.html?param=#23dd&noob=yes'; //or specify one

var encodedUrl = encodeURIComponent(url);
console.log(encodedUrl);
//outputs folder%2Findex.html%3Fparam%3D%2323dd%26noob%3Dyes


for more info go http://www.sitepoint.com/jquery-decode-url-string

    You can use esapi library and encode your url using the below function. The function ensures that '/' are not lost to encoding while the remainder of the text contents are encoded:

function encodeUrl(url)
{
    String arr[] = url.split(""/"");
    String encodedUrl = """";
    for(int i = 0; i<arr.length; i++)
    {
        encodedUrl = encodedUrl + ESAPI.encoder().encodeForHTML(ESAPI.encoder().encodeForURL(arr[i]));
        if(i<arr.length-1) encodedUrl = encodedUrl + ""/"";
    }
    return url;
}


https://www.owasp.org/index.php/ESAPI_JavaScript_Readme
    var myOtherUrl = 
   ""http://example.com/index.html?url="" + encodeURIComponent(myUrl).replace(/%20/g,'+');

Don't forget the /g flag to replace all encoded ' '
    Similar kind of thing I tried with normal javascript

function fixedEncodeURIComponent(str){
     return encodeURIComponent(str).replace(/[!'()]/g, escape).replace(/\*/g, ""%2A"");
}

    Nothing worked for me. All I was seeing was the HTML of the login page, coming back to the client side with code 200. (302 at first but the same Ajax request loading login page inside another Ajax request, which was supposed to be a redirect rather than loading plain text of the login page).

In the login controller, I added this line:

Response.Headers[""land""] = ""login"";


And in the global Ajax handler, I did this:

$(function () {
    var $document = $(document);
    $document.ajaxSuccess(function (e, response, request) {
        var land = response.getResponseHeader('land');
        var redrUrl = '/login?ReturnUrl=' + encodeURIComponent(window.location);
        if(land) {
            if (land.toString() === 'login') {
                window.location = redrUrl;
            }
        }
    });
});


Now I don't have any issue, and it works like a charm.
    I always use this to encode stuff for URLs. This is completely safe because it will encode every single character even if it doesn't have to be encoded.
function urlEncode(text) {
    let encoded = '';
    for (let char of text) {
        encoded += '%' + char.charCodeAt(0).toString(16);
    }
    return encoded;
}

    ","[2716, 3037, 1628, 197, 19, 89, 5, 43, 5, 14, 6, 3, 6, 12, 3, 2, 3, 2, 1, 6, 3, -1]",1723263,505,2008-12-02T02:37:08,2022-01-31 08:01:35Z,javascript 
Find the version of an installed npm package,"
                
How to find the version of an installed node.js/npm package?

This prints the version of npm itself:

npm -v <package-name>


This prints a cryptic error:

npm version <package-name>


This prints the package version on the registry (i.e. the latest version available):

npm view <package-name> version


How do I get the installed version?
    npm list for local packages or npm list -g for globally installed packages.

You can find the version of a specific package by passing its name as an argument. For example, npm list grunt will result in:

projectName@projectVersion /path/to/project/folder
 grunt@0.4.1


Alternatively, you can just run npm list without passing a package name as an argument to see the versions of all your packages:

 cli-color@0.1.6 
  es5-ext@0.7.1 
 coffee-script@1.3.3 
 less@1.3.0 
 sentry@0.1.2 
  file@0.2.1 
  underscore@1.3.3 
 uglify-js@1.2.6 


You can also add --depth=0 argument to list installed packages without their dependencies.
    Another quick way of finding out what packages are installed locally and without their dependencies is to use:
npm list --depth=0

Which gives you something like
 bower@0.8.6
 grunt@0.4.1
 grunt-bower-requirejs@0.4.3
 grunt-contrib-clean@0.4.1
 grunt-contrib-coffee@0.7.0
 grunt-contrib-copy@0.4.1
 grunt-contrib-imagemin@0.1.4
 grunt-contrib-jshint@0.1.1
 grunt-contrib-livereload@0.1.2
 grunt-contrib-requirejs@0.4.1
 grunt-regarde@0.1.1
 grunt-svgmin@0.1.0

Obviously, the same can be done globally with npm list -g --depth=0.
This method is clearer if you have installed a lot of packages.
To find out which packages need to be updated, you can use npm outdated -g --depth=0.
    npm view <package> version - returns the latest available version on the package.

npm list --depth=0 - returns versions of all installed modules without dependencies.

npm list - returns versions of all modules and dependencies.

And lastly to get node version: node -v
    It's very simple.. Just type below line
npm view <package-name> version

Example
npm view redux version

I have version 7.2.0 of redux
    npm info YOUR_PACKAGE version


e.g.

npm info grunt version
0.4.5

    From the root of the package do:

node -p ""require('./package.json').version""


EDIT: (so you need to cd into the module's home directory if you are not already there. If you have installed the module with npm install, then it will be under node_modules/<module_name>)

EDIT 2: updated as per answer from @jeff-dickey
    I just used 

npm list | grep <package name>


and it worked great 

On windows run:

npm list | find <package name>


In PowerShell run:

npm list | sls <package name>

    I've seen some very creative answers, but you can just do this (for global packages add the --global switch):
npm ls package

Example:
npm ls babel-cli
`-- babel-cli@6.26.0

The npm documentation says that npm -ls

This command will print to stdout all the versions of packages that
are installed, as well as their dependencies, in a tree-structure.

NPM documentation
    Combining some of the above answers and produces a super simple and super quick lookup.
Run from project root.  No need to cd into any folder, just 1 line:

node -p ""require('SOMEPACKAGE/package.json').version""
    For local packages 

npm list --depth=0


For Global packages 

npm list  -g --depth=0

    npm list package-name gives the currently installed version
    You can see package.json to see installed packages version
To get the list on command line
npm ls
It will give you all installed packages in a project with their respective versions.
For particular package version
npm ls <package-name>

for eg
npm ls next

It will return version
-- next@10.1.3

    If you agree to install jq, you can use the JSON output of npm list.

npm -j ls <package-name> | jq -r .version


or, if you want to be verbose 

npm --json list <package-name> | jq --raw-output '.version'


For instance:

$ npm -j ls ghost | jq -r .version
0.4.2


Also, the JSON format is slightly different for global packages, so you'll need to change the query.

For instance:

$ npm -j -g ls | jq -r .dependencies.ghost.version
0.4.2

    To list local packages with the version number use:

npm ls --depth=0

To list global packages with the version number use:

npm ls -g --depth=0
    I am using

npm list --depth=0 | grep module_name@

it brings me results like this

 module_name@2.1033.0

    npm list --depth 0 is the command which shows all libraries with version but you can use npm-check

npm-check is a good library to manage all those things regarding the version system event it will show libraries versions, new version update, and unused version and many more.

to install it just run

npm install -g npm-check


and simply run

npm-check


check the screenshot it is showing everything about the package version, new version update, and unused version.



It works globally too. give it a try.
Hope this help someone.
    I've built a tool that does exactly that - qnm

qnm - A simple CLI utility for querying the node_modules directory.

Install it using:

npm i --global qnm


and run:

qnm [module]


for example: 

> qnm lodash

lodash
 4.17.5
 cli-table2
  3.10.1
 karma
   3.10.1


Which means we have lodash installed in the root of the node_modules and two other copies in the node_modules of cli-table2 and karma.

It's really fast, and has some nice features like tab completion and match search.
    If you'd like to check for a particular module installed globally, on *nix systems use:
npm list -g --depth=0 | grep <module_name>

    This is simple question, and should have a simpler answer than what I see above. 

To see the installed npm packages with their version, the command is npm ls --depth=0, which, by default, displays what is installed locally. To see the globally installed packages, add the -global argument: npm ls --depth=0 -global.

--depth=0 returns a list of installed packages without their dependencies, which is what you're wanting to do most of the time.

ls is the name of the command, and list is an alias for ls.
    You can also check the version with this command:

npm info <package name> version
    To see all the installed packages locally or globally, use these commands:


npm list for local packages or npm list -g for globally installed packages.
npm list --depth=0
npm list | sls <package name>
node -v

    If you are brave enough (and have node installed), you can always do something like:

echo ""console.log(require('./package.json').version);"" | node


This will print the version of the current package.
You can also modify it to go insane, like this:

echo ""eval('var result='+require('child_process').execSync('npm version',{encoding:'utf8'})); console.log(result.WHATEVER_PACKAGE_NAME);"" | node


That will print the version of WHATEVER_PACKAGE_NAME package, that is seen by npm version.
    Try with:

npm list --depth 1 --global packagename

    Here's a portable Unix (using grep and sed) one-liner that returns the version string of a globally-installed npm package (remove the g from -pg to query local packages instead):

$ npm ll -pg --depth=0 grunt | grep -o ""@.*:"" | sed 's/.$//; s/^.//'
0.4.5



the npm ll outputs a parseable string formatted like: /usr/lib/node_modules/npm:npm@2.14.8:;
the grep command extracts the value between @ and :, inclusive;
the sed command removes the surrounding characters.

    I added this to my .bashrc 

function npmv {
    case $# in # number of arguments passed
    0) v=""$(npm -v)"" ; #store output from npm -v in variable
        echo ""NPM version is: $v""; #can't use single quotes 
                                   #${v} would also work
    ;;   
    1) s=""$(npm list --depth=0 $1 | grep $1 | cut -d @ -f 2)"";
       echo ""$s"";
    ;;
    2) case ""$2"" in # second argument
        g) #global|#Syntax to compare bash string to literal
             s=""$(npm list --depth=0 -g $1 | grep $1 | cut -d @ -f 2)"";
        echo ""$s"";
        ;;
        l) #latest
             npm view $1 version; #npm info $1 version does same thing
       ;;
       *) echo 'Invalid arguments';
       ;;
       esac;
    ;;
    *) echo 'Invalid arguments';
    ;;
    esac;
}
export -f npmv


Now all I have to do is type:


npmv for the version of npm eg: NPM version is: 4.2.0
npmv <package-name> for the local version eg: 0.8.08 
npmv <package-name> g for global version eg: 0.8.09 
npmv <package-name> l for latest version eg: 0.8.10 


Note -d on cut command means delimit by, followed by @, then f means field the 2 means second field since there will be one either side of the @ symbol.
    We can  use npm view any-promise(your module name) -v
    You may try this:
npm show {package} version shows the latest package version.
And if your package is outdated, npm outdated will show it with version info.
    To get ONLY the installed version number, try:
npm list -g --depth=0 packagename | grep packagename | cut -d'@' -f2
e.g. Installed version number of PM2:
npm list -g --depth=0 pm2 | grep pm2 | cut -d'@' -f2
    Access the package.json

You can access the package.json or bower.json of the package with:

notepad ./node_modules/:packageName/package.json

This will open the package.json in notepad which has the version number of the :packageName you included in the command.

For example :

notepad ./node_modules/vue-template-compiler/package.json

Good Luck.
    There is a simple way of doing this.
first, go to the desired location (where the package.json is located).
and simple open package.json file as a text editor.
by this method, you can find all module versions in one place
package.json looks like this
{
  ""name"": ""raj"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""index.js"",
  ""scripts"": {
    ""test"": ""echo \""Error: no test specified\"" && exit 1""
  },
  ""author"": """",
  ""license"": ""ISC"",
  ""dependencies"": {
    ""bcrypt"": ""^5.0.1"",
    ""connect-flash"": ""^0.1.1"",
    ""dotenv"": ""^10.0.0"",
    ""ejs"": ""^3.1.6"",
    ""express"": ""^4.17.1"",
    ""express-session"": ""^1.17.2"",
    ""mysql2"": ""^2.2.5"",
    ""passport"": ""^0.4.1"",
    ""passport-local"": ""^1.0.0"",
    ""sequelize"": ""^6.6.2"",
    ""socket.io"": ""^4.1.2""
  }
}

so thus you can read every installed dependency (modules)in your pc
i.e. ""socket.io"": ""^4.1.2"" so 'socket.io' having version 4.1.2
    ","[2709, 3116, 993, 269, 30, 107, 75, 72, 12, 28, 28, 9, 6, 21, 12, 3, 11, 12, 4, 7, 20, 13, 15, 11, 8, 6, 3, 5, 0, 3, 0]",1693524,426,2012-06-10T20:36:27,2022-01-25 23:41:07Z,
HTTP GET with request body,"
                
I'm developing a new RESTful webservice for our application.
When doing a GET on certain entities, clients can request the contents of the entity.
If they want to add some parameters (for example sorting a list) they can add these parameters in the query string.
Alternatively I want people to be able to specify these parameters in the request body.
HTTP/1.1 does not seem to explicitly forbid this. This will allow them to specify more information, might make it easier to specify complex XML requests.
My questions:

Is this a good idea altogether?
Will HTTP clients have issues with using request bodies within a GET request?

https://www.rfc-editor.org/rfc/rfc2616
    Roy Fielding's comment about including a body with a GET request.

Yes. In other words, any HTTP request message is allowed to contain a message body, and thus must parse messages with that in mind. Server semantics for GET, however, are restricted such that a body, if any, has no semantic meaning to the request. The requirements on parsing are separate from the requirements on method semantics.
So, yes, you can send a body with GET, and no, it is never useful to do so.
This is part of the layered design of HTTP/1.1 that will become clear again once the spec is partitioned (work in progress).
....Roy

Yes, you can send a request body with GET but it should not have any meaning. If you give it meaning by parsing it on the server and changing your response based on its contents, then you are ignoring this recommendation in the HTTP/1.1 spec, section 4.3:

...if the request method does not include defined semantics for an entity-body, then the message-body SHOULD be ignored when handling the request.

And the description of the GET method in the HTTP/1.1 spec, section 9.3:

The GET method means retrieve whatever information ([...]) is identified by the Request-URI.

which states that the request-body is not part of the identification of the resource in a GET request, only the request URI.
Update
The RFC2616 referenced as ""HTTP/1.1 spec"" is now obsolete. In 2014 it was replaced by RFCs 7230-7237. Quote ""the message-body SHOULD be ignored when handling the request"" has been deleted. It's now just ""Request message framing is independent of method semantics, even if the method doesn't define any use for a message body"" The 2nd quote ""The GET method means retrieve whatever information ... is identified by the Request-URI"" was deleted.  - From a comment
From the HTTP 1.1 2014 Spec:

A payload within a GET request message has no defined semantics; sending a payload body on a GET request might cause some existing implementations to reject the request.

    While you can do that, insofar as it isn't explicitly precluded by the HTTP specification, I would suggest avoiding it simply because people don't expect things to work that way.  There are many phases in an HTTP request chain and while they ""mostly"" conform to the HTTP spec, the only thing you're assured is that they will behave as traditionally used by web browsers.  (I'm thinking of things like transparent proxies, accelerators, A/V toolkits, etc.)

This is the spirit behind the Robustness Principle roughly ""be liberal in what you accept, and conservative in what you send"", you don't want to push the boundaries of a specification without good reason.  

However, if you have a good reason, go for it.
    You will likely encounter problems if you ever try to take advantage of caching. Proxies are not going to look in the GET body to see if the parameters have an impact on the response.
    GET, with a body!?
Specification-wise you could, but, it's not a good idea to do so injudiciously, as we shall see.
RFC 7231 4.3.1 states that a body ""has no defined semantics"", but that's not to say it is forbidden. If you attach a body to the request and what your server/app makes out of it is up to you. The RFC goes on to state that GET can be ""a programmatic view on various database records"". Obviously such view is many times tailored by a large number of input parameters, which are not always convenient or even safe to put in the query component of the request-target.
The good: I like the verbiage. It's clear that one read/get a resource without any observable side-effects on the server (the method is ""safe""), and, the request can be repeated with the same intended effect regardless of the outcome of the first request (the method is ""idempotent"").
The bad: An early draft of HTTP/1.1 forbade GET to have a body, and - allegedly - some implementations will even up until today drop the body, ignore the body or reject the message. For example, a dumb HTTP cache may construct a cache key out of the request-target only, being oblivious to the presence or content of a body. An even dumber server could be so ignorant that it treats the body as a new request, which effectively is called ""request smuggling"" (which is the act of sending ""a request to one device without the other device being aware of it"" - source).
Due to what I believe is primarily a concern with inoperability amongst implementations, work in progress suggests to categorize a GET body as a ""SHOULD NOT"", ""unless [the request] is made directly to an origin server that has previously indicated, in or out of band, that such a request has a purpose and will be adequately supported"" (emphasis mine).
The fix: There's a few hacks that can be employed for some of the problems with this approach. For example, body-unaware caches can indirectly become body-aware simply by appending a hash derived from the body to the query component, or disable caching altogether by responding a cache-control: no-cache header from the server.
Alas when it comes to the request chain, one is often not in control of- or even aware, of all present and future HTTP intermediaries and how they will deal with a GET body. That's why this approach must be considered generally unreliable.
But POST, is not idempotent!
POST is an alternative. The POST request usually includes a message body (just for the record, body is not a requirement, see RFC 7230 3.3.2). The very first use case example from RFC 7231 (4.3.3) is ""providing a block of data [...] to a data-handling process"". So just like GET with a body, what happens with the body on the back-end side is up to you.
The good: Perhaps a more common method to apply when one wish to send a request body, for whatever purpose, and so, will likely yield the least amount of noise from your team members (some may still falsely believe that POST must create a resource).
Also, what we often pass parameters to is a search function operating upon constantly evolving data, and a POST response is only cacheable if explicit freshness information is provided in the response.
The bad: POST requests are not defined as idempotent, leading to request retry hesitancy. For example, on page reload, browsers are unwilling to resubmit an HTML form without prompting the user with a nonreadable cryptic message.
The fix: Well, just because POST is not defined to be idempotent doesn't mean it mustn't be. Indeed, RFC 7230 6.3.1 writes: ""a user agent that knows (through design or configuration) that a POST request to a given resource is safe can repeat that request automatically"". So, unless your client is an HTML form, this is probably not a real problem.
QUERY is the holy grail
There's a proposal for a new method QUERY which does define semantics for a message body and defines the method as idempotent. See this.
Edit: As a side-note, I stumbled into this StackOverflow question after having discovered a codebase where they solely used PUT requests for server-side search functions. This were their idea to include a body with parameters and also be idempotent. Alas the problem with PUT is that the request body has very precise semantics. Specifically, the PUT ""requests that the state of the target resource be created or replaced with the state [in the body]"" (RFC 7231 4.3.4). Clearly, this excludes PUT as a viable option.
    Elasticsearch accepts GET requests with a body. It even seems that this is the preferred way: Elasticsearch guide

Some client libraries (like the Ruby driver) can log the cry command to stdout in development mode and it is using this syntax extensively.
    Neither restclient nor REST console support this but curl does.

The HTTP specification says in section 4.3


  A message-body MUST NOT be included in a request if the specification of the request method (section 5.1.1) does not allow sending an entity-body in requests.


Section 5.1.1 redirects us to section 9.x for the various methods. None of them explicitly prohibit the inclusion of a message body. However...

Section 5.2 says 


  The exact resource identified by an Internet request is determined by examining both the Request-URI and the Host header field.


and Section 9.3 says


  The GET method means retrieve whatever information (in the form of an entity) is identified by the Request-URI.


Which together suggest that when processing a GET request, a server is not required to examine anything other that the Request-URI and Host header field.

In summary, the HTTP spec doesn't prevent you from sending a message-body with GET but there is sufficient ambiguity that it wouldn't surprise me if it was not supported by all servers. 
    I put this question to the IETF HTTP WG.  The comment from Roy Fielding (author of http/1.1 document in 1998) was that


  ""... an implementation would be broken to do anything other than to parse and discard that body if received""


RFC 7213 (HTTPbis) states:


  ""A payload within a GET request message has no defined semantics;""


It seems clear now that the intention was that semantic meaning on GET request bodies is prohibited, which means that the request body can't be used to affect the result.

There are proxies out there that will definitely break your request in various ways if you include a body on GET.

So in summary, don't do it.
    You can either send a GET with a body or send a POST and give up RESTish religiosity (it's not so bad, 5 years ago there was only one member of that faith -- his comments linked above).

Neither are great decisions, but sending a GET body may prevent problems for some clients -- and some servers.  

Doing a POST might have obstacles with some RESTish frameworks. 

Julian Reschke suggested above using a non-standard HTTP header like ""SEARCH"" which could be an elegant solution, except that it's even less likely to be supported.

It might be most productive to list clients that can and cannot do each of the above.

Clients that cannot send a GET with body (that I know of):


XmlHTTPRequest Fiddler


Clients that can send a GET with body:


most browsers


Servers & libraries that can retrieve a body from GET:


Apache
PHP


Servers (and proxies) that strip a body from GET:


?

    What you're trying to achieve has been done for a long time with a much more common method, and one that doesn't rely on using a payload with GET.

You can simply build your specific search mediatype, or if you want to be more RESTful, use something like OpenSearch, and POST the request to the URI the server instructed, say /search. The server can then generate the search result or build the final URI and redirect using a 303.

This has the advantage of following the traditional PRG method, helps cache intermediaries cache the results, etc.

That said, URIs are encoded anyway for anything that is not ASCII, and so are application/x-www-form-urlencoded and multipart/form-data. I'd recommend using this rather than creating yet another custom json format if your intention is to support ReSTful scenarios.
    According to XMLHttpRequest, it's not valid. From the standard:


  4.5.6 The send() method
  
  
client . send([body = null])
    
    Initiates the request. The optional argument provides the request
    body. The argument is ignored if request method is GET or HEAD.
    
    Throws an InvalidStateError exception if either state is not
    opened or the send() flag is set.
  
  
  The send(body) method must run these steps:
  
  
  If state is not opened, throw an InvalidStateError exception.
  If the send() flag is set, throw an InvalidStateError exception.
  If the request method is GET or HEAD, set body to null.
  If body is null, go to the next step.
  


Although, I don't think it should because GET request might need big body content.

So, if you rely on XMLHttpRequest of a browser, it's likely it won't work.
    Even if a popular tool use this, as cited frequently on this page, I think it is still quite a bad idea, being too exotic, despite not forbidden by the spec.

Many intermediate infrastructures may just reject such requests.

By example, forget about using some of the available CDN in front of your web site, like this one:


  If a viewer GET request includes a body, CloudFront returns an HTTP status code 403 (Forbidden) to the viewer.


And yes, your client libraries may also not support emitting such requests, as reported in this comment.
    You have a list of options which are far better than using a request body with GET.

Let' assume you have categories and items for each category. Both to be identified by an id (""catid"" / ""itemid"" for the sake of this example). You want to sort according to another parameter ""sortby"" in a specific ""order"". You want to pass parameters for ""sortby"" and ""order"":

You can:


Use query strings, e.g.
example.com/category/{catid}/item/{itemid}?sortby=itemname&order=asc
Use mod_rewrite (or similar) for paths:
example.com/category/{catid}/item/{itemid}/{sortby}/{order}
Use individual HTTP headers you pass with the request
Use a different method, e.g. POST, to retrieve a resource.


All have their downsides, but are far better than using a GET with a body.
    If you really want to send cachable JSON/XML body to web application the only reasonable place to put your data is query string encoded with RFC4648: Base 64 Encoding with URL and Filename Safe Alphabet. Of course you could just urlencode JSON and put is in URL param's value, but Base64 gives smaller result. Keep in mind that there are URL size restrictions, see What is the maximum length of a URL in different browsers? .
You may think that Base64's padding = character may be bad for URL's param value, however it seems not - see this discussion: http://mail.python.org/pipermail/python-bugs-list/2007-February/037195.html . However you shouldn't put encoded data without param name because encoded string with padding will be interpreted as param key with empty value.
I would use something like ?_b64=<encodeddata>.
    From RFC 2616, section 4.3, ""Message Body"":

A server SHOULD read and forward a message-body on any request; if the
request method does not include defined semantics for an entity-body,
then the message-body SHOULD be ignored when handling the request.

That is, servers should always read any provided request body from the network (check Content-Length or read a chunked body, etc). Also, proxies should forward any such request body they receive. Then, if the RFC defines semantics for the body for the given method, the server can actually use the request body in generating a response. However, if the RFC does not define semantics for the body, then the server should ignore it.
This is in line with the quote from Fielding above.
Section 9.3, ""GET"", describes the semantics of the GET method, and doesn't mention request bodies. Therefore, a server should ignore any request body it receives on a GET request.
    I'm upset that REST as protocol doesn't support OOP and Get method is proof. As a solution, you can serialize your a DTO to JSON and then create a query string. On server side you'll able to deserialize the query string to the DTO.

Take a look on:


Message-based design in ServiceStack
Building RESTful Message Based Web Services with WCF


Message based approach can help you to solve Get method restriction. You'll able to send any DTO as with request body

Nelibur web service framework provides functionality which you can use

var client = new JsonServiceClient(Settings.Default.ServiceAddress);
var request = new GetClientRequest
    {
        Id = new Guid(""2217239b0e-b35b-4d32-95c7-5db43e2bd573"")
    };
var response = client.Get<GetClientRequest, ClientResponse>(request);

as you can see, the GetClientRequest was encoded to the following query string

http://localhost/clients/GetWithResponse?type=GetClientRequest&data=%7B%22Id%22:%2217239b0e-b35b-4d32-95c7-5db43e2bd573%22%7D

    I wouldn't advise this, it goes against standard practices, and doesn't offer that much in return. You want to keep the body for content, not options.
    What about nonconforming base64 encoded headers? ""SOMETHINGAPP-PARAMS:sdfSD45fdg45/aS""

Length restrictions hm. Can't you make your POST handling distinguish between the meanings? If you want simple parameters like sorting, I don't see why this would be a problem. I guess it's certainty you're worried about.
    IMHO you could just send the JSON encoded (ie. encodeURIComponent) in the URL, this way you do not violate the HTTP specs and get your JSON to the server.
    For example, it works with Curl, Apache and PHP.

PHP file:

<?php
echo $_SERVER['REQUEST_METHOD'] . PHP_EOL;
echo file_get_contents('php://input') . PHP_EOL;


Console command:

$ curl -X GET -H ""Content-Type: application/json"" -d '{""the"": ""body""}' 'http://localhost/test/get.php'


Output:

GET
{""the"": ""body""}

    
  Which server will ignore it?  fijiaaron Aug 30 '12 at 21:27


Google for instance is doing worse than ignoring it, it will consider it an error!

Try it yourself with a simple netcat:

$ netcat www.google.com 80
GET / HTTP/1.1
Host: www.google.com
Content-length: 6

1234


(the 1234 content is followed by CR-LF, so that is a total of 6 bytes)

and you will get:

HTTP/1.1 400 Bad Request
Server: GFE/2.0
(....)
Error 400 (Bad Request)
400. Thats an error.
Your client has issued a malformed or illegal request. Thats all we know.


You do also get 400 Bad Request from Bing, Apple, etc... which are served by AkamaiGhost.

So I wouldn't advise using GET requests with a body entity.
    Create a Requestfactory class
import java.net.URI;

import javax.annotation.PostConstruct;

import org.apache.http.client.methods.HttpEntityEnclosingRequestBase;
import org.apache.http.client.methods.HttpUriRequest;
import org.springframework.http.HttpMethod;
import org.springframework.http.client.HttpComponentsClientHttpRequestFactory;
import org.springframework.stereotype.Component;
import org.springframework.web.client.RestTemplate;

@Component
public class RequestFactory {
    private RestTemplate restTemplate = new RestTemplate();

    @PostConstruct
    public void init() {
        this.restTemplate.setRequestFactory(new HttpComponentsClientHttpRequestWithBodyFactory());
    }

    private static final class HttpComponentsClientHttpRequestWithBodyFactory extends HttpComponentsClientHttpRequestFactory {
        @Override
        protected HttpUriRequest createHttpUriRequest(HttpMethod httpMethod, URI uri) {
            if (httpMethod == HttpMethod.GET) {
                return new HttpGetRequestWithEntity(uri);
            }
            return super.createHttpUriRequest(httpMethod, uri);
        }
    }

    private static final class HttpGetRequestWithEntity extends HttpEntityEnclosingRequestBase {
        public HttpGetRequestWithEntity(final URI uri) {
            super.setURI(uri);
        }

        @Override
        public String getMethod() {
            return HttpMethod.GET.name();
        }
    }

    public RestTemplate getRestTemplate() {
        return restTemplate;
    }
}

and @Autowired where ever you require and use, Here is one sample code GET request with RequestBody
 @RestController
 @RequestMapping(""/v1/API"")
public class APIServiceController {
    
    @Autowired
    private RequestFactory requestFactory;
    

    @RequestMapping(method = RequestMethod.GET, path = ""/getData"")
    public ResponseEntity<APIResponse> getLicenses(@RequestBody APIRequest2 APIRequest){
        APIResponse response = new APIResponse();
        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.APPLICATION_JSON);
        Gson gson = new Gson();
        try {
            StringBuilder createPartUrl = new StringBuilder(PART_URL).append(PART_URL2);
            
            HttpEntity<String> entity = new HttpEntity<String>(gson.toJson(APIRequest),headers);
            ResponseEntity<APIResponse> storeViewResponse = requestFactory.getRestTemplate().exchange(createPartUrl.toString(), HttpMethod.GET, entity, APIResponse.class); //.getForObject(createLicenseUrl.toString(), APIResponse.class, entity);
    
            if(storeViewResponse.hasBody()) {
                response = storeViewResponse.getBody();
            }
            return new ResponseEntity<APIResponse>(response, HttpStatus.OK);
        }catch (Exception e) {
            e.printStackTrace();
            return new ResponseEntity<APIResponse>(response, HttpStatus.INTERNAL_SERVER_ERROR);
        }
        
    }
}

    ","[2706, 2193, 343, 207, 20, 81, 88, 33, 40, 38, 14, 4, 7, 11, 21, 6, 7, 5, 4, 4, 22, -5]",1742804,678,2009-06-10T20:47:24,2021-12-02 09:04:44Z,
Ignore files that have already been committed to a Git repository [duplicate],"
                    
            
        
            
                
                    
                        This question already has answers here:
                        
                    
                
            
                    
                        How can I make Git ""forget"" about a file that was tracked, but is now in .gitignore?
                            
                                (31 answers)
                            
                    
                Closed 4 years ago.
        

    

I have an already initialized Git repository that I added a .gitignore file to. How can I refresh the file index so the files I want ignored get ignored?
    To untrack a single file that has already been added/initialized to your repository, i.e., stop tracking the file but not delete it from your system use: git rm --cached filename
To untrack every file that is now in your .gitignore:
First commit any outstanding code changes, and then, run this command:
git rm -r --cached .

This removes any changed files from the index(staging area), then just run:
git add .

Commit it:
git commit -m "".gitignore is now working""


To undo git rm --cached filename, use git add filename.

Make sure to commit all your important changes before running git add .
Otherwise, you will lose any changes to other files.


Please be careful, when you push this to a repository and pull from somewhere else into a state where those files are still tracked, the files will be DELETED

    If you are trying to ignore changes to a file that's already tracked in the repository (e.g. a dev.properties file that you would need to change for your local environment but you would never want to check in these changes) than what you want to do is:

git update-index --assume-unchanged <file>


If you wanna start tracking changes again

git update-index --no-assume-unchanged <file>


See git-update-index(1) Manual Page.

Also have a look at the skip-worktree and no-skip-worktree options for update-index if you need this to persist past a git-reset (via)



Update: 
Since people have been asking, here's a convenient (and updated since commented on below) alias for seeing which files are currently ""ignored"" (--assume-unchanged) in your local workspace

$ git config --global alias.ignored = !git ls-files -v | grep ""^[[:lower:]]""

    To untrack a file that has already been added/initialized to your repository, ie stop tracking the file but not delete it from your system use: git rm --cached filename
    Complex answers everywhere!

Just use the following 

git rm -r --cached .



  It will remove the files you are trying to ignore from the origin and not from the master on your computer! 


After that just commit and push!
    Yes - .gitignore system only ignores files not currently under version control from git. 

I.e. if you've already added a file called test.txt using git-add, then adding test.txt to .gitignore will still cause changes to test.txt to be tracked.

You would have to git rm test.txt first and commit that change. Only then will changes to test.txt be ignored.
    i followed these steps

git rm -r --cached .
git add .
git reset HEAD


after that, git delete all files (*.swp in my case) that should be ignoring.
    Remove trailing whitespace in .gitignore

Also, make sure you have no trailing whitespace in your .gitignore.  I got to this question because I was searching for an answer, then I had a funny feeling I should open the editor instead of just cat'ing .gitignore.  Removed a single extra space from the end and poof it works now :)
    If you want to stop tracking file without deleting the file from your local system, which I prefer for ignoring config/database.yml file. Simply try:

git rm --cached config/database.yml
# this will delete your file from git history but not from your local system.


now, add this file to .gitignore file and commit the changes. And from now on, any changes made to config/database.yml will not get tracked by git.

$ echo config/database.yml >> .gitignore


Thanks
    If you need to stop tracking a lot of ignored files, you can combine some commands:

git ls-files -i --exclude-standard | xargs -L1 git rm --cached

This would stop tracking the ignored files. If you want to actually remove files from filesystem, do not use the --cached option. You can also specify a folder to limit the search, such as:

git ls-files -i --exclude-standard -- ${FOLDER} | xargs -L1 git rm
    If the files are already in version control you need to remove them manually.
    None of the answers worked for me.


Instead:


Move the file out of the git-controlled directory
Check the removal into git
Move the file back into the git-controlled directory



After moving the file back, git will ignore it.


Works with directories too!
    Thanks to your answer, I was able to write this little one-liner to improve it.  I ran it on my .gitignore and repo, and had no issues, but if anybody sees any glaring problems, please comment.  This should git rm -r --cached from  .gitignore:

cat $(git rev-parse --show-toplevel)/.gitIgnore | sed ""s/\/$//"" | grep -v ""^#"" | xargs -L 1 -I {} find $(git rev-parse --show-toplevel) -name ""{}"" | xargs -L 1 git rm -r --cached


Note that you'll get a lot of fatal: pathspec '<pathspec>' did not match any files.  That's just for the files which haven't been modified.
    There is another suggestion maybe for the slow guys like me =) Put the .gitignore file into your repository root not in .git folder. Cheers!
    To remove just a few specific files from being tracked:

git update-index --assume-unchanged path/to/file


If ever you want to start tracking it again:

git update-index --no-assume-unchanged path/to/file                      

    Not knowing quite what the 'answer' command did, I ran it, much to my dismay. It recursively removes every file from your git repo. 

Stackoverflow to the rescue... How to revert a ""git rm -r .""?

git reset HEAD


Did the trick, since I had uncommitted local files that I didn't want to overwrite. 
    another problem I had was I placed an inline comment.

tmp/*   # ignore my tmp folder (this doesn't work)


this works

# ignore my tmp folder
tmp/

    As dav_i says, in order to keep the file in repo and yet removing it from changes without creating an extra commit you can use:

git update-index --assume-unchanged filename

    One other problem not mentioned here is if you've created your .gitignore in Windows notepad it can look like gibberish on other platforms as I found out.  The key is to make sure you the encoding is set to ANSI in notepad, (or make the file on linux as I did).

From my answer here: https://stackoverflow.com/a/11451916/406592
    I have found a weird problem with .gitignore. Everything was in place and seemed correct. The only reason why my .gitignore was ""ignored"" was, that the line-ending was in Mac-Format (\r). So after saving the file with the correct line-ending (in vi using :set ff=unix) everything worked like a charm!
    One thing to also keep in mind if .gitignore does not seem to be ignoring untracked files is that you should not have comments on the same line as the ignores. So this is okay

# ignore all foo.txt, foo.markdown, foo.dat, etc.
foo*


But this will not work:

foo*   # ignore all foo.txt, foo.markdown, foo.dat, etc.


.gitignore interprets the latter case as ""ignore files named ""foo*   # ignore all foo.txt, foo.markdown, foo.dat, etc."", which, of course, you don't have.
    On my server linux server (not true on my local dev mac), directories are ignored as long as I don't add an asterisk:


  www/archives/*


I don't know why but it made me loose a couple of hours, so I wanted to share...
    ","[2696, 4478, 666, 373, 44, 90, 46, 55, 42, 9, 23, 25, 15, 25, 31, 25, 21, 29, 10, 10, 7, 8]",814036,1468,2009-07-16T19:26:35,2021-10-23 14:53:52Z,
How can I get query string values in JavaScript?,"
                    
            
        
            
                    
                        
                    
                
                    
                        This question's answers are a community effort. Edit existing answers to improve this post. It is not currently accepting new answers or interactions.
                        
                    
                
            
        

    

Is there a plugin-less way of retrieving query string values via jQuery (or without)? 

If so, how? If not, is there a plugin which can do so?
    Update: Jan-2022
Using Proxy() is more performant than using Object.fromEntries() and better supported
const params = new Proxy(new URLSearchParams(window.location.search), {
  get: (searchParams, prop) => searchParams.get(prop),
});
// Get the value of ""some_key"" in eg ""https://example.com/?some_key=some_value""
let value = params.some_key; // ""some_value""

Update: June-2021
For a specific case when you need all query params:
const urlSearchParams = new URLSearchParams(window.location.search);
const params = Object.fromEntries(urlSearchParams.entries());

Update: Sep-2018
You can use URLSearchParams which is simple and has decent (but not complete) browser support.
const urlParams = new URLSearchParams(window.location.search);
const myParam = urlParams.get('myParam');

Original
You don't need jQuery for that purpose. You can use just some pure JavaScript:
function getParameterByName(name, url = window.location.href) {
    name = name.replace(/[\[\]]/g, '\\$&');
    var regex = new RegExp('[?&]' + name + '(=([^&#]*)|&|#|$)'),
        results = regex.exec(url);
    if (!results) return null;
    if (!results[2]) return '';
    return decodeURIComponent(results[2].replace(/\+/g, ' '));
}

Usage:
// query string: ?foo=lorem&bar=&baz
var foo = getParameterByName('foo'); // ""lorem""
var bar = getParameterByName('bar'); // """" (present with empty value)
var baz = getParameterByName('baz'); // """" (present with no value)
var qux = getParameterByName('qux'); // null (absent)

NOTE: If a parameter is present several times (?foo=lorem&foo=ipsum), you will get the first value (lorem). There is no standard about this and usages vary, see for example this question: Authoritative position of duplicate HTTP GET query keys.
NOTE: The function is case-sensitive. If you prefer case-insensitive parameter name, add 'i' modifier to RegExp
NOTE: If you're getting a no-useless-escape eslint error, you can replace name = name.replace(/[\[\]]/g, '\\$&'); with name = name.replace(/[[\]]/g, '\\$&').

This is an update based on the new URLSearchParams specs to achieve the same result more succinctly. See answer titled ""URLSearchParams"" below.
    URLSearchParams
Firefox 44+, Opera 36+, Edge 17+, Safari 10.3+ and Chrome 49+ support the URLSearchParams API:

Chrome Announcement and details
Opera Announcement and details
Firefox Announcement and details

There is a google-suggested URLSearchParams polyfill for the stable versions of IE.
It is not standardized by W3C, but it is a living standard by WhatWG.
You can use it on location:
const params = new URLSearchParams(location.search);

or
const params = (new URL(location)).searchParams;

Or of course on any URL:
const url = new URL('https://example.com?foo=1&bar=2');
const params = new URLSearchParams(url.search);

You can get params also using a shorthand .searchParams property on the URL object, like this:
const params = new URL('https://example.com?foo=1&bar=2').searchParams;
params.get('foo'); // ""1""
params.get('bar'); // ""2"" 

You read/set parameters through the get(KEY), set(KEY, VALUE), append(KEY, VALUE) API. You can also iterate over all values for (let p of params) {}.
A reference implementation and a sample page are available for auditing and testing.
    Some of the solutions posted here are inefficient. Repeating the regular expression search every time the script needs to access a parameter is completely unnecessary, one single function to split up the parameters into an associative-array style object is enough. If you're not working with the HTML 5 History API, this is only necessary once per page load.  The other suggestions here also fail to decode the URL correctly.
var urlParams;
(window.onpopstate = function () {
    var match,
        pl     = /\+/g,  // Regex for replacing addition symbol with a space
        search = /([^&=]+)=?([^&]*)/g,
        decode = function (s) { return decodeURIComponent(s.replace(pl, "" "")); },
        query  = window.location.search.substring(1);
  
    urlParams = {};
    while (match = search.exec(query))
       urlParams[decode(match[1])] = decode(match[2]);
})();
Example querystring:

?i=main&mode=front&sid=de8d49b78a85a322c4155015fdce22c4&enc=+Hello%20&empty

Result:
 urlParams = {
    enc: "" Hello "",
    i: ""main"",
    mode: ""front"",
    sid: ""de8d49b78a85a322c4155015fdce22c4"",
    empty: """"
}

alert(urlParams[""mode""]);
// -> ""front""

alert(""empty"" in urlParams);
// -> true

This could easily be improved upon to handle array-style query strings too.  An example of this is here, but since array-style parameters aren't defined in RFC 3986 I won't pollute this answer with the source code. For those interested in a ""polluted"" version, look at campbeln's answer below.
Also, as pointed out in the comments, ; is a legal delimiter for key=value pairs.  It would require a more complicated regex to handle ; or &, which I think is unnecessary because it's rare that ; is used and I would say even more unlikely that both would be used.  If you need to support ; instead of &, just swap them in the regex.
 
If you're using a server-side preprocessing language, you might want to use its native JSON functions to do the heavy lifting for you.  For example, in PHP you can write:
<script>var urlParams = <?php echo json_encode($_GET, JSON_HEX_TAG);?>;</script>
Much simpler!
#UPDATED

A new capability would be to retrieve repeated params as following myparam=1&myparam=2. There is not a specification, however, most of the current approaches follow the generation of an array.

myparam = [""1"", ""2""]

So, this is the approach to manage it:
let urlParams = {};
(window.onpopstate = function () {
    let match,
        pl = /\+/g,  // Regex for replacing addition symbol with a space
        search = /([^&=]+)=?([^&]*)/g,
        decode = function (s) {
            return decodeURIComponent(s.replace(pl, "" ""));
        },
        query = window.location.search.substring(1);

    while (match = search.exec(query)) {
        if (decode(match[1]) in urlParams) {
            if (!Array.isArray(urlParams[decode(match[1])])) {
                urlParams[decode(match[1])] = [urlParams[decode(match[1])]];
            }
            urlParams[decode(match[1])].push(decode(match[2]));
        } else {
            urlParams[decode(match[1])] = decode(match[2]);
        }
    }
})();

    ES2015 (ES6)
getQueryStringParams = query => {
    return query
        ? (/^[?#]/.test(query) ? query.slice(1) : query)
            .split('&')
            .reduce((params, param) => {
                    let [key, value] = param.split('=');
                    params[key] = value ? decodeURIComponent(value.replace(/\+/g, ' ')) : '';
                    return params;
                }, {}
            )
        : {}
};


Without jQuery
var qs = (function(a) {
    if (a == """") return {};
    var b = {};
    for (var i = 0; i < a.length; ++i)
    {
        var p=a[i].split('=', 2);
        if (p.length == 1)
            b[p[0]] = """";
        else
            b[p[0]] = decodeURIComponent(p[1].replace(/\+/g, "" ""));
    }
    return b;
})(window.location.search.substr(1).split('&'));

With an URL like ?topic=123&name=query+string, the following will return:
qs[""topic""];    // 123
qs[""name""];     // query string
qs[""nothere""];  // undefined (object)


Google method
Tearing Google's code I found the method they use: getUrlParameters
function (b) {
    var c = typeof b === ""undefined"";
    if (a !== h && c) return a;
    for (var d = {}, b = b || k[B][vb], e = b[p](""?""), f = b[p](""#""), b = (f === -1 ? b[Ya](e + 1) : [b[Ya](e + 1, f - e - 1), ""&"", b[Ya](f + 1)][K](""""))[z](""&""), e = i.dd ? ia : unescape, f = 0, g = b[w]; f < g; ++f) {
        var l = b[f][p](""="");
        if (l !== -1) {
            var q = b[f][I](0, l),
                l = b[f][I](l + 1),
                l = l[Ca](/\+/g, "" "");
            try {
                d[q] = e(l)
            } catch (A) {}
        }
    }
    c && (a = d);
    return d
}

It is obfuscated, but it is understandable. It does not work because some variables are undefined.
They start to look for parameters on the url from ? and also from the hash #. Then for each parameter they split in the equal sign b[f][p](""="") (which looks like indexOf, they use the position of the char to get the key/value). Having it split they check whether the parameter has a value or not, if it has then they store the value of d, otherwise they just continue.
In the end the object d is returned, handling escaping and the + sign. This object is just like mine, it has the same behavior.

My method as a jQuery plugin
(function($) {
    $.QueryString = (function(paramsArray) {
        let params = {};

        for (let i = 0; i < paramsArray.length; ++i)
        {
            let param = paramsArray[i]
                .split('=', 2);
            
            if (param.length !== 2)
                continue;
            
            params[param[0]] = decodeURIComponent(param[1].replace(/\+/g, "" ""));
        }
            
        return params;
    })(window.location.search.substr(1).split('&'))
})(jQuery);

Usage
//Get a param
$.QueryString.param
//-or-
$.QueryString[""param""]
//This outputs something like...
//""val""

//Get all params as object
$.QueryString
//This outputs something like...
//Object { param: ""val"", param2: ""val"" }

//Set a param (only in the $.QueryString object, doesn't affect the browser's querystring)
$.QueryString.param = ""newvalue""
//This doesn't output anything, it just updates the $.QueryString object

//Convert object into string suitable for url a querystring (Requires jQuery)
$.param($.QueryString)
//This outputs something like...
//""param=newvalue&param2=val""

//Update the url/querystring in the browser's location bar with the $.QueryString object
history.replaceState({}, '', ""?"" + $.param($.QueryString));
//-or-
history.pushState({}, '', ""?"" + $.param($.QueryString));


Performance test (split method against regex method) (jsPerf)
Preparation code: methods declaration
Split test code
var qs = window.GetQueryString(query);

var search = qs[""q""];
var value = qs[""value""];
var undef = qs[""undefinedstring""];

Regex test code
var search = window.getParameterByName(""q"");
var value = window.getParameterByName(""value"");
var undef = window.getParameterByName(""undefinedstring"");

Testing in Firefox 4.0 x86 on Windows Server 2008 R2 / 7 x64

Split method: 144,780 2.17% fastest
Regex method: 13,891 0.85% | 90% slower

    tl;dr
A quick, complete solution, which handles multivalued keys and encoded characters.
// using ES5   (200 characters)
var qd = {};
if (location.search) location.search.substr(1).split(""&"").forEach(function(item) {var s = item.split(""=""), k = s[0], v = s[1] && decodeURIComponent(s[1]); (qd[k] = qd[k] || []).push(v)})

// using ES6   (23 characters cooler)
var qd = {};
if (location.search) location.search.substr(1).split`&`.forEach(item => {let [k,v] = item.split`=`; v = v && decodeURIComponent(v); (qd[k] = qd[k] || []).push(v)})

// as a function with reduce
function getQueryParams() {
  return location.search
    ? location.search.substr(1).split`&`.reduce((qd, item) => {let [k,v] = item.split`=`; v = v && decodeURIComponent(v); (qd[k] = qd[k] || []).push(v); return qd}, {})
    : {}
}

Multi-lined:
var qd = {};
if (location.search) location.search.substr(1).split(""&"").forEach(function(item) {
    var s = item.split(""=""),
        k = s[0],
        v = s[1] && decodeURIComponent(s[1]); //  null-coalescing / short-circuit
    //(k in qd) ? qd[k].push(v) : qd[k] = [v]
    (qd[k] = qd[k] || []).push(v) // null-coalescing / short-circuit
})

What is all this code...
""null-coalescing"", short-circuit evaluation
ES6 Destructuring assignments, Arrow functions, Template strings
####Example:
""?a=1&b=0&c=3&d&e&a=5&a=t%20e%20x%20t&e=http%3A%2F%2Fw3schools.com%2Fmy%20test.asp%3Fname%3Dstle%26car%3Dsaab""
> qd
a: [""1"", ""5"", ""t e x t""]
b: [""0""]
c: [""3""]
d: [undefined]
e: [undefined, ""http://w3schools.com/my test.asp?name=stle&car=saab""]

> qd.a[1]    // ""5""
> qd[""a""][1] // ""5""



Read more... about the Vanilla JavaScript solution.
To access different parts of a URL use location.(search|hash)
Easiest (dummy) solution
var queryDict = {};
location.search.substr(1).split(""&"").forEach(function(item) {queryDict[item.split(""="")[0]] = item.split(""="")[1]})


Handles empty keys correctly.
Overrides multi-keys with last value found.

""?a=1&b=0&c=3&d&e&a=5""
> queryDict
a: ""5""
b: ""0""
c: ""3""
d: undefined
e: undefined

Multi-valued keys
Simple key check (item in dict) ? dict.item.push(val) : dict.item = [val]
var qd = {};
location.search.substr(1).split(""&"").forEach(function(item) {(item.split(""="")[0] in qd) ? qd[item.split(""="")[0]].push(item.split(""="")[1]) : qd[item.split(""="")[0]] = [item.split(""="")[1]]})


Now returns arrays instead.
Access values by qd.key[index] or qd[key][index]

> qd
a: [""1"", ""5""]
b: [""0""]
c: [""3""]
d: [undefined]
e: [undefined]

Encoded characters?
Use decodeURIComponent() for the second or both splits.
var qd = {};
location.search.substr(1).split(""&"").forEach(function(item) {var k = item.split(""="")[0], v = decodeURIComponent(item.split(""="")[1]); (k in qd) ? qd[k].push(v) : qd[k] = [v]})

####Example:
""?a=1&b=0&c=3&d&e&a=5&a=t%20e%20x%20t&e=http%3A%2F%2Fw3schools.com%2Fmy%20test.asp%3Fname%3Dstle%26car%3Dsaab""
> qd
a: [""1"", ""5"", ""t e x t""]
b: [""0""]
c: [""3""]
d: [""undefined""]  // decodeURIComponent(undefined) returns ""undefined"" !!!*
e: [""undefined"", ""http://w3schools.com/my test.asp?name=stle&car=saab""]



# From comments
**\*!!!** Please note, that `decodeURIComponent(undefined)` returns string `""undefined""`. The solution lies in a simple usage of [`&&`][5], which ensures that `decodeURIComponent()` is not called on undefined values. _(See the ""complete solution"" at the top.)_
v = v && decodeURIComponent(v);


If the querystring is empty (`location.search == """"`), the result is somewhat misleading `qd == {"""": undefined}`. It is suggested to check the querystring before launching the parsing function likeso:
if (location.search) location.search.substr(1).split(""&"").forEach(...)

    Improved version of Artem Barger's answer:

function getParameterByName(name) {
    var match = RegExp('[?&]' + name + '=([^&]*)').exec(window.location.search);
    return match && decodeURIComponent(match[1].replace(/\+/g, ' '));
}


For more information on improvement see: http://james.padolsey.com/javascript/bujs-1-getparameterbyname/
    This one works fine. Regular expressions in some of the other answers introduce unnecessary overhead.
function getQuerystring(key) {
    var query = window.location.search.substring(1);
    var vars = query.split(""&"");
    for (var i = 0; i < vars.length; i++) {
        var pair = vars[i].split(""="");
        if (pair[0] == key) {
            return pair[1];
        }
    }
}

taken from here
    Roshambo on snipplr.com has a simple script to achieve this described in Get URL Parameters with jQuery | Improved. With his script you also easily get to pull out just the parameters you want.

Here's the gist:

$.urlParam = function(name, url) {
    if (!url) {
     url = window.location.href;
    }
    var results = new RegExp('[\\?&]' + name + '=([^&#]*)').exec(url);
    if (!results) { 
        return undefined;
    }
    return results[1] || undefined;
}


Then just get your parameters from the query string.

So if the URL/query string was xyz.com/index.html?lang=de.

Just call var langval = $.urlParam('lang');, and you've got it.

UZBEKJON has a great blog post on this as well, Get URL parameters & values with jQuery.
    Here is a fast way to get an object similar to the PHP $_GET array:

function get_query(){
    var url = location.search;
    var qs = url.substring(url.indexOf('?') + 1).split('&');
    for(var i = 0, result = {}; i < qs.length; i++){
        qs[i] = qs[i].split('=');
        result[qs[i][0]] = decodeURIComponent(qs[i][1]);
    }
    return result;
}


Usage:

var $_GET = get_query();


For the query string x=5&y&z=hello&x=6 this returns the object:

{
  x: ""6"",
  y: undefined,
  z: ""hello""
}

    If you are using Browserify, you can use the url module from Node.js:

var url = require('url');

url.parse('http://example.com/?bob=123', true).query;

// returns { ""bob"": ""123"" }


Further reading: URL Node.js v0.12.2 Manual & Documentation

EDIT: You can use URL interface, its quite widely adopted in almost all the new browser and if the code is going to run on an old browser you can use a polyfill like this one. Here's a code example on how to use URL interface to get query parameters (aka search parameters)

const url = new URL('http://example.com/?bob=123');
url.searchParams.get('bob'); 


You can also use URLSearchParams for it, here's an example from MDN to do it with URLSearchParams:

var paramsString = ""q=URLUtils.searchParams&topic=api"";
var searchParams = new URLSearchParams(paramsString);

//Iterate the search parameters.
for (let p of searchParams) {
  console.log(p);
}

searchParams.has(""topic"") === true; // true
searchParams.get(""topic"") === ""api""; // true
searchParams.getAll(""topic""); // [""api""]
searchParams.get(""foo"") === null; // true
searchParams.append(""topic"", ""webdev"");
searchParams.toString(); // ""q=URLUtils.searchParams&topic=api&topic=webdev""
searchParams.set(""topic"", ""More webdev"");
searchParams.toString(); // ""q=URLUtils.searchParams&topic=More+webdev""
searchParams.delete(""topic"");
searchParams.toString(); // ""q=URLUtils.searchParams""

    Keep it simple in plain JavaScript code:

function qs(key) {
    var vars = [], hash;
    var hashes = window.location.href.slice(window.location.href.indexOf('?') + 1).split('&');
    for(var i = 0; i < hashes.length; i++)
    {
        hash = hashes[i].split('=');
        vars.push(hash[0]);
        vars[hash[0]] = hash[1];
    }
    return vars[key];
}


Call it from anywhere in the JavaScript code:

var result = qs('someKey');

    If you're using jQuery, you can use a library, such as jQuery BBQ: Back Button & Query Library.


  ...jQuery BBQ provides a full .deparam() method, along with both hash state management, and fragment / query string parse and merge utility methods.


Edit: Adding Deparam Example:

 var DeparamExample = function() {
            var params = $.deparam.querystring();

            //nameofparam is the name of a param from url
            //code below will get param if ajax refresh with hash
            if (typeof params.nameofparam == 'undefined') {
                params = jQuery.deparam.fragment(window.location.href);
            }
            
            if (typeof params.nameofparam != 'undefined') {
                var paramValue = params.nameofparam.toString();
                  
            }
        };


If you want to just use plain JavaScript, you could use...

var getParamValue = (function() {
    var params;
    var resetParams = function() {
            var query = window.location.search;
            var regex = /[?&;](.+?)=([^&;]+)/g;
            var match;

            params = {};

            if (query) {
                while (match = regex.exec(query)) {
                    params[match[1]] = decodeURIComponent(match[2]);
                }
            }    
        };

    window.addEventListener
    && window.addEventListener('popstate', resetParams);

    resetParams();

    return function(param) {
        return params.hasOwnProperty(param) ? params[param] : null;
    }

})();


Because of the new HTML History API and specifically history.pushState() and history.replaceState(), the URL can change which will invalidate the cache of parameters and their values.

This version will update its internal cache of parameters each time the history changes.
    I would rather use split() instead of Regex for this operation:

function getUrlParams() {
    var result = {};
    var params = (window.location.search.split('?')[1] || '').split('&');
    for(var param in params) {
        if (params.hasOwnProperty(param)) {
            var paramParts = params[param].split('=');
            result[paramParts[0]] = decodeURIComponent(paramParts[1] || """");
        }
    }
    return result;
}

    I like Ryan Phelan's solution. But I don't see any point of extending jQuery for that? There is no usage of jQuery functionality.

On the other hand, I like the built-in function in Google Chrome: window.location.getParameter.

So why not to use this? Okay, other browsers don't have. So let's create this function if it does not exist:

if (!window.location.getParameter ) {
  window.location.getParameter = function(key) {
    function parseParams() {
        var params = {},
            e,
            a = /\+/g,  // Regex for replacing addition symbol with a space
            r = /([^&=]+)=?([^&]*)/g,
            d = function (s) { return decodeURIComponent(s.replace(a, "" "")); },
            q = window.location.search.substring(1);

        while (e = r.exec(q))
            params[d(e[1])] = d(e[2]);

        return params;
    }

    if (!this.queryStringParams)
        this.queryStringParams = parseParams(); 

    return this.queryStringParams[key];
  };
}


This function is more or less from Ryan Phelan, but it is wrapped differently: clear name and no dependencies of other javascript libraries. More about this function on my blog.
    Just use two splits:

function get(n) {
    var half = location.search.split(n + '=')[1];
    return half !== undefined ? decodeURIComponent(half.split('&')[0]) : null;
}


I was reading all the previous and more complete answers. But I think that is the simplest and faster method. You can check in this jsPerf benchmark

To solve the problem in Rup's comment, add a conditional split by changing the first line to the two below. But absolute accuracy means it's now slower than regexp (see jsPerf).

function get(n) {
    var half = location.search.split('&' + n + '=')[1];
    if (!half) half = location.search.split('?' + n + '=')[1];
    return half !== undefined ? decodeURIComponent(half.split('&')[0]) : null;
}


So if you know you won't run into Rup's counter-case, this wins. Otherwise, regexp.


  Or if you have control of the querystring and can guarantee that a value you are trying to get will never contain any URL encoded
  characters (having these in a value would be a bad idea) - you can use
  the following slightly more simplified and readable version of the 1st option:

    function getQueryStringValueByName(name) {
        var queryStringFromStartOfValue = location.search.split(name + '=')[1];
         return queryStringFromStartOfValue !== undefined ? queryStringFromStartOfValue.split('&')[0] : null;


    Here's an extended version of Andy E's linked ""Handle array-style query strings""-version. Fixed a bug (?key=1&key[]=2&key[]=3; 1 is lost and replaced with [2,3]), made a few minor performance improvements (re-decoding of values, recalculating ""["" position, etc.) and added a number of improvements (functionalized, support for ?key=1&key=2, support for ; delimiters). I left the variables annoyingly short, but added comments galore to make them readable (oh, and I reused v within the local functions, sorry if that is confusing ;).

It will handle the following querystring...


  ?test=Hello&person=neek&person[]=jeff&person[]=jim&person[extra]=john&test3&nocache=1398914891264


...making it into an object that looks like...

{
    ""test"": ""Hello"",
    ""person"": {
        ""0"": ""neek"",
        ""1"": ""jeff"",
        ""2"": ""jim"",
        ""length"": 3,
        ""extra"": ""john""
    },
    ""test3"": """",
    ""nocache"": ""1398914891264""
}


As you can see above, this version handles some measure of ""malformed"" arrays, i.e. - person=neek&person[]=jeff&person[]=jim or person=neek&person=jeff&person=jim as the key is identifiable and valid (at least in dotNet's NameValueCollection.Add):


  If the specified key already exists in the target NameValueCollection
  instance, the specified value is added to the existing comma-separated
  list of values in the form ""value1,value2,value3"".


It seems the jury is somewhat out on repeated keys as there is no spec. In this case, multiple keys are stored as an (fake)array. But do note that I do not process values based on commas into arrays.

The code:

getQueryStringKey = function(key) {
    return getQueryStringAsObject()[key];
};


getQueryStringAsObject = function() {
    var b, cv, e, k, ma, sk, v, r = {},
        d = function (v) { return decodeURIComponent(v).replace(/\+/g, "" ""); }, //# d(ecode) the v(alue)
        q = window.location.search.substring(1), //# suggested: q = decodeURIComponent(window.location.search.substring(1)),
        s = /([^&;=]+)=?([^&;]*)/g //# original regex that does not allow for ; as a delimiter:   /([^&=]+)=?([^&]*)/g
    ;

    //# ma(make array) out of the v(alue)
    ma = function(v) {
        //# If the passed v(alue) hasn't been setup as an object
        if (typeof v != ""object"") {
            //# Grab the cv(current value) then setup the v(alue) as an object
            cv = v;
            v = {};
            v.length = 0;

            //# If there was a cv(current value), .push it into the new v(alue)'s array
            //#     NOTE: This may or may not be 100% logical to do... but it's better than loosing the original value
            if (cv) { Array.prototype.push.call(v, cv); }
        }
        return v;
    };

    //# While we still have key-value e(ntries) from the q(uerystring) via the s(earch regex)...
    while (e = s.exec(q)) { //# while((e = s.exec(q)) !== null) {
        //# Collect the open b(racket) location (if any) then set the d(ecoded) v(alue) from the above split key-value e(ntry) 
        b = e[1].indexOf(""["");
        v = d(e[2]);

        //# As long as this is NOT a hash[]-style key-value e(ntry)
        if (b < 0) { //# b == ""-1""
            //# d(ecode) the simple k(ey)
            k = d(e[1]);

            //# If the k(ey) already exists
            if (r[k]) {
                //# ma(make array) out of the k(ey) then .push the v(alue) into the k(ey)'s array in the r(eturn value)
                r[k] = ma(r[k]);
                Array.prototype.push.call(r[k], v);
            }
            //# Else this is a new k(ey), so just add the k(ey)/v(alue) into the r(eturn value)
            else {
                r[k] = v;
            }
        }
        //# Else we've got ourselves a hash[]-style key-value e(ntry) 
        else {
            //# Collect the d(ecoded) k(ey) and the d(ecoded) sk(sub-key) based on the b(racket) locations
            k = d(e[1].slice(0, b));
            sk = d(e[1].slice(b + 1, e[1].indexOf(""]"", b)));

            //# ma(make array) out of the k(ey) 
            r[k] = ma(r[k]);

            //# If we have a sk(sub-key), plug the v(alue) into it
            if (sk) { r[k][sk] = v; }
            //# Else .push the v(alue) into the k(ey)'s array
            else { Array.prototype.push.call(r[k], v); }
        }
    }

    //# Return the r(eturn value)
    return r;
};

    From the MDN:

function loadPageVar (sVar) {
return unescape(window.location.search.replace(new RegExp(""^(?:.*[&\\?]"" + escape(sVar).replace(/[\.\+\*]/g, ""\\$&"") + ""(?:\\=([^&]*))?)?.*$"", ""i""), ""$1""));
}

alert(loadPageVar(""name""));

    Here's my stab at making Andy E's excellent solution into a full fledged jQuery plugin:

;(function ($) {
    $.extend({      
        getQueryString: function (name) {           
            function parseParams() {
                var params = {},
                    e,
                    a = /\+/g,  // Regex for replacing addition symbol with a space
                    r = /([^&=]+)=?([^&]*)/g,
                    d = function (s) { return decodeURIComponent(s.replace(a, "" "")); },
                    q = window.location.search.substring(1);

                while (e = r.exec(q))
                    params[d(e[1])] = d(e[2]);

                return params;
            }

            if (!this.queryStringParams)
                this.queryStringParams = parseParams(); 

            return this.queryStringParams[name];
        }
    });
})(jQuery);


The syntax is:

var someVar = $.getQueryString('myParam');


Best of both worlds!
    Code golf:

var a = location.search&&location.search.substr(1).replace(/\+/gi,"" "").split(""&"");
for (var i in a) {
    var s = a[i].split(""="");
    a[i]  = a[unescape(s[0])] = unescape(s[1]);
}


Display it!

for (i in a) {
    document.write(i + "":"" + a[i] + ""<br/>"");   
};


On my Mac: test.htm?i=can&has=cheezburger displays

0:can
1:cheezburger
i:can
has:cheezburger

    I use regular expressions a lot, but not for that.

It seems easier and more efficient to me to read the query string once in my application, and build an object from all the key/value pairs like:

var search = function() {
  var s = window.location.search.substr(1),
    p = s.split(/\&/), l = p.length, kv, r = {};
  if (l === 0) {return false;}
  while (l--) {
    kv = p[l].split(/\=/);
    r[kv[0]] = decodeURIComponent(kv[1] || '') || true;
  }
  return r;
}();


For a URL like http://domain.com?param1=val1&param2=val2 you can get their value later in your code as search.param1 and search.param2.
    Here is my version of query string parsing code on GitHub.

It's ""prefixed"" with jquery.*, but the parsing function itself don't use jQuery. It's pretty fast, but still open for few simple performance optimizations.

Also it supports list & hash-tables encoding in the URL, like:

arr[]=10&arr[]=20&arr[]=100


or

hash[key1]=hello&hash[key2]=moto&a=How%20are%20you




jQuery.toQueryParams = function(str, separator) {
    separator = separator || '&'
    var obj = {}
    if (str.length == 0)
        return obj
    var c = str.substr(0,1)
    var s = c=='?' || c=='#'  ? str.substr(1) : str; 

    var a = s.split(separator)
    for (var i=0; i<a.length; i++) {
        var p = a[i].indexOf('=')
        if (p < 0) {
            obj[a[i]] = ''
            continue
        }
        var k = decodeURIComponent(a[i].substr(0,p)),
            v = decodeURIComponent(a[i].substr(p+1))

        var bps = k.indexOf('[')
        if (bps < 0) {
            obj[k] = v
            continue;
        } 

        var bpe = k.substr(bps+1).indexOf(']')
        if (bpe < 0) {
            obj[k] = v
            continue;
        }

        var bpv = k.substr(bps+1, bps+bpe-1)
        var k = k.substr(0,bps)
        if (bpv.length <= 0) {
            if (typeof(obj[k]) != 'object') obj[k] = []
            obj[k].push(v)
        } else {
            if (typeof(obj[k]) != 'object') obj[k] = {}
            obj[k][bpv] = v
        }
    }
    return obj;

}

    Just another recommendation. The plugin Purl allows to retrieve all parts of URL, including anchor, host, etc.

It can be used with or without jQuery.

Usage is very simple and cool:

var url = $.url('http://allmarkedup.com/folder/dir/index.html?item=value'); // jQuery version
var url = purl('http://allmarkedup.com/folder/dir/index.html?item=value'); // plain JS version
url.attr('protocol'); // returns 'http'
url.attr('path'); // returns '/folder/dir/index.html'


However, as of Nov 11, 2014, Purl is no longer maintained and the author recommends using URI.js instead.  The jQuery plugin is different in that it focuses on elements - for usage with strings, just use URI directly, with or without jQuery.  Similar code would look as such, fuller docs here:

var url = new URI('http://allmarkedup.com/folder/dir/index.html?item=value'); // plain JS version
url.protocol(); // returns 'http'
url.path(); // returns '/folder/dir/index.html'

    The following code will create an object which has two methods:


isKeyExist: Check if a particular parameter exist
getValue: Get the value of a particular parameter.




var QSParam = new function() {
       var qsParm = {};
       var query = window.location.search.substring(1);
       var params = query.split('&');
       for (var i = 0; i < params.length; i++) {
           var pos = params[i].indexOf('=');
           if (pos > 0) {
               var key = params[i].substring(0, pos);
               var val = params[i].substring(pos + 1);
               qsParm[key] = val;
           }
       }
       this.isKeyExist = function(query){
           if(qsParm[query]){
               return true;
           }
           else{
              return false;
           }
       };
       this.getValue = function(query){
           if(qsParm[query])
           {
               return qsParm[query];
           }
           throw ""URL does not contain query ""+ query;
       }
};

    Try this:

String.prototype.getValueByKey = function(k){
    var p = new RegExp('\\b'+k+'\\b','gi');
    return this.search(p) != -1 ? decodeURIComponent(this.substr(this.search(p)+k.length+1).substr(0,this.substr(this.search(p)+k.length+1).search(/(&|;|$)/))) : """";
};


Then call it like so:

if(location.search != """") location.search.getValueByKey(""id"");


You can use this for cookies also:

if(navigator.cookieEnabled) document.cookie.getValueByKey(""username"");


This only works for strings that have key=value[&|;|$]... will not work on objects/arrays.

If you don't want to use String.prototype...
move it to a function and pass the string as an argument
    If you're doing more URL manipulation than simply parsing the querystring, you may find URI.js helpful. It is a library for manipulating URLs - and comes with all the bells and whistles. (Sorry for self-advertising here)

to convert your querystring into a map:

var data = URI('?foo=bar&bar=baz&foo=world').query(true);
data == {
  ""foo"": [""bar"", ""world""],
  ""bar"": ""baz""
}


(URI.js also ""fixes"" bad querystrings like ?&foo&&bar=baz& to ?foo&bar=baz)
    A very lightweight jQuery method: 

var qs = window.location.search.replace('?','').split('&'),
    request = {};
$.each(qs, function(i,v) {
    var initial, pair = v.split('=');
    if(initial = request[pair[0]]){
        if(!$.isArray(initial)) {
            request[pair[0]] = [initial]
        }
        request[pair[0]].push(pair[1]);
    } else {
        request[pair[0]] = pair[1];
    }
    return;
});
console.log(request);


And to alert, for example ?q

alert(request.q)

    This is a function I created a while back and I'm quite happy with. It is not case sensitive - which is handy. Also, if the requested QS doesn't exist, it just returns an empty string.

I use a compressed version of this. I'm posting uncompressed for the novice types to better explain what's going on.

I'm sure this could be optimized or done differently to work faster, but it's always worked great for what I need.

Enjoy.

function getQSP(sName, sURL) {
    var theItmToRtn = """";
    var theSrchStrg = location.search;
    if (sURL) theSrchStrg = sURL;
    var sOrig = theSrchStrg;
    theSrchStrg = theSrchStrg.toUpperCase();
    sName = sName.toUpperCase();
    theSrchStrg = theSrchStrg.replace(""?"", ""&"") theSrchStrg = theSrchStrg + ""&"";
    var theSrchToken = ""&"" + sName + ""="";
    if (theSrchStrg.indexOf(theSrchToken) != -1) {
        var theSrchTokenLth = theSrchToken.length;
        var theSrchTokenLocStart = theSrchStrg.indexOf(theSrchToken) + theSrchTokenLth;
        var theLocOfNextAndSign = theSrchStrg.indexOf(""&"", theSrchTokenLocStart);
        theItmToRtn = unescape(sOrig.substring(theSrchTokenLocStart, theLocOfNextAndSign));
    }
    return unescape(theItmToRtn);
}

    Here's my edit to this excellent answer - with added ability to parse query strings with keys without values.

var url = 'http://sb.com/reg/step1?param';
var qs = (function(a) {
    if (a == """") return {};
    var b = {};
    for (var i = 0; i < a.length; ++i) {
        var p=a[i].split('=', 2);
        if (p[1]) p[1] = decodeURIComponent(p[1].replace(/\+/g, "" ""));
        b[p[0]] = p[1];
    }
    return b;
})((url.split('?'))[1].split('&'));


IMPORTANT! The parameter for that function in the last line is different. It's just an example of how one can pass an arbitrary URL to it. You can use last line from Bruno's answer to parse the current URL.

So what exactly changed? With url http://sb.com/reg/step1?param= results will be same. But with url http://sb.com/reg/step1?param Bruno's solution returns an object without keys, while mine returns an object with key param and undefined value.
    function GET() {
        var data = [];
        for(x = 0; x < arguments.length; ++x)
            data.push(location.href.match(new RegExp(""/\?"".concat(arguments[x],""="",""([^\n&]*)"")))[1])
                return data;
    }


example:
data = GET(""id"",""name"",""foo"");
query string : ?id=3&name=jet&foo=b
returns:
    data[0] // 3
    data[1] // jet
    data[2] // b
or
    alert(GET(""id"")[0]) // return 3

    Roshambo jQuery method wasn't taking care of decode URL


  http://snipplr.com/view/26662/get-url-parameters-with-jquery--improved/


Just added that capability also while adding in  the return statement 

return decodeURIComponent(results[1].replace(/\+/g, "" "")) || 0;


Now you can find the updated gist:

$.urlParam = function(name){
var results = new RegExp('[\\?&]' + name + '=([^&#]*)').exec(window.location.href);
if (!results) { return 0; }
return decodeURIComponent(results[1].replace(/\+/g, "" "")) || 0;
}

    ","[2695, 9679, 672, 1765, 1314, 255, 675, 23, 223, 58, 28, 58, 169, 17, 65, 102, 34, 46, 97, 41, 41, 18, 399, 8, 8, 77, 18, 32, 37, 39, 39]",4635945,2155,2009-05-23T08:10:48,2022-03-10 21:07:34Z,javascript 
"Why is ""1000000000000000 in range(1000000000000001)"" so fast in Python 3?","
                
It is my understanding that the range() function, which is actually an object type in Python 3, generates its contents on the fly, similar to a generator.
This being the case, I would have expected the following line to take an inordinate amount of time because, in order to determine whether 1 quadrillion is in the range, a quadrillion values would have to be generated:
1_000_000_000_000_000 in range(1_000_000_000_000_001)

Furthermore: it seems that no matter how many zeroes I add on, the calculation more or less takes the same amount of time (basically instantaneous).
I have also tried things like this, but the calculation is still almost instant:
# count by tens
1_000_000_000_000_000_000_000 in range(0,1_000_000_000_000_000_000_001,10)

If I try to implement my own range function, the result is not so nice!
def my_crappy_range(N):
    i = 0
    while i < N:
        yield i
        i += 1
    return

What is the range() object doing under the hood that makes it so fast?

Martijn Pieters's answer was chosen for its completeness, but also see abarnert's first answer for a good discussion of what it means for range to be a full-fledged sequence in Python 3, and some information/warning regarding potential inconsistency for __contains__ function optimization across Python implementations. abarnert's other answer goes into some more detail and provides links for those interested in the history behind the optimization in Python 3 (and lack of optimization of xrange in Python 2). Answers by poke and by wim provide the relevant C source code and explanations for those who are interested.
    The Python 3 range() object doesn't produce numbers immediately; it is a smart sequence object that produces numbers on demand. All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.
The object also implements the object.__contains__ hook, and calculates if your number is part of its range. Calculating is a (near) constant time operation *. There is never a need to scan through all possible integers in the range.
From the range() object documentation:

The advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).

So at a minimum, your range() object would do:
class my_range:
    def __init__(self, start, stop=None, step=1, /):
        if stop is None:
            start, stop = 0, start
        self.start, self.stop, self.step = start, stop, step
        if step < 0:
            lo, hi, step = stop, start, -step
        else:
            lo, hi = start, stop
        self.length = 0 if lo > hi else ((hi - lo - 1) // step) + 1

    def __iter__(self):
        current = self.start
        if self.step < 0:
            while current > self.stop:
                yield current
                current += self.step
        else:
            while current < self.stop:
                yield current
                current += self.step

    def __len__(self):
        return self.length

    def __getitem__(self, i):
        if i < 0:
            i += self.length
        if 0 <= i < self.length:
            return self.start + i * self.step
        raise IndexError('my_range object index out of range')

    def __contains__(self, num):
        if self.step < 0:
            if not (self.stop < num <= self.start):
                return False
        else:
            if not (self.start <= num < self.stop):
                return False
        return (num - self.start) % self.step == 0

This is still missing several things that a real range() supports (such as the .index() or .count() methods, hashing, equality testing, or slicing), but should give you an idea.
I also simplified the __contains__ implementation to only focus on integer tests; if you give a real range() object a non-integer value (including subclasses of int), a slow scan is initiated to see if there is a match, just as if you use a containment test against a list of all the contained values. This was done to continue to support other numeric types that just happen to support equality testing with integers but are not expected to support integer arithmetic as well. See the original Python issue that implemented the containment test.

* Near constant time because Python integers are unbounded and so math operations also grow in time as N grows, making this a O(log N) operation. Since its all executed in optimised C code and Python stores integer values in 30-bit chunks, youd run out of memory before you saw any performance impact due to the size of the integers involved here.
    The fundamental misunderstanding here is in thinking that range is a generator. It's not. In fact, it's not any kind of iterator.
You can tell this pretty easily:
>>> a = range(5)
>>> print(list(a))
[0, 1, 2, 3, 4]
>>> print(list(a))
[0, 1, 2, 3, 4]

If it were a generator, iterating it once would exhaust it:
>>> b = my_crappy_range(5)
>>> print(list(b))
[0, 1, 2, 3, 4]
>>> print(list(b))
[]

What range actually is, is a sequence, just like a list. You can even test this:
>>> import collections.abc
>>> isinstance(a, collections.abc.Sequence)
True

This means it has to follow all the rules of being a sequence:
>>> a[3]         # indexable
3
>>> len(a)       # sized
5
>>> 3 in a       # membership
True
>>> reversed(a)  # reversible
<range_iterator at 0x101cd2360>
>>> a.index(3)   # implements 'index'
3
>>> a.count(3)   # implements 'count'
1


The difference between a range and a list is that a range is a lazy or dynamic sequence; it doesn't remember all of its values, it just remembers its start, stop, and step, and creates the values on demand on __getitem__.
(As a side note, if you print(iter(a)), you'll notice that range uses the same listiterator type as list. How does that work? A listiterator doesn't use anything special about list except for the fact that it provides a C implementation of __getitem__, so it works fine for range too.)

Now, there's nothing that says that Sequence.__contains__ has to be constant timein fact, for obvious examples of sequences like list, it isn't. But there's nothing that says it can't be. And it's easier to implement range.__contains__ to just check it mathematically ((val - start) % step, but with some extra complexity to deal with negative steps) than to actually generate and test all the values, so why shouldn't it do it the better way?
But there doesn't seem to be anything in the language that guarantees this will happen. As Ashwini Chaudhari points out, if you give it a non-integral value, instead of converting to integer and doing the mathematical test, it will fall back to iterating all the values and comparing them one by one. And just because CPython 3.2+ and PyPy 3.x versions happen to contain this optimization, and it's an obvious good idea and easy to do, there's no reason that IronPython or NewKickAssPython 3.x couldn't leave it out. (And in fact, CPython 3.0-3.1 didn't include it.)

If range actually were a generator, like my_crappy_range, then it wouldn't make sense to test __contains__ this way, or at least the way it makes sense wouldn't be obvious. If you'd already iterated the first 3 values, is 1 still in the generator? Should testing for 1 cause it to iterate and consume all the values up to 1 (or up to the first value >= 1)?
    Use the source, Luke!
In CPython, range(...).__contains__ (a method wrapper) will eventually delegate to a simple calculation which checks if the value can possibly be in the range.  The reason for the speed here is we're using mathematical reasoning about the bounds, rather than a direct iteration of the range object.  To explain the logic used:

Check that the number is between start and stop, and
Check that the stride value doesn't ""step over"" our number.

For example, 994 is in range(4, 1000, 2) because:

4 <= 994 < 1000, and
(994 - 4) % 2 == 0.

The full C code is included below, which is a bit more verbose because of memory management and reference counting details, but the basic idea is there:
static int
range_contains_long(rangeobject *r, PyObject *ob)
{
    int cmp1, cmp2, cmp3;
    PyObject *tmp1 = NULL;
    PyObject *tmp2 = NULL;
    PyObject *zero = NULL;
    int result = -1;

    zero = PyLong_FromLong(0);
    if (zero == NULL) /* MemoryError in int(0) */
        goto end;

    /* Check if the value can possibly be in the range. */

    cmp1 = PyObject_RichCompareBool(r->step, zero, Py_GT);
    if (cmp1 == -1)
        goto end;
    if (cmp1 == 1) { /* positive steps: start <= ob < stop */
        cmp2 = PyObject_RichCompareBool(r->start, ob, Py_LE);
        cmp3 = PyObject_RichCompareBool(ob, r->stop, Py_LT);
    }
    else { /* negative steps: stop < ob <= start */
        cmp2 = PyObject_RichCompareBool(ob, r->start, Py_LE);
        cmp3 = PyObject_RichCompareBool(r->stop, ob, Py_LT);
    }

    if (cmp2 == -1 || cmp3 == -1) /* TypeError */
        goto end;
    if (cmp2 == 0 || cmp3 == 0) { /* ob outside of range */
        result = 0;
        goto end;
    }

    /* Check that the stride does not invalidate ob's membership. */
    tmp1 = PyNumber_Subtract(ob, r->start);
    if (tmp1 == NULL)
        goto end;
    tmp2 = PyNumber_Remainder(tmp1, r->step);
    if (tmp2 == NULL)
        goto end;
    /* result = ((int(ob) - start) % step) == 0 */
    result = PyObject_RichCompareBool(tmp2, zero, Py_EQ);
  end:
    Py_XDECREF(tmp1);
    Py_XDECREF(tmp2);
    Py_XDECREF(zero);
    return result;
}

static int
range_contains(rangeobject *r, PyObject *ob)
{
    if (PyLong_CheckExact(ob) || PyBool_Check(ob))
        return range_contains_long(r, ob);

    return (int)_PySequence_IterSearch((PyObject*)r, ob,
                                       PY_ITERSEARCH_CONTAINS);
}

The ""meat"" of the idea is mentioned in the line:
/* result = ((int(ob) - start) % step) == 0 */ 

As a final note - look at the range_contains function at the bottom of the code snippet.  If the exact type check fails then we don't use the clever algorithm described, instead falling back to a dumb iteration search of the range using _PySequence_IterSearch!  You can check this behaviour in the interpreter (I'm using v3.5.0 here):
>>> x, r = 1000000000000000, range(1000000000000001)
>>> class MyInt(int):
...     pass
... 
>>> x_ = MyInt(x)
>>> x in r  # calculates immediately :) 
True
>>> x_ in r  # iterates for ages.. :( 
^\Quit (core dumped)

    To add to Martijns answer, this is the relevant part of the source (in C, as the range object is written in native code):
static int
range_contains(rangeobject *r, PyObject *ob)
{
    if (PyLong_CheckExact(ob) || PyBool_Check(ob))
        return range_contains_long(r, ob);

    return (int)_PySequence_IterSearch((PyObject*)r, ob,
                                       PY_ITERSEARCH_CONTAINS);
}

So for PyLong objects (which is int in Python 3), it will use the range_contains_long function to determine the result. And that function essentially checks if ob is in the specified range (although it looks a bit more complex in C).
If its not an int object, it falls back to iterating until it finds the value (or not).
The whole logic could be translated to pseudo-Python like this:
def range_contains (rangeObj, obj):
    if isinstance(obj, int):
        return range_contains_long(rangeObj, obj)

    # default logic by iterating
    return any(obj == x for x in rangeObj)

def range_contains_long (r, num):
    if r.step > 0:
        # positive step: r.start <= num < r.stop
        cmp2 = r.start <= num
        cmp3 = num < r.stop
    else:
        # negative step: r.start >= num > r.stop
        cmp2 = num <= r.start
        cmp3 = r.stop < num

    # outside of the range boundaries
    if not cmp2 or not cmp3:
        return False

    # num must be on a valid step inside the boundaries
    return (num - r.start) % r.step == 0

    If you're wondering why this optimization was added to range.__contains__, and why it wasn't added to xrange.__contains__ in 2.7:
First, as Ashwini Chaudhary discovered, issue 1766304 was opened explicitly to optimize [x]range.__contains__. A patch for this was accepted and checked in for 3.2, but not backported to 2.7 because ""xrange has behaved like this for such a long time that I don't see what it buys us to commit the patch this late."" (2.7 was nearly out at that point.)
Meanwhile:
Originally, xrange was a not-quite-sequence object. As the 3.1 docs say:

Range objects have very little behavior: they only support indexing, iteration, and the len function.

This wasn't quite true; an xrange object actually supported a few other things that come automatically with indexing and len,* including __contains__ (via linear search). But nobody thought it was worth making them full sequences at the time.
Then, as part of implementing the Abstract Base Classes PEP, it was important to figure out which builtin types should be marked as implementing which ABCs, and xrange/range claimed to implement collections.Sequence, even though it still only handled the same ""very little behavior"". Nobody noticed that problem until issue 9213. The patch for that issue not only added index and count to 3.2's range, it also re-worked the optimized __contains__ (which shares the same math with index, and is directly used by count).** This change went in for 3.2 as well, and was not backported to 2.x, because ""it's a bugfix that adds new methods"". (At this point, 2.7 was already past rc status.)
So, there were two chances to get this optimization backported to 2.7, but they were both rejected.

* In fact, you even get iteration for free with indexing alone, but in 2.3 xrange objects got a custom iterator.
** The first version actually reimplemented it, and got the details wronge.g., it would give you MyIntSubclass(2) in range(5) == False. But Daniel Stutzbach's updated version of the patch restored most of the previous code, including the fallback to the generic, slow _PySequence_IterSearch that pre-3.2 range.__contains__ was implicitly using when the optimization doesn't apply.
    It's all about a lazy approach to the evaluation and some extra optimization of range.
Values in ranges don't need to be computed until real use, or even further due to extra optimization.
By the way, your integer is not such big, consider sys.maxsize
sys.maxsize in range(sys.maxsize) is pretty fast
due to optimization - it's easy to compare given integers just with min and max of range.
but:
Decimal(sys.maxsize) in range(sys.maxsize) is pretty slow.
(in this case, there is no optimization in range, so if python receives unexpected Decimal, python will compare all numbers)
You should be aware of an implementation detail but should not be relied upon, because this may change in the future.
    The other answers explained it well already, but I'd like to offer another experiment illustrating the nature of range objects:
>>> r = range(5)
>>> for i in r:
        print(i, 2 in r, list(r))
        
0 True [0, 1, 2, 3, 4]
1 True [0, 1, 2, 3, 4]
2 True [0, 1, 2, 3, 4]
3 True [0, 1, 2, 3, 4]
4 True [0, 1, 2, 3, 4]

As you can see, a range object is an object that remembers its range and can be used many times (even while iterating over it), not just a one-time generator.
    TL;DR
The object returned by range() is actually a range object. This object implements the iterator interface so you can iterate over its values sequentially, just like a generator, list, or tuple.
But it also implements the __contains__ interface which is actually what gets called when an object appears on the right-hand side of the in operator. The __contains__() method returns a bool of whether or not the item on the left-hand side of the in is in the object. Since range objects know their bounds and stride, this is very easy to implement in O(1).
    Try x-1 in (i for i in range(x)) for large x values, which uses a generator comprehension to avoid invoking the range.__contains__ optimisation.
    TLDR;
the range is an arithmetic series so it can very easily calculate whether the object is there. It could even get the index of it if it were list like really quickly.
    
Due to optimization, it is very easy to compare given integers just with min and max range.
The reason that the range() function is so fast in Python3 is that here we use mathematical reasoning for the bounds, rather than a direct iteration of the range object.
So for explaining the logic here:


Check whether the number is between the start and stop.
Check whether the step precision value doesn't go over our number.


Take an example, 997 is in range(4, 1000, 3) because:
4 <= 997 < 1000, and (997 - 4) % 3 == 0.


    ","[2690, 2747, 1064, 447, 180, 126, 39, 58, 28, 4, 3, 4]",293400,506,2015-05-06T15:32:43,2021-12-31 06:02:32Z,python 
How can I install pip on Windows?,"
                
pip is a replacement for easy_install. But should I install pip using easy_install on Windows? Is there a better way?
    Python 2.7.9+ and 3.4+

Good news! Python 3.4 (released March 2014) and Python 2.7.9 (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Node.js, Haskell, Perl, Goalmost every other contemporary language with a majority open-source community. Thank you, Python.

If you do find that pip is not available when using Python 3.4+ or Python 2.7.9+, simply execute e.g.:

py -3 -m ensurepip


Of course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this in the Stack Overflow question Does Python have a package/module management system?.

And, alas for everyone using Python 2.7.8 or earlier (a sizable portion of the community). There's no plan to ship Pip to you. Manual instructions follow.

Python 2  2.7.8 and Python 3  3.3

Flying in the face of its 'batteries included' motto, Python ships without a package manager. To make matters worse, Pip wasuntil recentlyironically difficult to install.

Official instructions

Per https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip:

Download get-pip.py, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt:

python get-pip.py


You possibly need an administrator command prompt to do this. Follow Start a Command Prompt as an Administrator (Microsoft TechNet).

This installs the pip package, which (in Windows) contains ...\Scripts\pip.exe that path must be in PATH environment variable to use pip from the command line (see the second part of 'Alternative Instructions' for adding it to your PATH,

Alternative instructions

The official documentation tells users to install Pip and each of its dependencies from source. That's tedious for the experienced and prohibitively difficult for newbies.

For our sake, Christoph Gohlke prepares Windows installers (.msi) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:


Install setuptools
Install pip


For me, this installed Pip at C:\Python27\Scripts\pip.exe. Find pip.exe on your computer, then add its folder (for example, C:\Python27\Scripts) to your path (Start / Edit environment variables). Now you should be able to run pip from the command line. Try installing a package:

pip install httpie


There you go (hopefully)! Solutions for common problems are given below:

Proxy problems

If you work in an office, you might be behind an HTTP proxy. If so, set the environment variables http_proxy and https_proxy. Most Python applications (and other free software) respect these. Example syntax:

http://proxy_url:port
http://username:password@proxy_url:port


If you're really unlucky, your proxy might be a Microsoft NTLM proxy. Free software can't cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy. http://cntlm.sourceforge.net/

Unable to find vcvarsall.bat

Python modules can be partly written in C or C++. Pip tries to compile from source. If you don't have a C/C++ compiler installed and configured, you'll see this cryptic error message.


  Error: Unable to find vcvarsall.bat


You can fix that by installing a C++ compiler such as MinGW or Visual C++. Microsoft actually ships one specifically for use with Python. Or try Microsoft Visual C++ Compiler for Python 2.7.

Often though it's easier to check Christoph's site for your package.
    2016+ Update: 

These answers are outdated or otherwise wordy and difficult.

If you've got Python 3.4+ or 2.7.9+, it will be installed by default on Windows.  Otherwise, in short:


Download the pip installer: 
https://bootstrap.pypa.io/get-pip.py
If paranoid, inspect file to confirm it isn't malicious
(must b64 decode).
Open a console in the download folder as Admin and run
get-pip.py.  Alternatively, right-click its icon in Explorer and choose the ""run as Admin..."".


The new binaries pip.exe (and the deprecated easy_install.exe) will be found in the ""%ProgramFiles%\PythonXX\Scripts"" folder (or similar), which is often not in your PATH variable.  I recommend adding it.
    -- Outdated -- use distribute, not setuptools as described here. --
-- Outdated #2 -- use setuptools as distribute is deprecated.

As you mentioned pip doesn't include an independent installer, but you can install it with its predecessor easy_install.

So:


Download the last pip version from here: http://pypi.python.org/pypi/pip#downloads
Uncompress it
Download the last easy installer for Windows: (download the .exe at the bottom of http://pypi.python.org/pypi/setuptools ). Install it.
copy the uncompressed pip folder content into C:\Python2x\ folder (don't copy the whole folder into it, just the content), because python command doesn't work outside C:\Python2x folder and then run:  python setup.py install
Add your python C:\Python2x\Scripts to the path


You are done. 

Now you can use pip install package to easily install packages as in Linux :)
    2014 UPDATE:

1) If you have installed Python 3.4 or later, pip is included with Python and should already be working on your system.

2) If you are running a version below Python 3.4 or if pip was not installed with Python 3.4 for some reason, then you'd probably use pip's official installation script get-pip.py. The pip installer now grabs setuptools for you, and works regardless of architecture (32-bit or 64-bit).

The installation instructions are detailed here and involve:


  To install or upgrade pip, securely download get-pip.py.
  
  Then run the following (which may require administrator access):


python get-pip.py



  To upgrade an existing setuptools (or distribute), run pip install -U setuptools


I'll leave the two sets of old instructions below for posterity.

OLD Answers:

For Windows editions of the 64 bit variety - 64-bit Windows + Python used to require a separate installation method due to ez_setup, but I've tested the new distribute method on 64-bit Windows running 32-bit Python and 64-bit Python, and you can now use the same method for all versions of Windows/Python 2.7X:

OLD Method 2 using distribute:


Download distribute - I threw mine in C:\Python27\Scripts (feel free to create a Scripts directory if it doesn't exist.
Open up a command prompt (on Windows you should check out conemu2 if you don't use PowerShell) and change (cd) to the directory you've downloaded distribute_setup.py to.
Run distribute_setup: python distribute_setup.py (This will not work if your python installation directory is not added to your path - go here for help)
Change the current directory to the Scripts directory for your Python installation (C:\Python27\Scripts) or add that directory, as well as the Python base installation directory to your %PATH% environment variable.
Install pip using the newly installed setuptools: easy_install pip


The last step will not work unless you're either in the directory easy_install.exe is located in (C:\Python27\Scripts would be the default for Python 2.7), or you have that directory added to your path.

OLD Method 1 using ez_setup:

from the setuptools page --


  Download ez_setup.py and run it; it will download the appropriate .egg file and install it for you. (Currently, the provided .exe installer does not support 64-bit versions of Python for Windows, due to a distutils installer compatibility issue.


After this, you may continue with:


Add c:\Python2x\Scripts to the Windows path (replace the x in Python2x with the actual version number you have installed)
Open a new (!) DOS prompt. From there run easy_install pip

    For the latest Python download - I have Python 3.6 on Windows. You don't have to wonder. Everything you need is there. Take a breath, and I will show you how to do it.

Make sure where you install Python. For me, it was in the following directory


Now, lets add the Python and pip into environment variable path settings
if you are on Windows, so that typing pip or python anywhere call
python or pip from where they are installed.

So, PIP is found under the folder in the above screen ""SCRIPTS""
Let's add Python and PIP in the environment variable path.

Almost done. Let's test with CMD to install the google package using pip.
 pip install google




    When I have to use Windows, I use ActivePython, which automatically adds everything to your PATH and includes a package manager called PyPM which provides binary package management making it faster and simpler to install packages.

pip and easy_install aren't exactly the same thing, so there are some things you can get through pip but not easy_install and vice versa.

My recommendation is that you get ActivePython Community Edition and don't worry about the huge hassle of getting everything set up for Python on Windows. Then, you can just use pypm.

In case you want to use pip you have to check the PyPM option in the ActiveState installer. After installation you only need to logoff and log on again, and pip will be available on the commandline, because it is contained in the ActiveState installer PyPM option and the paths have been set by the installer for you already. PyPM will also be available, but you do not have to use it.
    Python 3.4, which  was released in March 2014, comes with pip included:
http://docs.python.org/3.4/whatsnew/3.4.html
So, since the release of Python 3.4, the up-to-date way to install pip on Windows is to just install Python.

The recommended way to use it is to call it as a module, especially with multiple python distributions or versions installed, to guarantee packages go to the correct place:
python -m pip install --upgrade packageXYZ

https://docs.python.org/3/installing/#work-with-multiple-versions-of-python-installed-in-parallel
    The up-to-date way is to use Windows' package manager Chocolatey.

Once this is installed, all you have to do is open a command prompt and run the following the three commands below, which will install Python 2.7, easy_install and pip. It will automatically detect whether you're on x64 or x86 Windows.

cinst python
cinst easy.install
cinst pip


All of the other Python packages on the Chocolatey Gallery can be found here.
    To install pip globally on Python 2.x, easy_install appears to be the best solution as Adrin states.

However the installation instructions for pip recommend using virtualenv since every virtualenv has pip installed in it automatically.  This does not require root access or modify your system Python installation.

Installing virtualenv still requires easy_install though.

2018 update: 

Python 3.3+ now includes the venv module for easily creating virtual environments like so:

python3 -m venv /path/to/new/virtual/environment

See documentation for different platform methods of activating the environment after creation, but typically one of:

$ source <venv>/bin/activate 

C:\> <venv>\Scripts\activate.bat

    Updated at 2016 : Pip should already be included in Python 2.7.9+ or 3.4+, but if for whatever reason it is not there, you can use the following one-liner.


Download https://bootstrap.pypa.io/get-pip.py and run it with Administrator permission python get-pip.py (If you are on Linux, use sudo python get-pip.py)


PS:


This should already be satisfied in most cases but, if necessary, be sure that your environment variable PATH includes Python's folders (for example, Python 2.7.x on Windows default install: C:\Python27 and C:\Python27\Scripts, for Python 3.3x: C:\Python33 and C:\Python33\Scripts, etc)
I encounter same problem and then found such perhaps easiest way (one liner!) mentioned on official website here: http://www.pip-installer.org/en/latest/installing.html


Can't believe there are so many lengthy (perhaps outdated?) answers out there. Feeling thankful to them but, please up-vote this short answer to help more new comers!
    I just wanted to add one more solution for those having issues installing setuptools from Windows 64-bit. The issue is discussed in this bug on python.org and is still unresolved as of the date of this comment. A simple workaround is mentioned and it works flawlessly. One registry change did the trick for me.

Link: http://bugs.python.org/issue6792#

Solution that worked for me...:

Add this registry setting for 2.6+ versions of Python:

 [HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Python\PythonCore\2.6\InstallPath]
 @=""C:\\Python26\\""


This is most likely the registry setting you will already have for Python 2.6+:

 [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6\InstallPath]
 @=""C:\\Python26\\""


Clearly, you will need to replace the 2.6 version with whatever version of Python you are running.
    Installers

I've built Windows installers for both distribute and pip here (the goal being to use pip without having to either bootstrap with easy_install or save and run Python scripts):


distribute-0.6.27.win32.exe
pip-1.1.win32.exe


On Windows, simply download and install first distribute, then pip from the above links. The distribute link above does contain stub .exe installers, and these are currently 32-bit only. I haven't tested the effect on 64-bit Windows.

Building on Windows

The process to redo this for new versions is not difficult, and I've included it here for reference.

Building distribute

In order to get the stub .exe files, you need to have a Visual C++ compiler (it is apparently compilable with MinGW as well)

hg clone https://bitbucket.org/tarek/distribute
cd distribute
hg checkout 0.6.27
rem optionally, comment out tag_build and tag_svn_revision in setup.cfg
msvc-build-launcher.cmd
python setup.py bdist_win32
cd ..
echo build is in distribute\dist


Building pip

git clone https://github.com/pypa/pip.git
cd pip
git checkout 1.1
python setup.py bdist_win32
cd ..
echo build is in pip\dist

    Update March 2015

Python 2.7.9 and later (on the Python 2 series), and Python 3.4 and later include pip by default, so you may have pip already.

If you don't, run this one line command on your prompt (which may require administrator access):

python -c ""exec('try: from urllib2 import urlopen \nexcept: from urllib.request import urlopen');f=urlopen('https://bootstrap.pypa.io/get-pip.py').read();exec(f)""


It will install pip. If Setuptools is not already installed, get-pip.py will install it for you too.

As mentioned in comments, the above command will download code from the Pip source code repository at GitHub, and dynamically run it at your environment. So be noticed that this is a shortcut of the steps download, inspect and run, all with a single command using Python itself. If you trust Pip, proceed without doubt.

Be sure that your Windows environment variable PATH includes Python's folders (for Python 2.7.x default install: C:\Python27 and C:\Python27\Scripts, for Python 3.3x: C:\Python33 and C:\Python33\Scripts, and so on).
    The following works for Python 2.7. Save this script and launch it:  
https://raw.github.com/pypa/pip/master/contrib/get-pip.py  
Pip is installed, then add the path to your environment : 

C:\Python27\Scripts


Finally

pip install virtualenv


Also you need Microsoft Visual C++ 2008 Express to get the good compiler and avoid these kind of messages when installing packages:

error: Unable to find vcvarsall.bat


If you have a 64-bit version of Windows 7, you may read 64-bit Python installation issues on 64-bit Windows 7 to successfully install the Python executable package (issue with registry entries).
    To use pip, it is not mandatory that you need to install pip in the system directly. You can use it through virtualenv. What you can do is follow these steps:


Download virtualenv tar.gz file from https://pypi.python.org/pypi/virtualenv
Unzip it with 7zip or some other tool


We normally need to install Python packages for one particular project. So, now create a project folder, lets say myproject.


Copy the virtualenv.py file from the decompressed folder of virtualenv, and paste inside the myproject folder


Now create a virtual environment, lets say myvirtualenv as follows, inside the myproject folder:

python virtualenv.py myvirtualenv


It will show you:

New python executable in myvirtualenv\Scripts\python.exe
Installing setuptools....................................done.
Installing pip.........................done.


Now your virtual environment, myvirtualenv, is created inside your project folder. You might notice, pip is now installed inside you virtual environment. All you need to do is activate the virtual environment with the following command.

myvirtualenv\Scripts\activate


You will see the following at the command prompt:

(myvirtualenv) PATH\TO\YOUR\PROJECT\FOLDER>pip install package_name


Now you can start using pip, but make sure you have activated the virtualenv looking at the left of your prompt.

This is one of the easiest way to install pip i.e. inside virtual environment, but you need to have virtualenv.py file with you.

For more ways to install pip/virtualenv/virtualenvwrapper, you can refer to thegauraw.tumblr.com.
    Simple CMD way
Use CURL to download get-pip.py:
curl --http1.1 https://bootstrap.pypa.io/get-pip.py --output get-pip.py

Execute the downloaded Python file
python get-pip.py

Then add C:\Python37\Scripts path to your environment variable. It assumes that there is a Python37 folder in your C drive. That folder name may vary according to the installed Python version
Now you can install Python packages by running
pip install awesome_package_name

    The best way I found so far, is just two lines of code:

curl http://python-distribute.org/distribute_setup.py | python
curl https://raw.github.com/pypa/pip/master/contrib/get-pip.py | python


It was tested on Windows 8 with PowerShell, Cmd, and Git Bash (MinGW).

And you probably want to add the path to your environment. It's somewhere like C:\Python33\Scripts.
    I use the cross-platform Anaconda package manager from continuum.io on Windows and it is reliable.  It has virtual environment management and a fully featured shell with common utilities (e.g. conda, pip).

> conda install <package>               # access distributed binaries

> pip install <package>                 # access PyPI packages 


conda also comes with binaries for libraries with non-Python dependencies, e.g. pandas, numpy, etc.  This proves useful particularly on Windows as it can be  hard to correctly compile C dependencies.
    Installing Pip for Python2 and Python3

Download get-pip.py to a folder on your computer.
Open a command prompt and navigate to the folder containing get-pip.py.
Run the following command:python get-pip.py, python3 get-pip.py or  python3.6 get-pip.py, depending on which version of Python you want to install pip
Pip should be now installed!


Old answer (still valid)
Try:
python -m ensurepip

It's probably the easiest way to install pip on any system.
    Here how to install pip the easy way.

Copy and paste this content in a file as get-pip.py.
Copy and paste get-pip.py into the Python folder.C:\Python27.
Double click on get-pip.py file. It will install pip on your computer.
Now you have to add C:\Python27\Scripts path to your environment variable. Because it includes the pip.exe file.
Now you are ready to use pip. Open cmd and type as pip install package_name

    I think the question makes it seem like the answer is simpler than it really is.
Running of pip will sometimes require native compilation of a module (64-bit NumPy is a common example of that). In order for pip's compilation to succeed, you need Python which was compiled with the same version of Microsoft Visual C++ as the one pip is using.
Standard Python distributions are compiled with Microsoft Visual C++ 2008. You can install an Express version of Microsoft Visual C++ 2008, but it is not maintained. Your best bet is to get an express version of a later Microsoft Visual C++ and compile Python. Then PIP and Python will be using the same Microsoft Visual C++ version.
    Windows 10 Update - GUI only
From now on you can just access Microsoft Store, and look for Python:

Which feature:

That's the easiest and safest way to install python and pip on windows.
    I had some issues installing in different ways when I followed instructions here. I think it's very tricky to install in every Windows environment in the same way. In my case I need Python 2.6, 2.7 and 3.3 in the same machine for different purposes so that's why I think there're more problems.
But the following instructions worked perfectly for me, so might be depending on your environment you should try this one:

http://docs.python-guide.org/en/latest/starting/install/win/

Also, due to the different environments I found incredible useful to use Virtual Environments, I had websites that use different libraries and it's much better to encapsulate them into a single folder, check out the instructions, briefly if PIP is installed you just install VirtualEnv:

pip install virtualenv


Into the folder you have all your files run

virtualenv venv


And seconds later you have a virtual environment with everything in venv folder, to activate it run venv/Scripts/activate.bat (deactivate the environment is easy, use deactivate.bat). Every library you install will end up in venv\Lib\site-packages and it's easy to move your whole environment somewhere.

The only downside I found is some code editors can't recognize this kind of environments, and you will see warnings in your code because imported libraries are not found. Of course there're tricky ways to do it but it would be nice editors keep in mind Virtual Environments are very normal nowadays.

Hope it helps.
    Now, it is bundled with Python. You don't need to install it.
pip -V

This is how you can check whether pip is installed or not.
In rare cases, if it is not installed, download the get-pip.py file and run it with Python as
python get-pip.py

    PythonXY comes with pip included, among others.
    
Download script: https://raw.github.com/pypa/pip/master/contrib/get-pip.py
Save it on drive somewhere like C:\pip-script\get-pip.py
Navigate to that path from command prompt and run "" python get-pip.py ""


Guide link: http://www.pip-installer.org/en/latest/installing.html#install-pip

Note: Make sure scripts path like this (C:\Python27\Scripts) is added int %PATH% environment variable as well.
    If you even have other problems with the pip version, you can try this:
pip install --trusted-host pypi.python.org --upgrade pip

    Even if I installed Python 3.7, added it to PATH, and checked the checkbox ""Install pip"", pip3.exe or pip.exe was finally not present on the computer (even in the Scripts subfolder).
This solved it:
python -m ensurepip

(The solution from the accepted answer did not work for me.)
    It's very simple:

Step 1: wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py
Step 2: wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py
Step 2: python ez_setup.py
Step 3: python get-pip.py


(Make sure your Python and Python script directory (for example, C:\Python27 and C:\Python27\Scripts) are in the PATH.)
    Working as of Feb 04 2014 :):

If you have tried installing pip through the Windows installer file from http://www.lfd.uci.edu/~gohlke/pythonlibs/#pip as suggested by @Colonel Panic, you might have installed the pip package manager successfully, but you might be unable to install any packages with pip. You might also have got the same SSL error as I got when I tried to install Beautiful Soup4 if you look in the pip.log file:

Downloading/unpacking beautifulsoup4
  Getting page https://pypi.python.org/simple/beautifulsoup4/
  Could not fetch URL https://pypi.python.org/simple/beautifulsoup4/: **connection error: [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed**
  Will skip URL https://pypi.python.org/simple/beautifulsoup4/ when looking for download links for beautifulsoup4


The problem is an issue with an old version of OpenSSL being incompatible with pip 1.3.1 and above versions. The easy workaround for now, is to install pip 1.2.1, which does not require SSL:

Installing Pip on Windows:


Download pip 1.2.1 from https://pypi.python.org/packages/source/p/pip/pip-1.2.1.tar.gz
Extract the pip-1.2.1.tar.gz file
Change directory to the extracted folder: cd <path to extracted folder>/pip-1.2.1
Run python setup.py install
Now make sure C:\Python27\Scripts is in PATH because pip is installed in the C:\Python27\Scripts directory unlike C:\Python27\Lib\site-packages where Python packages are normally installed


Now try to install any package using pip.

For example, to install the requests package using pip, run this from cmd:

pip install requests


Whola! requests will be successfully installed and you will get a success message.
    ","[2681, 1912, 211, 307, 223, 23, 47, 52, 42, 20, 17, 17, 28, 34, 23, 18, 5, 15, 13, 4, 13, 3, 1, 10, 3, 12, 8, 4, 2, 5, 5]",3215514,764,2011-01-20T18:08:59,2021-02-18 14:36:43Z,python 
"ssh ""permissions are too open"" error","
                
I had a problem with my mac where I couldn't save any kind of file on the disk anymore.
I had to reboot OSX lion and reset the permissions on files and acls.

But now when I want to commit a repository I get the following error from ssh:

Permissions 0777 for '/Users/username/.ssh/id_rsa' are too open.
It is recommended that your private key files are NOT accessible by others.
This private key will be ignored.


What permissions levels should i give to the id_rsa file?
    Keys need to be only readable by you:

chmod 400 ~/.ssh/id_rsa


If Keys need to be read-writable by you:

chmod 600 ~/.ssh/id_rsa


600 appears to be fine as well (in fact better in most cases, because you don't need to change file permissions later to edit it).

The relevant portion from the manpage (man ssh)


 ~/.ssh/id_rsa
         Contains the private key for authentication.  These files contain sensitive 
         data and should be readable by the user but not
         accessible by others (read/write/execute).  ssh will simply ignore a private 
         key file if it is              
         accessible by others.  It is possible to specify a
         passphrase when generating the key which will be used to encrypt the sensitive 
         part of this file using 3DES.

 ~/.ssh/identity.pub
 ~/.ssh/id_dsa.pub
 ~/.ssh/id_ecdsa.pub
 ~/.ssh/id_rsa.pub
         Contains the public key for authentication.  These files are not sensitive and 
         can (but need not) be readable by anyone.


    I've got the error in my windows 10 so I set permission as the following and it works.



In details, remove other users/groups until it has only 'SYSTEM' and 'Administrators'. Then add your windows login into it with Read permission only.

Note the id_rsa file is under the c:\users\<username> folder.
    I have got a similar issue when i was trying to login to remote ftp server using public keys.
To solve this issue I have done the following process:

First find the location of the public keys, because when you try to login to ftp, this public key is used.
Alternatively, you can create a key and set that key's permissions to 600.
Make sure you are in the correct location and perform this command:

chmod 600 id_rsa

    Using Cygwin in Windows 8.1, there is a command need to be run:
chgrp Users ~/.ssh/id_rsa

Then the solution posted here can be applied, 400 or 600 is OK.
chmod 600 ~/.ssh/id_rsa

Reference here
    Windows 10 ssh into Ubuntu EC2 permissions are too open error on AWS
I had this issue trying to ssh into an Ubuntu EC2 instance using the .pem file from AWS.
In windows this worked when I put this key in a folder created under the .ssh folder
C:\Users\USERNAME\.ssh\private_key

To change permission settings in Windows 10 :

File Settings > Security > Advanced
Disable inheritance
Convert Inherited Permissions Into Explicit Permissions
Remove all the permission entries except for Administrators

Could then connect securely.
    I got success with sudo
sudo chmod 400 pem-file.pem
sudo ssh -i pem-file.pem username@X.X.X.X

    AFAIK the values are:

700 for the hidden directory .ssh where key files are located

600  for the keyfile id_rsa


    On Windows 10, cygwin's chmod and chgrp weren't enough for me. I had to

right click on the file
-> Properties
-> Security (tab)
and remove all users and groups except for my active user.

    For windows users Only.
Goto file property --> security --> advanced

Disable inheritance property
Convert Inherited Permissions Into Explicit Permissions.
Remove all the permission entries except the Administrators.



    The locale-independent solution that works on Windows 8.1 is:

chgrp 545 ~/.ssh/id_rsa
chmod 600 ~/.ssh/id_rsa


GID 545 is a special ID that always refers to the 'Users' group, even if you locale uses a different word for Users.
    provide 400 permission,
execute below command  

chmod 400 /Users/username/.ssh/id_rsa



    This is what worked for me (on mac)

sudo chmod 600 path_to_your_key.pem 


then :

ssh -i path_to_your_key user@server_ip


Hope it help
    0600 is what mine is set at (and it's working)
    In case you are using WSL on windows
The most simple answer is to just type: sudo ssh -i keyfile.pem <user>@ip
without changing the file permissions.
The reason why this happens?
Another resource

You can't modify the permissions of files on Windows's filesystem
using chmod on Bash on Ubuntu on Windows. You'll have to copy the
private key to your WSL home directory (~) and do it there.


On the other hand, sudo should never be utilized with ssh. The reason why issuing with sudo works is that it's now likely being executed as root, and this is not the correct way to do this and is a massive security risk, as Allowing for anything other the 600/400 permissions defeats the purpose of utilizing an SSH key, compromising the security of the key.
The best way to do that is by copying the file to $HOME/.ssh:
cp keyfile.pem ~/.ssh
Doing sudo chmod 400 keyfile.pem to it.
Then ssh -i keyfile.pem <user>@ip.
    700  folder
644  id_rsa.pub

this works for me.
    There is one exception to the 0x00 permissions requirement on a key. If the key is owned by root and group-owned by a group with users in it, then it can be 0440 and any user in that group can use the key.
I believe this will work with any permissions in the set 0xx0 but I haven't tested every combination with every version. I have tried 0660 with 5.3p1-84 on CentOS 6, and the group not the primary group of the user but a secondary group, and it works fine.
This would typically not be done for someone's personal key, but for a key used for automation, in a situation where you don't want the application to be able to mess with the key.
Similar rules apply to the .ssh directory restrictions.
    I am using Windows 10 and trying to connect to EC2 instance via SSH. Rather than using Cygwin for Windows, try using Git Bash. After doing chmod 400 for key I am able to SSH into the EC2 instance, but the same is not working for me from Cygwin. Windows treats the .pem file as coming from internet and blocks it, even disabling inheritance doesn't work.
I converted the file to .ppk format and it's working fine from PuTTY also, but it's not working from Cygwin.
    I keep all my own certificates and keys in one directory, and this works for tools like PuTTY, but I got this too open error message from the scp command. I discovered that Windows already maintains a C:\users\ACCOUNTNAME\.ssh folder having the proper access rights for storing SSH keys. So long as you keep the contents backed up (Windows sometimes deletes it during updates), or create your own folder for ssh keys in your user folder, this will work fine, as only you and the administrators have access to that parent folder.
Be very careful about changing access rights on Windows folders. I did this, and once a day Windows is scanning, reading, and writing all the files on my C: drive, a process that slows the computer for many minutes.
    As people have said, in Windows, I just dropped my .pem file in C:\Users\[user]\.ssh\ and that solved it.  Although you can do chmod and other command line options from a bash or powershell prompt that didn't work.  I didn't change rsa or anything else.  Then when running the connection you have to put the path to the pem file in the .ssh folder:
ssh -i ""C:\Users\[user]\.ssh\ubuntukp01.pem"" ubuntu@ec[ipaddress].us-west-2.compute.amazonaws.com

    For me (using the Ubuntu Subsystem for Windows) the error message changed to:

 Permissions 0555 for 'key.pem' are too open


after using chmod 400.
It turns out that using root as a default user was the reason.

Change this using the cmd:

 ubuntu config --default-user your_username

    The other trick is to do that on the downloads folder.
After you download the private key from AWS EC2 instance, the file will be in this folder,then simply type the command
ssh-keygen -y -f myprivateKey.pem > mypublicKey.pub

    for Win10 need move your key to user's home dir
for linuxlike os you need to chmod to 700 like or 600 etc.
    PuTTY can do the work on windows 10. It generates a public key using a private key as input.

Download PuTTY
Install PuTTY.  Two applications come upon the installation: putty config, putty key gen
Launch puttyGen
Click load and select a private key file. Please note, you need to rename your private key file with .ppk extension, e.g. private-key.ppk


    I got same issue after migration from another mac. 
And it blocked to connect github by my key.

I reset permission as below and it works well now.

chmod 700 ~/.ssh     # (drwx------)
cd ~/.ssh            
chmod 644 *.pub      # (-rw-r--r--)
chmod 600 id_rsa     # (-rw-------)

    In my case the issue was a whitespace too much.
ssh -i mykey.pem  ubuntu@instace.eu-north-1.compute.amazonaws.com

but
ssh -i mykey.pem ubuntu@instace.eu-north-1.compute.amazonaws.com

worked fine. The problem is that the whitespace is taken as part of the username.
    I was getting this issue on WSL on Windows while connecting to AWS instance. My issue got resolved by switching to classic Command prompt. You can try switching to a different terminal interface and see if that helps.
    For Windows 10 this is what I've found works for me:

Move your key to the Linux file system:
mv ~/.ssh /home/{username}
Set the permission on that key:
chmod 700 /home/{username}/.ssh/id_rsa
Create a symbolic link to the key:
ln -s /home/{username}/.ssh ~/.ssh

This happens if you have set your home directory (~) to be stored in Windows instead of Linux (under /mnt/ vs /home/).
    I tried 600 level of permission for my private key and it worked for me.
chmod 600 privateKey 
[dev]$ ssh -i privateKey user@ip


On the other hand,
chmod 755 privateKey 
[dev]$ ssh -i privateKey user@ip

was giving below issue:
Permissions 0755 for 'privateKey' are too open.
It is required that your private key files are NOT accessible by others.
This private key will be ignored.
Load key ""privateKey"": bad permissions

    I have came across with this error while I was playing with Ansible. I have changed the permissions of the private key to 600 in order to solve this problem. And it worked!

chmod 600 .vagrant/machines/default/virtualbox/private_key

    Interesting message here.
Operating Systems are smart enough to deny remote connections if your private key is too open. It understands the risk where permissions for id_rsa is wide open (read, is editable by anyone).

{One may change your lock first and then open it with the keys he already has}

cd ~/.ssh
chmod 400 id_rsa


While working on the multiple servers (non-production), most of us feel need to connect remote server with ssh. A good idea is to have a piece of application level code (may be java using jsch) to create ssh trusts between servers. This way connection will be password-less. Incase, perl is installed - one may use net ssh module too.
    ","[2680, 4489, 53, 24, 118, 35, 18, 39, 24, 14, 50, 22, 13, 36, 3, 4, 19, 3, 3, 3, 5, 2, 0, 1, 4, 1, 1, 1, 1, 1, 3]",2014423,494,2012-02-14T02:02:31,2022-02-09 13:30:58Z,
Check if a given key already exists in a dictionary,"
                    
            
        
            
                    
                        
                    
                
                    
                        This question's answers are a community effort. Edit existing answers to improve this post. It is not currently accepting new answers or interactions.
                        
                    
                
            
        

    

I wanted to test if a key exists in a dictionary before updating the value for the key.
I wrote the following code:

if 'key1' in dict.keys():
  print ""blah""
else:
  print ""boo""


I think this is not the best way to accomplish this task. Is there a better way to test for a key in the dictionary?
    in is the intended way to test for the existence of a key in a dict.

d = {""key1"": 10, ""key2"": 23}

if ""key1"" in d:
    print(""this will execute"")

if ""nonexistent key"" in d:
    print(""this will not"")


If you wanted a default, you can always use dict.get():

d = dict()

for i in range(100):
    key = i % 10
    d[key] = d.get(key, 0) + 1


and if you wanted to always ensure a default value for any key you can either use dict.setdefault() repeatedly or defaultdict from the collections module, like so:

from collections import defaultdict

d = defaultdict(int)

for i in range(100):
    d[i % 10] += 1


but in general, the in keyword is the best way to do it.
    You don't have to call keys:

if 'key1' in dict:
  print(""blah"")
else:
  print(""boo"")


That will be much faster as it uses the dictionary's hashing as opposed to doing a linear search, which calling keys would do.
    You can shorten your code to this:
if 'key1' in my_dict:
    ...

However, this is at best a cosmetic improvement. Why do you believe this is not the best way?
    For additional information on speed execution of the accepted answer's proposed methods (10million loops):

'key' in mydict elapsed time 1.07 seconds
mydict.get('key') elapsed time 1.84 seconds
mydefaultdict['key'] elapsed time 1.07 seconds

Therefore using in or defaultdict are recommended against get.
    You can test for the presence of a key in a dictionary, using the in keyword:

d = {'a': 1, 'b': 2}
'a' in d # <== evaluates to True
'c' in d # <== evaluates to False


A common use for checking the existence of a key in a dictionary before mutating it is to default-initialize the value (e.g. if your values are lists, for example, and you want to ensure that there is an empty list to which you can append when inserting the first value for a key). In cases such as those, you may find the collections.defaultdict() type to be of interest.

In older code, you may also find some uses of has_key(), a deprecated method for checking the existence of keys in dictionaries (just use key_name in dict_name, instead).
    Using the Python ternary operator:
message = ""blah"" if 'key1' in my_dict else ""booh""
print(message)

    
Check if a given key already exists in a dictionary

To get the idea how to do that we first inspect what methods we can call on dictionary.
Here are the methods:
d={'clear':0, 'copy':1, 'fromkeys':2, 'get':3, 'items':4, 'keys':5, 'pop':6, 'popitem':7, 'setdefault':8, 'update':9, 'values':10}


Python Dictionary clear()        Removes all Items
Python Dictionary copy()         Returns Shallow Copy of a Dictionary
Python Dictionary fromkeys()     Creates dictionary from given sequence
Python Dictionary get()          Returns Value of The Key
Python Dictionary items()        Returns view of dictionary (key, value) pair
Python Dictionary keys()         Returns View Object of All Keys
Python Dictionary pop()          Removes and returns element having given key
Python Dictionary popitem()      Returns & Removes Element From Dictionary
Python Dictionary setdefault()   Inserts Key With a Value if Key is not Present
Python Dictionary update()       Updates the Dictionary
Python Dictionary values()       Returns view of all values in dictionary


The brutal method to check if the key already exists may be the get() method:
d.get(""key"")

The other two interesting methods items() and keys() sounds like too much of work. So let's examine if get() is the right method for us. We have our dict d:
d= {'clear':0, 'copy':1, 'fromkeys':2, 'get':3, 'items':4, 'keys':5, 'pop':6, 'popitem':7, 'setdefault':8, 'update':9, 'values':10}

Printing shows the key we don't have will return None:
print(d.get('key')) #None
print(d.get('clear')) #0
print(d.get('copy')) #1

We use that to get the information if the key is present or no.
But consider this if we create a dict with a single key:None:
d= {'key':None}
print(d.get('key')) #None
print(d.get('key2')) #None

Leading that get() method is not reliable in case some values may be None.
This story should have a happier ending. If we use the in comparator:
print('key' in d) #True
print('key2' in d) #False

We get the correct results.
We may examine the Python byte code:
import dis
dis.dis(""'key' in d"")
#   1           0 LOAD_CONST               0 ('key')
#               2 LOAD_NAME                0 (d)
#               4 COMPARE_OP               6 (in)
#               6 RETURN_VALUE

dis.dis(""d.get('key2')"")
#   1           0 LOAD_NAME                0 (d)
#               2 LOAD_METHOD              1 (get)
#               4 LOAD_CONST               0 ('key2')
#               6 CALL_METHOD              1
#               8 RETURN_VALUE

This shows that in compare operator is not just more reliable, but even faster than get().
    Use EAFP (easier to ask forgiveness than permission):
try:
   blah = dict[""mykey""]
   # key exists in dict
except KeyError:
   # key doesn't exist in dict

See other Stack Overflow posts:

Using 'try' vs. 'if' in Python
Checking for member existence in Python

    A dictionary in Python has a get('key', default) method. So you can just set a default value in case there isn't any key.
values = {...}
myValue = values.get('Key', None)

    I would recommend using the setdefault method instead.  It sounds like it will do everything you want.

>>> d = {'foo':'bar'}
>>> q = d.setdefault('foo','baz') #Do not override the existing key
>>> print q #The value takes what was originally in the dictionary
bar
>>> print d
{'foo': 'bar'}
>>> r = d.setdefault('baz',18) #baz was never in the dictionary
>>> print r #Now r has the value supplied above
18
>>> print d #The dictionary's been updated
{'foo': 'bar', 'baz': 18}

    The ways in which you can get the results are:


if your_dict.has_key(key) Removed in Python 3
if key in your_dict
try/except block


Which is better is dependent on 3 things:


Does the dictionary 'normally has the key' or 'normally does not have the key'.
Do you intend to use conditions like if...else...elseif...else?
How big is dictionary?


Read More: http://paltman.com/try-except-performance-in-python-a-simple-test/

Use of try/block instead of 'in' or 'if':

try:
    my_dict_of_items[key_i_want_to_check]
except KeyError:
    # Do the operation you wanted to do for ""key not present in dict"".
else:
    # Do the operation you wanted to do with ""key present in dict.""

    Python 2 only: (and Python2.7 supports `in` already)
You can use the has_key() method:
if dict.has_key('xyz')==1:
    # Update the value for the key
else:
    pass

    Just an FYI adding to Chris. B's (best) answer:
d = defaultdict(int)

Works as well; the reason is that calling int() returns 0 which is what defaultdict does behind the scenes (when constructing a dictionary), hence the name ""Factory Function"" in the documentation.
    You can use a for loop to iterate over the dictionary and get the name of key you want to find in the dictionary. After that, check if it exist or not using if condition:
dic = {'first' : 12, 'second' : 123}
for each in dic:
    if each == 'second':
        print('the key exists and the corresponding value can be updated in the dictionary')

    A Python dictionary has the method called __contains__. This method will return True if the dictionary has the key, else it returns False.
>>> temp = {}

>>> help(temp.__contains__)

Help on built-in function __contains__:

__contains__(key, /) method of builtins.dict instance
    True if D has a key k, else False.

    Another way of checking if a key exists using Boolean operators:
d = {'a': 1, 'b':2}
keys = 'abcd'

for k in keys:
    x = (k in d and 'blah') or 'boo'
    print(x)

This returns
>>> blah
>>> blah
>>> boo
>>> boo

Explanation
First, you should know that in Python, 0, None, or objects with zero length evaluate to False. Everything else evaluates to True. Boolean operations are evaluated left to right and return the operand not True or False.
Let's see an example:
>>> 'Some string' or 1/0
'Some string'
>>>

Since 'Some string' evaluates to True, the rest of the or is not evaluated and there is no division by zero error raised.
But if we switch the order 1/0 is evaluated first and raises an exception:
>>> 1/0 or 'Some string'
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ZeroDivisionError: division by zero
>>>

We can use this for pattern for checking if a key exists.
(k in d and 'blah')

does the same as
if k in d:
    'blah'
else:
    False

This already returns the correct result if the key exists, but we want it to print 'boo' when it doesn't. So, we take the result and or it with 'boo'
>>> False or 'boo'
'boo'
>>> 'blah' or 'boo'
'blah'
>>>

    ","[2679, 4690, 1718, 156, 86, 292, 49, 26, 45, 55, 63, 22, 20, 18, 3, 8, 5]",5180215,572,2009-10-21T19:05:09,2022-02-02 18:43:04Z,python 
How do I get a YouTube video thumbnail from the YouTube API?,"
                
If I have a YouTube video URL, is there any way to use PHP and cURL to get the associated thumbnail from the YouTube API?
    


Each YouTube video has four generated images. They are predictably formatted as follows:

https://img.youtube.com/vi/<insert-youtube-video-id-here>/0.jpg
https://img.youtube.com/vi/<insert-youtube-video-id-here>/1.jpg
https://img.youtube.com/vi/<insert-youtube-video-id-here>/2.jpg
https://img.youtube.com/vi/<insert-youtube-video-id-here>/3.jpg


The first one in the list is a full size image and others are thumbnail images. The default thumbnail image (i.e., one of 1.jpg, 2.jpg, 3.jpg) is:

https://img.youtube.com/vi/<insert-youtube-video-id-here>/default.jpg


For the high quality version of the thumbnail use a URL similar to this:

https://img.youtube.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg


There is also a medium quality version of the thumbnail, using a URL similar to the HQ:

https://img.youtube.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg


For the standard definition version of the thumbnail, use a URL similar to this:

https://img.youtube.com/vi/<insert-youtube-video-id-here>/sddefault.jpg


For the maximum resolution version of the thumbnail use a URL similar to this:

https://img.youtube.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg


All of the above URLs are available over HTTP too. Additionally, the slightly shorter hostname i3.ytimg.com works in place of img.youtube.com in the example URLs above.

Alternatively, you can use the YouTube Data API (v3) to get thumbnail images.
    What Asaph said is right. However, not every YouTube video contains all nine
thumbnails. Also, the thumbnails' image sizes depends on the video (the numbers
below are based on one). There are some thumbnails guaranteed to exist:

Width | Height | URL
------|--------|----
120   | 90     | https://i.ytimg.com/vi/<VIDEO ID>/1.jpg
120   | 90     | https://i.ytimg.com/vi/<VIDEO ID>/2.jpg
120   | 90     | https://i.ytimg.com/vi/<VIDEO ID>/3.jpg
120   | 90     | https://i.ytimg.com/vi/<VIDEO ID>/default.jpg
320   | 180    | https://i.ytimg.com/vi/<VIDEO ID>/mq1.jpg
320   | 180    | https://i.ytimg.com/vi/<VIDEO ID>/mq2.jpg
320   | 180    | https://i.ytimg.com/vi/<VIDEO ID>/mq3.jpg
320   | 180    | https://i.ytimg.com/vi/<VIDEO ID>/mqdefault.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/0.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/hq1.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/hq2.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/hq3.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/hqdefault.jpg

Additionally, the some other thumbnails may or may not exist. Their presence is
probably based on whether the video is high-quality.
Width | Height | URL
------|--------|----
640   | 480    | https://i.ytimg.com/vi/<VIDEO ID>/sd1.jpg
640   | 480    | https://i.ytimg.com/vi/<VIDEO ID>/sd2.jpg
640   | 480    | https://i.ytimg.com/vi/<VIDEO ID>/sd3.jpg
640   | 480    | https://i.ytimg.com/vi/<VIDEO ID>/sddefault.jpg
1280  | 720    | https://i.ytimg.com/vi/<VIDEO ID>/hq720.jpg
1920  | 1080   | https://i.ytimg.com/vi/<VIDEO ID>/maxresdefault.jpg

You can find JavaScript and PHP scripts to retrieve thumbnails and other
YouTube information in:

How to get YouTube Video Info with PHP
Retrieve YouTube Video Details using JavaScript - JSON & API v2

You can also use the YouTube Video Information Generator tool to get all
the information about a YouTube video by submitting a URL or video id.
    You can use YouTube Data API to retrieve video thumbnails, caption, description, rating, statistics and more. API version 3 requires a key*. Obtain the key and create a videos: list request:
https://www.googleapis.com/youtube/v3/videos?key=YOUR_API_KEY&part=snippet&id=VIDEO_ID

Example PHP Code
$data = file_get_contents(""https://www.googleapis.com/youtube/v3/videos?key=YOUR_API_KEY&part=snippet&id=T0Jqdjbed40"");
$json = json_decode($data);
var_dump($json->items[0]->snippet->thumbnails);

Output
object(stdClass)#5 (5) {
  [""default""]=>
  object(stdClass)#6 (3) {
    [""url""]=>
    string(46) ""https://i.ytimg.com/vi/T0Jqdjbed40/default.jpg""
    [""width""]=>
    int(120)
    [""height""]=>
    int(90)
  }
  [""medium""]=>
  object(stdClass)#7 (3) {
    [""url""]=>
    string(48) ""https://i.ytimg.com/vi/T0Jqdjbed40/mqdefault.jpg""
    [""width""]=>
    int(320)
    [""height""]=>
    int(180)
  }
  [""high""]=>
  object(stdClass)#8 (3) {
    [""url""]=>
    string(48) ""https://i.ytimg.com/vi/T0Jqdjbed40/hqdefault.jpg""
    [""width""]=>
    int(480)
    [""height""]=>
    int(360)
  }
  [""standard""]=>
  object(stdClass)#9 (3) {
    [""url""]=>
    string(48) ""https://i.ytimg.com/vi/T0Jqdjbed40/sddefault.jpg""
    [""width""]=>
    int(640)
    [""height""]=>
    int(480)
  }
  [""maxres""]=>
  object(stdClass)#10 (3) {
    [""url""]=>
    string(52) ""https://i.ytimg.com/vi/T0Jqdjbed40/maxresdefault.jpg""
    [""width""]=>
    int(1280)
    [""height""]=>
    int(720)
  }
}

* Not only that you need a key, you might be asked for billing information depending on the number of API requests you plan to make. However, few thousand requests per day are free.
Source article.
    YouTube is serving thumbnails from 2 servers. You just need to replace <YouTube_Video_ID_HERE> with your own YouTube video id. These days webP is best format for fast loading of images due to small image size.
https://img.youtube.com
https://i.ytimg.com
Examples are with https://i.ytimg.com server just because its shorter, no other particular reason. You can use both.
Player Background Thumbnail (480x360):
WebP
https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/0.webp

JPG
https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/0.jpg

Video frames thumbnails (120x90)
WebP:
Start: https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/1.webp
Middle: https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/2.webp
End: https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/3.webp

JPG:
Start: https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/1.jpg
Middle: https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/2.jpg
End: https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/3.jpg

Lowest quality thumbnail (120x90)
WebP
https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/default.webp

JPG
https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/default.jpg

Medium quality thumbnail (320x180)
WebP
https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/mqdefault.webp

JPG
https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/mqdefault.jpg

High quality thumbnail (480x360)
WebP
https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/hqdefault.webp

JPG
https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/hqdefault.jpg

Standard quality thumbnail (640x480)
WebP
https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/sddefault.webp

JPG
https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/sddefault.jpg

Unscaled thumbnail resolution
WebP
https://i.ytimg.com/vi_webp/<YouTube_Video_ID_HERE>/maxresdefault.webp

JPG
https://i.ytimg.com/vi/<YouTube_Video_ID_HERE>/maxresdefault.jpg

    In YouTube API V3 we can also use these URLs for obtaining thumbnails... They are classified based on their quality.

https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/default.jpg -   default
https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg - medium 
https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg - high
https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/sddefault.jpg - standard


And for the maximum resolution..

https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg


One advantage of these URLs over the URLs in the first answer is that these URLs don't get blocked by firewalls.
    If you want to get rid of the ""black bars"" and do it like YouTube does it, you can use:
https://i.ytimg.com/vi_webp/<video id>/mqdefault.webp

And if you can't use the .webp file extension you can do it like this:
https://i.ytimg.com/vi/<video id>/mqdefault.jpg

Also, if you need the unscaled version, use maxresdefault instead of mqdefault.
Note: I'm not sure about the aspect ratio if you're planning to use maxresdefault.
    YouTube is owned by Google and Google likes to have a reasonable number of images for different screen sizes, hence its images are stored in different sizes. Here is an example of how your thumbnail will be like:

Low quality thumbnail:

http://img.youtube.com/vi/<YouTube_Video_ID_HERE>/sddefault.jpg


Medium quality thumbnail:

http://img.youtube.com/vi/<YouTube_Video_ID_HERE>/mqdefault.jpg


High quality thumbnail:

http://img.youtube.com/vi/<YouTube_Video_ID_HERE>/hqdefault.jpg


Maximum quality thumbnail:

http://img.youtube.com/vi/<YouTube_Video_ID_HERE>/maxresdefault.jpg

    https://i.ytimg.com/vi/<--Video ID-->/default.jpg


Image Size Weight 120px Height 90px

https://i.ytimg.com/vi/<--Video ID-->/mqdefault.jpg


Image Size Weight 320px Height 180px

https://i.ytimg.com/vi/<--Video ID-->/hqdefault.jpg


Image Size Weight 480px Height 360px

https://i.ytimg.com/vi/<--Video ID-->/sddefault.jpg


Image Size Weight 640px Height 480px

https://i.ytimg.com/vi/<--Video ID-->/maxresdefault.jpg


Image Size Weight 1280px Height 720px

    If you want the biggest image from YouTube for a specific video ID, then the URL should be something like this:
http://i3.ytimg.com/vi/SomeVideoIDHere/0.jpg

Using the API, you can pick up the default thumbnail image. Simple code should be something like this:
//Grab the default thumbnail image
$attrs = $media->group->thumbnail[1]->attributes();
$thumbnail = $attrs['url'];
$thumbnail = substr($thumbnail, 0, -5);
$thumb1 = $thumbnail.""default.jpg"";

// Grab the third thumbnail image
$thumb2 = $thumbnail.""2.jpg"";

// Grab the fourth thumbnail image.
$thumb3 = $thumbnail.""3.jpg"";

// Using simple cURL to save it your server.
// You can extend the cURL below if you want it as fancy, just like
// the rest of the folks here.

$ch = curl_init (""$thumb1"");
curl_setopt($ch, CURLOPT_HEADER, 0);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
curl_setopt($ch, CURLOPT_BINARYTRANSFER,1);
$rawdata = curl_exec($ch);
curl_close($ch);

// Using fwrite to save the above
$fp = fopen(""SomeLocationInReferenceToYourScript/AnyNameYouWant.jpg"", 'w');

// Write the file
fwrite($fp, $rawdata);

// And then close it.
fclose($fp);

    You can get the video ID from the YouTube video url using parse_url ,parse_str and then insert in to the predictive urls for images. Thanks to YouTube for the predictive URLs
$videoUrl = ""https://www.youtube.com/watch?v=8zy7wGbQgfw"";
parse_str( parse_url( $videoUrl, PHP_URL_QUERY ), $my_array_of_vars );
$ytID = $my_array_of_vars['v']; //gets video ID

print ""https://img.youtube.com/vi/$ytID/maxresdefault.jpg"";
print ""https://img.youtube.com/vi/$ytID/mqdefault.jpg"";
print ""https://img.youtube.com/vi/$ytID/hqdefault.jpg"";
print ""https://img.youtube.com/vi/$ytID/sddefault.jpg"";
print ""https://img.youtube.com/vi/$ytID/default.jpg"";

You can use this tool to generate YouTube thumbnails
https://youtube-thumbnail-tool.com
    // Get image form video URL
$url = $video['video_url'];

$urls = parse_url($url);

//Expect the URL to be http://youtu.be/abcd, where abcd is the video ID
if ($urls['host'] == 'youtu.be') :

    $imgPath = ltrim($urls['path'],'/');

//Expect the URL to be http://www.youtube.com/embed/abcd
elseif (strpos($urls['path'],'embed') == 1) :

    $imgPath = end(explode('/',$urls['path']));

//Expect the URL to be abcd only
elseif (strpos($url,'/') === false):

    $imgPath = $url;

//Expect the URL to be http://www.youtube.com/watch?v=abcd
else :

    parse_str($urls['query']);

    $imgPath = $v;

endif;

    Method 1:

You can find all information for a YouTube video with a JSON page which has even ""thumbnail_url"",
http://www.youtube.com/oembed?format=json&url={your video URL goes here}

Like final URL look + PHP test code

$data = file_get_contents(""https://www.youtube.com/oembed?format=json&url=https://www.youtube.com/watch?v=_7s-6V_0nwA"");
$json = json_decode($data);
var_dump($json);


Output

object(stdClass)[1]
  public 'width' => int 480
  public 'version' => string '1.0' (length=3)
  public 'thumbnail_width' => int 480
  public 'title' => string 'how to reminder in window as display message' (length=44)
  public 'provider_url' => string 'https://www.youtube.com/' (length=24)
  public 'thumbnail_url' => string 'https://i.ytimg.com/vi/_7s-6V_0nwA/hqdefault.jpg' (length=48)
  public 'author_name' => string 'H2 ZONE' (length=7)
  public 'type' => string 'video' (length=5)
  public 'author_url' => string 'https://www.youtube.com/channel/UC9M35YwDs8_PCWXd3qkiNzg' (length=56)
  public 'provider_name' => string 'YouTube' (length=7)
  public 'height' => int 270
  public 'html' => string '<iframe width=""480"" height=""270"" src=""https://www.youtube.com/embed/_7s-6V_0nwA?feature=oembed"" frameborder=""0"" allow=""autoplay; encrypted-media"" allowfullscreen></iframe>' (length=171)
  public 'thumbnail_height' => int 360


For details, you can also see How to get a YouTube video thumbnail using id
or https://www.youtube.com/watch?v=mXde7q59BI8 video tutorial 1

Method 2:

Using YouTube image link,
https://img.youtube.com/vi/""insert-youtube-video-id-here""/default.jpg

Method 3:

Using browser source code for getting thumbnail using video URL link
-go to video source code and search for thumbnailurl.
Now you can use this URL into
your source code:

{img src=""https://img.youtube.com/vi/""insert-youtube-video-id-here""/default.jpg""}


For details you can also see How to get a YouTube video thumbnail using id
or
https://www.youtube.com/watch?v=9f6E8MeM6PI
video tutorial 2
    In YouTube Data API v3, you can get video's thumbnails with the videos->list function. From snippet.thumbnails.(key), you can pick the default, medium or high resolution thumbnail, and get its width, height and URL.

You can also update thumbnails with the thumbnails->set functionality.

For examples, you can check out the YouTube API Samples project. (PHP ones.)
    You can get the Video Entry which contains the URL to the video's thumbnail. There's example code in the link. Or, if you want to parse XML, there's information here. The XML returned has a media:thumbnail element, which contains the thumbnail's URL.
    I made a function to only fetch existing images from YouTube 

function youtube_image($id) {
    $resolution = array (
        'maxresdefault',
        'sddefault',
        'mqdefault',
        'hqdefault',
        'default'
    );

    for ($x = 0; $x < sizeof($resolution); $x++) {
        $url = '//img.youtube.com/vi/' . $id . '/' . $resolution[$x] . '.jpg';
        if (get_headers($url)[0] == 'HTTP/1.0 200 OK') {
            break;
        }
    }
    return $url;
}

    Another good alternative would be to use the oEmbed API which is supported by YouTube.

You simply add your YouTube URL to the oEmbed URL and you'll receive a JSON including a thumbnail and the HTML code for embedding.

Example:

http://www.youtube.com/oembed?format=json&url=http%3A//youtube.com/watch%3Fv%3DxUeJdWYdMmQ


Would give you:

{
  ""height"":270,
  ""width"":480,
  ""title"":""example video for 2020"",
  ""thumbnail_width"":480,
  ""html"":""..."",
  ""thumbnail_height"":360,
  ""version"":""1.0"",
  ""provider_name"":""YouTube"",
  ""author_url"":""https:\/\/www.youtube.com\/channel\/UCza6VSQUzCON- AzlsrOLwaA"",
  ""thumbnail_url"":""https:\/\/i.ytimg.com\/vi\/xUeJdWYdMmQ\/hqdefault.jpg"",
  ""author_name"":""Pokics"",
  ""provider_url"":""https:\/\/www.youtube.com\/"",
  ""type"":""video""
}


Read the documentation for more information.
    YouTube Data API

YouTube provides us the four generated images for every video through the Data API (v3), for example,


https://i.ytimg.com/vi/V_zwalcR8DU/maxresdefault.jpg
https://i.ytimg.com/vi/V_zwalcR8DU/sddefault.jpg
https://i.ytimg.com/vi/V_zwalcR8DU/hqdefault.jpg
https://i.ytimg.com/vi/V_zwalcR8DU/mqdefault.jpg


Getting access to the images via the API


First get your public API key at Google API Console.
As per YouTube's thumbnail reference in the API documentation, you need to access the resources on snippet.thumbnails.
As per this, you need to phrase your URL like this - 

www.googleapis.com/youtube/v3/videos?part=snippet&id=`yourVideoId`&key=`yourApiKey`



Now change your video ID and your API key to the your respective video-id and api-key and its response will be a JSON output providing you the four links in the thumbnails of snippet variable (if all are available).
    I think there is a lot of answers for thumbnail, but I want to add some other URLs to get YouTube thumbnail very easily. I am just taking some text from Asaph's answer. Here are some URLs to get YouTube thumbnails:
https://ytimg.googleusercontent.com/vi/<insert-youtube-video-id-here>/default.jpg

For the high quality version of the thumbnail use a URL similar to this:
https://ytimg.googleusercontent.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg

There is also a medium quality version of the thumbnail, using a URL similar to the high quality:
https://ytimg.googleusercontent.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg

For the standard definition version of the thumbnail, use a URL similar to this:
https://ytimg.googleusercontent.com/vi/<insert-youtube-video-id-here>/sddefault.jpg

For the maximum resolution version of the thumbnail use a URL similar to this:
https://ytimg.googleusercontent.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg

    Here's the top answer optimized for manual use. The video ID token without separators enables selecting with a double click.

Each YouTube video has four generated images. They are predictably formatted as follows:

https://img.youtube.com/vi/YOUTUBEVIDEOID/0.jpg
https://img.youtube.com/vi/YOUTUBEVIDEOID/1.jpg
https://img.youtube.com/vi/YOUTUBEVIDEOID/2.jpg
https://img.youtube.com/vi/YOUTUBEVIDEOID/3.jpg


The first one in the list is a full size image and others are thumbnail images. The default thumbnail image (ie. one of 1.jpg, 2.jpg, 3.jpg) is:

https://img.youtube.com/vi/YOUTUBEVIDEOID/default.jpg


For the high quality version of the thumbnail use a URL  similar to this:

https://img.youtube.com/vi/YOUTUBEVIDEOID/hqdefault.jpg


There is also a medium quality version of the thumbnail, using a URL similar to the HQ:

https://img.youtube.com/vi/YOUTUBEVIDEOID/mqdefault.jpg


For the standard definition version of the thumbnail, use a URL similar to this:

https://img.youtube.com/vi/YOUTUBEVIDEOID/sddefault.jpg


For the maximum resolution version of the thumbnail use a URL similar to this:

https://img.youtube.com/vi/YOUTUBEVIDEOID/maxresdefault.jpg


All of the above URLs are available over HTTP too. Additionally, the slightly shorter hostname i3.ytimg.com works in place of img.youtube.com in the example URLs above.

Alternatively, you can use the YouTube Data API (v3) to get thumbnail images.
    This is my client-side-only no-API-key-required solution.

YouTube.parse('https://www.youtube.com/watch?v=P3DGwyl0mJQ').then(_ => console.log(_))


The code:

import { parseURL, parseQueryString } from './url'
import { getImageSize } from './image'

const PICTURE_SIZE_NAMES = [
    // 1280 x 720.
    // HD aspect ratio.
    'maxresdefault',
    // 629 x 472.
    // non-HD aspect ratio.
    'sddefault',
    // For really old videos not having `maxresdefault`/`sddefault`.
    'hqdefault'
]

// - Supported YouTube URL formats:
//   - http://www.youtube.com/watch?v=My2FRPA3Gf8
//   - http://youtu.be/My2FRPA3Gf8
export default
{
    parse: async function(url)
    {
        // Get video ID.
        let id
        const location = parseURL(url)
        if (location.hostname === 'www.youtube.com') {
            if (location.search) {
                const query = parseQueryString(location.search.slice('/'.length))
                id = query.v
            }
        } else if (location.hostname === 'youtu.be') {
            id = location.pathname.slice('/'.length)
        }

        if (id) {
            return {
                source: {
                    provider: 'YouTube',
                    id
                },
                picture: await this.getPicture(id)
            }
        }
    },

    getPicture: async (id) => {
        for (const sizeName of PICTURE_SIZE_NAMES) {
            try {
                const url = getPictureSizeURL(id, sizeName)
                return {
                    type: 'image/jpeg',
                    sizes: [{
                        url,
                        ...(await getImageSize(url))
                    }]
                }
            } catch (error) {
                console.error(error)
            }
        }
        throw new Error(`No picture found for YouTube video ${id}`)
    },

    getEmbeddedVideoURL(id, options = {}) {
        return `https://www.youtube.com/embed/${id}`
    }
}

const getPictureSizeURL = (id, sizeName) => `https://img.youtube.com/vi/${id}/${sizeName}.jpg`


Utility image.js:

// Gets image size.
// Returns a `Promise`.
function getImageSize(url)
{
    return new Promise((resolve, reject) =>
    {
        const image = new Image()
        image.onload = () => resolve({ width: image.width, height: image.height })
        image.onerror = reject
        image.src = url
    })
}


Utility url.js:

// Only on client side.
export function parseURL(url)
{
    const link = document.createElement('a')
    link.href = url
    return link
}

export function parseQueryString(queryString)
{
    return queryString.split('&').reduce((query, part) =>
    {
        const [key, value] = part.split('=')
        query[decodeURIComponent(key)] = decodeURIComponent(value)
        return query
    },
    {})
}

    Just to add/expand on the solutions given, I feel it is necessary to note that, as I had this problem myself, one can actually grab multiple YouTube videos content, in this case, thumbnails,  with one HTTP request:

Using a Rest Client, in this case, HTTPFUL, you can do something like this:

<?php
header(""Content-type"", ""application/json"");

//download the httpfull.phar file from http://phphttpclient.com
include(""httpful.phar"");

$youtubeVidIds= array(""nL-rk4bgJWU"", ""__kupr7KQos"", ""UCSynl4WbLQ"", ""joPjqEGJGqU"", ""PBwEBjX3D3Q"");


$response = \Httpful\Request::get(""https://www.googleapis.com/youtube/v3/videos?key=YourAPIKey4&part=snippet&id="".implode ("","",$youtubeVidIds)."""")

->send();

print ($response);

?>

    YouTube API version 3 up and running in 2 minutes
If all you want to do is search YouTube and get associated properties:

Get a public API -- This link gives a good direction

Use below query string. The search query (denoted by q=) in the URL string is stackoverflow for example purposes. YouTube will then send you back a JSON reply where you can then parse for Thumbnail, Snippet, Author, etc.
https://www.googleapis.com/youtube/v3/search?part=id%2Csnippet&maxResults=50&q=stackoverflow&key=YOUR_API_KEY_HERE


    I have used YouTube thumbnails in this way:

$url = 'http://img.youtube.com/vi/' . $youtubeId . '/0.jpg';
$img = dirname(__FILE__) . '/youtubeThumbnail_'  . $youtubeId . '.jpg';
file_put_contents($img, file_get_contents($url));


Remember YouTube prevents to include images directly from their server.
    Use img.youtube.com/vi/YouTubeID/ImageFormat.jpg

Here image formats are different like default, hqdefault, 
maxresdefault.
    If you're using the public API, the best way to do it is using if statements.

If the video is public or unlisted, you set the thumbnail using the URL method.
If the video is private you use the API to get the thumbnail.

<?php
    if($video_status == 'unlisted'){
        $video_thumbnail = 'http://img.youtube.com/vi/'.$video_url.'/mqdefault.jpg';
        $video_status = '<i class=""fa fa-lock""></i>&nbsp;Unlisted';
    }
    elseif($video_status == 'public'){
        $video_thumbnail = 'http://img.youtube.com/vi/'.$video_url.'/mqdefault.jpg';
        $video_status = '<i class=""fa fa-eye""></i>&nbsp;Public';
    }
    elseif($video_status == 'private'){
        $video_thumbnail = $playlistItem['snippet']['thumbnails']['maxres']['url'];
        $video_status = '<i class=""fa fa-lock""></i>&nbsp;Private';
    }

        function get_video_thumbnail( $src ) {
            $url_pieces = explode('/', $src);
            if( $url_pieces[2] == 'dai.ly'){
                $id = $url_pieces[3];
                $hash = json_decode(file_get_contents('https://api.dailymotion.com/video/'.$id.'?fields=thumbnail_large_url'), TRUE);
                $thumbnail = $hash['thumbnail_large_url'];
            }else if($url_pieces[2] == 'www.dailymotion.com'){
                $id = $url_pieces[4];
                $hash = json_decode(file_get_contents('https://api.dailymotion.com/video/'.$id.'?fields=thumbnail_large_url'), TRUE);
                $thumbnail = $hash['thumbnail_large_url'];
            }else if ( $url_pieces[2] == 'vimeo.com' ) { // If Vimeo
                $id = $url_pieces[3];
                $hash = unserialize(file_get_contents('http://vimeo.com/api/v2/video/' . $id . '.php'));
                $thumbnail = $hash[0]['thumbnail_large'];
            } elseif ( $url_pieces[2] == 'youtu.be' ) { // If Youtube
                $extract_id = explode('?', $url_pieces[3]);
                $id = $extract_id[0];
                $thumbnail = 'http://img.youtube.com/vi/' . $id . '/mqdefault.jpg';
            }else if ( $url_pieces[2] == 'player.vimeo.com' ) { // If Vimeo
                $id = $url_pieces[4];
                $hash = unserialize(file_get_contents('http://vimeo.com/api/v2/video/' . $id . '.php'));
                $thumbnail = $hash[0]['thumbnail_large'];
            } elseif ( $url_pieces[2] == 'www.youtube.com' ) { // If Youtube
                $extract_id = explode('=', $url_pieces[3]);
                $id = $extract_id[1];
                $thumbnail = 'http://img.youtube.com/vi/' . $id . '/mqdefault.jpg';
            } else{
                $thumbnail = tim_thumb_default_image('video-icon.png', null, 147, 252);
            }
            return $thumbnail;
        }

get_video_thumbnail('https://vimeo.com/154618727');
get_video_thumbnail('https://www.youtube.com/watch?v=SwU0I7_5Cmc');
get_video_thumbnail('https://youtu.be/pbzIfnekjtM');
get_video_thumbnail('http://www.dailymotion.com/video/x5thjyz');

    A simple PHP function I created for the YouTube thumbnail and the types are


default
hqdefault
mqdefault
sddefault
maxresdefault




function get_youtube_thumb($link,$type){

    $video_id = explode(""?v="", $link);

    if (empty($video_id[1])){
        $video_id = explode(""/v/"", $link);
        $video_id = explode(""&"", $video_id[1]);
        $video_id = $video_id[0];
    }
    $thumb_link = """";

    if($type == 'default'   || $type == 'hqdefault' ||
       $type == 'mqdefault' || $type == 'sddefault' ||
       $type == 'maxresdefault'){

        $thumb_link = 'http://img.youtube.com/vi/'.$video_id.'/'.$type.'.jpg';

    }elseif($type == ""id""){
        $thumb_link = $video_id;
    }
    return $thumb_link;}

    Save this code in empty.php file and test it.
<img src=""<?php echo youtube_img_src('9bZkp7q19f0', 'high');?>"" />
<?php
// Get a YOUTUBE video thumb image's source url for IMG tag ""src"" attribute:
// $ID = YouYube video ID (string)
// $size = string (default, medium, high or standard)
function youtube_img_src ($ID = null, $size = 'default') {
    switch ($size) {
        case 'medium':
            $size = 'mqdefault';
            break;
        case 'high':
            $size = 'hqdefault';
            break;
        case 'standard':
            $size = 'sddefault';
            break;
        default:
            $size = 'default';
            break;
    }
    if ($ID) {
        return sprintf('https://img.youtube.com/vi/%s/%s.jpg', $ID, $size);
    }
    return 'https://img.youtube.com/vi/ERROR/1.jpg';
}

There are some thumbnails guaranteed to exist:
Width | Height | URL
------|--------|----
120   | 90     | https://i.ytimg.com/vi/<VIDEO ID>/1.jpg
120   | 90     | https://i.ytimg.com/vi/<VIDEO ID>/2.jpg
120   | 90     | https://i.ytimg.com/vi/<VIDEO ID>/3.jpg
120   | 90     | https://i.ytimg.com/vi/<VIDEO ID>/default.jpg
320   | 180    | https://i.ytimg.com/vi/<VIDEO ID>/mq1.jpg
320   | 180    | https://i.ytimg.com/vi/<VIDEO ID>/mq2.jpg
320   | 180    | https://i.ytimg.com/vi/<VIDEO ID>/mq3.jpg
320   | 180    | https://i.ytimg.com/vi/<VIDEO ID>/mqdefault.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/0.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/hq1.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/hq2.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/hq3.jpg
480   | 360    | https://i.ytimg.com/vi/<VIDEO ID>/hqdefault.jpg

Thanks.
    Use:

https://www.googleapis.com/youtube/v3/videoCategories?part=snippet,id&maxResults=100&regionCode=us&key=**Your YouTube ID**


Above is the link. Using that, you can find the YouTube characteristics of videos. After finding characteristics, you can get videos of the selected category. After then you can find selected video images using Asaph's answer.

Try the above approach and you can parse everything from the YouTube API.
    I found this nifty tool that allows you to create the image with the YouTube play button placed over the image:


Installed on the server for scripting: https://github.com/halgatewood/youtube-thumbnail-enhancer

    ","[2677, 5153, 314, 426, 18, 87, 57, 31, 5, 59, 16, 29, 9, 34, 34, 36, 18, 14, 10, 6, 4, 14, 17, 15, 4, 11, 7, 12, 2, 15, 13]",1871782,1234,2010-01-14T23:34:24,2022-03-29 03:44:20Z,php 
Do I cast the result of malloc?,"
                
In this question, someone suggested in a comment that I should not cast the result of malloc. i.e., I should do this:
int *sieve = malloc(sizeof(*sieve) * length);

rather than:
int *sieve = (int *) malloc(sizeof(*sieve) * length);

Why would this be the case?
    TL;DR
int *sieve = (int *) malloc(sizeof(int) * length);

has two problems. The cast and that you're using the type instead of variable as argument for sizeof. Instead, do like this:
int *sieve = malloc(sizeof *sieve * length);

Long version
No; you don't cast the result, since:

It is unnecessary, as void * is automatically and safely promoted to any other pointer type in this case.
It adds clutter to the code, casts are not very easy to read (especially if the pointer type is long).
It makes you repeat yourself, which is generally bad.
It can hide an error if you forgot to include <stdlib.h>. This can cause crashes (or, worse, not cause a crash until way later in some totally different part of the code). Consider what happens if pointers and integers are differently sized; then you're hiding a warning by casting and might lose bits of your returned address. Note: as of C99 implicit functions are gone from C, and this point is no longer relevant since there's no automatic assumption that undeclared functions return int.

As a clarification, note that I said ""you don't cast"", not ""you don't need to cast"". In my opinion, it's a failure to include the cast, even if you got it right. There are simply no benefits to doing it, but a bunch of potential risks, and including the cast indicates that you don't know about the risks.
Also note, as commentators point out, that the above talks about straight C, not C++. I very firmly believe in C and C++ as separate languages.
To add further, your code needlessly repeats the type information (int) which can cause errors. It's better to de-reference the pointer being used to store the return value, to ""lock"" the two together:
int *sieve = malloc(length * sizeof *sieve);

This also moves the length to the front for increased visibility, and drops the redundant parentheses with sizeof; they are only needed when the argument is a type name. Many people seem to not know (or ignore) this, which makes their code more verbose. Remember: sizeof is not a function! :)

While moving length to the front may increase visibility in some rare cases, one should also pay attention that in the general case, it should be better to write the expression as:
int *sieve = malloc(sizeof *sieve * length);

Since keeping the sizeof first, in this case, ensures multiplication is done with at least size_t math.
Compare: malloc(sizeof *sieve * length * width) vs. malloc(length * width * sizeof *sieve) the second may overflow the length * width when width and length are smaller types than size_t.
    In C, you don't need to cast the return value of malloc. The pointer to void returned by malloc is automagically converted to the correct type. However, if you want your code to compile with a C++ compiler, a cast is needed. A preferred alternative among the community is to use the following:

int *sieve = malloc(sizeof *sieve * length);


which additionally frees you from having to worry about changing the right-hand side of the expression if ever you change the type of sieve.

Casts are bad, as people have pointed out. Especially pointer casts. 
    You do cast, because:


It makes your code more portable between C and C++, and as SO experience shows, a great many programmers claim they are writing in C when they are really writing in C++ (or C plus local compiler extensions).
Failing to do so can hide an error: note all the SO examples of confusing when to write type * versus type **.
The idea that it keeps you from noticing you failed to #include an appropriate header file misses the forest for the trees. It's the same as saying ""don't worry about the fact you failed to ask the compiler to complain about not seeing prototypes -- that pesky stdlib.h is the REAL important thing to remember!""
It forces an extra cognitive cross-check. It puts the (alleged) desired type right next to the arithmetic you're doing for the raw size of that variable. I bet you could do an SO study that shows that malloc() bugs are caught much faster when there's a cast. As with assertions, annotations that reveal intent decrease bugs. 
Repeating yourself in a way that the machine can check is often a great idea. In fact, that's what an assertion is, and this use of cast is an assertion. Assertions are still the most general technique we have for getting code correct, since Turing came up with the idea so many years ago.

    From the Wikipedia:


  Advantages to casting
  
  
  Including the cast may allow a C program or function to compile as C++.
  The cast allows for pre-1989 versions of malloc that originally returned a char *.
  Casting can help the developer identify inconsistencies in type sizing should the destination pointer type change, particularly if the pointer is declared far from the malloc() call (although modern compilers and static analyzers can warn on such behaviour without requiring the cast).
  
  
  Disadvantages to casting
  
  
  Under the ANSI C standard, the cast is redundant.
  Adding the cast may mask failure to include the header stdlib.h, in
  which the prototype for malloc is found. In the absence of a
  prototype for malloc, the standard requires that the C compiler
  assume malloc returns an int. If there is no cast, a warning is
  issued when this integer is assigned to the pointer; however, with
  the cast, this warning is not produced, hiding a bug. On certain
  architectures and data models (such as LP64 on 64-bit systems, where
  long and pointers are 64-bit and int is 32-bit), this error can
  actually result in undefined behaviour, as the implicitly declared
  malloc returns a 32-bit value whereas the actually defined function
  returns a 64-bit value. Depending on calling conventions and memory
  layout, this may result in stack smashing. This issue is less likely
  to go unnoticed in modern compilers, as they uniformly produce
  warnings that an undeclared function has been used, so a warning will
  still appear. For example, GCC's default behaviour is to show a
  warning that reads ""incompatible implicit declaration of built-in
  function"" regardless of whether the cast is present or not.
  If the type of the pointer is changed at its declaration, one may
  also, need to change all lines where malloc is called and cast.
  


Although malloc without casting is preferred method and most experienced programmers choose it, you should use whichever you like having aware of the issues.

i.e: If you need to compile C program as C++ (Although it is a separate language) you must cast the result of use malloc. 
    As others stated, it is not needed for C, but necessary for C++. If you think you are going to compile your C code with a C++ compiler, for whatever reasons, you can use a macro instead, like:

#ifdef __cplusplus
# define NEW(type, count) ((type *)calloc(count, sizeof(type)))
#else
# define NEW(type, count) (calloc(count, sizeof(type)))
#endif


That way you can still write it in a very compact way:

int *sieve = NEW(int, 1);


and it will compile for C and C++.
    This question is subject of opinion-based abuse.
Sometimes I notice comments like that:

Don't cast the result of malloc

or

Why you don't cast the result of malloc

on questions where OP uses casting. The comments itself contain a hyperlink to this question.
That is in any possible manner inappropriate and incorrect as well. There is no right and no wrong when it is truly a matter of one's own coding-style.

Why is this happening?
It's based upon two reasons:

This question is indeed opinion-based. Technically, the question should have been closed as opinion-based years ago. A ""Do I"" or ""Don't I"" or equivalent ""Should I"" or ""Shouldn't I"" question, you just can't answer focused without an attitude of one's own opinion. One of the reason to close a question is because it ""might lead to opinion-based answers"" as it is well shown here.

Many answers (including the most apparent and accepted answer of @unwind) are either completely or almost entirely opinion-based (f.e. a mysterious ""clutter"" that would be added to your code if you do casting or repeating yourself would be bad) and show a clear and focused tendency to omit the cast. They argue about the redundancy of the cast on one side but also and even worse argue to solve a bug caused by a bug/failure of programming itself - to not #include <stdlib.h> if one want to use malloc().



I want to bring a true view of some points discussed, with less of my personal opinion. A few points need to be noted especially:

Such a very susceptible question to fall into one's own opinion needs an answer with neutral pros and cons. Not only cons or pros.
A good overview of pros and cons is listed in this answer:
https://stackoverflow.com/a/33047365/12139179
(I personally consider this because of that reason the best answer, so far.)




One reason which is encountered at most to reason the omission of the cast is that the cast might hide a bug.
If someone uses an implicit declared malloc() that returns int (implicit functions are gone from the standard since C99) and sizeof(int) != sizeof(int*), as shown in this question
Why does this code segfault on 64-bit architecture but work fine on 32-bit?
the cast would hide a bug.
While this is true, it only shows half of the story as the omission of the cast would only be a forward-bringing solution to an even bigger bug - not including stdlib.h when using malloc().
This will never be a serious issue, If you,

Use a compiler compliant to C99 or above (which is recommended and should be mandatory), and

Aren't so absent to forgot to include stdlib.h, when you want to use malloc() in your code, which is a huge bug itself.






Some people argue about C++ compliance of C code, as the cast is obliged in C++.
First of all to say in general: Compiling C code with a C++ compiler is not a good practice.
C and C++ are in fact two completely different languages with different semantics.
But If you really want/need to make C code compliant to C++ and vice versa use compiler switches instead of any cast.
Since the cast is with tendency declared as redundant or even harmful, I want to take a focus on these questions, which give good reasons why casting can be useful or even necessary:

https://stackoverflow.com/a/34094068/12139179

https://stackoverflow.com/a/36297486/12139179

https://stackoverflow.com/a/33044300/12139179






The cast can be non-beneficial when your code, respectively the type of the assigned pointer (and with that the type of the cast), changes, although this is in most cases unlikely. Then you would need to maintain/change all casts too and if you have a few thousand calls to memory-management functions in your code, this can really summarizing up and decrease the maintenance efficiency.


Summary:
Fact is, that the cast is redundant per the C standard (already since ANSI-C (C89/C90)) if the assigned pointer point to an object of fundamental alignment requirement (which includes the most of all objects).
You don't need to do the cast as the pointer is automatically aligned in this case:

""The order and contiguity of storage allocated by successive calls to the aligned_alloc, calloc, malloc, and realloc functions is unspecified. The pointer returned if the allocation succeeds is suitably aligned so that it may be assigned to a pointer to any type of object with a fundamental alignment requirement and then used to access such an object or an array of such objects in the space allocated (until the space is explicitly deallocated).""
Source: C18, 7.22.3/1



""A fundamental alignment is a valid alignment less than or equal to _Alignof (max_align_t). Fundamental alignments shall be supported by the implementation for objects of all storage durations. The alignment requirements of the following types shall be fundamental alignments:
  all atomic, qualified, or unqualified basic types;
  all atomic, qualified, or unqualified enumerated types;
  all atomic, qualified, or unqualified pointer types;
  all array types whose element type has a fundamental alignment requirement;57)
  all types specified in Clause 7 as complete object types;
  all structure or union types all of whose elements have types with fundamental alignment requirements and none of whose elements have an alignment specifier specifying an alignment that is not a fundamental alignment.

As specified in 6.2.1, the later declaration might hide the prior declaration.""

Source: C18, 6.2.8/2

However, if you allocate memory for an implementation-defined object of extended alignment requirement, the cast would be needed.

An extended alignment is represented by an alignment greater than _Alignof (max_align_t). It is implementation-defined whether any extended alignments are supported and the storage durations for which they are supported. A type having an extended alignment requirement is an over-aligned type.58)
Source. C18, 6.2.8/3

Everything else is a matter of the specific use case and one's own opinion.
Please be careful how you educate yourself.
I recommend you to read all of the answers made so far carefully first (as well as their comments which may point at a failure) and then build your own opinion if you or if you not cast the result of malloc() at a specific case.
Please note:
There is no right and wrong answer to that question. It is a matter of style and you yourself decide which way you choose (if you aren't forced to by education or job of course). Please be aware of that and don't let trick you.

Last note: I voted to lately close this question as opinion-based, which is indeed needed since years. If you got the close/reopen privilege I would like to invite you to do so, too.
    You don't cast the result of malloc, because doing so adds pointless clutter to your code.
The most common reason why people cast the result of malloc is because they are unsure about how the C language works. That's a warning sign: if you don't know how a particular language mechanism works, then don't take a guess. Look it up or ask on Stack Overflow.
Some comments:

A void pointer can be converted to/from any other pointer type without an explicit cast (C11 6.3.2.3 and 6.5.16.1).

C++ will however not allow an implicit cast between void* and another pointer type. So in C++, the cast would have been correct. But if you program in C++, you should use new and not malloc(). And you should never compile C code using a C++ compiler.
If you need to support both C and C++ with the same source code, use compiler switches to mark the differences. Do not attempt to sate both language standards with the same code, because they are not compatible.

If a C compiler cannot find a function because you forgot to include the header, you will get a compiler/linker error about that. So if you forgot to include <stdlib.h> that's no biggie, you won't be able to build your program.

On ancient compilers that follow a version of the standard which is more than 25 years old, forgetting to include <stdlib.h> would result in dangerous behavior. Because in that ancient standard, functions without a visible prototype implicitly converted the return type to int. Casting the result from malloc explicitly would then hide away this bug.
But that is really a non-issue. You aren't using a 25 years old computer, so why would you use a 25 years old compiler?


    In C you get an implicit conversion from void * to any other (data) pointer.
    In C you can implicitly convert a void pointer to any other kind of pointer, so a cast is not necessary. Using one may suggest to the casual observer that there is some reason why one is needed, which may be misleading.
    This is what The GNU C Library Reference manual says: 


  You can store the result of malloc into any pointer variable without a
  cast, because ISO C automatically converts the type void * to another
  type of pointer when necessary. But the cast is necessary in contexts
  other than assignment operators or if you might want your code to run
  in traditional C.


And indeed the ISO C11 standard (p347) says so: 


  The pointer returned if the allocation succeeds is suitably aligned so
  that it may be assigned to a pointer to any type of object with a
  fundamental alignment requirement and then used to access such an
  object or an array of such objects in the space allocated (until the
  space is explicitly deallocated)

    Casting the value returned by malloc() is not necessary now, but I'd like to add one point that seems no one has pointed out:

In the ancient days, that is, before ANSI C provides the void * as the generic type of pointers, char * is the type for such usage. In that case, the cast can shut down the compiler warnings.

Reference: C FAQ
    Just adding my experience, studying computer engineering I see that the two or three professors that I have seen writing in C always cast malloc, however the one I asked (with an immense CV and understanding of C) told me that it is absolutely unnecessary but only used to be absolutely specific, and to get the students into the mentality of being absolutely specific. Essentially casting will not change anything in how it works, it does exactly what it says, allocates memory, and casting does not effect it, you get the same memory, and even if you cast it to something else by mistake (and somehow evade compiler errors) C will access it the same way.

Edit: Casting has a certain point. When you use array notation, the code generated has to know how many memory places it has to advance to reach the beginning of the next element, this is achieved through casting. This way you know that for a double you go 8 bytes ahead while for an int you go 4, and so on. Thus it has no effect if you use pointer notation, in array notation it becomes necessary.
    It is not mandatory to cast the results of malloc, since it returns void* , and a void* can be pointed to any datatype. 
    It depends on the programming language and compiler. If you use malloc in C, there is no need to type cast it, as it will automatically type cast. However, if you are using C++, then you should type cast because malloc will return a void* type.
    A void pointer is a generic object pointer and C supports implicit conversion from a void pointer type to other types, so there is no need of explicitly typecasting it.

However, if you want the same code work perfectly compatible on a C++ platform, which does not support implicit conversion, you need to do the typecasting, so it all depends on usability.
    The casting of malloc is unnecessary in C but mandatory in C++.

Casting is unnecessary in C because of:


void * is automatically and safely promoted to any other pointer type in the case of C.
It can hide an error if you forgot to include <stdlib.h>. This can cause crashes.
If pointers and integers are differently sized, then you're hiding a warning by casting and might lose bits of your returned address.
If the type of the pointer is changed at its declaration, one may also need to change all lines where malloc is called and cast.


On the other hand, casting may increase the portability of your program. i.e, it allows a C program or function to compile as C++.
    People used to GCC and Clang are spoiled.  It's not all that good out there.

I have been pretty horrified over the years by the staggeringly aged compilers I've been required to use.  Often companies and managers adopt an ultra-conservative approach to changing compilers and will not even test if a new compiler ( with better standards compliance and code optimization ) will work in their system.  The practical reality for working developers is that when you're coding you need to cover your bases and, unfortunately, casting mallocs is a good habit if you cannot control what compiler may be applied to your code.

I would also suggest that many organizations apply a coding standard of their own and that that should be the method people follow if it is defined.  In the absence of explicit guidance I tend to go for most likely to compile everywhere, rather than slavish adherence to a standard.

The argument that it's not necessary under current standards is quite valid.  But that argument omits the practicalities of the real world.  We do not code in a world ruled exclusively by the standard of the day, but by the practicalities of what I like to call ""local management's reality field"".  And that's bent and twisted more than space time ever was. :-)

YMMV.

I tend to think of casting malloc as a defensive operation.  Not pretty, not perfect, but generally safe.  ( Honestly, if you've not included stdlib.h then you've way more problems than casting malloc ! ).
    No, you don't cast the result of malloc().

In general, you don't cast to or from void *.

A typical reason given for not doing so is that failure to #include <stdlib.h> could go unnoticed. This isn't an issue anymore for a long time now as C99 made implicit function declarations illegal, so if your compiler conforms to at least C99, you will get a diagnostic message.

But there's a much stronger reason not to introduce unnecessary pointer casts:

In C, a pointer cast is almost always an error. This is because of the following rule (6.5 p7 in N1570, the latest draft for C11):


  An object shall have its stored value accessed only by an lvalue expression that has one of
  the following types:
   a type compatible with the effective type of the object,
   a qualified version of a type compatible with the effective type of the object,
   a type that is the signed or unsigned type corresponding to the effective type of the
  object,
   a type that is the signed or unsigned type corresponding to a qualified version of the
  effective type of the object,
   an aggregate or union type that includes one of the aforementioned types among its
  members (including, recursively, a member of a subaggregate or contained union), or
   a character type.


This is also known as the strict aliasing rule. So the following code is undefined behavior:

long x = 5;
double *p = (double *)&x;
double y = *p;


And, sometimes surprisingly, the following is as well:

struct foo { int x; };
struct bar { int x; int y; };
struct bar b = { 1, 2};
struct foo *p = (struct foo *)&b;
int z = p->x;


Sometimes, you do need to cast pointers, but given the strict aliasing rule, you have to be very careful with it. So, any occurrence of a pointer cast in your code is a place you have to double-check for its validity. Therefore, you never write an unnecessary pointer cast.

tl;dr

In a nutshell: Because in C, any occurrence of a pointer cast should raise a red flag for code requiring special attention, you should never write unnecessary pointer casts.



Side notes:


There are cases where you actually need a cast to void *, e.g. if you want to print a pointer:

int x = 5;
printf(""%p\n"", (void *)&x);


The cast is necessary here, because printf() is a variadic function, so implicit conversions don't work.
In C++, the situation is different. Casting pointer types is somewhat common (and correct) when dealing with objects of derived classes. Therefore, it makes sense that in C++, the conversion to and from void * is not implicit. C++ has a whole set of different flavors of casting.

    In the C language, a void pointer can be assigned to any pointer, which is why you should not use a type cast. If you want ""type safe"" allocation, I can recommend the following macro functions, which I always use in my C projects:

#include <stdlib.h>
#define NEW_ARRAY(ptr, n) (ptr) = malloc((n) * sizeof *(ptr))
#define NEW(ptr) NEW_ARRAY((ptr), 1)


With these in place you can simply say

NEW_ARRAY(sieve, length);


For non-dynamic arrays, the third must-have function macro is

#define LEN(arr) (sizeof (arr) / sizeof (arr)[0])


which makes array loops safer and more convenient:

int i, a[100];

for (i = 0; i < LEN(a); i++) {
   ...
}

    The returned type is void*, which can be cast to the desired type of data pointer in order to be dereferenceable.
    The best thing to do when programming in C whenever it is possible:


Make your program compile through a C compiler with all warnings turned on -Wall and fix all errors and warnings
Make sure there are no variables declared as auto
Then compile it using a C++ compiler with -Wall and -std=c++11. Fix all errors and warnings.
Now compile using the C compiler again. Your program should now compile without any warning and contain fewer bugs.


This procedure lets you take advantage of C++ strict type checking, thus reducing the number of bugs. In particular, this procedure forces you to include stdlib.hor you will get


  malloc was not declared within this scope


and also forces you to cast the result of malloc or you will get


  invalid conversion from void* to T*


or what ever your target type is.

The only benefits from writing in C instead of C++ I can find are


C has a well specified ABI
C++ may generate more code [exceptions, RTTI, templates, runtime polymorphism]


Notice that the second cons should in the ideal case disappear when using the subset common to C together with the static polymorphic feature.

For those that finds C++ strict rules inconvenient, we can use the C++11 feature with inferred  type

auto memblock=static_cast<T*>(malloc(n*sizeof(T))); //Mult may overflow...

    I put in the cast simply to show disapproval of the ugly hole in the type system, which allows code such as the following snippet to compile without diagnostics, even though no casts are used to bring about the bad conversion:

double d;
void *p = &d;
int *q = p;


I wish that didn't exist (and it doesn't in C++) and so I cast. It represents my taste, and my programming politics. I'm not only casting a pointer, but effectively, casting a ballot, and casting out demons of stupidity. If I can't actually cast out stupidity, then at least let me express the wish to do so with a gesture of protest.

In fact, a good practice is to wrap malloc (and friends) with functions that return unsigned char *, and basically never to use void * in your code.  If you need a generic pointer-to-any-object, use a char * or unsigned char *, and have casts in both directions. The one relaxation that can be indulged, perhaps, is using functions like memset and memcpy without casts.

On the topic of casting and C++ compatibility, if you write your code so that it compiles as both C and C++ (in which case you have to cast the return value of malloc when assigning it to something other than void *), you can do a very helpful thing for yourself: you can use macros for casting which translate to C++ style casts when compiling as C++, but reduce to a C cast when compiling as C:

/* In a header somewhere */
#ifdef __cplusplus
#define strip_qual(TYPE, EXPR) (const_cast<TYPE>(EXPR))
#define convert(TYPE, EXPR) (static_cast<TYPE>(EXPR))
#define coerce(TYPE, EXPR) (reinterpret_cast<TYPE>(EXPR))
#else
#define strip_qual(TYPE, EXPR) ((TYPE) (EXPR))
#define convert(TYPE, EXPR) ((TYPE) (EXPR))
#define coerce(TYPE, EXPR) ((TYPE) (EXPR))
#endif


If you adhere to these macros, then a simple grep search of your code base for these identifiers will show you where all your casts are, so you can review whether any of them are incorrect.

Then, going forward, if you regularly compile the code with C++, it will enforce the use of an appropriate cast. For instance, if you use strip_qual just to remove a const or volatile, but the program changes in such a way that a type conversion is now involved, you will get a diagnostic, and you will have to use a combination of casts to get the desired conversion.

To help you adhere to these macros, the the GNU C++ (not C!) compiler has a beautiful feature: an optional diagnostic which is produced for all occurrences of C style casts.


     -Wold-style-cast (C++ and Objective-C++ only)
         Warn if an old-style (C-style) cast to a non-void type is used
         within a C++ program.  The new-style casts (dynamic_cast,
         static_cast, reinterpret_cast, and const_cast) are less vulnerable
         to unintended effects and much easier to search for.


If your C code compiles as C++, you can use this -Wold-style-cast option to find out all occurrences of the (type) casting syntax that may creep into the code, and follow up on these diagnostics by replacing it with an appropriate choice from among the above macros (or a combination, if necessary).

This treatment of conversions is the single largest standalone technical justification for working in a ""Clean C"": the combined C and C++ dialect, which in turn technically justifies casting the return value of malloc.
    I prefer to do the cast, but not manually. My favorite is using g_new and g_new0 macros from glib. If glib is not used, I would add similar macros. Those macros reduce code duplication without compromising type safety. If you get the type wrong, you would get an implicit cast between non-void pointers, which would cause a warning (error in C++). If you forget to include the header that defines g_new and g_new0, you would get an error. g_new and g_new0 both take the same arguments, unlike malloc that takes fewer arguments than calloc. Just add 0 to get zero-initialized memory. The code can be compiled with a C++ compiler without changes.
    Casting is only for C++ not C.In case you are using a C++ compiler you better change it to C compiler.
    The main issue with malloc is to get the right size.
The memory returned form malloc() is untyped, and it will not magically gain an effective type due to a simple cast.
I guess that both approaches are fine and the choice should depend on programmer intention.

If allocating memory for a type, then use a cast.

ptr = (T*)malloc(sizeof(T));

If allocating memory for a given pointer, then don't use a cast.

ptr = malloc(sizeof *ptr);
Ad 1
The first method assures the correct size by allocating memory for a given type, and then casting it to assure that it is assigned to the right pointer. If incorrect type of ptr is used then the compiler will issue a warning/error. If the type of ptr is changed, then the compiler will point the places where the code needs refactoring.
Moreover, the first method can be combined into a macro similar to new operator in C++.
#define NEW(T) ((T*)malloc(sizeof(T)))
...
ptr = NEW(T);

Moreover this method works if ptr is void*.
Ad 2
The second methods does not care about the types, it assures the correct size by taking it from the pointer's type. The main advantage of this method is the automatic adjustment of storage size whenever the type of ptr is changed.
It can save some time (or errors) when refactoring.
The disadvantage is that the method does not work if ptr is void* but it may be perceived as a good thing. And that it does not work with C++ so it should not be used in inlined functions in headers that are going to be used by C++ programs.
Personally, I prefer the second option.
    
As other stated, it is not needed for C, but for C++.
Including the cast may allow a C program or function to compile as C++.
In C it is unnecessary, as void * is automatically and safely promoted to any other pointer type.
But if you cast then, it can hide an error if you forgot to include
stdlib.h. This can cause crashes (or, worse, not cause a crash
until way later in some totally different part of the code).

Because stdlib.h contains the prototype for malloc is found. In the
absence of a prototype for malloc, the standard requires that the C
compiler assumes malloc returns an int. If there is no cast, a
warning is issued when this integer is assigned to the pointer;
however, with the cast, this warning is not produced, hiding a bug.

    A void pointer is a generic pointer and C supports implicit conversion from a void pointer type to other types, so there is no need of explicitly typecasting it.

However, if you want the same code work perfectly compatible on a C++ platform, which does not support implicit conversion, you need to do the typecasting, so it all depends on usability.
    The concept behind void pointer is that it can be casted to any data type that is why malloc returns void. Also you must be aware of automatic typecasting. So it is not mandatory to cast the pointer though you must do it. It helps in keeping the code clean and helps debugging
    For me, the take home and conclusion here is that casting malloc in C is totally NOT necessary but if you however cast, it wont affect malloc as malloc will still allocate to you your requested blessed memory space.
Another take home is the reason or one of the reasons people do casting and this is to enable them compile same program either in C or C++.

There may be other reasons but other reasons, almost certainly, would land you in serious trouble sooner or later.
    ","[2668, 2426, 442, 390, 163, 191, 21, 111, 102, 114, 41, 76, 58, 57, 32, 37, 12, 21, 18, 30, 32, 16, 17, 16, 14, 1, 9, 9, 10, 0]",305474,987,2009-03-03T10:13:02,2022-02-03 15:22:49Z,c 
How do I check if a string contains a specific word?,"
                    
            
        
            
                    
                        
                    
                
                    
                        This question's answers are a community effort. Edit existing answers to improve this post. It is not currently accepting new answers or interactions.
                        
                    
                
            
        

    

Consider:

$a = 'How are you?';

if ($a contains 'are')
    echo 'true';


Suppose I have the code above, what is the correct way to write the statement if ($a contains 'are')?
    Now with PHP 8 you can do this using str_contains:
if (str_contains('How are you', 'are')) { 
    echo 'true';
}

RFC
Before PHP 8
You can use the strpos() function which is used to find the occurrence of one string inside another one:
$a = 'How are you?';

if (strpos($a, 'are') !== false) {
    echo 'true';
}

Note that the use of !== false is deliberate (neither != false nor === true will return the desired result); strpos() returns either the offset at which the needle string begins in the haystack string, or the boolean false if the needle isn't found. Since 0 is a valid offset and 0 is ""falsey"", we can't use simpler constructs like !strpos($a, 'are').
    You could use regular expressions as it's better for word matching compared to strpos, as mentioned by other users. A strpos check for are will also return true for strings such as: fare, care, stare, etc. These unintended matches can simply be avoided in regular expression by using word boundaries.
A simple match for are could look something like this:
$a = 'How are you?';

if (preg_match('/\bare\b/', $a)) {
    echo 'true';
}

On the performance side, strpos is about three times faster. When I did one million compares at once, it took preg_match 1.5 seconds to finish and for strpos it took 0.5 seconds.
Edit:
In order to search any part of the string, not just word by word, I would recommend using a regular expression like
$a = 'How are you?';
$search = 'are y';
if(preg_match(""/{$search}/i"", $a)) {
    echo 'true';
}

The i at the end of regular expression changes regular expression to be case-insensitive, if you do not want that, you can leave it out.
Now, this can be quite problematic in some cases as the $search string isn't sanitized in any way, I mean, it might not pass the check in some cases as if $search is a user input they can add some string that might behave like some different regular expression...
Also, here's a great tool for testing and seeing explanations of various regular expressions Regex101
To combine both sets of functionality into a single multi-purpose function (including with selectable case sensitivity), you could use something like this:
function FindString($needle,$haystack,$i,$word)
{   // $i should be """" or ""i"" for case insensitive
    if (strtoupper($word)==""W"")
    {   // if $word is ""W"" then word search instead of string in string search.
        if (preg_match(""/\b{$needle}\b/{$i}"", $haystack)) 
        {
            return true;
        }
    }
    else
    {
        if(preg_match(""/{$needle}/{$i}"", $haystack)) 
        {
            return true;
        }
    }
    return false;
    // Put quotes around true and false above to return them as strings instead of as bools/ints.
}

One more thing to take in mind, is that \b will not work in different languages other than english.
The explanation for this and the solution is taken from here:

\b represents the beginning or end of a word (Word Boundary). This
regex would match apple in an apple pie, but wouldnt match apple in
pineapple, applecarts or bakeapples.
How about caf? How can we extract the word caf in regex?
Actually, \bcaf\b wouldnt work. Why? Because caf contains
non-ASCII character: . \b cant be simply used with Unicode such as
, ,  and  .
When you want to extract Unicode characters, you should directly
define characters which represent word boundaries.
The answer: (?<=[\s,.:;""']|^)UNICODE_WORD(?=[\s,.:;""']|$)

So in order to use the answer in PHP, you can use this function:
function contains($str, array $arr) {
    // Works in Hebrew and any other unicode characters
    // Thanks https://medium.com/@shiba1014/regex-word-boundaries-with-unicode-207794f6e7ed
    // Thanks https://www.phpliveregex.com/
    if (preg_match('/(?<=[\s,.:;""\']|^)' . $word . '(?=[\s,.:;""\']|$)/', $str)) return true;
}

And if you want to search for array of words, you can use this:
function arrayContainsWord($str, array $arr)
{
    foreach ($arr as $word) {
        // Works in Hebrew and any other unicode characters
        // Thanks https://medium.com/@shiba1014/regex-word-boundaries-with-unicode-207794f6e7ed
        // Thanks https://www.phpliveregex.com/
        if (preg_match('/(?<=[\s,.:;""\']|^)' . $word . '(?=[\s,.:;""\']|$)/', $str)) return true;
    }
    return false;
}

As of PHP 8.0.0 you can now use str_contains
<?php
    if (str_contains('abc', '')) {
        echo ""Checking the existence of the empty string will always 
        return true"";
    }

    To determine whether a string contains another string you can use the PHP function strpos().
int strpos ( string $haystack , mixed $needle [, int $offset = 0 ] )`

<?php

$haystack = 'how are you';
$needle = 'are';

if (strpos($haystack,$needle) !== false) {
    echo ""$haystack contains $needle"";
}

?>

CAUTION:
If the needle you are searching for is at the beginning of the haystack it will return position 0, if you do a == compare that will not work, you will need to do a ===
A == sign is a comparison and tests whether the variable / expression / constant to the left has the same value as the variable / expression / constant to the right.
A === sign is a comparison to see whether two variables / expresions / constants are equal AND have the same type - i.e. both are strings or both are integers.
    Here is a little utility function that is useful in situations like this

// returns true if $needle is a substring of $haystack
function contains($needle, $haystack)
{
    return strpos($haystack, $needle) !== false;
}

    While most of these answers will tell you if a substring appears in your string, that's usually not what you want if you're looking for a particular word, and not a substring.

What's the difference?  Substrings can appear within other words:


The ""are"" at the beginning of ""area""
The ""are"" at the end of ""hare""
The ""are"" in the middle of ""fares""


One way to mitigate this would be to use a regular expression coupled with word boundaries (\b):

function containsWord($str, $word)
{
    return !!preg_match('#\\b' . preg_quote($word, '#') . '\\b#i', $str);
}


This method doesn't have the same false positives noted above, but it does have some edge cases of its own.  Word boundaries match on non-word characters (\W), which are going to be anything that isn't a-z, A-Z, 0-9, or _.  That means digits and underscores are going to be counted as word characters and scenarios like this will fail:


The ""are"" in ""What _are_ you thinking?""
The ""are"" in ""lol u dunno wut those are4?""


If you want anything more accurate than this, you'll have to start doing English language syntax parsing, and that's a pretty big can of worms (and assumes proper use of syntax, anyway, which isn't always a given).
    Peer to SamGoody and Lego Stormtroopr comments.

If you are looking for a PHP algorithm to rank search results based on proximity/relevance of multiple words
here comes a quick and easy way of generating search results with PHP only:

Issues with the other boolean search methods such as strpos(), preg_match(), strstr() or stristr() 


can't search for multiple words
results are unranked 


PHP method based on Vector Space Model and tf-idf (term frequencyinverse document frequency):

It sounds difficult but is surprisingly easy.

If we want to search for multiple words in a string the core problem is how we assign a weight to each one of them?

If we could weight the terms in a string based on how representative they are of the string as a whole, 
we could order our results by the ones that best match the query. 

This is the idea of the vector space model, not far from how SQL full-text search works:

function get_corpus_index($corpus = array(), $separator=' ') {

    $dictionary = array();

    $doc_count = array();

    foreach($corpus as $doc_id => $doc) {

        $terms = explode($separator, $doc);

        $doc_count[$doc_id] = count($terms);

        // tfidf, short for term frequencyinverse document frequency, 
        // according to wikipedia is a numerical statistic that is intended to reflect 
        // how important a word is to a document in a corpus

        foreach($terms as $term) {

            if(!isset($dictionary[$term])) {

                $dictionary[$term] = array('document_frequency' => 0, 'postings' => array());
            }
            if(!isset($dictionary[$term]['postings'][$doc_id])) {

                $dictionary[$term]['document_frequency']++;

                $dictionary[$term]['postings'][$doc_id] = array('term_frequency' => 0);
            }

            $dictionary[$term]['postings'][$doc_id]['term_frequency']++;
        }

        //from http://phpir.com/simple-search-the-vector-space-model/

    }

    return array('doc_count' => $doc_count, 'dictionary' => $dictionary);
}

function get_similar_documents($query='', $corpus=array(), $separator=' '){

    $similar_documents=array();

    if($query!=''&&!empty($corpus)){

        $words=explode($separator,$query);

        $corpus=get_corpus_index($corpus, $separator);

        $doc_count=count($corpus['doc_count']);

        foreach($words as $word) {

            if(isset($corpus['dictionary'][$word])){

                $entry = $corpus['dictionary'][$word];


                foreach($entry['postings'] as $doc_id => $posting) {

                    //get term frequencyinverse document frequency
                    $score=$posting['term_frequency'] * log($doc_count + 1 / $entry['document_frequency'] + 1, 2);

                    if(isset($similar_documents[$doc_id])){

                        $similar_documents[$doc_id]+=$score;

                    }
                    else{

                        $similar_documents[$doc_id]=$score;

                    }
                }
            }
        }

        // length normalise
        foreach($similar_documents as $doc_id => $score) {

            $similar_documents[$doc_id] = $score/$corpus['doc_count'][$doc_id];

        }

        // sort from  high to low

        arsort($similar_documents);

    }   

    return $similar_documents;
}


CASE 1

$query = 'are';

$corpus = array(
    1 => 'How are you?',
);

$match_results=get_similar_documents($query,$corpus);
echo '<pre>';
    print_r($match_results);
echo '</pre>';


RESULT

Array
(
    [1] => 0.52832083357372
)


CASE 2

$query = 'are';

$corpus = array(
    1 => 'how are you today?',
    2 => 'how do you do',
    3 => 'here you are! how are you? Are we done yet?'
);

$match_results=get_similar_documents($query,$corpus);
echo '<pre>';
    print_r($match_results);
echo '</pre>';


RESULTS

Array
(
    [1] => 0.54248125036058
    [3] => 0.21699250014423
)


CASE 3

$query = 'we are done';

$corpus = array(
    1 => 'how are you today?',
    2 => 'how do you do',
    3 => 'here you are! how are you? Are we done yet?'
);

$match_results=get_similar_documents($query,$corpus);
echo '<pre>';
    print_r($match_results);
echo '</pre>';


RESULTS

Array
(
    [3] => 0.6813781191217
    [1] => 0.54248125036058
)


There are plenty of improvements to be made
but the model provides a way of getting good results from natural queries, 
which don't have boolean operators such as strpos(), preg_match(), strstr() or stristr().

NOTA BENE

Optionally eliminating redundancy prior to search the words


thereby reducing index size and resulting in less storage requirement
less disk I/O
faster indexing and a consequently faster search.


1. Normalisation


Convert all text to lower case 


2. Stopword elimination


Eliminate words from the text which carry no real meaning (like 'and', 'or', 'the', 'for', etc.)


3. Dictionary substitution


Replace words with others which have an identical or similar meaning. 
(ex:replace instances of 'hungrily' and 'hungry' with 'hunger')
Further algorithmic measures (snowball) may be performed to further reduce words to their essential meaning. 
The replacement of colour names with their hexadecimal equivalents 
The reduction of numeric values by reducing precision are other ways of normalising the text.


RESOURCES 


http://linuxgazette.net/164/sephton.html
http://snowball.tartarus.org/
MySQL Fulltext Search Score Explained
http://dev.mysql.com/doc/internals/en/full-text-search.html
http://en.wikipedia.org/wiki/Vector_space_model
http://en.wikipedia.org/wiki/Tf%E2%80%93idf
http://phpir.com/simple-search-the-vector-space-model/

    Look at strpos():

<?php
    $mystring = 'abc';
    $findme   = 'a';
    $pos = strpos($mystring, $findme);

    // Note our use of ===. Simply, == would not work as expected
    // because the position of 'a' was the 0th (first) character.
    if ($pos === false) {
        echo ""The string '$findme' was not found in the string '$mystring'."";
    }
    else {
        echo ""The string '$findme' was found in the string '$mystring',"";
        echo "" and exists at position $pos."";
    }
?>

    if (preg_match('/(are)/', $a)) {
   echo 'true';
}

    Make use of case-insensitve matching using stripos():

if (stripos($string,$stringToSearch) !== false) {
    echo 'true';
}

    Lot of answers that use substr_count checks if the result is >0. But since the if statement considers zero the same as false, you can avoid that check and write directly:

if (substr_count($a, 'are')) {


To check if not present, add the ! operator: 

if (!substr_count($a, 'are')) {

    In PHP, the best way to verify if a string contains a certain substring, is to use a simple helper function like this:

function contains($haystack, $needle, $caseSensitive = false) {
    return $caseSensitive ?
            (strpos($haystack, $needle) === FALSE ? FALSE : TRUE):
            (stripos($haystack, $needle) === FALSE ? FALSE : TRUE);
}


Explanation:


strpos finds the position of the first occurrence of a case-sensitive substring in a string.
stripos finds the position of the first occurrence of a case-insensitive substring in a string.
myFunction($haystack, $needle) === FALSE ? FALSE : TRUE ensures that myFunction always returns a boolean and fixes unexpected behavior when the index of the substring is 0.
$caseSensitive ? A : B selects either strpos or stripos to do the work, depending on the value of $caseSensitive.


Output:

var_dump(contains('bare','are'));            // Outputs: bool(true)
var_dump(contains('stare', 'are'));          // Outputs: bool(true)
var_dump(contains('stare', 'Are'));          // Outputs: bool(true)
var_dump(contains('stare', 'Are', true));    // Outputs: bool(false)
var_dump(contains('hair', 'are'));           // Outputs: bool(false)
var_dump(contains('aren\'t', 'are'));        // Outputs: bool(true)
var_dump(contains('Aren\'t', 'are'));        // Outputs: bool(true)
var_dump(contains('Aren\'t', 'are', true));  // Outputs: bool(false)
var_dump(contains('aren\'t', 'Are'));        // Outputs: bool(true)
var_dump(contains('aren\'t', 'Are', true));  // Outputs: bool(false)
var_dump(contains('broad', 'are'));          // Outputs: bool(false)
var_dump(contains('border', 'are'));         // Outputs: bool(false)

    A string can be checked with the below function:
function either_String_existor_not($str, $character) {
    return strpos($str, $character) !== false;
}

    You can use the strstr function:

$haystack = ""I know programming"";
$needle   = ""know"";
$flag = strstr($haystack, $needle);

if ($flag){

    echo ""true"";
}


Without using an inbuilt function:

$haystack  = ""hello world"";
$needle = ""llo"";

$i = $j = 0;

while (isset($needle[$i])) {
    while (isset($haystack[$j]) && ($needle[$i] != $haystack[$j])) {
        $j++;
        $i = 0;
    }
    if (!isset($haystack[$j])) {
        break;
    }
    $i++;
    $j++;

}
if (!isset($needle[$i])) {
    echo ""YES"";
}
else{
    echo ""NO "";
}

    Using strstr() or stristr() if your search should be case insensitive would be another option.
    I'm a bit impressed that none of the answers here that used strpos, strstr and similar functions mentioned Multibyte String Functions yet (2015-05-08).

Basically, if you're having trouble finding words with characters specific to some languages, such as German, French, Portuguese, Spanish, etc. (e.g.: , , , , , ), you may want to precede the functions with mb_. Therefore, the accepted answer would use mb_strpos or mb_stripos (for case-insensitive matching) instead:

if (mb_strpos($a,'are') !== false) {
    echo 'true';
}


If you cannot guarantee that all your data is 100% in UTF-8, you may want to use the mb_ functions.

A good article to understand why is The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky.
    Another option to finding the occurrence of a word from a string using strstr() and stristr() is like the following:

<?php
    $a = 'How are you?';
    if (strstr($a,'are'))  // Case sensitive
        echo 'true';
    if (stristr($a,'are'))  // Case insensitive
        echo 'true';
?>

    If you want to avoid the ""falsey"" and ""truthy"" problem, you can use substr_count:

if (substr_count($a, 'are') > 0) {
    echo ""at least one 'are' is present!"";
}


It's a bit slower than strpos but it avoids the comparison problems.
    Do not use preg_match() if you only want to check if one string is contained in another string. Use strpos() or strstr() instead as they will be faster. (http://in2.php.net/preg_match)

if (strpos($text, 'string_name') !== false){
   echo 'get the string';
}

    I had some trouble with this, and finally I chose to create my own solution. Without using regular expression engine:

function contains($text, $word)
{
    $found = false;
    $spaceArray = explode(' ', $text);

    $nonBreakingSpaceArray = explode(chr(160), $text);

    if (in_array($word, $spaceArray) ||
        in_array($word, $nonBreakingSpaceArray)
       ) {

        $found = true;
    }
    return $found;
 }


You may notice that the previous solutions are not an answer for the word being used as a prefix for another. In order to use your example:

$a = 'How are you?';
$b = ""a skirt that flares from the waist"";
$c = ""are"";


With the samples above, both $a and $b contains $c, but you may want your function to tell you that only $a contains $c.
    The function below also works and does not depend on any other function; it uses only native PHP string manipulation. Personally, I do not recommend this, but you can see how it works:

<?php

if (!function_exists('is_str_contain')) {
  function is_str_contain($string, $keyword)
  {
    if (empty($string) || empty($keyword)) return false;
    $keyword_first_char = $keyword[0];
    $keyword_length = strlen($keyword);
    $string_length = strlen($string);

    // case 1
    if ($string_length < $keyword_length) return false;

    // case 2
    if ($string_length == $keyword_length) {
      if ($string == $keyword) return true;
      else return false;
    }

    // case 3
    if ($keyword_length == 1) {
      for ($i = 0; $i < $string_length; $i++) {

        // Check if keyword's first char == string's first char
        if ($keyword_first_char == $string[$i]) {
          return true;
        }
      }
    }

    // case 4
    if ($keyword_length > 1) {
      for ($i = 0; $i < $string_length; $i++) {
        /*
        the remaining part of the string is equal or greater than the keyword
        */
        if (($string_length + 1 - $i) >= $keyword_length) {

          // Check if keyword's first char == string's first char
          if ($keyword_first_char == $string[$i]) {
            $match = 1;
            for ($j = 1; $j < $keyword_length; $j++) {
              if (($i + $j < $string_length) && $keyword[$j] == $string[$i + $j]) {
                $match++;
              }
              else {
                return false;
              }
            }

            if ($match == $keyword_length) {
              return true;
            }

            // end if first match found
          }

          // end if remaining part
        }
        else {
          return false;
        }

        // end for loop
      }

      // end case4
    }

    return false;
  }
}


Test:

var_dump(is_str_contain(""test"", ""t"")); //true
var_dump(is_str_contain(""test"", """")); //false
var_dump(is_str_contain(""test"", ""test"")); //true
var_dump(is_str_contain(""test"", ""testa"")); //flase
var_dump(is_str_contain(""a----z"", ""a"")); //true
var_dump(is_str_contain(""a----z"", ""z"")); //true 
var_dump(is_str_contain(""mystringss"", ""strings"")); //true 

    It can be done in three different ways:

 $a = 'How are you?';


1- stristr()

 if (strlen(stristr($a,""are""))>0) {
    echo ""true""; // are Found
 } 


2- strpos()

 if (strpos($a, ""are"") !== false) {
   echo ""true""; // are Found
 }


3- preg_match()

 if( preg_match(""are"",$a) === 1) {
   echo ""true""; // are Found
 }

    Another solution for a specific string:

$subject = 'How are you?';
$pattern = '/are/';
preg_match($pattern, $subject, $match);
if ($match[0] == 'are') {
    echo true;
}


You can also use strpos() function.
    You can also use built-in functions strchr() and strrchr() and extensions for multibyte strings mb_strchr() and mb_strrchr(). 
These functions return parts of strings, and FALSE if nothing is found.


strchr() - Find the first occurrence of a string (is an alias of strstr()). 
strrchr() - Find the last occurrence of a character in a string. 

    The strpos function works fine, but if you want to do case-insensitive checking for a word in a paragraph then you can make use of the stripos function of PHP.

For example,

$result = stripos(""I love PHP, I love PHP too!"", ""php"");
if ($result === false) {
    // Word does not exist
}
else {
    // Word exists
}


Find the position of the first occurrence of a case-insensitive substring in a string.

If the word doesn't exist in the string then it will return false else it will return the position of the word.
    Another option is to use the strstr() function. Something like:

if (strlen(strstr($haystack,$needle))>0) {
// Needle Found
}


Point to note: The strstr() function is case-sensitive. For a case-insensitive search, use the stristr() function.
    The short-hand version

$result = false!==strpos($a, 'are');

    Use:

$text = 'This is a test';
echo substr_count($text, 'is'); // 2

// So if you want to check if is exists in the text just put
// in a condition like this:
if (substr_count($text, 'is') > 0) {
    echo ""is exists"";
}

    If you want to check if the string contains several specifics words, you can do:

$badWords = array(""dette"", ""capitale"", ""rembourser"", ""ivoire"", ""mandat"");

$string = ""a string with the word ivoire"";

$matchFound = preg_match_all(""/\b("" . implode($badWords,""|"") . "")\b/i"", $string, $matches);

if ($matchFound) {
    echo ""a bad word has been found"";
}
else {
    echo ""your string is okay"";
}


This is useful to avoid spam when sending emails for example.
    I think that a good idea is to use mb_stpos:

$haystack = 'How are you?';
$needle = 'are';

if (mb_strpos($haystack, $needle) !== false) {

    echo 'true';
}


Because this solution is case sensitive and safe for all Unicode characters.



But you can also do it like this (sauch response was not yet):

if (count(explode($needle, $haystack)) > 1) {

    echo 'true';
}


This solution is also case sensitive and safe for Unicode characters.

In addition you do not use the negation in the expression, which increases the readability of the code.



Here is other solution using function:

function isContainsStr($haystack, $needle) {

    return count(explode($needle, $haystack)) > 1;
}

if (isContainsStr($haystack, $needle)) {

    echo 'true';
}

    Maybe you could use something like this:

<?php
    findWord('Test all OK');

    function findWord($text) {
        if (strstr($text, 'ok')) {
            echo 'Found a word';
        }
        else
        {
            echo 'Did not find a word';
        }
    }
?>

    ","[2659, 7745, 726, 168, 289, 162, 56, 75, 38, 51, 24, 30, 12, 27, 66, 30, 21, 43, 15, 23, 25, 19, 9, 8, 12, 32, 16, 9, 13, 7, 14]",6089445,690,2010-12-06T13:14:05,2022-04-01 01:39:43Z,php 
How can I refresh a page with jQuery?,"
                
How can I refresh a page with jQuery?
    Use location.reload():

$('#something').click(function() {
    location.reload();
});


The reload() function takes an optional parameter that can be set to true to force a reload from the server rather than the cache. The parameter defaults to false, so by default the page may reload from the browser's cache.
    There are multiple unlimited ways to refresh a page with JavaScript:


location.reload()
history.go(0)
location.href = location.href
location.href = location.pathname
location.replace(location.pathname)
location.reload(false) 


  If we needed to pull the document from
   the web-server again (such as where the document contents
   change dynamically) we would pass the argument as true.



You can continue the list being creative:


window.location = window.location
window.self.window.self.window.window.location = window.location
...and other 534 ways


var methods = [
  ""location.reload()"",
  ""history.go(0)"",
  ""location.href = location.href"",
  ""location.href = location.pathname"",
  ""location.replace(location.pathname)"",
  ""location.reload(false)""
];

var $body = $(""body"");
for (var i = 0; i < methods.length; ++i) {
  (function(cMethod) {
    $body.append($(""<button>"", {
      text: cMethod
    }).on(""click"", function() {
      eval(cMethod); // don't blame me for using eval
    }));
  })(methods[i]);
}button {
  background: #2ecc71;
  border: 0;
  color: white;
  font-weight: bold;
  font-family: ""Monaco"", monospace;
  padding: 10px;
  border-radius: 4px;
  cursor: pointer;
  transition: background-color 0.5s ease;
  margin: 2px;
}
button:hover {
  background: #27ae60;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>

    This should work on all browsers even without jQuery:

location.reload();

    Lots of ways will work, I suppose:


window.location.reload();
history.go(0);
window.location.href=window.location.href;

    <i id=""refresh"" class=""fa fa-refresh"" aria-hidden=""true""></i>

<script>
$(document).on('click','#refresh',function(){
   location.reload(true);
});
</script>

    Three approaches with different cache-related behaviours:

location.reload(true)
In browsers that implement the forcedReload parameter of location.reload(), reloads by fetching a fresh copy of the page and all of its resources (scripts, stylesheets, images, etc.). Will not serve any resources from the cache - gets fresh copies from the server without sending any if-modified-since or if-none-match headers in the request.
Equivalent to the user doing a ""hard reload"" in browsers where that's possible.
Note that passing true to location.reload() is supported in Firefox (see MDN) and Internet Explorer (see MSDN) but is not supported universally and is not part of the W3 HTML 5 spec, nor the W3 draft HTML 5.1 spec, nor the WHATWG HTML Living Standard.
In unsupporting browsers, like Google Chrome, location.reload(true) behaves the same as location.reload().

location.reload() or location.reload(false)
Reloads the page, fetching a fresh, non-cached copy of the page HTML itself, and performing RFC 7234 revalidation requests for any resources (like scripts) that the browser has cached, even if they are fresh are RFC 7234 permits the browser to serve them without revalidation.
Exactly how the browser should utilise its cache when performing a location.reload() call isn't specified or documented as far as I can tell; I determined the behaviour above by experimentation.
This is equivalent to the user simply pressing the ""refresh"" button in their browser.

location = location (or infinitely many other possible techniques that involve assigning to location or to its properties)
Only works if the page's URL doesn't contain a fragid/hashbang!
Reloads the page without refetching or revalidating any fresh resources from the cache. If the page's HTML itself is fresh, this will reload the page without performing any HTTP requests at all.
This is equivalent (from a caching perspective) to the user opening the page in a new tab.
However, if the page's URL contains a hash, this will have no effect.
Again, the caching behaviour here is unspecified as far as I know; I determined it by testing.


So, in summary, you want to use:

location = location for maximum use of the cache, as long as the page doesn't have a hash in its URL, in which case this won't work
location.reload(true) to fetch new copies of all resources without revalidating (although it's not universally supported and will behave no differently to location.reload() in some browsers, like Chrome)
location.reload() to faithfully reproduce the effect of the user clicking the 'refresh' button.

    If the current page was loaded by a POST request, you may want to use

window.location = window.location.pathname;


instead of

window.location.reload();


because window.location.reload() will prompt for confirmation if called on a page that was loaded by a POST request.
    The question should be,

How to refresh a page with JavaScript

window.location.href = window.location.href; //This is a possibility
window.location.reload(); //Another possiblity
history.go(0); //And another


You're spoiled for choice.
    you may need to use
 location.reload()

or also may need to use
location.reload(forceGet)

forceGet is a boolean and optional.
Set this parameter to true if you want to force the browser to take the page from the server to receive rid of the cache as well
    You may want to use

location.reload(forceGet)


forceGet is a boolean and optional.

The default is false which reloads the page from the cache.

Set this parameter to true if you want to force the browser to get the page from the server to get rid of the cache as well.

Or just

location.reload()


if you want quick and easy with caching.
    This works for me.
function reload(){
    location.reload(true);
}

    As the question is generic, let's try to sum up possible solutions for the answer:

Simple plain JavaScript Solution:

The easiest way is a one line solution placed in an appropriate way:

location.reload();


What many people are missing here, because they hope to get some ""points"" is that the reload() function itself offers a Boolean as a parameter (details: https://developer.mozilla.org/en-US/docs/Web/API/Location/reload).


  The Location.reload() method reloads the resource from the current
  URL. Its optional unique parameter is a Boolean, which, when it is
  true, causes the page to always be reloaded from the server. If it is
  false or not specified, the browser may reload the page from its
  cache.


This means there are two ways:

Solution1: Force reloading the current page from the server

location.reload(true);


Solution2: Reloading from cache or server (based on browser and your config)

location.reload(false);
location.reload();


And if you want to combine it with jQuery an listening to an event, I would recommend using the "".on()"" method instead of "".click"" or other event wrappers, e.g. a more proper solution would be:

$('#reloadIt').on('eventXyZ', function() {
    location.reload(true);
});

    You can write it in two ways. 1st is the standard way of reloading the page also called as simple refresh  

location.reload(); //simple refresh


And another is called the hard refresh. Here you pass the boolean expression and set it to true. This will reload the page destroying the older cache and displaying the contents from scratch.

location.reload(true);//hard refresh

    use 

location.reload();


or

window.location.reload();

    window.location.reload() will reload from the server and will load all your data, scripts, images, etc. again.

So if you just want to refresh the HTML, the window.location = document.URL will return much quicker and with less traffic. But it will not reload the page if there is a hash (#) in the URL.
    You can use  JavaScript  location.reload()  method.
This method accepts a boolean parameter. true or false. If the parameter is  true; the page always reloaded from the server. If it is false; which  is  the default or with empty parameter browser reload the page from it's cache.

With true parameter

<button type=""button"" onclick=""location.reload(true);"">Reload page</button>


With default/ false parameter

 <button type=""button"" onclick=""location.reload();"">Reload page</button>


Using jquery

<button id=""Reloadpage"">Reload page</button>
<script type=""text/javascript"">
    $('#Reloadpage').click(function() {
        location.reload();
    }); 
</script>

    I found

window.location.href = """";


or

window.location.href = null;


also makes a page refresh.

This makes it very much easier to reload the page removing any hash.
This is very nice when I am using AngularJS in the iOS simulator, so that I don't have to rerun the app.
    You don't need anything from jQuery, to reload a page using pure JavaScript, just use reload function on location property like this:

window.location.reload();


By default, this will reload the page using the browser cache (if exists)...

If you'd like to do force reload the page, just pass a true value to reload method like below...

window.location.reload(true);


Also if you are already in window scope, you can get rid of window and do:

location.reload();

    To reload a page with jQuery, do:

$.ajax({
    url: """",
    context: document.body,
    success: function(s,x){
        $(this).html(s);
    }
});


The approach here that I used was Ajax jQuery. I tested it on Chrome 13. Then I put the code in the handler that will trigger the reload. The URL is """", which means this page.
    Here is a solution that asynchronously reloads a page using jQuery. It avoids the flicker caused by window.location = window.location. This example shows a page that reloads continuously, as in a dashboard. It is battle-tested and is running on an information display TV in Times Square.

<!DOCTYPE html>
<html lang=""en"">
  <head>
    ...
    <meta http-equiv=""refresh"" content=""300"">
    <script src=""//ajax.googleapis.com/ajax/libs/jquery/2.2.3/jquery.min.js""></script>
    <script>
    function refresh() {
      $.ajax({
        url: """",
        dataType: ""text"",
        success: function(html) {
          $('#fu').replaceWith($.parseHTML(html));
          setTimeout(refresh,2000);
        }
      });
    }
    refresh();
    </script>
  </head>
  <body>
    <div id=""fu"">
      ...
    </div>
  </body>
</html>


Notes:


Using $.ajax directly like $.get('',function(data){$(document.body).html(data)}) causes css/js files to get cache-busted, even if you use cache: true, that's why we use parseHTML
parseHTML will NOT find a body tag so your whole body needs to go in an extra div, I hope this nugget of knowledge helps you one day, you can guess how we chose the id for that div
Use http-equiv=""refresh"" just in case something goes wrong with javascript/server hiccup, then the page will STILL reload without you getting a phone call
This approach probably leaks memory somehow, the http-equiv refresh fixes that

    There are many ways to reload the current pages, but somehow using those approaches you can see page updated but not with few cache values will be there, so overcome that issue or if you wish to make hard requests then use the below code.

    location.reload(true);
    //Here, it will make a hard request or reload the current page and clear the cache as well.


    location.reload(false); OR location.reload();
    //It can be reload the page with cache

    The jQuery Load function can also perform a page refresh:

$('body').load('views/file.html', function () {
    $(this).fadeIn(5000);
});

    $(document).on(""click"", ""#refresh_btn"", function(event) 
{
     window.location.replace(window.location.href);
});

    Use onclick=""return location.reload();"" within the button tag.

<button id=""refersh-page"" name=""refersh-page"" type=""button"" onclick=""return location.reload();"">Refesh Page</button>

    All the answers here are good. Since the question specifies about reloading the page with jquery, I just thought adding something more for future readers.

jQuery is a cross-platform JavaScript library designed to simplify the client-side scripting of HTML.
~ Wikipedia ~

So you'll understand that the foundation of jquery, or jquery is based on javascript. So going with pure javascript is way better when it comes to simple things.
But if you need a jquery solution, here's one.
$(location).attr('href', '');

    If you are using jQuery and want to refresh, then try adding your jQuery in a javascript function:

I wanted to hide an iframe from a page when clicking oh an h3, for me it worked but I wasn't able to click the item that allowed me to view the iframe to begin with unless I refreshed the browser manually...not ideal.

I tried the following:

var hide = () => {
    $(""#frame"").hide();//jQuery
    location.reload(true);//javascript
};


Mixing plain Jane javascript with your jQuery should work.

// code where hide (where location.reload was used)function was integrated, below    
    iFrameInsert = () => {
        var file = `Fe1FVoW0Nt4`;
        $(""#frame"").html(`<iframe width=\""560\"" height=\""315\"" src=\""https://www.youtube.com/embed/${file}\"" frameborder=\""0\"" allow=\""autoplay; encrypted-media\"" allowfullscreen></iframe><h3>Close Player</h3>`);
        $(""h3"").enter code hereclick(hide);
}

// View Player 
$(""#id-to-be-clicked"").click(iFrameInsert);

    Probably shortest (12 chars) - use history

history.go()

    It is shortest in JavaScript.

window.location = '';

    Simple Javascript Solution:

 location = location; 


<button onClick=""location = location;"">Reload</button>

    Here are some lines of code you can use to reload the page using jQuery.

It uses the jQuery wrapper and extracts the native dom element. 

Use this if you just want a jQuery feeling on your code and you don't care about speed/performance of the code.

Just pick from 1 to 10 that suits your needs or add some more based on the pattern and answers before this.

<script>
  $(location)[0].reload(); //1
  $(location).get(0).reload(); //2
  $(window)[0].location.reload(); //3
  $(window).get(0).location.reload(); //4
  $(window)[0].$(location)[0].reload(); //5
  $(window).get(0).$(location)[0].reload(); //6
  $(window)[0].$(location).get(0).reload(); //7
  $(window).get(0).$(location).get(0).reload(); //8
  $(location)[0].href = ''; //9
  $(location).get(0).href = ''; //10
  //... and many other more just follow the pattern.
</script>

    ","[2657, 4087, 496, 472, 211, 7, 44, 111, 66, 3, 56, 2, 17, 3, 8, 26, 9, 14, 8, 122, 15, 3, 17, 1, 3, 3, 3, 1, 0, 1, -1]",2879281,368,2011-03-23T11:55:31,2021-11-15 09:44:21Z,javascript 
Is there an equivalent of 'which' on the Windows command line?,"
                
As I sometimes have path problems, where one of my own cmd scripts is hidden (shadowed) by another program (earlier on the path), I would like to be able to find the full path to a program on the Windows command line, given just its name.

Is there an equivalent to the UNIX command 'which'?

On UNIX, which command prints the full path of the given command to easily find and repair these shadowing problems.
    Windows Server 2003 and later (i.e. anything after Windows XP 32 bit) provide the where.exe program which does some of what which does, though it matches all types of files, not just executable commands.  (It does not match built-in shell commands like cd.)  It will even accept wildcards, so where nt* finds all files in your %PATH% and current directory whose names start with nt.
Try where /? for help.
Note that Windows PowerShell defines where as an alias for the Where-Object cmdlet, so if you want where.exe, you need to type the full name instead of omitting the .exe extension. Alternatively, you can set an alias for it:
Set-Alias which where.exe

Update: Using Get-Command (alias: gcm) is recommended since it's native to PS and will get all command types: aliases, cmdlets, executables, and functions. Example:
gcm notepad*

    Under PowerShell, Get-Command will find executables anywhere in $Env:PATH.
$ Get-Command eventvwr

CommandType   Name          Definition
-----------   ----          ----------
Application   eventvwr.exe  c:\windows\system32\eventvwr.exe
Application   eventvwr.msc  c:\windows\system32\eventvwr.msc

And since powershell let's you define aliases, which can be defined like so.
$ sal which gcm   # short form of `Set-Alias which Get-Command`
$ which foo
...

PowerShell commands are not just executable files (.exe, .ps1, etc). They can also be cmdlets, functions, aliases, custom executable suffixes set in $Env:PATHEXT, etc. Get-Command is able to find and list all of these commands (quite akin to Bash's type -a foo). This alone makes it better than where.exe, which.exe, etc which are typically limited to finding just executables.
Finding executables using only part of the name
$ gcm *disk*

CommandType     Name                             Version    Source
-----------     ----                             -------    ------
Alias           Disable-PhysicalDiskIndication   2.0.0.0    Storage
Alias           Enable-PhysicalDiskIndication    2.0.0.0    Storage
Function        Add-PhysicalDisk                 2.0.0.0    Storage
Function        Add-VirtualDiskToMaskingSet      2.0.0.0    Storage
Function        Clear-Disk                       2.0.0.0    Storage
Cmdlet          Get-PmemDisk                     1.0.0.0    PersistentMemory
Cmdlet          New-PmemDisk                     1.0.0.0    PersistentMemory
Cmdlet          Remove-PmemDisk                  1.0.0.0    PersistentMemory
Application     diskmgmt.msc                     0.0.0.0    C:\WINDOWS\system32\diskmgmt.msc
Application     diskpart.exe                     10.0.17... C:\WINDOWS\system32\diskpart.exe
Application     diskperf.exe                     10.0.17... C:\WINDOWS\system32\diskperf.exe
Application     diskraid.exe                     10.0.17... C:\WINDOWS\system32\diskraid.exe
...

Finding custom executables
Unlike UNIX, where executables are files with the executable (+x) bit set, executables on windows are files present in one of the directories specified in the $PATH env. variable whose filename suffixes are named in the $PATHEXT env. variable (defaults to .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.CPL).
As Get-Command also honours this env. variable, it can be extended to list custom executables. e.g.
$ $Env:PATHEXT=""$Env:PATHEXT;.dll;.ps1;.psm1;.py""     # temporary assignment, only for this shell's process

$ gcm user32,kernel32,*WASM*,*http*py

CommandType     Name                        Version    Source
-----------     ----                        -------    ------
ExternalScript  Invoke-WASMProfiler.ps1                C:\WINDOWS\System32\WindowsPowerShell\v1.0\Invoke-WASMProfiler.ps1
Application     http-server.py              0.0.0.0    C:\Users\ME\AppData\Local\Microsoft\WindowsApps\http-server.py
Application     kernel32.dll                10.0.17... C:\WINDOWS\system32\kernel32.dll
Application     user32.dll                  10.0.17... C:\WINDOWS\system32\user32.dll

See Get-Command for more options and examples.
    While later versions of Windows have a where command, you can also do this with Windows XP by using the environment variable modifiers, as follows:

c:\> for %i in (cmd.exe) do @echo.   %~$PATH:i
   C:\WINDOWS\system32\cmd.exe

c:\> for %i in (python.exe) do @echo.   %~$PATH:i
   C:\Python25\python.exe


You don't need any extra tools and it's not limited to PATH since you can substitute any environment variable (in the path format, of course) that you wish to use.



And, if you want one that can handle all the extensions in PATHEXT (as Windows itself does), this one does the trick:

@echo off
setlocal enableextensions enabledelayedexpansion

:: Needs an argument.

if ""x%1""==""x"" (
    echo Usage: which ^<progName^>
    goto :end
)

:: First try the unadorned filenmame.

set fullspec=
call :find_it %1

:: Then try all adorned filenames in order.

set mypathext=!pathext!
:loop1
    :: Stop if found or out of extensions.

    if ""x!mypathext!""==""x"" goto :loop1end

    :: Get the next extension and try it.

    for /f ""delims=;"" %%j in (""!mypathext!"") do set myext=%%j
    call :find_it %1!myext!

:: Remove the extension (not overly efficient but it works).

:loop2
    if not ""x!myext!""==""x"" (
        set myext=!myext:~1!
        set mypathext=!mypathext:~1!
        goto :loop2
    )
    if not ""x!mypathext!""==""x"" set mypathext=!mypathext:~1!

    goto :loop1
:loop1end

:end
endlocal
goto :eof

:: Function to find and print a file in the path.

:find_it
    for %%i in (%1) do set fullspec=%%~$PATH:i
    if not ""x!fullspec!""==""x"" @echo.   !fullspec!
    goto :eof


It actually returns all possibilities but you can tweak it quite easily for specific search rules.
    In Windows PowerShell:

set-alias which where.exe

    In Windows CMD which calls where:

$ where php
C:\Program Files\PHP\php.exe

    If you have PowerShell installed (which I recommend), you can use the following command as a rough equivalent (substitute programName for your executable's name):

($Env:Path).Split("";"") | Get-ChildItem -filter programName*


More is here:
My Manwich! PowerShell Which 
    I have a function in my PowerShell profile named 'which'

function which {
    get-command $args[0]| format-list
}


Here's what the output looks like:

PS C:\Users\fez> which python


Name            : python.exe
CommandType     : Application
Definition      : C:\Python27\python.exe
Extension       : .exe
Path            : C:\Python27\python.exe
FileVersionInfo : File:             C:\Python27\python.exe
                  InternalName:
                  OriginalFilename:
                  FileVersion:
                  FileDescription:
                  Product:
                  ProductVersion:
                  Debug:            False
                  Patched:          False
                  PreRelease:       False
                  PrivateBuild:     False
                  SpecialBuild:     False
                  Language:

    Go get unxutils from here: http://sourceforge.net/projects/unxutils/

gold on windows platforms, puts all the nice unix utilities on a standard windows DOS. Been using it for years.

It has a 'which' included. Note that it's case sensitive though.

NB: to install it explode the zip somewhere and add ...\UnxUtils\usr\local\wbin\ to your system path env variable.
    In PowerShell, it is gcm, which gives formatted information about other commands. If you want to retrieve only path to executable, use .Source.

For instance: gcm git or (gcm git).Source

Tidbits:


Available for Windows XP.
Available since PowerShell 1.0.
gcm is an alias of Get-Command cmdlet.
Without any parameters, it lists down all the available commands offered by the host shell.
You can create a custom alias with Set-Alias which gcm and use it like: (which git).Source.
Official docs: https://technet.microsoft.com/en-us/library/ee176842.aspx

    The GnuWin32 tools have which, along with a whole slew of other Unix tools.
    Just have to post this Windows' one liner batch file:

C:>type wh.cmd
@for %%f in (%*) do for %%e in (%PATHEXT% .dll .lnk) do for %%b in (%%f%%e) do for %%d in (%PATH%) do if exist %%d\%%b echo %%d\%%b


A test:

C:>wh ssh
C:\cygwin64\bin\ssh.EXE
C:\Windows\System32\OpenSSH\\ssh.EXE


Not quite a one-liner if you wrap the code in setlocal enableextensions and endlocal.
    Cygwin is a solution. If you don't mind using a third-party solution, then Cygwin is the way to go.

Cygwin gives you the comfort of *nix in the Windows environment (and you can use it in your Windows command shell, or use a *nix shell of your choice). It gives you a whole host of *nix commands (like which) for Windows, and you can just include that directory in your PATH.
    If you can find a free Pascal compiler, you can compile this. At least it works and shows the algorithm necessary.

program Whence (input, output);
  Uses Dos, my_funk;
  Const program_version = '1.00';
        program_date    = '17 March 1994';
  VAR   path_str          : string;
        command_name      : NameStr;
        command_extension : ExtStr;
        command_directory : DirStr;
        search_dir        : DirStr;
        result            : DirStr;


  procedure Check_for (file_name : string);
    { Check existence of the passed parameter. If exists, then state so   }
    { and exit.                                                           }
  begin
    if Fsearch(file_name, '') <> '' then
    begin
      WriteLn('DOS command = ', Fexpand(file_name));
      Halt(0);    { structured ? whaddayamean structured ? }
    end;
  end;

  function Get_next_dir : DirStr;
    { Returns the next directory from the path variable, truncating the   }
    { variable every time. Implicit input (but not passed as parameter)   }
    { is, therefore, path_str                                             }
    var  semic_pos : Byte;

  begin
      semic_pos := Pos(';', path_str);
      if (semic_pos = 0) then
      begin
        Get_next_dir := '';
        Exit;
      end;

      result := Copy(Path_str, 1, (semic_pos - 1));  { return result   }
      { Hmm! although *I* never reference a Root drive (my directory tree) }
      { is 1/2 way structured), some network logon software which I run    }
      { does (it adds Z:\ to the path). This means that I have to allow    }
      { path entries with & without a terminating backslash. I'll delete   }
      { anysuch here since I always add one in the main program below.     }
      if (Copy(result, (Length(result)), 1) = '\') then
         Delete(result, Length(result), 1);

      path_str := Copy(path_str,(semic_pos + 1),
                       (length(path_str) - semic_pos));
      Get_next_dir := result;
  end;  { Of function get_next_dir }

begin
  { The following is a kludge which makes the function Get_next_dir easier  }
  { to implement. By appending a semi-colon to the end of the path         }
  { Get_next_dir doesn't need to handle the special case of the last entry }
  { which normally doesn't have a semic afterwards. It may be a kludge,    }
  { but it's a documented kludge (you might even call it a refinement).    }
  path_str := GetEnv('Path') + ';';

  if (paramCount = 0) then
  begin
    WriteLn('Whence: V', program_version, ' from ', program_date);
    Writeln;
    WriteLn('Usage: WHENCE command[.extension]');
    WriteLn;
    WriteLn('Whence is a ''find file''type utility witha difference');
    Writeln('There are are already more than enough of those :-)');
    Write  ('Use Whence when you''re not sure where a command which you ');
    WriteLn('want to invoke');
    WriteLn('actually resides.');
    Write  ('If you intend to invoke the command with an extension e.g ');
    Writeln('""my_cmd.exe param""');
    Write  ('then invoke Whence with the same extension e.g ');
    WriteLn('""Whence my_cmd.exe""');
    Write  ('otherwise a simple ""Whence my_cmd"" will suffice; Whence will ');
    Write  ('then search the current directory and each directory in the ');
    Write  ('for My_cmd.com, then My_cmd.exe and lastly for my_cmd.bat, ');
    Write  ('just as DOS does');
    Halt(0);
  end;

  Fsplit(paramStr(1), command_directory, command_name, command_extension);
  if (command_directory <> '') then
  begin
WriteLn('directory detected *', command_directory, '*');
    Halt(0);
  end;

  if (command_extension <> '') then
  begin
    path_str := Fsearch(paramstr(1), '');    { Current directory }
    if   (path_str <> '') then WriteLn('Dos command = ""', Fexpand(path_str), '""')
    else
    begin
      path_str := Fsearch(paramstr(1), GetEnv('path'));
      if (path_str <> '') then WriteLn('Dos command = ""', Fexpand(path_str), '""')
                          else Writeln('command not found in path.');
    end;
  end
  else
  begin
    { O.K, the way it works, DOS looks for a command firstly in the current  }
    { directory, then in each directory in the Path. If no extension is      }
    { given and several commands of the same name exist, then .COM has       }
    { priority over .EXE, has priority over .BAT                             }

    Check_for(paramstr(1) + '.com');     { won't return if file is found }
    Check_for(paramstr(1) + '.exe');
    Check_for(paramstr(1) + '.bat');

    { Not in current directory, search through path ... }

    search_dir := Get_next_dir;

    while (search_dir <> '') do
    begin
       Check_for(search_dir + '\' + paramstr(1) + '.com');
       Check_for(search_dir + '\' + paramstr(1) + '.exe');
       Check_for(search_dir + '\' + paramstr(1) + '.bat');
       search_dir := Get_next_dir;
    end;

    WriteLn('DOS command not found: ', paramstr(1));
  end;
end.

    I am using GOW (GNU on Windows) which is a light version of Cygwin. You can grab it from GitHub here.


  GOW (GNU on Windows) is the lightweight alternative to Cygwin. It uses
  a convenient Windows installer that installs about 130 extremely
  useful open source UNIX applications compiled as native win32
  binaries. It is designed to be as small as possible, about 10 MB, as
  opposed to Cygwin which can run well over 100 MB depending upon
  options. - About Description(Brent R. Matzelle)


A screenshot of a list of commands included in GOW:


    You can first install Git from Downloading Git, and then open Git Bash and type:

which app-name

    This batch file uses CMD variable handling to find the command that would be executed in the path. Note: that the current directory is always done before the path) and depending on which API call is used other locations are searched before/after the path.

@echo off
echo. 
echo PathFind - Finds the first file in in a path
echo ======== = ===== === ===== ==== == == = ====
echo. 
echo Searching for %1 in %path%
echo. 
set a=%~$PATH:1
If ""%a%""=="""" (Echo %1 not found) else (echo %1 found at %a%)


See set /? for help.
    Not in stock Windows but it is provided by Services for Unix and there are several simple batch scripts floating around that accomplish the same thing such this this one.
    For you WindowsXP users (who have no where command built-in), I have written a ""where like"" command as a rubygem called whichr.

To install it, install Ruby.

Then

gem install whichr


Run it like:

C:> whichr cmd_here
    None of the Win32 ports of Unix which that I could find on the Internet are satistactory, because they all have one or more of these shortcomings:


No support for Windows PATHEXT variable. (Which defines the list of extensions implicitely added to each command before scanning the path, and in which order.) (I use a lot of tcl scripts, and no publicly available which tool could find them.)
No support for cmd.exe code pages, which makes them display paths with non-ascii characters incorrectly. (I'm very sensitive to that, with the  in my first name :-))
No support for the distinct search rules in cmd.exe and the PowerShell command line. (No publicly available tool will find .ps1 scripts in a PowerShell window, but not in a cmd window!)


So I eventually wrote my own which, that suports all the above correctly.

Available there:
http://jf.larvoire.free.fr/progs/which.exe
    TCC and TCC/LE from JPSoft are CMD.EXE replacements that add significant functionality.  Relevant to the OP's question, which is a builtin command for TCC family command processors.
    I have used the which module from npm for quite a while, and it works very well: https://www.npmjs.com/package/which
It is a great multi platform alternative.

Now I switched to the which that comes with Git. Just add to your path the /usr/bin path from Git, which is usually at C:\Program Files\Git\usr\bin\which.exe. The which binary will be at C:\Program Files\Git\usr\bin\which.exe. It is faster and also works as expected.
    try this

set a=%~$dir:1
If ""%for%""=="""" (Echo %1 not found) else (echo %1 found at %a%)

    I have created tool similar to Ned Batchelder:

Searching .dll and .exe files in PATH

While my tool is primarly for searching of various dll versions it shows more info (date, size, version) but it do not use PATHEXT (I hope to update my tool soon).
    The best version of this I've found on Windows is Joseph Newcomer's ""whereis"" utility, which is available (with source) from his site.

The article about the development of ""whereis"" is worth reading.
    Here is a function which I made to find executable similar to the Unix command 'WHICH`
app_path_func.cmd:
@ECHO OFF
CLS

FOR /F ""skip=2 tokens=1,2* USEBACKQ"" %%N IN (`reg query ""HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\App Paths\%~1"" /t REG_SZ  /v ""Path""`) DO (
 IF /I ""%%N"" == ""Path"" (
  SET wherepath=%%P%~1
  GoTo Found
 )
)

FOR /F ""tokens=* USEBACKQ"" %%F IN (`where.exe %~1`) DO (
 SET wherepath=%%F
 GoTo Found
)

FOR /F ""tokens=* USEBACKQ"" %%F IN (`where.exe /R ""%PROGRAMFILES%"" %~1`) DO (
 SET wherepath=%%F
 GoTo Found
)

FOR /F ""tokens=* USEBACKQ"" %%F IN (`where.exe /R ""%PROGRAMFILES(x86)%"" %~1`) DO (
 SET wherepath=%%F
 GoTo Found
)

FOR /F ""tokens=* USEBACKQ"" %%F IN (`where.exe /R ""%WINDIR%"" %~1`) DO (
 SET wherepath=%%F
 GoTo Found
)

:Found
SET %2=%wherepath%
:End

Test:
@ECHO OFF
CLS

CALL ""app_path_func.cmd"" WINWORD.EXE PROGPATH
ECHO %PROGPATH%

PAUSE

Result:
C:\Program Files (x86)\Microsoft Office\Office15\
Press any key to continue . . .

https://www.freesoftwareservers.com/display/FREES/Find+Executable+via+Batch+-+Microsoft+Office+Example+-+WINWORD+-+Find+Microsoft+Office+Path
    It is possible to download all of the UNIX commands compiled for Windows, including which from this GitHub repository: https://github.com/George-Ogden/UNIX
    ","[2656, 2840, 210, 309, 69, 30, 44, 12, 11, 13, 37, 5, 18, 10, 6, 6, 6, 8, 4, 6, 3, 2, 1, 5, 7, -1, 0]",744463,364,2008-11-20T04:19:35,2021-12-22 10:43:55Z,
Why shouldn't I use mysql_* functions in PHP?,"
                    
            
        
            
                    
                        
                    
                
                    
                        Want to improve this post? Provide detailed answers to this question, including citations and an explanation of why your answer is correct. Answers without enough detail may be edited or deleted.
                        
                    
                
            
        

    

What are the technical reasons for why one shouldn't use mysql_* functions? (e.g. mysql_query(), mysql_connect() or mysql_real_escape_string())?

Why should I use something else even if they work on my site?

If they don't work on my site, why do I get errors like 


  Warning: mysql_connect(): No such file or directory

    The MySQL extension:


Is not under active development
Is officially deprecated as of PHP 5.5 (released June 2013).
Has been removed entirely as of PHP 7.0 (released December 2015)


This means that as of 31 Dec 2018 it does not exist in any supported version of PHP. If you are using a version of PHP which supports it, you are using a version which doesn't get security problems fixed.

Lacks an OO interface
Doesn't support:


Non-blocking, asynchronous queries
Prepared statements or parameterized queries
Stored procedures
Multiple Statements
Transactions
The ""new"" password authentication method (on by default in MySQL 5.6; required in 5.7)
Any of the new functionality in MySQL 5.1 or later



Since it is deprecated, using it makes your code less future proof. 

Lack of support for prepared statements is particularly important as they provide a clearer, less error-prone method of escaping and quoting external data than manually escaping it with a separate function call.

See the comparison of SQL extensions.
    PHP offers three different APIs to connect to MySQL. These are the mysql(removed as of PHP 7), mysqli, and PDO extensions.

The mysql_* functions used to be very popular, but their use is not encouraged anymore. The documentation team is discussing the database security situation, and educating users to move away from the commonly used ext/mysql extension is part of this (check php.internals: deprecating ext/mysql).

And the later PHP developer team has taken the decision to generate E_DEPRECATED errors when users connect to MySQL, whether through mysql_connect(), mysql_pconnect() or the implicit connection functionality built into ext/mysql.

ext/mysql was officially deprecated as of PHP 5.5 and has been removed as of PHP 7.

See the Red Box?

When you go on any mysql_* function manual page, you see a red box, explaining it should not be used anymore.

Why



Moving away from ext/mysql is not only about security, but also about having access to all the features of the MySQL database.

ext/mysql was built for MySQL 3.23 and only got very few additions since then while mostly keeping compatibility with this old version which makes the code a bit harder to maintain. Missing features that is not supported by ext/mysql include: (from PHP manual).


Stored procedures (can't handle multiple result sets)
Prepared statements
Encryption (SSL)
Compression
Full Charset support


Reason to not use mysql_* function:


Not under active development
Removed as of PHP 7
Lacks an OO interface
Doesn't support non-blocking, asynchronous queries
Doesn't support prepared statements or parameterized queries
Doesn't support stored procedures
Doesn't support multiple statements
Doesn't support transactions
Doesn't support all of the functionality in MySQL 5.1


Above point quoted from Quentin's answer

Lack of support for prepared statements is particularly important as they provide a clearer, less error prone method of escaping and quoting external data than manually escaping it with a separate function call.

See the comparison of SQL extensions.



Suppressing deprecation warnings

While code is being converted to MySQLi/PDO, E_DEPRECATED errors can be suppressed by setting error_reporting in php.ini to exclude E_DEPRECATED:

error_reporting = E_ALL ^ E_DEPRECATED


Note that this will also hide other deprecation warnings, which, however, may be for things other than MySQL. (from PHP manual)

The article PDO vs. MySQLi: Which Should You Use? by Dejan Marjanovic will help you to choose.

And a better way is PDO, and I am now writing a simple PDO tutorial.



A simple and short PDO tutorial



Q. First question in my mind was: what is `PDO`?

A. PDO  PHP Data Objects  is a database access layer providing a uniform method of access to multiple databases.


  




Connecting to MySQL 

With mysql_* function or we can say it the old way (deprecated in PHP 5.5 and above)

$link = mysql_connect('localhost', 'user', 'pass');
mysql_select_db('testdb', $link);
mysql_set_charset('UTF-8', $link);


With PDO: All you need to do is create a new PDO object. The constructor accepts parameters for specifying the database source PDO's constructor mostly takes four parameters which are DSN (data source name) and  optionally username, password.

Here I think you are familiar with all except DSN; this is new in PDO. A DSN is basically a string of options that tell PDO which driver to use, and connection details. For further reference, check PDO MySQL DSN.

$db = new PDO('mysql:host=localhost;dbname=testdb;charset=utf8', 'username', 'password');


Note: you can also use charset=UTF-8, but sometimes it causes an error, so it's better to use utf8.

If there is any connection error, it will throw a PDOException object that can be caught to handle Exception further.

Good read: Connections and Connection management  

You can also pass in several driver options as an array to the fourth parameter. I recommend passing the parameter which puts PDO into exception mode. Because some PDO drivers don't support native prepared statements, so PDO performs emulation of the prepare. It also lets you manually enable this emulation. To use the native server-side prepared statements, you should explicitly set it false.

The other is to turn off prepare emulation which is enabled in the MySQL driver by default, but prepare emulation should be turned off to use PDO safely. 

I will later explain why prepare emulation should be turned off. To find reason please check this post.

It is only usable if you are using an old version of MySQL which I do not recommended.

Below is an example of how you can do it:

$db = new PDO('mysql:host=localhost;dbname=testdb;charset=UTF-8', 
              'username', 
              'password',
              array(PDO::ATTR_EMULATE_PREPARES => false,
              PDO::ATTR_ERRMODE => PDO::ERRMODE_EXCEPTION));


Can we set attributes after PDO construction?

Yes, we can also set some attributes after PDO construction with the setAttribute method:

$db = new PDO('mysql:host=localhost;dbname=testdb;charset=UTF-8', 
              'username', 
              'password');
$db->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
$db->setAttribute(PDO::ATTR_EMULATE_PREPARES, false);


Error Handling 



Error handling is much easier in PDO than mysql_*.

A common practice when using mysql_* is:

//Connected to MySQL
$result = mysql_query(""SELECT * FROM table"", $link) or die(mysql_error($link));


OR die() is not a good way to handle the error since we can not handle the thing in die. It will just end the script abruptly and then echo the error to the screen which you usually do NOT want to show to your end users, and let bloody hackers discover your schema. Alternately, the return values of mysql_* functions can often be used in conjunction with mysql_error() to handle errors.

PDO offers a better solution: exceptions. Anything we do with PDO should be wrapped in a try-catch block. We can force PDO into one of three error modes by setting the error mode attribute. Three error handling modes are below.


PDO::ERRMODE_SILENT. It's just setting error codes and acts pretty much the same as mysql_* where you must check each result and then look at $db->errorInfo(); to get the error details.
PDO::ERRMODE_WARNING Raise E_WARNING. (Run-time warnings (non-fatal errors). Execution of the script is not halted.)
PDO::ERRMODE_EXCEPTION: Throw exceptions. It represents an error raised by PDO. You should not throw a PDOException from your own code. See Exceptions for more information about exceptions in PHP. It acts very much like or die(mysql_error());, when it isn't caught. But unlike or die(), the PDOException can be caught and handled gracefully if you choose to do so.


Good read:


Errors and error handling 
The PDOException class 
Exceptions 


Like:

$stmt->setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_SILENT );
$stmt->setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_WARNING );
$stmt->setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION );


And you can wrap it in try-catch, like below:

try {
    //Connect as appropriate as above
    $db->query('hi'); //Invalid query!
} 
catch (PDOException $ex) {
    echo ""An Error occured!""; //User friendly message/message you want to show to user
    some_logging_function($ex->getMessage());
}


You do not have to handle with try-catch right now. You can catch it at any time appropriate, but I strongly recommend you to use try-catch. Also it may make more sense to catch it at outside the function that calls the PDO stuff:

function data_fun($db) {
    $stmt = $db->query(""SELECT * FROM table"");
    return $stmt->fetchAll(PDO::FETCH_ASSOC);
}

//Then later
try {
    data_fun($db);
}
catch(PDOException $ex) {
    //Here you can handle error and show message/perform action you want.
}


Also, you can handle by or die() or we can say like mysql_*, but it will be really varied. You can hide the dangerous error messages in production by turning display_errors off and just reading your error log.

Now, after reading all the things above, you are probably thinking: what the heck is that when I just want to start leaning simple SELECT, INSERT, UPDATE, or DELETE statements? Don't worry, here we go:



Selecting Data



So what you are doing in mysql_* is:

<?php
$result = mysql_query('SELECT * from table') or die(mysql_error());

$num_rows = mysql_num_rows($result);

while($row = mysql_fetch_assoc($result)) {
    echo $row['field1'];
}


Now in PDO, you can do this like:

<?php
$stmt = $db->query('SELECT * FROM table');

while($row = $stmt->fetch(PDO::FETCH_ASSOC)) {
    echo $row['field1'];
}


Or

<?php
$stmt = $db->query('SELECT * FROM table');
$results = $stmt->fetchAll(PDO::FETCH_ASSOC);

//Use $results


Note: If you are using the method like below (query()), this method returns a PDOStatement object. So if you want to fetch the result, use it like above.

<?php
foreach($db->query('SELECT * FROM table') as $row) {
    echo $row['field1'];
}


In PDO Data, it is obtained via the ->fetch(), a method of your statement handle. Before calling fetch, the best approach would be telling PDO how youd like the data to be fetched. In the below section I am explaining this.

Fetch Modes

Note the use of PDO::FETCH_ASSOC in the fetch() and fetchAll() code above. This tells PDO to return the rows as an associative array with the field names as keys. There are many other fetch modes too which I will explain one by one.

First of all, I explain how to select fetch mode:

 $stmt->fetch(PDO::FETCH_ASSOC)


In the above, I have been using fetch(). You can also use:


PDOStatement::fetchAll() - Returns an array containing all of the result set rows
PDOStatement::fetchColumn() - Returns a single column from the next row of a result set
PDOStatement::fetchObject() - Fetches the next row and returns it as an object.
PDOStatement::setFetchMode() - Set the default fetch mode for this statement


Now I come to fetch mode:


PDO::FETCH_ASSOC: returns an array indexed by column name as returned in your result set
PDO::FETCH_BOTH (default): returns an array indexed by both column name and 0-indexed column number as returned in your result set


There are even more choices! Read about them all in PDOStatement Fetch documentation..

Getting the row count:

Instead of using mysql_num_rows to get the number of returned rows, you can get a PDOStatement and do rowCount(), like:

<?php
$stmt = $db->query('SELECT * FROM table');
$row_count = $stmt->rowCount();
echo $row_count.' rows selected';


Getting the Last Inserted ID

<?php
$result = $db->exec(""INSERT INTO table(firstname, lastname) VAULES('John', 'Doe')"");
$insertId = $db->lastInsertId();




Insert and Update or Delete statements



What we are doing in mysql_* function is:

<?php
$results = mysql_query(""UPDATE table SET field='value'"") or die(mysql_error());
echo mysql_affected_rows($result);


And in pdo, this same thing can be done by:

<?php
$affected_rows = $db->exec(""UPDATE table SET field='value'"");
echo $affected_rows;


In the above query PDO::exec execute an SQL statement and returns the number of affected rows.

Insert and delete will be covered later.

The above method is only useful when you are not using variable in query. But when you need to use a variable in a query, do not ever ever try like the above and there for  prepared statement or parameterized statement is.



Prepared Statements

Q. What is a prepared statement and why do I need them?
A. A prepared statement is a pre-compiled SQL statement that can be executed multiple times by sending only the data to the server.

The typical workflow of using a prepared statement is as follows (quoted from Wikipedia three 3 point):


Prepare: The statement template is created by the application and sent to the database management system (DBMS). Certain values are left unspecified, called parameters, placeholders or bind variables (labelled ? below):

INSERT INTO PRODUCT (name, price) VALUES (?, ?)
The DBMS parses, compiles, and performs query optimization on the statement template, and stores the result without executing it.
Execute: At a later time, the application supplies (or binds) values for the parameters, and the DBMS executes the statement (possibly returning a result). The application may execute the statement as many times as it wants with different values. In this example, it might supply 'Bread' for the first parameter and 1.00 for the second parameter.


You can use a prepared statement by including placeholders in your SQL. There are basically three ones without placeholders (don't try this with variable its above one), one with unnamed placeholders, and one with named placeholders.

Q. So now, what are named placeholders and how do I use them?
A. Named placeholders. Use descriptive names preceded by a colon, instead of question marks. We don't care about position/order of value in name place holder:

 $stmt->bindParam(':bla', $bla);


bindParam(parameter,variable,data_type,length,driver_options)

You can also bind using an execute array as well:

<?php
$stmt = $db->prepare(""SELECT * FROM table WHERE id=:id AND name=:name"");
$stmt->execute(array(':name' => $name, ':id' => $id));
$rows = $stmt->fetchAll(PDO::FETCH_ASSOC);


Another nice feature for OOP friends is that named placeholders have the ability to insert objects directly into your database, assuming the properties match the named fields. For example:

class person {
    public $name;
    public $add;
    function __construct($a,$b) {
        $this->name = $a;
        $this->add = $b;
    }

}
$demo = new person('john','29 bla district');
$stmt = $db->prepare(""INSERT INTO table (name, add) value (:name, :add)"");
$stmt->execute((array)$demo);


Q. So now, what are unnamed placeholders and how do I use them?
A. Let's have an example:

<?php
$stmt = $db->prepare(""INSERT INTO folks (name, add) values (?, ?)"");
$stmt->bindValue(1, $name, PDO::PARAM_STR);
$stmt->bindValue(2, $add, PDO::PARAM_STR);
$stmt->execute();


and

$stmt = $db->prepare(""INSERT INTO folks (name, add) values (?, ?)"");
$stmt->execute(array('john', '29 bla district'));


In the above, you can see those ? instead of a name like in a name place holder. Now in the first example, we assign variables to the various placeholders ($stmt->bindValue(1, $name, PDO::PARAM_STR);). Then, we assign values to those placeholders and execute the statement. In the second example, the first array element goes to the first ? and the second to the second ?.

NOTE: In unnamed placeholders we must take care of the proper order of the elements in the array that we are passing to the PDOStatement::execute() method.



SELECT, INSERT, UPDATE, DELETE prepared queries


SELECT:

$stmt = $db->prepare(""SELECT * FROM table WHERE id=:id AND name=:name"");
$stmt->execute(array(':name' => $name, ':id' => $id));
$rows = $stmt->fetchAll(PDO::FETCH_ASSOC);

INSERT:

$stmt = $db->prepare(""INSERT INTO table(field1,field2) VALUES(:field1,:field2)"");
$stmt->execute(array(':field1' => $field1, ':field2' => $field2));
$affected_rows = $stmt->rowCount();

DELETE:

$stmt = $db->prepare(""DELETE FROM table WHERE id=:id"");
$stmt->bindValue(':id', $id, PDO::PARAM_STR);
$stmt->execute();
$affected_rows = $stmt->rowCount();

UPDATE:

$stmt = $db->prepare(""UPDATE table SET name=? WHERE id=?"");
$stmt->execute(array($name, $id));
$affected_rows = $stmt->rowCount();





NOTE:

However PDO and/or MySQLi are not completely safe. Check the answer Are PDO prepared statements sufficient to prevent SQL injection? by ircmaxell. Also, I am quoting some part from his answer:

$pdo->setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
$pdo->query('SET NAMES GBK');
$stmt = $pdo->prepare(""SELECT * FROM test WHERE name = ? LIMIT 1"");
$stmt->execute(array(chr(0xbf) . chr(0x27) . "" OR 1=1 /*""));

    First, let's begin with the standard comment we give everyone: 


  Please, don't use mysql_* functions in new code. They are no longer maintained and are officially deprecated. See the red box? Learn about prepared statements instead, and use PDO or MySQLi - this article will help you decide which. If you choose PDO, here is a good tutorial.


Let's go through this, sentence by sentence, and explain:


They are no longer maintained, and are officially deprecated

This means that the PHP community is gradually dropping support for these very old functions. They are likely to not exist in a future (recent) version of PHP! Continued use of these functions may break your code in the (not so) far future.

NEW! - ext/mysql is now officially deprecated as of PHP 5.5!

Newer! ext/mysql has been removed in PHP 7.
Instead, you should learn of prepared statements

mysql_* extension does not support prepared statements, which is (among other things) a very effective countermeasure against SQL Injection. It fixed a very serious vulnerability in MySQL dependent applications which allows attackers to gain access to your script and perform any possible query on your database.

For more information, see How can I prevent SQL injection in PHP?
See the Red Box?

When you go to any mysql function manual page, you see a red box, explaining it should not be used anymore.
Use either PDO or MySQLi

There are better, more robust and well-built alternatives, PDO - PHP Database Object, which offers a complete OOP approach to database interaction, and MySQLi, which is a MySQL specific improvement.

    The mysql_ functions:


are out of date - they're not maintained any more
don't allow you to move easily to another database backend
don't support prepared statements, hence
encourage programmers to use concatenation to build queries, leading to SQL injection vulnerabilities

    Ease of use

The analytic and synthetic reasons were already mentioned. For newcomers there's a more significant incentive to stop using the dated mysql_ functions. 

Contemporary database APIs are just easier to use.

It's mostly the bound parameters which can simplify code. And with excellent tutorials (as seen above) the transition to PDO isn't overly arduous.

Rewriting a larger code base at once however takes time. Raison d'tre for this intermediate alternative:

Equivalent pdo_* functions in place of mysql_*

Using <pdo_mysql.php> you can switch from the old mysql_ functions with minimal effort. It adds pdo_ function wrappers which replace their mysql_ counterparts.


Simply include_once(""pdo_mysql.php""); in each invocation script that has to interact with the database.

Remove the mysql_ function prefix everywhere and replace it with pdo_.


mysql_connect() becomes pdo_connect()
mysql_query() becomes pdo_query()
mysql_num_rows() becomes pdo_num_rows()
mysql_insert_id() becomes pdo_insert_id()
mysql_fetch_array() becomes pdo_fetch_array()
mysql_fetch_assoc() becomes pdo_fetch_assoc()
mysql_real_escape_string() becomes pdo_real_escape_string()
and so on...   

Your code will work alike and still mostly look the same:

include_once(""pdo_mysql.php""); 

pdo_connect(""localhost"", ""usrABC"", ""pw1234567"");
pdo_select_db(""test"");

$result = pdo_query(""SELECT title, html FROM pages"");  

while ($row = pdo_fetch_assoc($result)) {
    print ""$row[title] - $row[html]"";
}



Et voil.
Your code is using PDO.
Now it's time to actually utilize it.  

Bound parameters can be easy to use



You just need a less unwieldy API.

pdo_query() adds very facile support for bound parameters. Converting old code is straightforward:



Move your variables out of the SQL string.


Add them as comma delimited function parameters to pdo_query().
Place question marks ? as placeholders where the variables were before.
Get rid of ' single quotes that previously enclosed string values/variables.


The advantage becomes more obvious for lengthier code.

Often string variables aren't just interpolated into SQL, but concatenated with escaping calls in between.

pdo_query(""SELECT id, links, html, title, user, date FROM articles
   WHERE title='"" . pdo_real_escape_string($title) . ""' OR id='"".
   pdo_real_escape_string($title) . ""' AND user <> '"" .
   pdo_real_escape_string($root) . ""' ORDER BY date"")


With ? placeholders applied you don't have to bother with that:

pdo_query(""SELECT id, links, html, title, user, date FROM articles
   WHERE title=? OR id=? AND user<>? ORDER BY date"", $title, $id, $root)


Remember that pdo_* still allows either or.
Just don't escape a variable and bind it in the same query.


The placeholder feature is provided by the real PDO behind it.
Thus also allowed :named placeholder lists later.


More importantly you can pass $_REQUEST[] variables safely behind any query. When submitted <form> fields match the database structure exactly it's even shorter:

pdo_query(""INSERT INTO pages VALUES (?,?,?,?,?)"", $_POST);


So much simplicity. But let's get back to some more rewriting advises and technical reasons on why you may want to get rid of mysql_ and escaping.

Fix or remove any oldschool sanitize() function

Once you have converted all mysql_ calls to pdo_query with bound params, remove all redundant pdo_real_escape_string calls.

In particular you should fix any sanitize or clean or filterThis or clean_data functions as advertised by dated tutorials in one form or the other:

function sanitize($str) {
   return trim(strip_tags(htmlentities(pdo_real_escape_string($str))));
}


Most glaring bug here is the lack of documentation. More significantly the order of filtering was in exactly the wrong order.


Correct order would have been: deprecatedly stripslashes as the innermost call, then trim, afterwards strip_tags, htmlentities for output context, and only lastly the _escape_string as its application should directly preceed the SQL intersparsing.
But as first step just get rid of the _real_escape_string call.
You may have to keep the rest of your sanitize() function for now if your database and application flow expect HTML-context-safe strings. Add a comment that it applies only HTML escaping henceforth.
String/value handling is delegated to PDO and its parameterized statements.
If there was any mention of stripslashes() in your sanitize function, it may indicate a higher level oversight.


That was commonly there to undo damage (double escaping) from the deprecated magic_quotes. Which however is best fixed centrally, not string by string.
Use one of the userland reversal approaches. Then remove the stripslashes() in the sanitize function.



  Historic note on magic_quotes. That feature is rightly deprecated. It's often incorrectly portrayed as failed security feature however. But magic_quotes are as much a failed security feature as tennis balls have failed as nutrition source. That simply wasn't their purpose.
  
  The original implementation in PHP2/FI introduced it explicitly with just ""quotes will be automatically escaped making it easier to pass form data directly to msql queries"". Notably it was accidentially safe to use with mSQL, as that supported ASCII only.
  Then PHP3/Zend reintroduced magic_quotes for MySQL and misdocumented it. But originally it was just a convenience feature, not intend for security. 



How prepared statements differ

When you scramble string variables into the SQL queries, it doesn't just get more intricate for you to follow. It's also extraneous effort for MySQL to segregate code and data again.



SQL injections simply are when data bleeds into code context. A database server can't later spot where PHP originally glued variables inbetween query clauses.

With bound parameters you separate SQL code and SQL-context values in your PHP code. But it doesn't get shuffled up again behind the scenes (except with PDO::EMULATE_PREPARES). Your database receives the unvaried SQL commands and 1:1 variable values.



While this answer stresses that you should care about the readability advantages of dropping mysql_. There's occasionally also a performance advantage (repeated INSERTs with just differing values) due to this visible and technical data/code separation. 

Beware that parameter binding still isn't a magic one-stop solution against all SQL injections. It handles the most common use for data/values. But can't whitelist column name / table identifiers, help with dynamic clause construction, or just plain array value lists.

Hybrid PDO use

These pdo_* wrapper functions make a coding-friendly stop-gap API. (It's pretty much what MYSQLI could have been if it wasn't for the idiosyncratic function signature shift). They also expose the real PDO at most times.
Rewriting doesn't have to stop at using the new pdo_ function names. You could one by one transition each pdo_query() into a plain $pdo->prepare()->execute() call.

It's best to start at simplifying again however. For example the common result fetching:

$result = pdo_query(""SELECT * FROM tbl"");
while ($row = pdo_fetch_assoc($result)) {


Can be replaced with just an foreach iteration:

foreach ($result as $row) {


Or better yet a direct and complete array retrieval:

$result->fetchAll();


You'll get more helpful warnings in most cases than PDO or mysql_ usually provide after failed queries.

Other options

So this hopefully visualized some practical reasons and a worthwile pathway to drop mysql_.

Just switching to pdo doesn't quite cut it. pdo_query() is also just a frontend onto it.

Unless you also introduce parameter binding or can utilize something else from the nicer API, it's a pointless switch. I hope it's portrayed simple enough to not further the discouragement to newcomers. (Education usually works better than prohibition.)

While it qualifies for the simplest-thing-that-could-possibly-work category, it's also still very experimental code. I just wrote it over the weekend. There's a plethora of alternatives however. Just google for PHP database abstraction and browse a little. There always have been and will be lots of excellent libraries for such tasks.

If you want to simplify your database interaction further, mappers like Paris/Idiorm are worth a try. Just like nobody uses the bland DOM in JavaScript anymore, you don't have to babysit a raw database interface nowadays.
    Speaking of technical reasons, there are only a few, extremely specific and rarely used. Most likely you will never ever use them in your life.
Maybe I am too ignorant, but I never had an opportunity to use them things like 


non-blocking, asynchronous queries
stored procedures returning multiple resultsets
Encryption (SSL)
Compression


If you need them - these are no doubt technical reasons to move away from mysql extension toward something more stylish and modern-looking.

Nevertheless, there are also some non-technical issues, which can make your experience a bit harder


further use of these functions with modern PHP versions will raise deprecated-level notices. They simply can be turned off.
in a distant future, they can be possibly removed from the default PHP build. Not a big deal too, as mydsql ext will be moved into PECL and every hoster will be happy to compile PHP with it, as they don't want to lose clients whose sites were working for decades.   
strong resistance from Stackoverflow community. verytime you mention these honest functions, you being told that they are under strict taboo.
being an average PHP user, most likely your idea of using these functions is error-prone and wrong. Just because of all these numerous tutorials and manuals which teach you the wrong way. Not the functions themselves - I have to emphasize it - but the way they are used.


This latter issue is a problem.
But, in my opinion, the proposed solution is no better either.
It seems to me too idealistic a dream that all those PHP users will learn how to handle SQL queries properly at once. Most likely they would just change mysql_* to mysqli_* mechanically, leaving the approach the same. Especially because mysqli makes prepared statements usage incredible painful and troublesome.
Not to mention that native prepared statements aren't enough to protect from SQL injections, and neither mysqli nor PDO offers a solution. 

So, instead of fighting this honest extension, I'd prefer to fight wrong practices and educate people in the right ways. 

Also, there are some false or non-significant reasons, like


Doesn't support Stored Procedures (we were using mysql_query(""CALL my_proc""); for ages)
Doesn't support Transactions (same as above)
Doesn't support Multiple Statements (who need them?)
Not under active development (so what? does it affect you in any practical way?)
Lacks an OO interface (to create one is a matter of several hours)
Doesn't support Prepared Statements or Parametrized Queries


The last one is an interesting point. Although mysql ext do not support native prepared statements, they aren't required for the safety. We can easily fake prepared statements using manually handled placeholders (just like PDO does):

function paraQuery()
{
    $args  = func_get_args();
    $query = array_shift($args);
    $query = str_replace(""%s"",""'%s'"",$query); 

    foreach ($args as $key => $val)
    {
        $args[$key] = mysql_real_escape_string($val);
    }

    $query  = vsprintf($query, $args);
    $result = mysql_query($query);
    if (!$result)
    {
        throw new Exception(mysql_error()."" [$query]"");
    }
    return $result;
}

$query  = ""SELECT * FROM table where a=%s AND b LIKE %s LIMIT %d"";
$result = paraQuery($query, $a, ""%$b%"", $limit);


voila, everything is parameterized and safe.

But okay, if you don't like the red box in the manual, a problem of choice arises: mysqli or PDO?

Well, the answer would be as follows:


If you understand the necessity of using a database abstraction layer and looking for an API to create one, mysqli is a very good choice, as it indeed supports many mysql-specific features.
If, like vast majority of PHP folks, you are using raw API calls right in the application code (which is essentially wrong practice) - PDO is the only choice, as this extension pretends to be not just API but rather a semi-DAL, still incomplete but offers many important features, with two of them makes PDO critically distinguished from mysqli:


unlike mysqli, PDO can bind placeholders by value, which makes dynamically built queries feasible without several screens of quite messy code.
unlike mysqli, PDO can always return query result in a simple usual array, while mysqli can do it only on mysqlnd installations.



So, if you are an average PHP user and want to save yourself a ton of headaches when using native prepared statements, PDO - again - is the only choice.
However, PDO is not a silver bullet too and has its hardships.
So, I wrote solutions for all the common pitfalls and complex cases in the PDO tag wiki

Nevertheless, everyone talking about extensions always missing the 2 important facts about Mysqli and PDO:


Prepared statement isn't a silver bullet. There are dynamical identifiers which cannot be bound using prepared statements. There are dynamical queries with an unknown number of parameters which makes query building a difficult task.
Neither mysqli_* nor PDO functions should have appeared in the application code.
There ought to be an abstraction layer between them and application code, which will do all the dirty job of binding, looping, error handling, etc. inside, making application code DRY and clean. Especially for the complex cases like dynamical query building.


So, just switching to PDO or mysqli is not enough. One has to use an ORM, or a query builder, or whatever database abstraction class instead of calling raw API functions in their code.
And contrary - if you have an abstraction layer between your application code and mysql API - it doesn't actually matter which engine is used. You can use mysql ext until it goes deprecated and then easily rewrite your abstraction class to another engine, having all the application code intact.

Here are some examples based on my safemysql class to show how such an abstraction class ought to be:

$city_ids = array(1,2,3);
$cities   = $db->getCol(""SELECT name FROM cities WHERE is IN(?a)"", $city_ids);


Compare this one single line with amount of code you will need with PDO.
Then compare with crazy amount of code you will need with raw Mysqli prepared statements.
Note that error handling, profiling, query logging already built in and running.

$insert = array('name' => 'John', 'surname' => ""O'Hara"");
$db->query(""INSERT INTO users SET ?u"", $insert);


Compare it with usual PDO inserts, when every single field name being repeated six to ten times - in all these numerous named placeholders, bindings, and query definitions.

Another example:

$data = $db->getAll(""SELECT * FROM goods ORDER BY ?n"", $_GET['order']);


You can hardly find an example for PDO to handle such practical case.
And it will be too wordy and most likely unsafe.    

So, once more - it is not just raw driver should be your concern but abstraction class, useful not only for silly examples from beginner's manual but to solve whatever real-life problems. 
    There are many reasons, but perhaps the most important one is that those functions encourage insecure programming practices because they do not support prepared statements. Prepared statements help prevent SQL injection attacks.

When using mysql_* functions, you have to remember to run user-supplied parameters through mysql_real_escape_string(). If you forget in just one place or if you happen to escape only part of the input, your database may be subject to attack.

Using prepared statements in PDO or mysqli will make it so that these sorts of programming errors are more difficult to make.
    This answer is written to show just how trivial it is to bypass poorly written PHP user-validation code, how (and using what) these attacks work and how to replace the old MySQL functions with a secure prepared statement - and basically, why StackOverflow users (probably with a lot of rep) are barking at new users asking questions to improve their code.
First off, please feel free to create this test mysql database (I have called mine prep):
mysql> create table users(
    -> id int(2) primary key auto_increment,
    -> userid tinytext,
    -> pass tinytext);
Query OK, 0 rows affected (0.05 sec)

mysql> insert into users values(null, 'Fluffeh', 'mypass');
Query OK, 1 row affected (0.04 sec)

mysql> create user 'prepared'@'localhost' identified by 'example';
Query OK, 0 rows affected (0.01 sec)

mysql> grant all privileges on prep.* to 'prepared'@'localhost' with grant option;
Query OK, 0 rows affected (0.00 sec)

With that done, we can move to our PHP code.
Lets assume the following script is the verification process for an admin on a website (simplified but working if you copy and use it for testing):
<?php 

    if(!empty($_POST['user']))
    {
        $user=$_POST['user'];
    }   
    else
    {
        $user='bob';
    }
    if(!empty($_POST['pass']))
    {
        $pass=$_POST['pass'];
    }
    else
    {
        $pass='bob';
    }
    
    $database='prep';
    $link=mysql_connect('localhost', 'prepared', 'example');
    mysql_select_db($database) or die( ""Unable to select database"");

    $sql=""select id, userid, pass from users where userid='$user' and pass='$pass'"";
    //echo $sql.""<br><br>"";
    $result=mysql_query($sql);
    $isAdmin=false;
    while ($row = mysql_fetch_assoc($result)) {
        echo ""My id is "".$row['id']."" and my username is "".$row['userid']."" and lastly, my password is "".$row['pass'].""<br>"";
        $isAdmin=true;
        // We have correctly matched the Username and Password
        // Lets give this person full access
    }
    if($isAdmin)
    {
        echo ""The check passed. We have a verified admin!<br>"";
    }
    else
    {
        echo ""You could not be verified. Please try again...<br>"";
    }
    mysql_close($link);

?>

<form name=""exploited"" method='post'>
    User: <input type='text' name='user'><br>
    Pass: <input type='text' name='pass'><br>
    <input type='submit'>
</form>

Seems legit enough at first glance.
The user has to enter a login and password, right?
Brilliant, not enter in the following:
user: bob
pass: somePass

and submit it.
The output is as follows:
You could not be verified. Please try again...

Super! Working as expected, now lets try the actual username and password:
user: Fluffeh
pass: mypass

Amazing! Hi-fives all round, the code correctly verified an admin. It's perfect!
Well, not really. Lets say the user is a clever little person. Lets say the person is me.
Enter in the following:
user: bob
pass: n' or 1=1 or 'm=m

And the output is:
The check passed. We have a verified admin!

Congrats, you just allowed me to enter your super-protected admins only section with me entering a false username and a false password. Seriously, if you don't believe me, create the database with the code I provided, and run this PHP code - which at glance REALLY does seem to verify the username and password rather nicely.
So, in answer, THAT IS WHY YOU ARE BEING YELLED AT.
So, lets have a look at what went wrong, and why I just got into your super-admin-only-bat-cave. I took a guess and assumed that you weren't being careful with your inputs and simply passed them to the database directly. I constructed the input in a way tht would CHANGE the query that you were actually running. So, what was it supposed to be, and what did it end up being?
select id, userid, pass from users where userid='$user' and pass='$pass'

That's the query, but when we replace the variables with the actual inputs that we used, we get the following:
select id, userid, pass from users where userid='bob' and pass='n' or 1=1 or 'm=m'

See how I constructed my ""password"" so that it would first close the single quote around the password, then introduce a completely new comparison? Then just for safety, I added another ""string"" so that the single quote would get closed as expected in the code we originally had.
However, this isn't about folks yelling at you now, this is about showing you how to make your code more secure.
Okay, so what went wrong, and how can we fix it?
This is a classic SQL injection attack. One of the simplest for that matter. On the scale of attack vectors, this is a toddler attacking a tank - and winning.
So, how do we protect your sacred admin section and make it nice and secure? The first thing to do will be to stop using those really old and deprecated mysql_* functions. I know, you followed a tutorial you found online and it works, but it's old, it's outdated and in the space of a few minutes, I have just broken past it without so much as breaking a sweat.
Now, you have the better options of using mysqli_ or PDO. I am personally a big fan of PDO, so I will be using PDO in the rest of this answer. There are pro's and con's, but personally I find that the pro's far outweigh the con's. It's portable across multiple database engines - whether you are using MySQL or Oracle or just about bloody anything - just by changing the connection string, it has all the fancy features we want to use and it is nice and clean. I like clean.
Now, lets have a look at that code again, this time written using a PDO object:
<?php 

    if(!empty($_POST['user']))
    {
        $user=$_POST['user'];
    }   
    else
    {
        $user='bob';
    }
    if(!empty($_POST['pass']))
    {
        $pass=$_POST['pass'];
    }
    else
    {
        $pass='bob';
    }
    $isAdmin=false;
    
    $database='prep';
    $pdo=new PDO ('mysql:host=localhost;dbname=prep', 'prepared', 'example');
    $sql=""select id, userid, pass from users where userid=:user and pass=:password"";
    $myPDO = $pdo->prepare($sql, array(PDO::ATTR_CURSOR => PDO::CURSOR_FWDONLY));
    if($myPDO->execute(array(':user' => $user, ':password' => $pass)))
    {
        while($row=$myPDO->fetch(PDO::FETCH_ASSOC))
        {
            echo ""My id is "".$row['id']."" and my username is "".$row['userid']."" and lastly, my password is "".$row['pass'].""<br>"";
            $isAdmin=true;
            // We have correctly matched the Username and Password
            // Lets give this person full access
        }
    }
    
    if($isAdmin)
    {
        echo ""The check passed. We have a verified admin!<br>"";
    }
    else
    {
        echo ""You could not be verified. Please try again...<br>"";
    }

?>

<form name=""exploited"" method='post'>
    User: <input type='text' name='user'><br>
    Pass: <input type='text' name='pass'><br>
    <input type='submit'>
</form>

The major differences are that there are no more mysql_* functions. It's all done via a PDO object, secondly, it is using a prepared statement. Now, what's a prepared statement you ask? It's a way to tell the database ahead of running a query, what the query is that we are going to run. In this case, we tell the database: ""Hi, I am going to run a select statement wanting id, userid and pass from the table users where the userid is a variable and the pass is also a variable."".
Then, in the execute statement, we pass the database an array with all the variables that it now expects.
The results are fantastic. Lets try those username and password combinations from before again:
user: bob
pass: somePass

User wasn't verified. Awesome.
How about:
user: Fluffeh
pass: mypass

Oh, I just got a little excited, it worked: The check passed. We have a verified admin!
Now, lets try the data that a clever chap would enter to try to get past our little verification system:
user: bob
pass: n' or 1=1 or 'm=m

This time, we get the following:
You could not be verified. Please try again...

This is why you are being yelled at when posting questions - it's because people can see that your code can be bypassed wihout even trying. Please, do use this question and answer to improve your code, to make it more secure and to use functions that are current.
Lastly, this isn't to say that this is PERFECT code. There are many more things that you could do to improve it, use hashed passwords for example, ensure that when you store sensitive information in the database, you don't store it in plain text, have multiple levels of verification - but really, if you just change your old injection prone code to this, you will be WELL along the way to writing good code - and the fact that you have gotten this far and are still reading gives me a sense of hope that you will not only implement this type of code when writing your websites and applications, but that you might go out and research those other things I just mentioned - and more. Write the best code you can, not the most basic code that barely functions.
    Because (amongst other reasons) it's much harder to ensure the input data is sanitized. If you use parametrized queries, as one does with PDO or mysqli you can entirely avoid the risk.

As an example, someone could use ""enhzflep); drop table users"" as a username. The old functions will allow executing multiple statements per query, so something like that nasty bugger can delete a whole table.

If one were to use PDO of mysqli, the user-name would end-up being ""enhzflep); drop table users"".

See bobby-tables.com.
    It's possible to define almost all mysql_* functions using mysqli or PDO. Just include them on top of your old PHP application, and it will work on PHP7. My solution here.

<?php

define('MYSQL_LINK', 'dbl');
$GLOBALS[MYSQL_LINK] = null;

function mysql_link($link=null) {
    return ($link === null) ? $GLOBALS[MYSQL_LINK] : $link;
}

function mysql_connect($host, $user, $pass) {
    $GLOBALS[MYSQL_LINK] = mysqli_connect($host, $user, $pass);
    return $GLOBALS[MYSQL_LINK];
}

function mysql_pconnect($host, $user, $pass) {
    return mysql_connect($host, $user, $pass);
}

function mysql_select_db($db, $link=null) {
    $link = mysql_link($link);
    return mysqli_select_db($link, $db);
}

function mysql_close($link=null) {
    $link = mysql_link($link);
    return mysqli_close($link);
}

function mysql_error($link=null) {
    $link = mysql_link($link);
    return mysqli_error($link);
}

function mysql_errno($link=null) {
    $link = mysql_link($link);
    return mysqli_errno($link);
}

function mysql_ping($link=null) {
    $link = mysql_link($link);
    return mysqli_ping($link);
}

function mysql_stat($link=null) {
    $link = mysql_link($link);
    return mysqli_stat($link);
}

function mysql_affected_rows($link=null) {
    $link = mysql_link($link);
    return mysqli_affected_rows($link);
}

function mysql_client_encoding($link=null) {
    $link = mysql_link($link);
    return mysqli_character_set_name($link);
}

function mysql_thread_id($link=null) {
    $link = mysql_link($link);
    return mysqli_thread_id($link);
}

function mysql_escape_string($string) {
    return mysql_real_escape_string($string);
}

function mysql_real_escape_string($string, $link=null) {
    $link = mysql_link($link);
    return mysqli_real_escape_string($link, $string);
}

function mysql_query($sql, $link=null) {
    $link = mysql_link($link);
    return mysqli_query($link, $sql);
}

function mysql_unbuffered_query($sql, $link=null) {
    $link = mysql_link($link);
    return mysqli_query($link, $sql, MYSQLI_USE_RESULT);
}

function mysql_set_charset($charset, $link=null){
    $link = mysql_link($link);
    return mysqli_set_charset($link, $charset);
}

function mysql_get_host_info($link=null) {
    $link = mysql_link($link);
    return mysqli_get_host_info($link);
}

function mysql_get_proto_info($link=null) {
    $link = mysql_link($link);
    return mysqli_get_proto_info($link);
}
function mysql_get_server_info($link=null) {
    $link = mysql_link($link);
    return mysqli_get_server_info($link);
}

function mysql_info($link=null) {
    $link = mysql_link($link);
    return mysqli_info($link);
}

function mysql_get_client_info() {
    $link = mysql_link();
    return mysqli_get_client_info($link);
}

function mysql_create_db($db, $link=null) {
    $link = mysql_link($link);
    $db = str_replace('`', '', mysqli_real_escape_string($link, $db));
    return mysqli_query($link, ""CREATE DATABASE `$db`"");
}

function mysql_drop_db($db, $link=null) {
    $link = mysql_link($link);
    $db = str_replace('`', '', mysqli_real_escape_string($link, $db));
    return mysqli_query($link, ""DROP DATABASE `$db`"");
}

function mysql_list_dbs($link=null) {
    $link = mysql_link($link);
    return mysqli_query($link, ""SHOW DATABASES"");
}

function mysql_list_fields($db, $table, $link=null) {
    $link = mysql_link($link);
    $db = str_replace('`', '', mysqli_real_escape_string($link, $db));
    $table = str_replace('`', '', mysqli_real_escape_string($link, $table));
    return mysqli_query($link, ""SHOW COLUMNS FROM `$db`.`$table`"");
}

function mysql_list_tables($db, $link=null) {
    $link = mysql_link($link);
    $db = str_replace('`', '', mysqli_real_escape_string($link, $db));
    return mysqli_query($link, ""SHOW TABLES FROM `$db`"");
}

function mysql_db_query($db, $sql, $link=null) {
    $link = mysql_link($link);
    mysqli_select_db($link, $db);
    return mysqli_query($link, $sql);
}

function mysql_fetch_row($qlink) {
    return mysqli_fetch_row($qlink);
}

function mysql_fetch_assoc($qlink) {
    return mysqli_fetch_assoc($qlink);
}

function mysql_fetch_array($qlink, $result=MYSQLI_BOTH) {
    return mysqli_fetch_array($qlink, $result);
}

function mysql_fetch_lengths($qlink) {
    return mysqli_fetch_lengths($qlink);
}

function mysql_insert_id($qlink) {
    return mysqli_insert_id($qlink);
}

function mysql_num_rows($qlink) {
    return mysqli_num_rows($qlink);
}

function mysql_num_fields($qlink) {
    return mysqli_num_fields($qlink);
}

function mysql_data_seek($qlink, $row) {
    return mysqli_data_seek($qlink, $row);
}

function mysql_field_seek($qlink, $offset) {
    return mysqli_field_seek($qlink, $offset);
}

function mysql_fetch_object($qlink, $class=""stdClass"", array $params=null) {
    return ($params === null)
        ? mysqli_fetch_object($qlink, $class)
        : mysqli_fetch_object($qlink, $class, $params);
}

function mysql_db_name($qlink, $row, $field='Database') {
    mysqli_data_seek($qlink, $row);
    $db = mysqli_fetch_assoc($qlink);
    return $db[$field];
}

function mysql_fetch_field($qlink, $offset=null) {
    if ($offset !== null)
        mysqli_field_seek($qlink, $offset);
    return mysqli_fetch_field($qlink);
}

function mysql_result($qlink, $offset, $field=0) {
    if ($offset !== null)
        mysqli_field_seek($qlink, $offset);
    $row = mysqli_fetch_array($qlink);
    return (!is_array($row) || !isset($row[$field]))
        ? false
        : $row[$field];
}

function mysql_field_len($qlink, $offset) {
    $field = mysqli_fetch_field_direct($qlink, $offset);
    return is_object($field) ? $field->length : false;
}

function mysql_field_name($qlink, $offset) {
    $field = mysqli_fetch_field_direct($qlink, $offset);
    if (!is_object($field))
        return false;
    return empty($field->orgname) ? $field->name : $field->orgname;
}

function mysql_field_table($qlink, $offset) {
    $field = mysqli_fetch_field_direct($qlink, $offset);
    if (!is_object($field))
        return false;
    return empty($field->orgtable) ? $field->table : $field->orgtable;
}

function mysql_field_type($qlink, $offset) {
    $field = mysqli_fetch_field_direct($qlink, $offset);
    return is_object($field) ? $field->type : false;
}

function mysql_free_result($qlink) {
    try {
        mysqli_free_result($qlink);
    } catch (Exception $e) {
        return false;
    }
    return true;
}

    The MySQL extension is the oldest of the three and was the original way that developers used to communicate with MySQL. This extension is now being deprecated in favor of the other two alternatives because of improvements made in newer releases of both PHP and MySQL.


MySQLi is the 'improved' extension for working with MySQL databases. It takes advantage of features that are available in newer versions of the MySQL server, exposes both a function-oriented and an object-oriented interface to the developer and a does few other nifty things.
PDO offers an API that consolidates most of the functionality that was previously spread across the major database access extensions, i.e. MySQL, PostgreSQL, SQLite, MSSQL, etc. The interface exposes high-level objects for the programmer to work with database connections, queries and result sets, and low-level drivers perform communication and resource handling with the database server. A lot of discussion and work is going into PDO and its considered the appropriate method of working with databases in modern, professional code.

    I find the above answers really lengthy, so to summarize:


  The mysqli extension has a number of
  benefits, the key enhancements over
  the mysql extension being:
  
  
  Object-oriented interface
  Support for Prepared Statements
  Support for Multiple Statements
  Support for Transactions
  Enhanced debugging capabilities
  Embedded server support
  


Source: MySQLi overview



As explained in the above answers, the alternatives to mysql are mysqli and PDO (PHP Data Objects).


API supports server-side Prepared Statements: Supported by MYSQLi and PDO
API supports client-side Prepared Statements: Supported only by PDO
API supports Stored Procedures: Both MySQLi and PDO
API supports Multiple Statements and all MySQL 4.1+ functionality - Supported by MySQLi and mostly also by PDO


Both MySQLi and PDO were introduced in PHP 5.0, whereas MySQL was introduced prior to PHP 3.0. A point to note is that MySQL is included in PHP5.x though deprecated in later versions. 
    Don't use mysql because is deprecated use Mysqli Instead.
What Deprecated Means:
It means don't use some specific function/method/software feature/particular software practice it just means that it should not be used because there is (or there will be) a better alternative in that software that should be used instead.
Several common issues can arise when using deprecated functions:
1. Functions just flat-out stop working: Applications or scripts might rely on functions that are simply no longer supported, Thus use their improved versions or alternative.
2. Warning messages display about deprecation: These messages dont normally interfere with site functionality. However, in some cases, they might disrupt the process of the server sending headers.
For Example: This can cause login issues (cookies/sessions dont get set properly) or forwarding issues (301/302/303 redirects).
keep in mind that:
-Deprecated software is still a part of the software.
-Deprecated code is just a status (label) of the code.
Key Differences in MYSQL vs MYSQLI
mysql*

old database driver
MySQL can only be used procedurally
No protection from SQL injection attack
Was deprecated in PHP 5.5.0 and was removed in PHP 7

mysqli

new database driver
Currently under usage
prepared statements protect from attacks

    ","[2652, 2191, 1342, 311, 158, 227, 114, 103, 72, 80, 11, 38, 25, 1]",245602,529,2012-10-12T13:18:39,2022-04-19 17:51:42Z,php 
How do I make Git ignore file mode (chmod) changes?,"
                
I have a project in which I have to change the mode of files with chmod to 777 while developing, but which should not change in the main repo. 

Git picks up on chmod -R 777 . and marks all files as changed. Is there a way to make Git ignore mode changes that have been made to files?
    Try:
git config core.fileMode false

From git-config(1):

core.fileMode
    Tells Git if the executable bit of files in the working tree
    is to be honored.

    Some filesystems lose the executable bit when a file that is
    marked as executable is checked out, or checks out a
    non-executable file with executable bit on. git-clone(1)
    or git-init(1) probe the filesystem to see if it handles the 
    executable bit correctly and this variable is automatically
    set as necessary.

    A repository, however, may be on a filesystem that handles
    the filemode correctly, and this variable is set to true when
    created, but later may be made accessible from another
    environment that loses the filemode (e.g. exporting ext4
    via CIFS mount, visiting a Cygwin created repository with Git
    for Windows or Eclipse). In such a case it may be necessary
    to set this variable to false. See git-update-index(1).

    The default is true (when core.filemode is not specified
    in the config file).


The -c flag can be used to set this option for one-off commands:
git -c core.fileMode=false diff

Typing the -c core.fileMode=false can be bothersome and so you can set this flag for all git repos or just for one git repo:
# this will set your the flag for your user for all git repos (modifies `$HOME/.gitconfig`)
git config --global core.fileMode false

# this will set the flag for one git repo (modifies `$current_git_repo/.git/config`)
git config core.fileMode false

Additionally, git clone and git init explicitly set core.fileMode to true in the repo config as discussed in Git global core.fileMode false overridden locally on clone
Warning
core.fileMode is not the best practice and should be used carefully. This setting only covers the executable bit of mode and never the read/write bits. In many cases you think you need this setting because you did something like chmod -R 777, making all your files executable. But in most projects most files don't need and should not be executable for security reasons.
The proper way to solve this kind of situation is to handle folder and file permission separately, with something like:
find . -type d -exec chmod a+rwx {} \; # Make folders traversable and read/write
find . -type f -exec chmod a+rw {} \;  # Make files read/write

If you do that, you'll never need to use core.fileMode, except in very rare environment.
    undo mode change in working tree:
git diff --summary | grep --color 'mode change 100755 => 100644' | cut -d' ' -f7- | xargs -d'\n' chmod +x
git diff --summary | grep --color 'mode change 100644 => 100755' | cut -d' ' -f7- | xargs -d'\n' chmod -x

Or in mingw-git
git diff --summary | grep  'mode change 100755 => 100644' | cut -d' ' -f7- | xargs -e'\n' chmod +x
git diff --summary | grep  'mode change 100644 => 100755' | cut -d' ' -f7- | xargs -e'\n' chmod -x

Or in BSD/macOS
git diff --summary | grep --color 'mode change 100644 => 100755' | cut -d' ' -f7- | tr '\n' '\0' | xargs -0 chmod -x
git diff --summary | grep --color 'mode change 100755 => 100644' | cut -d' ' -f7- | tr '\n' '\0' | xargs -0 chmod -x

    If

git config --global core.filemode false


does not work for you, do it manually:

cd into yourLovelyProject folder


cd into .git folder:

cd .git


edit the config file:

nano config


change true to false

[core]
        repositoryformatversion = 0
        filemode = true


->

[core]
        repositoryformatversion = 0
        filemode = false


save, exit, go to upper folder:

cd ..


reinit the git

git init


you are done!
    If you want to set this option for all of your repos, use the --global option.

git config --global core.filemode false


If this does not work you are probably using a newer version of git so try the --add option.

git config --add --global core.filemode false


If you run it without the --global option and your working directory is not a repo, you'll get

error: could not lock config file .git/config: No such file or directory

    You can configure it globally:

git config --global core.filemode false

If the above doesn't work for you, the reason might be your local configuration overrides the global configuration. 

Remove your local configuration to make the global configuration take effect:

git config --unset core.filemode

Alternatively, you could change your local configuration to the right value:

git config core.filemode false
    By definining the following alias (in ~/.gitconfig) you can easily temporarily disable the fileMode per git command:

[alias]
nfm = ""!f(){ git -c core.fileMode=false $@; };f""


When this alias is prefixed to the git command, the file mode changes won't show up with commands that would otherwise show them. For example:

git nfm status

    Adding to Greg Hewgill answer (of using core.fileMode config variable):

You can use --chmod=(-|+)x option of git update-index (low-level version of ""git add"") to change execute permissions in the index, from where it would be picked up if you use ""git commit"" (and not ""git commit -a"").
    If you have used chmod command already then check the difference of file, It shows previous file mode and current file mode such as:

new mode : 755

old mode : 644

set old mode of all files using below command

sudo chmod 644 .

now set core.fileMode to false in config file either using command or manually.

git config core.fileMode false


then apply chmod command to change the permissions of all files such as 

sudo chmod 755 .


and again set core.fileMode to true.

git config core.fileMode true


For best practises don't Keep core.fileMode false always.
    Simple solution:

Hit this Simple command in project Folder(it won't remove your original changes) ...it will only remove changes that had been done while you changed project folder permission

command is below:

git config core.fileMode false

Why this all unnecessary file get modified:
because you have changed the project folder permissions 
with commend
sudo chmod -R 777 ./yourProjectFolder

when will you check changes what not you did?
you found like below while using git diff filename

old mode 100644
new mode 100755

    This may work: git config core.fileMode false
    This works for me:

find . -type f -exec chmod a-x {} \;


or reverse, depending on your operating system

find . -type f -exec chmod a+x {} \;

    If you want to set filemode to false in config files recursively (including submodules) :
find -name config | xargs sed -i -e 's/filemode = true/filemode = false/'
    ","[2643, 4398, 304, 105, 149, 44, 23, 57, 26, 7, 3, 3, 14]",1209201,936,2009-10-16T21:43:38,2022-04-03 14:18:58Z,
How to change the commit author for one specific commit?,"
                
I want to change the author of one specific commit in the history. It's not the last commit.

I know about this question - How do I change the author of a commit in git?

But I am thinking about something, where I identify the commit by hash or short-hash.
    Interactive rebase off of a point earlier in the history than the commit you need to modify (git rebase -i <earliercommit>). In the list of commits being rebased, change the text from pick to edit next to the hash of the one you want to modify. Then when git prompts you to change the commit, use this:

git commit --amend --author=""Author Name <email@address.com>"" --no-edit




For example, if your commit history is A-B-C-D-E-F with F as HEAD, and you want to change the author of C and D, then you would...


Specify git rebase -i B (here is an example of what you will see after executing the git rebase -i B command)


if you need to edit A, use git rebase -i --root

Change the lines for both C and D from pick to edit
Exit the editor (for vim, this would be pressing Esc and then typing :wq).
Once the rebase started, it would first pause at C
You would git commit --amend --author=""Author Name <email@address.com>""
Then git rebase --continue
It would pause again at D
Then you would git commit --amend --author=""Author Name <email@address.com>"" again
git rebase --continue
The rebase would complete.
Use git push -f to update your origin with the updated commits.

    The accepted answer to this question is a wonderfully clever use of interactive rebase, but it unfortunately exhibits conflicts if the commit we are trying to change the author of used to be on a branch which was subsequently merged in. More generally, it does not work when handling messy histories.
Since I am apprehensive about running scripts which depend on setting and unsetting environment variables to rewrite git history, I am writing a new answer based on this post which is similar to this answer but is more complete.
The following is tested and working, unlike the linked answer.
Assume for clarity of exposition that 03f482d6 is the commit whose author we are trying to replace, and 42627abe is the commit with the new author.

Checkout the commit we are trying to modify.
 git checkout 03f482d6


Make the author change.
 git commit --amend --author ""New Author Name <New Author Email>""



Now we have a new commit with hash assumed to be 42627abe.

Checkout the original branch.

Replace the old commit with the new one locally.
 git replace 03f482d6 42627abe


Rewrite all future commits based on the replacement.
 git filter-branch -- --all


Remove the replacement for cleanliness.
 git replace -d 03f482d6


Push the new history (only use --force if the below fails, and only after sanity checking with git log and/or git diff).
 git push --force-with-lease



Instead of 4-5 you can just rebase onto new commit:
git rebase -i 42627abe

    Github documentation contains a script that replaces the committer info for all commits in a branch (now irretrievable, this is the last snapshot).

Run the following script from terminal after changing the variable values
#!/bin/sh

git filter-branch --env-filter '

OLD_EMAIL=""your-old-email@example.com""
CORRECT_NAME=""Your Correct Name""
CORRECT_EMAIL=""your-correct-email@example.com""

if [ ""$GIT_COMMITTER_EMAIL"" = ""$OLD_EMAIL"" ]
then
    export GIT_COMMITTER_NAME=""$CORRECT_NAME""
    export GIT_COMMITTER_EMAIL=""$CORRECT_EMAIL""
fi
if [ ""$GIT_AUTHOR_EMAIL"" = ""$OLD_EMAIL"" ]
then
    export GIT_AUTHOR_NAME=""$CORRECT_NAME""
    export GIT_AUTHOR_EMAIL=""$CORRECT_EMAIL""
fi
' --tag-name-filter cat -- --branches --tags


Push the corrected history to GitHub:
git push --force --tags origin 'refs/heads/*'



OR if you like to push selected references of the branches then use
```
git push --force --tags origin 'refs/heads/develop'
```

    
Reset your email to the config globally:

git config --global user.email example@email.com
Now reset the author of your commit without edit required:

git commit --amend --reset-author --no-edit

    Find a way that can change user quickly and has no side effect to others commits.

Simple and clear way:

git config user.name ""New User""
git config user.email ""newuser@gmail.com""

git log
git rebase -i 1f1357
# change the word 'pick' to 'edit', save and exit

git commit --amend --reset-author --no-edit
git rebase --continue

git push --force-with-lease


detailed operations


show commit logs and find out the commit id that ahead of your commit which you want to change:


git log



git rebase start from the chosed commit id to the recent reversely:


git config user.name ""New User""
git config user.email ""newuser@gmail.com""
git rebase -i 1f1357

# change word pick to edit, save and exit
edit 809b8f7 change code order 
pick 9baaae5 add prometheus monitor kubernetes
edit 5d726c3 fix liquid escape issue   
edit 3a5f98f update tags
pick 816e21c add prometheus monitor kubernetes



rebase will Stopped at next commit id, output:


Stopped at 809b8f7...  change code order 
You can amend the commit now, with
  git commit --amend 

Once you are satisfied with your changes, run

  git rebase --continue



comfirm and continue your rebase untill it successfully to refs/heads/master.


# each continue will show you an amend message
# use git commit --amend --reset-author --no-edit to comfirm
# use git rebase --skip to skip
git commit --amend --reset-author --no-edit
git rebase --continue
git commit --amend --reset-author --no-edit
...
git rebase --continue
Successfully rebased and updated refs/heads/master.



git push to update


git push --force-with-lease

    You can change author of last commit using the command below.

git commit --amend --author=""Author Name <email@address.com>""

However, if you want to change more than one commits author name, it's a bit tricky. You need to start an interactive rebase then mark commits as edit then amend them one by one and finish.

Start rebasing with git rebase -i. It will show you something like this. 



Change the pick keyword to edit for the commits you want to change the author name.



Then close the editor. For the beginners, hit Escape then type :wq and hit Enter.

Then you will see your terminal like nothing happened. Actually you are in the middle of an interactive rebase. Now it's time to amend your commit's author name using the command above. It will open the editor again. Quit and continue rebase with git rebase --continue. Repeat the same for the commit count you want to edit. You can make sure that interactive rebase finished when you get the No rebase in progress? message.
    In furtherance to Eugen Konkov answer, to start from the root commit, use --root flag. The --no-edit flag is helpful too, because with it you are not prompted into an editor for each commit.

git rebase --root --exec ""git commit --amend --author='name <email>' --no-edit""

    Commit before:



To fix author for all commits you can apply command from @Amber's answer:

git commit --amend --author=""Author Name <email@address.com>""


Or to reuse your name and email you can just write:

git commit --amend --author=Eugen


Commit after the command:



For example to change all starting from 4025621:



You must run:

git rebase --onto 4025621 --exec ""git commit --amend --author=Eugen"" 4025621


Note: To include an author containing spaces such as a name and email address, the author must be surrounded by escaped quotes.  For example:

git rebase --onto 4025621 --exec ""git commit --amend --author=\""Foo Bar <foo@bar.com>\"""" 4025621


or add this alias into ~/.gitconfig:

[alias]
    reauthor = !bash -c 'git rebase --onto $1 --exec \""git commit --amend --author=$2\"" $1' --


And then run:

git reauthor 4025621 Eugen

    The answers in the question to which you linked are good answers and cover your situation (the other question is more general since it involves rewriting multiple commits).

As an excuse to try out git filter-branch, I wrote a script to rewrite the Author Name and/or Author Email for a given commit:

#!/bin/sh

#
# Change the author name and/or email of a single commit.
#
# change-author [-f] commit-to-change [branch-to-rewrite [new-name [new-email]]]
#
#     If -f is supplied it is passed to ""git filter-branch"".
#
#     If <branch-to-rewrite> is not provided or is empty HEAD will be used.
#     Use ""--all"" or a space separated list (e.g. ""master next"") to rewrite
#     multiple branches.
#
#     If <new-name> (or <new-email>) is not provided or is empty, the normal
#     user.name (user.email) Git configuration value will be used.
#

force=''
if test ""x$1"" = ""x-f""; then
    force='-f'
    shift
fi

die() {
    printf '%s\n' ""$@""
    exit 128
}
targ=""$(git rev-parse --verify ""$1"" 2>/dev/null)"" || die ""$1 is not a commit""
br=""${2:-HEAD}""

TARG_COMMIT=""$targ""
TARG_NAME=""${3-}""
TARG_EMAIL=""${4-}""
export TARG_COMMIT TARG_NAME TARG_EMAIL

filt='

    if test ""$GIT_COMMIT"" = ""$TARG_COMMIT""; then
        if test -n ""$TARG_EMAIL""; then
            GIT_AUTHOR_EMAIL=""$TARG_EMAIL""
            export GIT_AUTHOR_EMAIL
        else
            unset GIT_AUTHOR_EMAIL
        fi
        if test -n ""$TARG_NAME""; then
            GIT_AUTHOR_NAME=""$TARG_NAME""
            export GIT_AUTHOR_NAME
        else
            unset GIT_AUTHOR_NAME
        fi
    fi

'

git filter-branch $force --env-filter ""$filt"" -- $br

    It could happen if you're missing settings on your machine, e.g., after a format, or when not having a Git configured correctly without setting up (correctly) these commands.
git config user.name ""Author Name""
git config user.email ""<email@address.com>""

Why not make your life simpler by following this article by Atlassian?

git commit --amend --author=""Author Name <email@address.com>""
Unprotect your branch, if it's protected. In this example, it's master; therefore, it'll be protected by the source code repository
git push origin master --force

That's the simplest scenario for the last commit. For picking up any ""random"" commit, you need:

git rebase -i <Earlier Commit>.
Change pick on edit on that commit, in which you're interested in
git commit --amend --author=""Author Name <email@address.com>""
Unprotect your branch if it's protected. In this example, it's master; therefore, it'll be protected by the source code repository
git push origin master --force

You can always git log in between to be sure where you are before you push.
    OPTIONAL: Make sure to stash your local changes if you don't want to send them to remote.
$ git status
$ git stash

Update the author for the last commit.
$ git log   // Old author in local and remote
$ git commit --amend --author=""Author Name <email@address.com>""
$ git log   // New Author in local
$ git push origin <branch> --force-with-lease 
$ git log   // New Author in remote

Then, if you used git stash then recovers your staged changes
$ git stash pop
$ git status


Then, you should to update the configuration for the next commits of the current project.
$ git config user.name ""Author Name""
$ git config user.email ""<email@address.com>""

And check or also edit this with git config --edit

Clarification: In the rare case that you lose commits using $ ggpush -f you can recover them with reflog. Anyway using --force-with-lease you are protected even more than if you use only -f
GL

Source
ZSH

    Steps to rename author name after commit pushed


First type ""git log"" to get the commit id and more details
git rebase i HEAD~10 (10 is the total commit to display on rebase)

If you Get anything like below

fatal: It seems that there is already a rebase-merge directory, and
I wonder if you are in the middle of another rebase.  If that is the
case, please try

git rebase (--continue | --abort | --skip)
If that is not the case, please rm -fr 
"".git/rebase-merge""
and run me again.  I am stopping in case you still have something
valuable there.
Then type ""git rebase --continue"" or ""git rebase --abort"" as per your need


now your will rebase window opened, click ""i"" key from keyboard
then you will get list of commits to 10 [because we have passed 10 commit above]
Like below


pick 897fe9e simplify code a little

pick abb60f9 add new feature

pick dc18f70 bugfix
Now you need to add below command just below of the commit you want to edit, like below

pick 897fe9e simplify code a little
exec git commit --amend --author 'Author Name <author.name@mail.com>'
pick abb60f9 add new feature
exec git commit --amend --author 'Author Name <author.name@mail.com>' 
pick dc18f70 bugfix
exec git commit --amend --author 'Author Name <author.name@mail.com>'


That's it, now just press ESC, :wq and you are all set
Then git push origin HEAD:BRANCH NAME -f [please take care of -f Force push]


like  git push -f or git push origin HEAD: dev -f

    If the commit that you want to change is not the last commit, then follow the below steps. If your commit is in different branch then first switch to that branch.

git checkout branch_name

Find commit before the commit that you want to change and find its hash. Then issue rebase command.

git rebase -i -p hash of commit

Then an editor will open and enter 'edit' for the commits that you want to change. Leave others with default 'pick' option. Once changed enter 'esc' key and wq! to exit.

Then issue git commit command with amendment option.

git commit --amend --author=""Username email"" --no-edit

Then issue the following command.

git rebase --continue

Once commit author is updated in the local repository, push the changes to the remote repository.
    you can use these commands from official page of github

https://help.github.com/en/github/using-git/changing-author-info

here is the commands

#!/bin/sh

git filter-branch --env-filter '

OLD_EMAIL=""your-old-email@example.com""
CORRECT_NAME=""Your Correct Name""
CORRECT_EMAIL=""your-correct-email@example.com""

if [ ""$GIT_COMMITTER_EMAIL"" = ""$OLD_EMAIL"" ]
then
export GIT_COMMITTER_NAME=""$CORRECT_NAME""
export GIT_COMMITTER_EMAIL=""$CORRECT_EMAIL""
fi
if [ ""$GIT_AUTHOR_EMAIL"" = ""$OLD_EMAIL"" ]
then
export GIT_AUTHOR_NAME=""$CORRECT_NAME""
export GIT_AUTHOR_EMAIL=""$CORRECT_EMAIL""
fi
' --tag-name-filter cat -- --branches --tags


here u can change the old email to ur new user name and email address.
    SOLUTION

Install git filter-repo (Git project recommends filter-repo over filter-branch)
$ PACKAGE_TOOL install git-filter-repo


Create a file .mailmap in the root of the git repository containing
New Name <new@ema.il> <old@ema.il>


Run git filter-repo --use-mailmap



MORE DETAILS

git-filter-repo lists this as an example in their docs
Instead of replacing both name and email based on email filter as in example above, take a look at additional examples in git mailmap documentation
Instead of using default .mailmap file you can specify your own by invoking git filter-repo with argument --mailmap <filename>.
Many more examples on how to further filter branch/tag/whatever can be found in git-filter-repo project's README.md.

    When doing git rebase -i there is this interesting bit in the doc:


  If you want to fold two or more commits into one, replace the command ""pick"" for the second and subsequent commits with ""squash"" or ""fixup"". If the commits had different authors, the folded commit will be attributed to the author of the first commit. The suggested commit message for the folded commit is the concatenation of the commit messages of the first commit and of those with the ""squash"" command, but omits the commit messages of commits with the ""fixup"" command.



If you have an history of A-B-C-D-E-F,
and you want to change commits B and D (= 2 commits),


then you can do:


git config user.name ""Correct new name""
git config user.email ""correct@new.email""
create empty commits (one for each commit):

you need a message for rebase purpose
git commit --allow-empty -m ""empty""

start the rebase operation

git rebase -i B^
B^ selects the parent of B.

you will want to put one empty commit before each commit to modify
you will want to change pick to squash for those.


Example of what git rebase -i B^ will give you:

pick sha-commit-B some message
pick sha-commit-C some message
pick sha-commit-D some message
pick sha-commit-E some message
pick sha-commit-F some message
# pick sha-commit-empty1 empty
# pick sha-commit-empty2 empty


change that to:

# change commit B's author
pick sha-commit-empty1 empty
squash sha-commit-B some message
# leave commit C alone
pick sha-commit-C some message
# change commit D's author
pick sha-commit-empty2 empty
squash sha-commit-D some message
# leave commit E-F alone
pick sha-commit-E some message
pick sha-commit-F some message


It will prompt you to edit the messages:

# This is a combination of 2 commits.
# The first commit's message is:

empty

# This is the 2nd commit message:

...some useful commit message there...


and you can just remove the first few lines.
    There is one additional step to Amber's answer if you're using a centralized repository:

git push -f to force the update of the central repository.

Be careful that there are not a lot of people working on the same branch because it can ruin consistency.
    For the merge commit message, I found that I cannot amend it by using rebase, at least on gitlab. It shows the merge as a commit but I cannot rebase onto that #sha. I found this post is helpful.

git checkout <sha of merge>
git commit --amend # edit message
git rebase HEAD previous_branch


This three lines of code did the job for changing the merge commit message (like author).
    There is a shortcut applicable to the most voted question: using exec instead of edit.
exec allows to run a command on a specified commit.
Using it allows to avoid using edit, exiting to a terminal and running the git command for each git commit.
This is especially helpful if you have to change multiple commits in the history.
The steps are:

perform a rebase to an earlier commit (git rebase -i <earliercommit>)
in the editor that opens up, add a line after each commit line you want to edit and add exec git commit --amend --author=""Author Name <email@address.com>"" --no-edit (or using --reset-author if you want to reset to the value set in the git config)
save and exit - this will run the specified command for each commit, effectively changing the author

Example editor content (to change first 2 commits author):
pick 1fc6c95 Patch A
exec git commit --amend --author=""Author Name <email@address.com>"" --no-edit
pick 6b2481b Patch B
exec git commit --amend --author=""Author Name <email@address.com>"" --no-edit
pick dd1475d something I want to split
pick c619268 A fix for Patch B
pick fa39187 something to add to patch A
pick 4ca2acc i cant' typ goods
pick 7b36971 something to move before patch B

# Rebase 41a72e6..7b36971 onto 41a72e6
#
# Commands:
#  p, pick = use commit
#  r, reword = use commit, but edit the commit message
#  e, edit = use commit, but stop for amending
#  s, squash = use commit, but meld into previous commit
#  f, fixup = like ""squash"", but discard this commit's log message
#  x, exec = run command (the rest of the line) using shell
#
# If you remove a line here THAT COMMIT WILL BE LOST.
# However, if you remove everything, the rebase will be aborted.
#

    There is also a lazy approach to this problem, especially if you have more than one commit that you want to change. In my case, I had a new branch with several commits with a wrong author, so what helped me: 

Go to your original branch:

git checkout develop


Create new branch from it:

git checkout -b myFeature develop 


Merge it without commit info as one commit: 

git merge --no-commit --squash branchWrongAuthor


You might also want to stage changes:

git stage .


Change the name of the author and commit changes:

git commit --amend --author ""New Author Name <New Author Email>"" -m ""new feature added""


And that's it, you can push the changes.

git push


You can delete the branch with a wrong author after that.
    Changing Your Committer Name & Email Globally:

$ git config --global user.name ""John Doe""
$ git config --global user.email ""john@doe.org""


Changing Your Committer Name & Email per Repository:

$ git config user.name ""John Doe""
$ git config user.email ""john@doe.org""


Changing the Author Information Just for the Next Commit:

$ git commit --author=""John Doe <john@doe.org>""


Hint: For other situation and read more information read the post reference.
    If what you need to change is the AUTHOR OF THE LAST commit and no other is using your repository, you may undo your last commit with:

git push -f origin last_commit_hash:branch_name 


change the author name of your commit with:

git commit --amend --author ""type new author here""


Exit the editor that opens and push again your code:

git push

    ","[2632, 4413, 634, 319, 242, 41, 112, 36, 50, 67, 6, 7, 7, 9, 5, 4, 18, 19, 4, 2, 4, 3, 2]",1262625,1078,2010-06-15T04:00:08,2021-11-24 22:36:34Z,
How can I fix 'android.os.NetworkOnMainThreadException'?,"
                
I got an error while running my Android project for RssReader. 

Code:

URL url = new URL(urlToRssFeed);
SAXParserFactory factory = SAXParserFactory.newInstance();
SAXParser parser = factory.newSAXParser();
XMLReader xmlreader = parser.getXMLReader();
RssHandler theRSSHandler = new RssHandler();
xmlreader.setContentHandler(theRSSHandler);
InputSource is = new InputSource(url.openStream());
xmlreader.parse(is);
return theRSSHandler.getFeed();


And it shows the below error:

android.os.NetworkOnMainThreadException


How can I fix this issue?
    NOTE : AsyncTask was deprecated in API level 30.
AsyncTask | Android Developers
This exception is thrown when an application attempts to perform a networking operation on its main thread. Run your code in AsyncTask:
class RetrieveFeedTask extends AsyncTask<String, Void, RSSFeed> {

    private Exception exception;

    protected RSSFeed doInBackground(String... urls) {
        try {
            URL url = new URL(urls[0]);
            SAXParserFactory factory = SAXParserFactory.newInstance();
            SAXParser parser = factory.newSAXParser();
            XMLReader xmlreader = parser.getXMLReader();
            RssHandler theRSSHandler = new RssHandler();
            xmlreader.setContentHandler(theRSSHandler);
            InputSource is = new InputSource(url.openStream());
            xmlreader.parse(is);

            return theRSSHandler.getFeed();
        } catch (Exception e) {
            this.exception = e;

            return null;
        } finally {
            is.close();
        }
    }

    protected void onPostExecute(RSSFeed feed) {
        // TODO: check this.exception
        // TODO: do something with the feed
    }
}

How to execute the task:
In MainActivity.java file you can add this line within your oncreate() method
new RetrieveFeedTask().execute(urlToRssFeed);

Don't forget to add this to AndroidManifest.xml file:
<uses-permission android:name=""android.permission.INTERNET""/>

    You should almost always run network operations on a thread or as an asynchronous task.
But it is possible to remove this restriction and you override the default behavior, if you are willing to accept the consequences.
Add:
StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder().permitAll().build();

StrictMode.setThreadPolicy(policy);

In your class,
and
Add this permission in the Android manifest.xml file:
<uses-permission android:name=""android.permission.INTERNET""/>

Consequences:
Your app will (in areas of spotty Internet connection) become unresponsive and lock up, the user perceives slowness and has to do a force kill, and you risk the activity manager killing your app and telling the user that the app has stopped.
Android has some good tips on good programming practices to design for responsiveness:
NetworkOnMainThreadException | Android Developers
    I solved this problem using a new Thread.

Thread thread = new Thread(new Runnable() {

    @Override
    public void run() {
        try  {
            //Your code goes here
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
});

thread.start(); 

    The accepted answer has some significant downsides. It is not advisable to use AsyncTask for networking unless you really know what you are doing. Some of the down-sides include:

AsyncTask's created as non-static inner classes have an implicit reference to the enclosing Activity object, its context, and the entire View hierarchy created by that activity. This reference prevents the Activity from being garbage collected until the AsyncTask's background work completes. If the user's connection is slow, and/or the download is large, these short-term memory leaks can become a problem - for example, if the orientation changes several times (and you don't cancel the executing tasks), or the user navigates away from the Activity.
AsyncTask has different execution characteristics depending on the platform it executes on: prior to API level 4 AsyncTasks execute serially on a single background thread; from API level 4 through API level 10, AsyncTasks execute on a pool of up to 128 threads; from API level 11 onwards AsyncTask executes serially on a single background thread (unless you use the overloaded executeOnExecutor method and supply an alternative executor). Code that works fine when running serially on ICS may break when executed concurrently on Gingerbread, say if you have inadvertent order-of-execution dependencies.

If you want to avoid short-term memory leaks, have well-defined execution characteristics across all platforms, and have a base to build really robust network handling, you might want to consider:

Using a library that does a nice job of this for you - there's a nice comparison of networking libs in this question, or
Using a Service or IntentService instead, perhaps with a PendingIntent to return the result via the Activity's onActivityResult method.

IntentService approach
Downsides:

More code and complexity than AsyncTask, though not as much as you might think
Will queue requests and run them on a single background thread. You can easily control this by replacing IntentService with an equivalent Service implementation, perhaps like this one.
Um, I can't think of any others right now actually

Upsides:

Avoids the short-term memory leak problem
If your activity restarts while network operations are in-flight it can still receive the result of the download via its onActivityResult method
A better platform than AsyncTask to build and reuse robust networking code. Example: if you need to do an important upload, you could do it from AsyncTask in an Activity, but if the user context-switches out of the app to take a phone call, the system may kill the app before the upload completes. It is less likely to kill an application with an active Service.
If you use your own concurrent version of IntentService (like the one I linked above) you can control the level of concurrency via the Executor.

Implementation summary
You can implement an IntentService to perform downloads on a single background thread quite easily.
Step 1: Create an IntentService to perform the download. You can tell it what to download via Intent extras, and pass it a PendingIntent to use to return the result to the Activity:
import android.app.IntentService;
import android.app.PendingIntent;
import android.content.Intent;
import android.util.Log;

import java.io.InputStream;
import java.net.MalformedURLException;
import java.net.URL;

public class DownloadIntentService extends IntentService {

    private static final String TAG = DownloadIntentService.class.getSimpleName();

    public static final String PENDING_RESULT_EXTRA = ""pending_result"";
    public static final String URL_EXTRA = ""url"";
    public static final String RSS_RESULT_EXTRA = ""url"";

    public static final int RESULT_CODE = 0;
    public static final int INVALID_URL_CODE = 1;
    public static final int ERROR_CODE = 2;

    private IllustrativeRSSParser parser;

    public DownloadIntentService() {
        super(TAG);

        // make one and reuse, in the case where more than one intent is queued
        parser = new IllustrativeRSSParser();
    }

    @Override
    protected void onHandleIntent(Intent intent) {
        PendingIntent reply = intent.getParcelableExtra(PENDING_RESULT_EXTRA);
        InputStream in = null;
        try {
            try {
                URL url = new URL(intent.getStringExtra(URL_EXTRA));
                IllustrativeRSS rss = parser.parse(in = url.openStream());

                Intent result = new Intent();
                result.putExtra(RSS_RESULT_EXTRA, rss);

                reply.send(this, RESULT_CODE, result);
            } catch (MalformedURLException exc) {
                reply.send(INVALID_URL_CODE);
            } catch (Exception exc) {
                // could do better by treating the different sax/xml exceptions individually
                reply.send(ERROR_CODE);
            }
        } catch (PendingIntent.CanceledException exc) {
            Log.i(TAG, ""reply cancelled"", exc);
        }
    }
}

Step 2: Register the service in the manifest:
<service
        android:name="".DownloadIntentService""
        android:exported=""false""/>

Step 3: Invoke the service from the Activity, passing a PendingResult object which the Service will use to return the result:
PendingIntent pendingResult = createPendingResult(
    RSS_DOWNLOAD_REQUEST_CODE, new Intent(), 0);
Intent intent = new Intent(getApplicationContext(), DownloadIntentService.class);
intent.putExtra(DownloadIntentService.URL_EXTRA, URL);
intent.putExtra(DownloadIntentService.PENDING_RESULT_EXTRA, pendingResult);
startService(intent);

Step 4: Handle the result in onActivityResult:
@Override
protected void onActivityResult(int requestCode, int resultCode, Intent data) {
    if (requestCode == RSS_DOWNLOAD_REQUEST_CODE) {
        switch (resultCode) {
            case DownloadIntentService.INVALID_URL_CODE:
                handleInvalidURL();
                break;
            case DownloadIntentService.ERROR_CODE:
                handleError(data);
                break;
            case DownloadIntentService.RESULT_CODE:
                handleRSS(data);
                break;
        }
        handleRSS(data);
    }
    super.onActivityResult(requestCode, resultCode, data);
}

A GitHub project containing a complete working Android Studio/Gradle project is available here.
    Do the network actions on another thread.
For example:
new Thread(new Runnable(){
    @Override
    public void run() {
        // Do network action in this function
    }
}).start();

And add this to file AndroidManifest.xml:
<uses-permission android:name=""android.permission.INTERNET""/>

    There are two solutions of this problem.

Don't use a network call in the main UI thread. Use an async task for that.

Write the below code into your MainActivity file after setContentView(R.layout.activity_main);:
if (android.os.Build.VERSION.SDK_INT > 9) {
StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder().permitAll().build();
StrictMode.setThreadPolicy(policy);
}


And the below import statement into your Java file.
import android.os.StrictMode;

    You disable the strict mode using following code:

if (android.os.Build.VERSION.SDK_INT > 9) {
    StrictMode.ThreadPolicy policy = 
        new StrictMode.ThreadPolicy.Builder().permitAll().build();
    StrictMode.setThreadPolicy(policy);
}


This is not recommended: use the AsyncTask interface.

Full code for both the methods
    Network-based operations cannot be run on the main thread. You need to run all network-based tasks on a child thread or implement AsyncTask.

This is how you run a task in a child thread:

new Thread(new Runnable(){
    @Override
    public void run() {
        try {
            // Your implementation goes here
        } 
        catch (Exception ex) {
            ex.printStackTrace();
        }
    }
}).start();

    You cannot perform network I/O on the UI thread on Honeycomb. Technically, it is possible on earlier versions of Android, but it is a really bad idea as it will cause your app to stop responding, and can result in the OS killing your app for being badly behaved. You'll need to run a background process or use AsyncTask to perform your network transaction on a background thread.

There is an article about Painless Threading on the Android developer site which is a good introduction to this, and it will provide you with a much better depth of an answer than can be realistically provided here.
    Android does not allow to run long-running operations on the main thread.
So just use a different thread and post the result to the main thread when needed.
new Thread(new Runnable() {
        @Override
        public void run() {
            /*
            // Run operation here
            */
            // After getting the result
            runOnUiThread(new Runnable() {
                @Override
                public void run() {
                    // Post the result to the main thread
                }
            });
        }
    }).start();

    
Do not use strictMode (only in debug mode)
Do not change SDK version
Do not use a separate thread


Use Service or AsyncTask

See also StackOverflow question:

android.os.NetworkOnMainThreadException sending an email from Android
     Kotlin 
If you are using Kotlin, you can use a coroutine:
fun doSomeNetworkStuff() {
    GlobalScope.launch(Dispatchers.IO) {
        // ...
    }
}

    This happens in Android 3.0 and above. From Android 3.0 and above, they have restricted using network operations (functions that access the Internet) from running in the main thread/UI thread (what spawns from your on create and on resume methods in the activity).

This is to encourage using separate threads for network operations. See AsyncTask for more details on how to perform network activities the right way.
    The error is due to executing long running operations in main thread,You can easily rectify the problem by using AsynTask or Thread. You can checkout this library AsyncHTTPClient for better handling. 

AsyncHttpClient client = new AsyncHttpClient();
client.get(""http://www.google.com"", new AsyncHttpResponseHandler() {

    @Override
    public void onStart() {
        // Called before a request is started
    }

    @Override
    public void onSuccess(int statusCode, Header[] headers, byte[] response) {
        // Called when response HTTP status is ""200 OK""
    }

    @Override
    public void onFailure(int statusCode, Header[] headers, byte[] errorResponse, Throwable e) {
        // Called when response HTTP status is ""4XX"" (for example, 401, 403, 404)
    }

    @Override
    public void onRetry(int retryNo) {
        // Called when request is retried
    }
});

    Put your code inside:

new Thread(new Runnable(){
    @Override
    public void run() {
        try {
            // Your implementation
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }
    }
}).start();


Or:

class DemoTask extends AsyncTask<Void, Void, Void> {

    protected Void doInBackground(Void... arg0) {
        //Your implementation
    }

    protected void onPostExecute(Void result) {
        // TODO: do something with the feed
    }
}

    The top answer of spektom works perfect.

If you are writing the AsyncTask inline and not extending as a class, and on top of this, if there is a need to get a response out of the AsyncTask, one can use the get() method as below.

RSSFeed feed = new RetreiveFeedTask().execute(urlToRssFeed).get();


(From his example.)
    Using Android Annotations is an option. It will allow you to simply run any method in a background thread:

// normal method
private void normal() {
    doSomething(); // do something in background
}

@Background
protected void doSomething() 
    // run your networking code here
}


Note, that although it provides benefits of simplicity and readability, it has its disadvantages. 
    You should not do any time-consuming task on the main thread (UI thread), like any network operation, file I/O, or SQLite database operations. So for this kind of operation, you should create a worker thread, but the problem is that you can not directly perform any UI related operation from your worker thread. For that, you have to use Handler and pass the Message. 

To simplify all these things, Android provides various ways, like AsyncTask, AsyncTaskLoader, CursorLoader or IntentService. So you can use any of these according to your requirements.
    Google deprecated the Android AsyncTask API in Android 11.
Even if you create a thread class outside the main activity, just by calling it in main, you will get the same error. The calls must be inside a runnable thread, but if you need some asynchronous code to execute on the background or some on post afterwards here, you can check out some alternatives for both Kotlin and Java:
*https://stackoverflow.com/questions/58767733/android-asynctask-api-deprecating-in-android-11-what-are-the-alternatives*

The one that worked for me specifically was an answer by mayank1513 for a Java8 implementation of runnable thread found on the above link. The code is as follows:
new Thread(() -> {
        // do background stuff here
        runOnUiThread(()->{
            // OnPostExecute stuff here
        });
    }).start();

However, you can define the thread first in some part of your code and start it somewhere else like this:
Thread definition
Thread thread = new Thread(() -> {
            // do background stuff here
            runOnUiThread(()->{
                // OnPostExecute stuff here
            });
        });

Thread call
thread.start();

I hope this saves someone the headache of seeing deprecated AsyncTask.
    You are able to move a part of your code into another thread to offload the main thread and avoid getting ANR, NetworkOnMainThreadException, IllegalStateException (e.g., cannot access database on the main thread since it may potentially lock the UI for a long period of time).
There are some approaches that you should choose depends on the situation
Java Thread or Android HandlerThread:

Java threads are one-time use only and die after executing its run method.
HandlerThread is a handy class for starting a new thread that has a looper.

AsyncTask (deprecated in API level 30)

AsyncTask is designed to be a helper class around Thread and Handler and does not constitute a generic threading framework. AsyncTasks should ideally be used for short operations (a few seconds at the most.) If you need to keep threads running for long periods of time, it is highly recommended you use the various APIs provided by the java.util.concurrent package such as Executor, ThreadPoolExecutor and FutureTask.

Since the main thread monopolizes UI components, it is not possible to access to some View, and that is why Handler comes to the rescue
[Executor framework]

ThreadPoolExecutor class that implements ExecutorService which gives fine control on the thread pool (E.g., core pool size, max pool size, keep alive time, etc.)
ScheduledThreadPoolExecutor - a class that extends ThreadPoolExecutor. It can schedule tasks after a given delay or periodically.

FutureTask

FutureTask performs asynchronous processing, however, if the result is not ready yet or processing has not complete, calling get() will be block the thread

AsyncTaskLoaders

AsyncTaskLoaders as they solve a lot of problems that are inherent to AsyncTask

IntentService

This is the de facto choice for long running processing on Android, a good example would be to upload or download large files. The upload and download may continue even if the user exits the app and you certainly do not want to block the user from being able to use the app while these tasks are going on.

JobScheduler

Effectively, you have to create a Service and create a job using JobInfo.Builder that specifies your criteria for when to run the service.

RxJava

Library for composing asynchronous and event-based programs by using observable sequences.

Coroutines (Kotlin)

The main gist of it is, it makes asynchronous code looks so much like synchronous

Read more here, here, here, and here.
    There are many great answers already on this question, but a lot of great libraries have come out since those answers were posted. This is intended as a kind of newbie-guide.
I will cover several use cases for performing network operations and a solution or two for each.
REST over HTTP
Typically JSON, but it can be XML or something else.
Full API Access
Let's say you are writing an app that lets users track stock prices, interest rates and currency exchange rates. You find an JSON API that looks something like this:
http://api.example.com/stocks                       // ResponseWrapper<String> object containing a
                                                    // list of strings with ticker symbols
http://api.example.com/stocks/$symbol               // Stock object
http://api.example.com/stocks/$symbol/prices        // PriceHistory<Stock> object
http://api.example.com/currencies                   // ResponseWrapper<String> object containing a
                                                    // list of currency abbreviation
http://api.example.com/currencies/$currency         // Currency object
http://api.example.com/currencies/$id1/values/$id2  // PriceHistory<Currency> object comparing the prices
                                                    // of the first currency (id1) to the second (id2)

Retrofit from Square
This is an excellent choice for an API with multiple endpoints and allows you to declare the REST endpoints instead of having to code them individually as with other libraries like Amazon Ion Java or Volley (website: Retrofit).
How do you use it with the finances API?
File build.gradle
Add these lines to your module level build.gradle file:
implementation 'com.squareup.retrofit2:retrofit:2.3.0' // Retrofit library, current as of September 21, 2017
implementation 'com.squareup.retrofit2:converter-gson:2.3.0' // Gson serialization and deserialization support for retrofit, version must match retrofit version

File FinancesApi.java
public interface FinancesApi {
    @GET(""stocks"")
    Call<ResponseWrapper<String>> listStocks();
    @GET(""stocks/{symbol}"")
    Call<Stock> getStock(@Path(""symbol"")String tickerSymbol);
    @GET(""stocks/{symbol}/prices"")
    Call<PriceHistory<Stock>> getPriceHistory(@Path(""symbol"")String tickerSymbol);

    @GET(""currencies"")
    Call<ResponseWrapper<String>> listCurrencies();
    @GET(""currencies/{symbol}"")
    Call<Currency> getCurrency(@Path(""symbol"")String currencySymbol);
    @GET(""currencies/{symbol}/values/{compare_symbol}"")
    Call<PriceHistory<Currency>> getComparativeHistory(@Path(""symbol"")String currency, @Path(""compare_symbol"")String currencyToPriceAgainst);
}

Class FinancesApiBuilder
public class FinancesApiBuilder {
    public static FinancesApi build(String baseUrl){
        return new Retrofit.Builder()
                    .baseUrl(baseUrl)
                    .addConverterFactory(GsonConverterFactory.create())
                    .build()
                    .create(FinancesApi.class);
    }
}

Class FinancesFragment snippet
FinancesApi api = FinancesApiBuilder.build(""http://api.example.com/""); //trailing '/' required for predictable behavior
api.getStock(""INTC"").enqueue(new Callback<Stock>(){
    @Override
    public void onResponse(Call<Stock> stockCall, Response<Stock> stockResponse){
        Stock stock = stockCall.body();
        // Do something with the stock
    }
    @Override
    public void onResponse(Call<Stock> stockCall, Throwable t){
        // Something bad happened
    }
}

If your API requires an API key or other header, like a user token, etc. to be sent, Retrofit makes this easy (see this awesome answer to Add Header Parameter in Retrofit for details).
One-off REST API access
Let's say you're building a ""mood weather"" app that looks up the user's GPS location and checks the current temperature in that area and tells them the mood. This type of app doesn't need to declare API endpoints; it just needs to be able to access one API endpoint.
Ion
This is a great library for this type of access.
Please read msysmilu's great answer to How can I fix 'android.os.NetworkOnMainThreadException'?.
Load images via HTTP
Volley
Volley can also be used for REST APIs, but due to the more complicated setup required, I prefer to use Retrofit from Square as above.
Let's say you are building a social networking app and want to load profile pictures of friends.
File build.gradle
Add this line to your module level build.gradle file:
implementation 'com.android.volley:volley:1.0.0'

File ImageFetch.java
Volley requires more setup than Retrofit. You will need to create a class like this to setup a RequestQueue, an ImageLoader and an ImageCache, but it's not too bad:
public class ImageFetch {
    private static ImageLoader imageLoader = null;
    private static RequestQueue imageQueue = null;

    public static ImageLoader getImageLoader(Context ctx){
        if(imageLoader == null){
            if(imageQueue == null){
                imageQueue = Volley.newRequestQueue(ctx.getApplicationContext());
            }
            imageLoader = new ImageLoader(imageQueue, new ImageLoader.ImageCache() {
                Map<String, Bitmap> cache = new HashMap<String, Bitmap>();
                @Override
                public Bitmap getBitmap(String url) {
                    return cache.get(url);
                }
                @Override
                public void putBitmap(String url, Bitmap bitmap) {
                    cache.put(url, bitmap);
                }
            });
        }
        return imageLoader;
    }
}

File user_view_dialog.xml
Add the following to your layout XML file to add an image:
<com.android.volley.toolbox.NetworkImageView
    android:id=""@+id/profile_picture""
    android:layout_width=""32dp""
    android:layout_height=""32dp""
    android:layout_alignParentTop=""true""
    android:layout_centerHorizontal=""true""
    app:srcCompat=""@android:drawable/spinner_background""/>

File UserViewDialog.java
Add the following code to the onCreate method (Fragment, Activity) or the constructor (Dialog):
NetworkImageView profilePicture = view.findViewById(R.id.profile_picture);
profilePicture.setImageUrl(""http://example.com/users/images/profile.jpg"", ImageFetch.getImageLoader(getContext());

Picasso
Picasso is another excellent library from Square. Please see the website for some great examples.
    Use this in Your Activity

    btnsub.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            new Thread(new Runnable() {

                @Override
                public void run() {
                    // TODO Auto-generated method stub

                    //Initialize soap request + add parameters
                    SoapObject request = new SoapObject(NAMESPACE, METHOD_NAME1);

                    //Use this to add parameters
                    request.addProperty(""pincode"", txtpincode.getText().toString());
                    request.addProperty(""bg"", bloodgroup.getSelectedItem().toString());

                    //Declare the version of the SOAP request
                    SoapSerializationEnvelope envelope = new SoapSerializationEnvelope(SoapEnvelope.VER11);

                    envelope.setOutputSoapObject(request);
                    envelope.dotNet = true;

                    try {
                        HttpTransportSE androidHttpTransport = new HttpTransportSE(URL);

                        //this is the actual part that will call the webservice
                        androidHttpTransport.call(SOAP_ACTION1, envelope);

                        // Get the SoapResult from the envelope body.
                        SoapObject result = (SoapObject) envelope.getResponse();
                        Log.e(""result data"", ""data"" + result);
                        SoapObject root = (SoapObject) result.getProperty(0);
                        // SoapObject s_deals = (SoapObject) root.getProperty(0);
                        // SoapObject s_deals_1 = (SoapObject) s_deals.getProperty(0);
                        //

                        System.out.println(""********Count : "" + root.getPropertyCount());

                        value = new ArrayList<Detailinfo>();

                        for (int i = 0; i < root.getPropertyCount(); i++) {
                            SoapObject s_deals = (SoapObject) root.getProperty(i);
                            Detailinfo info = new Detailinfo();

                            info.setFirstName(s_deals.getProperty(""Firstname"").toString());
                            info.setLastName(s_deals.getProperty(""Lastname"").toString());
                            info.setDOB(s_deals.getProperty(""DOB"").toString());
                            info.setGender(s_deals.getProperty(""Gender"").toString());
                            info.setAddress(s_deals.getProperty(""Address"").toString());
                            info.setCity(s_deals.getProperty(""City"").toString());
                            info.setState(s_deals.getProperty(""State"").toString());
                            info.setPinecode(s_deals.getProperty(""Pinecode"").toString());
                            info.setMobile(s_deals.getProperty(""Mobile"").toString());
                            info.setEmail(s_deals.getProperty(""Email"").toString());
                            info.setBloodgroup(s_deals.getProperty(""Bloodgroup"").toString());
                            info.setAdddate(s_deals.getProperty(""Adddate"").toString());
                            info.setWaight(s_deals.getProperty(""waight"").toString());
                            value.add(info);
                        }

                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                    Intent intent = new Intent(getApplicationContext(), ComposeMail.class);
                    //intent.putParcelableArrayListExtra(""valuesList"", value);

                    startActivity(intent);
                }
            }).start();
        }
    });

    This works. I just made Dr.Luiji's answer a little simpler.
new Thread() {
    @Override
    public void run() {
        try {
            //Your code goes here
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}.start();

    From developer-android:

AsyncTasks should ideally be used for short operations (a few seconds at the most.)

Using newCachedThreadPool is the good one. also you can consider other options like newSingleThreadExecutor, newFixedThreadPool
    ExecutorService myExecutor = Executors.newCachedThreadPool();
    myExecutor.execute(new Runnable() {
        @Override
        public void run() {
            URL url = new URL(urls[0]);
            SAXParserFactory factory = SAXParserFactory.newInstance();
            SAXParser parser = factory.newSAXParser();
            XMLReader xmlreader = parser.getXMLReader();
            RssHandler theRSSHandler = new RssHandler();
            xmlreader.setContentHandler(theRSSHandler);
            InputSource is = new InputSource(url.openStream());
            xmlreader.parse(is);
        }
    });


ThreadPoolExecutor is a helper class to make this process easier. This
class manages the creation of a group of threads, sets their
priorities, and manages how work is distributed among those threads.
As workload increases or decreases, the class spins up or destroys
more threads to adjust to the workload.

See this for more information about Android threads.
    On Android, network operations cannot be run on the main thread. You can use Thread, AsyncTask (short-running tasks), Service (long-running tasks) to do network operations. android.os.NetworkOnMainThreadException is thrown when an application attempts to perform a networking operation on its main thread. If your task took above five seconds, it takes a force close.
Run your code in AsyncTask:
class FeedTask extends AsyncTask<String, Void, Boolean> {

    protected RSSFeed doInBackground(String... urls) {
       // TODO: Connect
    }

    protected void onPostExecute(RSSFeed feed) {
        // TODO: Check this.exception
        // TODO: Do something with the feed
    }
}

Or
new Thread(new Runnable(){
    @Override
    public void run() {
        try {
            // Your implementation
        }
        catch (Exception ex) {
            ex.printStackTrace();
        }
    }
}).start();

This is not recommended.
But for debugging purposes, you can disable the strict mode as well using the following code:
if (android.os.Build.VERSION.SDK_INT > 9) {
    StrictMode.ThreadPolicy policy =
        new StrictMode.ThreadPolicy.Builder().permitAll().build();
    StrictMode.setThreadPolicy(policy);
}

    This exception occurs due to any heavy task performed on the main thread if that performing task takes too much time.

To avoid this, we can handle it using threads or executers

Executors.newSingleThreadExecutor().submit(new Runnable() {
    @Override
    public void run() {
        // You can perform your task here.
    }
});

    This is only thrown for applications targeting the Honeycomb SDK or higher. Applications targeting earlier SDK versions are allowed to do networking on their main event loop threads.

The error is the SDK warning!
    For me it was this:

<uses-sdk
        android:minSdkVersion=""8""
        android:targetSdkVersion=""10"" />


The device I was testing my app on was 4.1.2 which is SDK Version 16!

Make the sure the target version is the same as your Android Target Library. If you are unsure what your target library is, right click your Project -> Build Path -> Android, and it should be the one that is ticked.

Also, as others have mentioned, include the correct permissions to access the Internet:

<uses-permission android:name=""android.permission.INTERNET""/>

    The main thread is the UI thread, and you cannot do an operation in the main thread which may block the user interaction. You can solve this in two ways:

Force to do the task in the main thread like this

StrictMode.ThreadPolicy threadPolicy = new StrictMode.ThreadPolicy.Builder().permitAll().build();
StrictMode.setThreadPolicy(threadPolicy);


Or create a simple handler and update the main thread if you want.

Runnable runnable;
Handler newHandler;

newHandler = new Handler();
runnable = new Runnable() {
    @Override
    public void run() {
         try {
            //update UI
        } catch (Exception e) {
            e.printStackTrace();
        } 
    }
};
newHandler.post(runnable);


And to stop the thread use:

newHandler.removeCallbacks(runnable);


For more information check this out: Painless threading
    Just to spell out something explicitly:

The main thread is basically the UI thread.

So saying that you cannot do networking operations in the main thread means you cannot do networking operations in the UI thread, which means you cannot do networking operations in a *runOnUiThread(new Runnable() { ... }* block inside some other thread, either.

(I just had a long head-scratching moment trying to figure out why I was getting that error somewhere other than my main thread.  This was why; this thread helped; and hopefully this comment will help someone else.)
    ","[2625, 2649, 751, 495, 187, 91, 128, 74, 62, 150, 7, 81, 8, 50, 45, 52, 43, 49, 44, 5, 14, 20, 26, 10, 4, 6, 23, 32, 28, 9, 24]",1387558,543,2011-06-14T12:02:00,2022-05-01 09:58:34Z,java 
Can (a== 1 && a ==2 && a==3) ever evaluate to true?,"
                    
            
        
            
                    
                        
                    
                
                    
                        Want to improve this post? Provide detailed answers to this question, including citations and an explanation of why your answer is correct. Answers without enough detail may be edited or deleted.
                        
                    
                
            
        

    


  Moderator note: Please resist the urge to edit the code or remove this notice. The pattern of whitespace may be part of the question and therefore should not be tampered with unnecessarily. If you are in the ""whitespace is insignificant"" camp, you should be able to accept the code as is.


Is it ever possible that (a== 1 && a ==2 && a==3) could evaluate to true in JavaScript?

This is an interview question asked by a major tech company. It happened two weeks back, but I'm still trying to find the answer. I know we never write such code in our day-to-day job, but I'm curious.
    If you take advantage of how == works, you could simply create an object with a custom toString (or valueOf) function that changes what it returns each time it is used such that it satisfies all three conditions.

const a = {
  i: 1,
  toString: function () {
    return a.i++;
  }
}

if(a == 1 && a == 2 && a == 3) {
  console.log('Hello World!');
}




The reason this works is due to the use of the loose equality operator. When using loose equality, if one of the operands is of a different type than the other, the engine will attempt to convert one to the other. In the case of an object on the left and a number on the right, it will attempt to convert the object to a number by first calling valueOf if it is callable, and failing that, it will call toString. I used toString in this case simply because it's what came to mind, valueOf would make more sense. If I instead returned a string from toString, the engine would have then attempted to convert the string to a number giving us the same end result, though with a slightly longer path.
    I couldn't resist - the other answers are undoubtedly true, but you really can't walk past the following code:

var a = 1;
var a = 2;
var a = 3;
if(a==1 && a== 2 &&a==3) {
    console.log(""Why hello there!"")
}


Note the weird spacing in the if statement (that I copied from your question). It is the half-width Hangul (that's Korean for those not familiar) which is an Unicode space character that is not interpreted by ECMA script as a space character - this means that it is a valid character for an identifier. Therefore there are three completely different variables, one with the Hangul after the a, one with it before and the last one with just a. Replacing the space with _ for readability, the same code would look like this:

var a_ = 1;
var a = 2;
var _a = 3;
if(a_==1 && a== 2 &&_a==3) {
    console.log(""Why hello there!"")
}


Check out the validation on Mathias' variable name validator. If that weird spacing was actually included in their question, I feel sure that it's a hint for this kind of answer.

Don't do this. Seriously.

Edit: It has come to my attention that (although not allowed to start a variable) the Zero-width joiner and Zero-width non-joiner characters are also permitted in variable names - see Obfuscating JavaScript with zero-width characters - pros and cons?.

This would look like the following:

var a= 1;
var a= 2; //one zero-width character
var a= 3; //two zero-width characters (or you can use the other one)
if(a==1&&a==2&&a==3) {
    console.log(""Why hello there!"")
}

    Example without getters or valueOf:

a = [1,2,3];
a.join = a.shift;
console.log(a == 1 && a == 2 && a == 3);


This works because == invokes toString which calls .join for Arrays.

Another solution, using Symbol.toPrimitive which is an ES6 equivalent of toString/valueOf: 

let i = 0;
let a = { [Symbol.toPrimitive]: () => ++i };

console.log(a == 1 && a == 2 && a == 3);

    IT IS POSSIBLE!

var i = 0;

with({
  get a() {
    return ++i;
  }
}) {
  if (a == 1 && a == 2 && a == 3)
    console.log(""wohoo"");
}


This uses a getter inside of a with statement to let a evaluate to three different values.

... this still does not mean this should be used in real code...

Even worse, this trick will also work with the use of ===.

  var i = 0;

  with({
    get a() {
      return ++i;
    }
  }) {
    if (a !== a)
      console.log(""yep, this is printed."");
  }

    This is possible in case of variable a being accessed by, say 2 web workers through a SharedArrayBuffer as well as some main script. The possibility is low, but it is possible that when the code is compiled to machine code, the web workers update the variable a just in time so the conditions a==1, a==2 and a==3 are satisfied.

This can be an example of race condition in multi-threaded environment provided by web workers and SharedArrayBuffer in JavaScript.

Here is the basic implementation of above:

main.js

// Main Thread

const worker = new Worker('worker.js')
const modifiers = [new Worker('modifier.js'), new Worker('modifier.js')] // Let's use 2 workers
const sab = new SharedArrayBuffer(1)

modifiers.forEach(m => m.postMessage(sab))
worker.postMessage(sab)


worker.js

let array

Object.defineProperty(self, 'a', {
  get() {
    return array[0]
  }
});

addEventListener('message', ({data}) => {
    array = new Uint8Array(data)
    let count = 0
    do {
        var res = a == 1 && a == 2 && a == 3
        ++count
    } while(res == false) // just for clarity. !res is fine
    console.log(`It happened after ${count} iterations`)
    console.log('You should\'ve never seen this')
})


modifier.js

addEventListener('message' , ({data}) => {
    setInterval( () => {
        new Uint8Array(data)[0] = Math.floor(Math.random()*3) + 1
    })
})


On my MacBook Air, it happens after around 10 billion iterations on the first attempt:



Second attempt:



As I said, the chances will be low, but given enough time, it'll hit the condition.

Tip: If it takes too long on your system. Try only a == 1 && a == 2 and change Math.random()*3 to Math.random()*2. Adding more and more to list drops the chance of hitting.
    If it is asked if it is possible (not MUST), it can ask ""a"" to return a random number. It would be true if it generates 1, 2, and 3 sequentially.

with({
  get a() {
    return Math.floor(Math.random()*4);
  }
}){
  for(var i=0;i<1000;i++){
    if (a == 1 && a == 2 && a == 3){
      console.log(""after "" + (i+1) + "" trials, it becomes true finally!!!"");
      break;
    }
  }
}

    When you can't do anything without regular expressions:

var a = {
  r: /\d/g, 
  valueOf: function(){
    return this.r.exec(123)[0]
  }
}

if (a == 1 && a == 2 && a == 3) {
    console.log(""!"")
}


It works because of custom valueOf method that is called when Object compared with primitive (such as Number). Main trick is that a.valueOf returns new value every time because it's calling exec on regular expression with g flag, which causing updating lastIndex of that regular expression every time match is found. So first time this.r.lastIndex == 0, it matches 1 and updates lastIndex: this.r.lastIndex == 1, so next time regex will match 2 and so on.
    It can be accomplished using the following in the global scope. For nodejs use global instead of window in the code below.

var val = 0;
Object.defineProperty(window, 'a', {
  get: function() {
    return ++val;
  }
});
if (a == 1 && a == 2 && a == 3) {
  console.log('yay');
}


This answer abuses the implicit variables provided by the global scope in the execution context by defining a getter to retrieve the variable.
    Yes, it is possible! 

 JavaScript

if=()=>!0;
var a = 9;

if(a==1 && a== 2 && a==3)
{
    document.write(""<h1>Yes, it is possible!</h1>"")
}


The above code is a short version (thanks to  @Forivin for its note in comments) and the following code is original:

var a = 9;

if(a==1 && a== 2 && a==3)
{
    //console.log(""Yes, it is possible!"")
    document.write(""<h1>Yes, it is possible!</h1>"")
}

//--------------------------------------------

function if(){return true;}



  If you just see top side of my code and run it you say WOW, how?
  
  So I think it is enough to say Yes, it is possible to someone that said to
  you: Nothing is impossible
  
  Trick: I used a hidden character after if to make a function that its name is similar to if. In JavaScript we can not override keywords so I forced to use this way. It is a fake if, but it works for you in this case!




 C#

Also I wrote a C# version (with increase property value technic):

static int _a;
public static int a => ++_a;

public static void Main()
{
    if(a==1 && a==2 && a==3)
    {
        Console.WriteLine(""Yes, it is possible!"");
    }
}


Live Demo
    Alternatively, you could use a class for it and an instance for the check.

function A() {
    var value = 0;
    this.valueOf = function () { return ++value; };
}

var a = new A;

if (a == 1 && a == 2 && a == 3) {
    console.log('bingo!');
}


EDIT

Using ES6 classes it would look like this

class A {
  constructor() {
    this.value = 0;
    this.valueOf();
  }
  valueOf() {
    return this.value++;
  };
}

let a = new A;

if (a == 1 && a == 2 && a == 3) {
  console.log('bingo!');
}

    This is also possible using a series of self-overwriting getters:

(This is similar to jontro's solution, but doesn't require a counter variable.)

(() => {
    ""use strict"";
    Object.defineProperty(this, ""a"", {
        ""get"": () => {
            Object.defineProperty(this, ""a"", {
                ""get"": () => {
                    Object.defineProperty(this, ""a"", {
                        ""get"": () => {
                            return 3;
                        }
                    });
                    return 2;
                },
                configurable: true
            });
            return 1;
        },
        configurable: true
    });
    if (a == 1 && a == 2 && a == 3) {
        document.body.append(""Yes, its possible."");
    }
})();

    I don't see this answer already posted, so I'll throw this one into the mix too. This is similar to Jeff's answer with the half-width Hangul space.

var a = 1;
var  = 2;
var  = 3;
if(a == 1 &&  == 2 &&  == 3) {
    console.log(""Why hello there!"")
}


You might notice a slight discrepancy with the second one, but the first and third are identical to the naked eye. All 3 are distinct characters:

a - Latin lower case A
 - Full Width Latin lower case A
 - Cyrillic lower case A

The generic term for this is ""homoglyphs"": different unicode characters that look the same. Typically hard to get three that are utterly indistinguishable, but in some cases you can get lucky. A, , , and  would work better (Latin-A, Greek Alpha, Cyrillic-A, and Cherokee-A respectively; unfortunately the Greek and Cherokee lower-case letters are too different from the Latin a: ,, and so doesn't help with the above snippet).

There's an entire class of Homoglyph Attacks out there, most commonly in fake domain names (eg. wikipedi.org (Cyrillic) vs wikipedia.org (Latin)), but it can show up in code as well; typically referred to as being underhanded (as mentioned in a comment, [underhanded] questions are now off-topic on PPCG, but used to be a type of challenge where these sorts of things would show up).  I used this website to find the homoglyphs used for this answer.
    JavaScript
a == a +1
In JavaScript, there are no integers but only Numbers, which are implemented as double precision floating point numbers.
It means that if a Number a is large enough, it can be considered equal to three consecutive integers:
a = 100000000000000000
if (a == a+1 && a == a+2 && a == a+3){
  console.log(""Precision loss!"");
}

True, it's not exactly what the interviewer asked (it doesn't work with a=0), but it doesn't involve any trick with hidden functions or operator overloading.
Other languages
For reference, there are a==1 && a==2 && a==3 solutions in Ruby and Python. With a slight modification, it's also possible in Java.
Ruby
With a custom ==:
class A
  def ==(o)
    true
  end
end

a = A.new

if a == 1 && a == 2 && a == 3
  puts ""Don't do this!""
end

Or an increasing a:
def a
  @a ||= 0
  @a += 1
end

if a == 1 && a == 2 && a == 3
  puts ""Don't do this!""
end

Python
You can either define == for a new class:
class A:
    def __eq__(self, who_cares):
        return True
a = A()

if a == 1 and a == 2 and a == 3:
    print(""Don't do that!"")

or, if you're feeling adventurous, redefine the values of integers:
import ctypes

def deref(addr, typ):
    return ctypes.cast(addr, ctypes.POINTER(typ))

deref(id(2), ctypes.c_int)[6] = 1
deref(id(3), ctypes.c_int)[6] = 1
deref(id(4), ctypes.c_int)[6] = 1

print(1 == 2 == 3 == 4)
# True

It might segfault, depending on your system/interpreter.
The python console crashes with the above code, because 2 or 3 are probably used in the background. It works fine if you use less-common integers:
>>> import ctypes
>>> 
>>> def deref(addr, typ):
...     return ctypes.cast(addr, ctypes.POINTER(typ))
... 
>>> deref(id(12), ctypes.c_int)[6] = 11
>>> deref(id(13), ctypes.c_int)[6] = 11
>>> deref(id(14), ctypes.c_int)[6] = 11
>>> 
>>> print(11 == 12 == 13 == 14)
True

Java
It's possible to modify Java Integer cache:
package stackoverflow;

import java.lang.reflect.Field;

public class IntegerMess
{
    public static void main(String[] args) throws Exception {
        Field valueField = Integer.class.getDeclaredField(""value"");
        valueField.setAccessible(true);
        valueField.setInt(1, valueField.getInt(42));
        valueField.setInt(2, valueField.getInt(42));
        valueField.setInt(3, valueField.getInt(42));
        valueField.setAccessible(false);

        Integer a = 42;

        if (a.equals(1) && a.equals(2) && a.equals(3)) {
            System.out.println(""Bad idea."");
        }
    }
}

    This is an inverted version of @Jeff's answer* where a hidden character (U+115F, U+1160 or U+3164) is used to create variables that look like 1, 2 and 3.

var  a = 1;
var 1 = a;
var 2 = a;
var 3 = a;
console.log( a ==1 && a ==2 && a ==3 );


* That answer can be simplified by using zero width non-joiner (U+200C) and zero width joiner (U+200D). Both of these characters are allowed inside identifiers but not at the beginning:

var a = 1;
var a = 2;
var a = 3;
console.log(a == 1 && a == 2 && a == 3);

/****
var a = 1;
var a\u200c = 2;
var a\u200d = 3;
console.log(a == 1 && a\u200c == 2 && a\u200d == 3);
****/


Other tricks are possible using the same idea e.g. by using Unicode variation selectors to create variables that look exactly alike (a = 1; a = 2; a == 1 && a == 2; // true).
    Rule number one of interviews; never say impossible.
No need for hidden character trickery.
window.__defineGetter__( 'a', function(){
    if( typeof i !== 'number' ){
        // define i in the global namespace so that it's not lost after this function runs
        i = 0;
    }
    return ++i;
});

if( a == 1 && a == 2 && a == 3 ){
    console.log( 'Oh dear, what have we done?' );
}

    Honestly though, whether there is a way for it to evaluate to true or not (and as others have shown, there are multiple ways), the answer I'd be looking for, speaking as someone who has conducted hundreds of interviews, would be something along the lines of:

""Well, maybe yes under some weird set of circumstances that aren't immediately obvious to me... but if I encountered this in real code then I would use common debugging techniques to figure out how and why it was doing what it was doing and then immediately refactor the code to avoid that situation... but more importantly: I would absolutely NEVER write that code in the first place because that is the very definition of convoluted code, and I strive to never write convoluted code"".

I guess some interviewers would take offense to having what is obviously meant to be a very tricky question called out, but I don't mind developers who have an opinion, especially when they can back it up with reasoned thought and can dovetail my question into a meaningful statement about themselves.
    If you ever get such an interview question (or notice some equally unexpected behavior in your code) think about what kind of things could possibly cause a behavior that looks impossible at first glance:


Encoding: In this case the variable you are looking at is not the one you think it is. This can happen if you intentionally mess around with Unicode using homoglyphs or space characters to make the name of a variable look like another one, but encoding issues can also be introduced accidentally, e.g. when copying & pasting code from the Web that contains unexpected Unicode code points (e.g. because a content management system  did some ""auto-formatting"" such as replacing fl with Unicode 'LATIN SMALL LIGATURE FL' (U+FB02)).
Race conditions: A race-condition might occur, i.e. a situation where code is not executing in the sequence expected by the developer. Race conditions often happen in multi-threaded code, but multiple threads are not a requirement for race conditions to be possible  asynchronicity is sufficient (and don't get confused, async does not mean multiple threads are used under the hood). 

Note that therefore JavaScript is also not free from race conditions just because it is single-threaded. See here for a simple single-threaded  but async  example. In the context of an single statement the race condition however would be rather hard to hit in JavaScript.

JavaScript with web workers is a bit different, as you can have multiple threads. @mehulmpt has shown us a great proof-of-concept using web workers.
Side-effects:  A side-effect of the equality comparison operation (which doesn't have to be as obvious as in the examples here, often side-effects are very subtle).  


These kind of issues can appear in many programming languages, not only JavaScript, so we aren't seeing one of the classical JavaScript WTFs here1. 

Of course, the interview question and the samples here all look very contrived. But they are a good reminder that:


Side-effects can get really nasty and that a well-designed program should be free from unwanted side-effects.
Multi-threading and mutable state can be problematic.
Not doing character encoding and string processing right can lead to nasty bugs.


1 For example, you can find an example in a totally different programming language (C#) exhibiting a side-effect (an obvious one) here.
    Using Proxies:

var a = new Proxy({ i: 0 }, {
    get: (target, name) => name === Symbol.toPrimitive ? () => ++target.i : target[name],
});
console.log(a == 1 && a == 2 && a == 3);


Proxies basically pretend to be a target object (the first parameter), but intercept operations on the target object (in this case the ""get property"" operation) so that there is an opportunity to do something other than the default object behavior. In this case the ""get property"" action is called on a when == coerces its type in order to compare it to each number. This happens:


We create a target object, { i: 0 }, where the i property is our counter
We create a Proxy for the target object and assign it to a
For each a == comparison, a's type is coerced to a primitive value
This type coercion results in calling a[Symbol.toPrimitive]() internally
The Proxy intercepts getting the a[Symbol.toPrimitive] function using the ""get handler""
The Proxy's ""get handler"" checks that the property being gotten is Symbol.toPrimitive, in which case it increments and then returns the counter from the target object: ++target.i. If a different property is being retrieved, we just fall back to returning the default property value, target[name]


So:

var a = ...; // a.valueOf == target.i == 0
a == 1 && // a == ++target.i == 1
a == 2 && // a == ++target.i == 2
a == 3    // a == ++target.i == 3


As with most of the other answers, this only works with a loose equality check (==), because strict equality checks (===) do not do type coercion that the Proxy can intercept.
    Here's another variation, using an array to pop off whatever values you want.

const a = {
  n: [3,2,1],
  toString: function () {
    return a.n.pop();
  }
}

if(a == 1 && a == 2 && a == 3) {
  console.log('Yes');
}

    Okay, another hack with generators:

const value = function* () {
  let i = 0;
  while(true) yield ++i;
}();

Object.defineProperty(this, 'a', {
  get() {
    return value.next().value;
  }
});

if (a === 1 && a === 2 && a === 3) {
  console.log('yo!');
}

    An ECMAScript6 answer that makes use of Symbols:

const a = {value: 1};
a[Symbol.toPrimitive] = function() { return this.value++ };
console.log((a == 1 && a == 2 && a == 3));


Due to == usage, JavaScript is supposed to coerce a into something close to the second operand (1, 2, 3 in this case). But before JavaScript tries to figure coercing on its own, it tries to call Symbol.toPrimitive. If you provide Symbol.toPrimitive JavaScript would use the value your function returns. If not, JavaScript would call valueOf.
    I think this is the minimal code to implement it:

i=0,a={valueOf:()=>++i}

if (a == 1 && a == 2 && a == 3) {
  console.log('Mind === Blown');
}


Creating a dummy object with a custom valueOf that increments a global variable i on each call. 23 characters!
    Same, but different, but still same (can be ""tested"" multiple times):

const a = { valueOf: () => this.n = (this.n || 0) % 3 + 1}
    
if(a == 1 && a == 2 && a == 3) {
  console.log('Hello World!');
}

if(a == 1 && a == 2 && a == 3) {
  console.log('Hello World!');
}


My idea started from how Number object type equation works.
    This one uses the defineProperty with a nice side-effect causing global variable!

var _a = 1

Object.defineProperty(this, ""a"", {
  ""get"": () => {
    return _a++;
  },
  configurable: true
});

console.log(a)
console.log(a)
console.log(a)

    Actually the answer to the first part of the question is ""Yes"" in every programming language. For example, this is in the case of C/C++:

#define a   (b++)
int b = 1;
if (a ==1 && a== 2 && a==3) {
    std::cout << ""Yes, it's possible!"" << std::endl;
} else {
    std::cout << ""it's impossible!"" << std::endl;
}

    By overriding valueOf in a class declaration, it can be done:

class Thing {
    constructor() {
        this.value = 1;
    }

    valueOf() {
        return this.value++;
    }
}

const a = new Thing();

if(a == 1 && a == 2 && a == 3) {
    console.log(a);
}


What happens is that valueOf is called in each comparison operator.  On the first one, a will equal 1, on the second, a will equal 2, and so on and so forth, because each time valueOf is called, the value of a is incremented.

Therefore the console.log will fire and output (in my terminal anyways) Thing: { value: 4}, indicating the conditional was true.
    As we already know that the secret of loose equality operator (==) will try to convert both values to a common type. As a result, some functions will be invoked.

ToPrimitive(A) attempts to convert its object argument to a primitive
value, by invoking varying sequences of A.toString and A.valueOf
methods on A.

So as other answers using Symbol.toPrimitive, .toString, .valueOf from integer. I would suggest the solution using an array with Array.pop like this.
let a = { array: [3, 2, 1], toString: () => a.array.pop() };

if(a == 1 && a == 2 && a == 3) {
  console.log('Hello World!');
}

In this way, we can work with text like this
let a = { array: [""World"", ""Hello""], toString: () => a.array.pop() };

if(a == ""Hello"" && a == ""World"") {
  console.log('Hello World!');
}

    ","[2607, 3435, 2116, 552, 638, 202, 270, 216, 196, 116, 135, 148, 131, 97, 81, 75, 67, 43, 30, 41, 32, 25, 24, 26, 14, 26, 3, -1]",394669,917,2018-01-15T20:20:47,2022-02-06 10:24:36Z,javascript 
How do I parse a string to a float or int?,"
                

How can I convert a str to float?
""545.2222""    545.2222


How can I convert a str to int?
""31""          31



    >>> a = ""545.2222""
>>> float(a)
545.22220000000004
>>> int(float(a))
545

    Python method to check if a string is a float:

def is_float(value):
  try:
    float(value)
    return True
  except:
    return False


A longer and more accurate name for this function could be: is_convertible_to_float(value)

What is, and is not a float in Python may surprise you:

val                   is_float(val) Note
--------------------  ----------   --------------------------------
""""                    False        Blank string
""127""                 True         Passed string
True                  True         Pure sweet Truth
""True""                False        Vile contemptible lie
False                 True         So false it becomes true
""123.456""             True         Decimal
""      -127    ""      True         Spaces trimmed
""\t\n12\r\n""          True         whitespace ignored
""NaN""                 True         Not a number
""NaNanananaBATMAN""    False        I am Batman
""-iNF""                True         Negative infinity
""123.E4""              True         Exponential notation
"".1""                  True         mantissa only
""1,234""               False        Commas gtfo
u'\x30'               True         Unicode is fine.
""NULL""                False        Null is not special
0x3fade               True         Hexadecimal
""6e7777777777777""     True         Shrunk to infinity
""1.797693e+308""       True         This is max value
""infinity""            True         Same as inf
""infinityandBEYOND""   False        Extra characters wreck it
""12.34.56""            False        Only one dot allowed
u''                 False        Japanese '4' is not a float.
""#56""                 False        Pound sign
""56%""                 False        Percent of what?
""0E0""                 True         Exponential, move dot 0 places
0**0                  True         0___0  Exponentiation
""-5e-5""               True         Raise to a negative number
""+1e1""                True         Plus is OK with exponent
""+1e1^5""              False        Fancy exponent not interpreted
""+1e1.3""              False        No decimals in exponent
""-+1""                 False        Make up your mind
""(1)""                 False        Parenthesis is bad


You think you know what numbers are? You are not so good as you think! Not big surprise.

Don't use this code on life-critical software!

Catching broad exceptions this way, killing canaries and gobbling the exception creates a tiny chance that a valid float as string will return false.  The float(...) line of code can failed for any of a thousand reasons that have nothing to do with the contents of the string.  But if you're writing life-critical software in a duck-typing prototype language like Python, then you've got much larger problems.
    def num(s):
    try:
        return int(s)
    except ValueError:
        return float(s)

    This is another method which deserves to be mentioned here, ast.literal_eval:


  This can be used for safely evaluating strings containing Python expressions from untrusted sources without the need to parse the values oneself.


That is, a safe 'eval'

>>> import ast
>>> ast.literal_eval(""545.2222"")
545.2222
>>> ast.literal_eval(""31"")
31

    Localization and commas

You should consider the possibility of commas in the string representation of a number, for cases like  float(""545,545.2222"") which throws an exception. Instead, use methods in locale to convert the strings to numbers and interpret commas correctly. The locale.atof method converts to a float in one step once the locale has been set for the desired number convention.

Example 1 -- United States number conventions 

In the United States and the UK, commas can be used as a thousands separator.  In this example with American locale, the comma is handled properly as a separator:

>>> import locale
>>> a = u'545,545.2222'
>>> locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
'en_US.UTF-8'
>>> locale.atof(a)
545545.2222
>>> int(locale.atof(a))
545545
>>>


Example 2 -- European number conventions

In the majority of countries of the world,  commas are used for decimal marks instead of periods.  In this example with French locale, the comma is correctly handled as a decimal mark:

>>> import locale
>>> b = u'545,2222'
>>> locale.setlocale(locale.LC_ALL, 'fr_FR')
'fr_FR'
>>> locale.atof(b)
545.2222


The method locale.atoi is also available, but the argument should be an integer.
    a = int(float(a)) if int(float(a)) == float(a) else float(a)

    If you aren't averse to third-party modules, you could check out the fastnumbers module. It provides a function called fast_real that does exactly what this question is asking for and does it faster than a pure-Python implementation:

>>> from fastnumbers import fast_real
>>> fast_real(""545.2222"")
545.2222
>>> type(fast_real(""545.2222""))
float
>>> fast_real(""31"")
31
>>> type(fast_real(""31""))
int

    
  In Python, how can I parse a numeric string like ""545.2222"" to its corresponding float value, 542.2222? Or parse the string ""31"" to an integer, 31?
  I just want to know how to parse a float string to a float, and (separately) an int string to an int.


It's good that you ask to do these separately. If you're mixing them, you may be setting yourself up for problems later. The simple answer is:

""545.2222"" to float:

>>> float(""545.2222"")
545.2222


""31"" to an integer:

>>> int(""31"")
31


Other conversions, ints to and from strings and literals:

Conversions from various bases, and you should know the base in advance (10 is the default). Note you can prefix them with what Python expects for its literals (see below) or remove the prefix:

>>> int(""0b11111"", 2)
31
>>> int(""11111"", 2)
31
>>> int('0o37', 8)
31
>>> int('37', 8)
31
>>> int('0x1f', 16)
31
>>> int('1f', 16)
31


If you don't know the base in advance, but you do know they will have the correct prefix, Python can infer this for you if you pass 0 as the base:

>>> int(""0b11111"", 0)
31
>>> int('0o37', 0)
31
>>> int('0x1f', 0)
31


Non-Decimal (i.e. Integer) Literals from other Bases

If your motivation is to have your own code clearly represent hard-coded specific values, however, you may not need to convert from the bases - you can let Python do it for you automatically with the correct syntax.

You can use the apropos prefixes to get automatic conversion to integers with the following literals. These are valid for Python 2 and 3:

Binary, prefix 0b

>>> 0b11111
31


Octal, prefix 0o

>>> 0o37
31


Hexadecimal, prefix 0x

>>> 0x1f
31


This can be useful when describing binary flags, file permissions in code, or hex values for colors - for example, note no quotes:

>>> 0b10101 # binary flags
21
>>> 0o755 # read, write, execute perms for owner, read & ex for group & others
493
>>> 0xffffff # the color, white, max values for red, green, and blue
16777215


Making ambiguous Python 2 octals compatible with Python 3

If you see an integer that starts with a 0, in Python 2, this is (deprecated) octal syntax.

>>> 037
31


It is bad because it looks like the value should be 37. So in Python 3, it now raises a SyntaxError:

>>> 037
  File ""<stdin>"", line 1
    037
      ^
SyntaxError: invalid token


Convert your Python 2 octals to octals that work in both 2 and 3 with the 0o prefix:

>>> 0o37
31

    You could use json.loads:
>>> import json
>>> json.loads('123.456')
123.456
>>> type(_)
<class 'float'>
>>> 

As you can see it becomes a type of float.
    Users codelogic and harley are correct, but keep in mind if you know the string is an integer (for example, 545) you can call int(""545"") without first casting to float.

If your strings are in a list, you could use the map function as well. 

>>> x = [""545.0"", ""545.6"", ""999.2""]
>>> map(float, x)
[545.0, 545.60000000000002, 999.20000000000005]
>>>


It is only good if they're all the same type.
    float(""545.2222"") and int(float(""545.2222""))
    Handles hex, octal, binary, decimal, and float

This solution will handle all of the string conventions for numbers (all that I know about).

def to_number(n):
    ''' Convert any number representation to a number 
    This covers: float, decimal, hex, and octal numbers.
    '''

    try:
        return int(str(n), 0)
    except:
        try:
            # python 3 doesn't accept ""010"" as a valid octal.  You must use the
            # '0o' prefix
            return int('0o' + n, 0)
        except:
            return float(n)


This test case output illustrates what I'm talking about.

======================== CAPTURED OUTPUT =========================
to_number(3735928559)   = 3735928559 == 3735928559
to_number(""0xFEEDFACE"") = 4277009102 == 4277009102
to_number(""0x0"")        =          0 ==          0
to_number(100)          =        100 ==        100
to_number(""42"")         =         42 ==         42
to_number(8)            =          8 ==          8
to_number(""0o20"")       =         16 ==         16
to_number(""020"")        =         16 ==         16
to_number(3.14)         =       3.14 ==       3.14
to_number(""2.72"")       =       2.72 ==       2.72
to_number(""1e3"")        =     1000.0 ==       1000
to_number(0.001)        =      0.001 ==      0.001
to_number(""0xA"")        =         10 ==         10
to_number(""012"")        =         10 ==         10
to_number(""0o12"")       =         10 ==         10
to_number(""0b01010"")    =         10 ==         10
to_number(""10"")         =         10 ==         10
to_number(""10.0"")       =       10.0 ==         10
to_number(""1e1"")        =       10.0 ==         10


Here is the test:

class test_to_number(unittest.TestCase):

    def test_hex(self):
        # All of the following should be converted to an integer
        #
        values = [

                 #          HEX
                 # ----------------------
                 # Input     |   Expected
                 # ----------------------
                (0xDEADBEEF  , 3735928559), # Hex
                (""0xFEEDFACE"", 4277009102), # Hex
                (""0x0""       ,          0), # Hex

                 #        Decimals
                 # ----------------------
                 # Input     |   Expected
                 # ----------------------
                (100         ,        100), # Decimal
                (""42""        ,         42), # Decimal
            ]



        values += [
                 #        Octals
                 # ----------------------
                 # Input     |   Expected
                 # ----------------------
                (0o10        ,          8), # Octal
                (""0o20""      ,         16), # Octal
                (""020""       ,         16), # Octal
            ]


        values += [
                 #        Floats
                 # ----------------------
                 # Input     |   Expected
                 # ----------------------
                (3.14        ,       3.14), # Float
                (""2.72""      ,       2.72), # Float
                (""1e3""       ,       1000), # Float
                (1e-3        ,      0.001), # Float
            ]

        values += [
                 #        All ints
                 # ----------------------
                 # Input     |   Expected
                 # ----------------------
                (""0xA""       ,         10), 
                (""012""       ,         10), 
                (""0o12""      ,         10), 
                (""0b01010""   ,         10), 
                (""10""        ,         10), 
                (""10.0""      ,         10), 
                (""1e1""       ,         10), 
            ]

        for _input, expected in values:
            value = to_number(_input)

            if isinstance(_input, str):
                cmd = 'to_number(""{}"")'.format(_input)
            else:
                cmd = 'to_number({})'.format(_input)

            print(""{:23} = {:10} == {:10}"".format(cmd, value, expected))
            self.assertEqual(value, expected)

    float(x) if '.' in x else int(x)

    Pass your string to this function: 

def string_to_number(str):
  if(""."" in str):
    try:
      res = float(str)
    except:
      res = str  
  elif(str.isdigit()):
    res = int(str)
  else:
    res = str
  return(res)


It will return int, float or string depending on what was passed. 

string that is an int

print(type(string_to_number(""124"")))
<class 'int'>


string that is a float

print(type(string_to_number(""12.4"")))
<class 'float'>


string that is a string

print(type(string_to_number(""hello"")))
<class 'str'>


string that looks like a float

print(type(string_to_number(""hel.lo"")))
<class 'str'>

    To typecast in Python use the constructor functions of the type, passing the string (or whatever value you are trying to cast) as a parameter.
For example:
>>>float(""23.333"")
   23.333

Behind the scenes, Python is calling the objects __float__ method, which should return a float representation of the parameter. This is especially powerful, as you can define your own types (using classes) with a __float__ method so that it can be casted into a float using float(myobject).
    I use this function for that

import ast

def parse_str(s):
   try:
      return ast.literal_eval(str(s))
   except:
      return


It will convert the string to its type

value = parse_str('1')  # Returns Integer
value = parse_str('1.5')  # Returns Float

    The YAML parser can help you figure out what datatype your string is. Use yaml.load(), and then you can use type(result) to test for type:

>>> import yaml

>>> a = ""545.2222""
>>> result = yaml.load(a)
>>> result
545.22220000000004
>>> type(result)
<type 'float'>

>>> b = ""31""
>>> result = yaml.load(b)
>>> result
31
>>> type(result)
<type 'int'>

>>> c = ""HI""
>>> result = yaml.load(c)
>>> result
'HI'
>>> type(result)
<type 'str'>

    The question seems a little bit old. But let me suggest a function, parseStr, which makes something similar, that is, returns integer or float and if a given ASCII string cannot be converted to none of them it returns it untouched. The code of course might be adjusted to do only what you want:

   >>> import string
   >>> parseStr = lambda x: x.isalpha() and x or x.isdigit() and \
   ...                      int(x) or x.isalnum() and x or \
   ...                      len(set(string.punctuation).intersection(x)) == 1 and \
   ...                      x.count('.') == 1 and float(x) or x
   >>> parseStr('123')
   123
   >>> parseStr('123.3')
   123.3
   >>> parseStr('3HC1')
   '3HC1'
   >>> parseStr('12.e5')
   1200000.0
   >>> parseStr('12$5')
   '12$5'
   >>> parseStr('12.2.2')
   '12.2.2'

    this is an old question and got already a lot of answers.
but if you are dealing with mixed integers and floats and want a consistent way to deal with your mixed data, here is my solution with the proper docstring:
def parse_num(candidate):
    """"""parse string to number if possible
    work equally well with negative and positive numbers, integers and floats.

    Args:
        candidate (str): string to convert

    Returns:
        float | int | None: float or int if possible otherwise None
    """"""
    try:
        float_value = float(candidate)
    except ValueError:
        return None

    # optional part if you prefer int to float when decimal part is 0 
    if float_value.is_integer():
        return int(float_value)
    # end of the optional part

    return float_value

# test
candidates = ['34.77', '-13', 'jh', '8990', '76_3234_54']
res_list = list(map(parse_num, candidates))
print('Before:')
print(candidates)
print('After:')
print(res_list)

output:
Before:
['34.77', '-13', 'jh', '8990', '76_3234_54']
After:
[34.77, -13, None, 8990, 76323454]

    I am surprised nobody mentioned regex because sometimes string must be prepared and normalized before casting to number

import re
def parseNumber(value, as_int=False):
    try:
        number = float(re.sub('[^.\-\d]', '', value))
        if as_int:
            return int(number + 0.5)
        else:
            return number
    except ValueError:
        return float('nan')  # or None if you wish


usage:

parseNumber('13,345')
> 13345.0

parseNumber('- 123 000')
> -123000.0

parseNumber('99999\n')
> 99999.0


and by the way, something to verify you have a number:

import numbers
def is_number(value):
    return isinstance(value, numbers.Number)
    # will work with int, float, long, Decimal

    def num(s):
    """"""num(s)
    num(3),num(3.7)-->3
    num('3')-->3, num('3.7')-->3.7
    num('3,700')-->ValueError
    num('3a'),num('a3'),-->ValueError
    num('3e4') --> 30000.0
    """"""
    try:
        return int(s)
    except ValueError:
        try:
            return float(s)
        except ValueError:
            raise ValueError('argument is not a string of number')

    def get_int_or_float(v):
    number_as_float = float(v)
    number_as_int = int(number_as_float)
    return number_as_int if number_as_float == number_as_int else number_as_float

    This is a corrected version of 
https://stackoverflow.com/a/33017514/5973334

This will try to parse a string and return either int or float depending on what the string represents.
It might rise parsing exceptions or have some unexpected behaviour.

  def get_int_or_float(v):
        number_as_float = float(v)
        number_as_int = int(number_as_float)
        return number_as_int if number_as_float == number_as_int else 
        number_as_float

    Use:

def num(s):
    try:
        for each in s:
            yield int(each)
    except ValueError:
        yield float(each)
a = num([""123.55"",""345"",""44""])
print a.next()
print a.next()


This is the most Pythonic way I could come up with. 
    Use:

>>> str_float = ""545.2222""
>>> float(str_float)
545.2222
>>> type(_) # Check its type
<type 'float'>

>>> str_int = ""31""
>>> int(str_int)
31
>>> type(_) # Check its type
<type 'int'>

    You need to take into account rounding to do this properly.
i.e. - int(5.1) => 5
int(5.6) => 5  -- wrong, should be 6 so we do int(5.6 + 0.5) => 6
def convert(n):
    try:
        return int(n)
    except ValueError:
        return float(n + 0.5)

    By using int and float methods we can convert a string to integer and floats.

s=""45.8""
print(float(s))

y='67'
print(int(y))

    Here's another interpretation of your question (hint: it's vague). It's possible you're looking for something like this:

def parseIntOrFloat( aString ):
    return eval( aString )


It works like this...

>>> parseIntOrFloat(""545.2222"")
545.22220000000004
>>> parseIntOrFloat(""545"")
545




Theoretically, there's an injection vulnerability. The string could, for example be ""import os; os.abort()"". Without any background on where the string comes from, however, the possibility is theoretical speculation.  Since the question is vague, it's not at all clear if this vulnerability actually exists or not.
    This is a function which will convert any object (not just str) to int or float, based on if the actual string supplied looks like int or float. Further if it's an object which has both __float and __int__ methods, it defaults to using __float__

def conv_to_num(x, num_type='asis'):
    '''Converts an object to a number if possible.
    num_type: int, float, 'asis'
    Defaults to floating point in case of ambiguity.
    '''
    import numbers

    is_num, is_str, is_other = [False]*3

    if isinstance(x, numbers.Number):
        is_num = True
    elif isinstance(x, str):
        is_str = True

    is_other = not any([is_num, is_str])

    if is_num:
        res = x
    elif is_str:
        is_float, is_int, is_char = [False]*3
        try:
            res = float(x)
            if '.' in x:
                is_float = True
            else:
                is_int = True
        except ValueError:
            res = x
            is_char = True

    else:
        if num_type == 'asis':
            funcs = [int, float]
        else:
            funcs = [num_type]

        for func in funcs:
            try:
                res = func(x)
                break
            except TypeError:
                continue
        else:
            res = x

    for number and char together :
string_for_int = ""498 results should get""
string_for_float = ""498.45645765 results should get""

first import re:
 import re

 #for get integer part:
 print(int(re.search(r'\d+', string_for_int).group())) #498

 #for get float part:
 print(float(re.search(r'\d+\.\d+', string_for_float).group())) #498.45645765

for easy model :
value1 = ""10""
value2 = ""10.2""
print(int(value1)) #10
print(float(value2)) #10.2

    ","[2597, 2980, 578, 567, 154, 82, 6, 32, 26, 4, 27, 18, 5, 81, 6, 6, 13, 15, 21, 1, 6, 9, 11, 3, 1, 0, 6, -1, -12, 0, 0]",4511974,296,2008-12-19T01:52:26,2022-03-28 12:09:09Z,python 
Does a finally block always get executed in Java?,"
                
Considering this code, can I be absolutely sure that the finally block always executes, no matter what something() is?

try {  
    something();  
    return success;  
}  
catch (Exception e) {   
    return failure;  
}  
finally {  
    System.out.println(""I don't know if this will get printed out"");
}

    Yes, finally will be called after the execution of the try or catch code blocks.

The only times finally won't be called are:


If you invoke System.exit()
If you invoke Runtime.getRuntime().halt(exitStatus)
If the JVM crashes first
If the JVM reaches an infinite loop (or some other non-interruptable, non-terminating statement) in the try or catch block
If the OS forcibly terminates the JVM process; e.g., kill -9 <pid> on UNIX
If the host system dies; e.g., power failure, hardware error, OS panic, et cetera
If the finally block is going to be executed by a daemon thread and all other non-daemon threads exit before finally is called

    Example code:
public static void main(String[] args) {
    System.out.println(Test.test());
}

public static int test() {
    try {
        return 0;
    }
    finally {
        System.out.println(""something is printed"");
    }
}

Output:
something is printed. 
0

    Also, although it's bad practice, if there is a return statement within the finally block, it will trump any other return from the regular block. That is, the following block would return false:

try { return true; } finally { return false; }


Same thing with throwing exceptions from the finally block.
    In addition to the other responses, it is important to point out that 'finally' has the right to override any exception/returned value by the try..catch block. For example, the following code returns 12:

public static int getMonthsInYear() {
    try {
        return 10;
    }
    finally {
        return 12;
    }
}


Similarly, the following method does not throw an exception:

public static int getMonthsInYear() {
    try {
        throw new RuntimeException();
    }
    finally {
        return 12;
    }
}


While the following method does throw it:

public static int getMonthsInYear() {
    try {
        return 12;          
    }
    finally {
        throw new RuntimeException();
    }
}

    Here's an elaboration of Kevin's answer. It's important to know that the expression to be returned is evaluated before finally, even if it is returned after.

public static void main(String[] args) {
    System.out.println(Test.test());
}

public static int printX() {
    System.out.println(""X"");
    return 0;
}

public static int test() {
    try {
        return printX();
    }
    finally {
        System.out.println(""finally trumps return... sort of"");
    }
}


Output:

X
finally trumps return... sort of
0

    I tried the above example with slight modification-

public static void main(final String[] args) {
    System.out.println(test());
}

public static int test() {
    int i = 0;
    try {
        i = 2;
        return i;
    } finally {
        i = 12;
        System.out.println(""finally trumps return."");
    }
}


The above code outputs:


  finally trumps return.
  2


This is because when return i; is executed i has a value 2. After this the finally block is executed where 12 is assigned to i and then System.out out is executed.

After executing the finally block the try block returns 2, rather than returning 12, because this return statement is not executed again.

If you will debug this code in Eclipse then you'll get a feeling that after executing System.out of finally block the return statement of try block is executed again. But this is not the case. It simply returns the value 2.
    Here's the official words from the Java Language Specification.

14.20.2. Execution of try-finally and try-catch-finally


  A try statement with a finally block is executed by first executing the try block. Then there is a choice:
  
  
  If execution of the try block completes normally, [...]
  If execution of the try block completes abruptly because of a throw of a value V, [...]
  If execution of the try block completes abruptly for any other reason R, then the finally block is executed. Then there is a choice:
  
  
  If the finally block completes normally, then the try statement completes abruptly for reason R. 
  If the finally block completes abruptly for reason S, then the try statement completes abruptly for reason S (and reason R is discarded).
  
  


The specification for return actually makes this explicit:

JLS 14.17 The return Statement


ReturnStatement:
     return Expression(opt) ;

  
  A return statement with no Expression attempts to transfer control to the invoker of the method or constructor that contains it. 
  
  A return statement with an Expression attempts to transfer control to the invoker of the method that contains it; the value of the Expression becomes the value of the method invocation.
  
  The preceding descriptions say ""attempts to transfer control"" rather than just ""transfers control"" because if there are any try statements within the method or constructor whose try blocks contain the return statement, then any finally clauses of those try statements will be executed, in order, innermost to outermost, before control is transferred to the invoker of the method or constructor. Abrupt completion of a finally clause can disrupt the transfer of control initiated by a return statement.

    finally block is always executed and before returning x's (calculated) value.
System.out.println(""x value from foo() = "" + foo());

...

int foo() {
  int x = 2;
  try {
    return x++;
  } finally {
    System.out.println(""x value in finally = "" + x);
  }
}

Output:

x value in finally = 3
x value from foo() = 2

    A logical way to think about this is:


Code placed in a finally block must be executed whatever occurs within the try block
So if code in the try block tries to return a value or throw an exception the item is placed 'on the shelf' till the finally block can execute
Because code in the finally block has (by definition) a high priority it can return or throw whatever it likes. In which case anything left 'on the shelf' is discarded.
The only exception to this is if the VM shuts down completely during the try block e.g. by 'System.exit'

    No, not always one exception case is//
System.exit(0);
before the finally block prevents finally to be  executed.

  class A {
    public static void main(String args[]){
        DataInputStream cin = new DataInputStream(System.in);
        try{
            int i=Integer.parseInt(cin.readLine());
        }catch(ArithmeticException e){
        }catch(Exception e){
           System.exit(0);//Program terminates before executing finally block
        }finally{
            System.out.println(""Won't be executed"");
            System.out.println(""No error"");
        }
    }
}

    NOT ALWAYS

The Java Language specification describes how try-catch-finally and try-catch blocks work at 14.20.2
In no place it specifies that the finally block is always executed.
But for all cases in which the try-catch-finally and try-finally blocks complete it does specify that before completion finally must be executed.

try {
  CODE inside the try block
}
finally {
  FIN code inside finally block
}
NEXT code executed after the try-finally block (may be in a different method).


The JLS does not guarantee that FIN is executed after CODE.
The JLS guarantees that if CODE and NEXT are executed then FIN will always be executed after CODE and before NEXT.

Why doesn't the JLS guarantee that the finally block is always executed after the try block? Because it is impossible. It is unlikely but possible that the JVM will be aborted (kill, crash, power off) just after completing the try block but before execution of the finally block. There is nothing the JLS can do to avoid this.

Thus, any software which for their proper behaviour depends on finally blocks always being executed after their try blocks complete are bugged.

return instructions in the try block are irrelevant to this issue. If execution reaches code after the try-catch-finally it is guaranteed that the finally block will have been executed before, with or without return instructions inside the try block.
    finally is always executed unless there is abnormal program termination (like calling System.exit(0)..). so, your sysout will get printed
    Yes, it will. No matter what happens in your try or catch block unless otherwise System.exit() called or JVM crashed. if there is any return statement in the block(s),finally will be executed prior to that return statement.
    The finally block is always executed unless there is abnormal program termination, either resulting from a JVM crash or from a call to System.exit(0).

On top of that, any value returned from within the finally block will override the value returned prior to execution of the finally block, so be careful of checking all exit points when using try finally.
    I was very confused with all the answers provided on different forums and decided to finally code and see. The ouput is :
finally will be executed even if there is return in try and catch block.
try {  
  System.out.println(""try""); 
  return;
  //int  i =5/0;
  //System.exit(0 ) ;
} catch (Exception e) {   
  System.out.println(""catch"");
  return;
  //int  i =5/0;
  //System.exit(0 ) ;
} finally {  
   System.out.println(""Print me FINALLY"");
}

Output

try
Print me FINALLY


If return is replaced by System.exit(0) in try and catch block in above code and an exception occurs before it,for any reason.

    Concisely, in the official Java Documentation (Click here), it is written that - 


  If the JVM exits while the try or catch code is being executed, then
  the finally block may not execute. Likewise, if the thread executing
  the try or catch code is interrupted or killed, the finally block may
  not execute even though the application as a whole continues.

    finally will execute and that is for sure.

finally will not execute in below cases: 

case 1 :

When you are executing System.exit().

case 2 :

When your JVM / Thread crashes.

case 3 : 

When your execution is stopped in between manually.
    Yes it will get called. That's the whole point of having a finally keyword. If jumping out of the try/catch block could just skip the finally block it was the same as putting the System.out.println outside the try/catch.
    Consider the following program:

public class SomeTest {

    private static StringBuilder sb = new StringBuilder();

    public static void main(String args[]) {

        System.out.println(someString());
        System.out.println(""---AGAIN---"");
        System.out.println(someString());
        System.out.println(""---PRINT THE RESULT---"");
        System.out.println(sb.toString());
    }

    private static String someString() {

        try {
            sb.append(""-abc-"");
            return sb.toString();

        } finally {
            sb.append(""xyz"");
        }
    }
}


As of Java 1.8.162, the above code block gives the following output:

-abc-
---AGAIN---
-abc-xyz-abc-
---PRINT THE RESULT---
-abc-xyz-abc-xyz


this means that using finally to free up objects is a good practice like the following code:

private static String someString() {

    StringBuilder sb = new StringBuilder();

    try {
        sb.append(""abc"");
        return sb.toString();

    } finally {
        sb = null; // Just an example, but you can close streams or DB connections this way.
    }
}

    
Finally  Block always get executed. Unless and until
System.exit() statement exists there (first statement in finally block).
If system.exit() is first statement then finally block won't get executed and control come out of the finally block.
Whenever System.exit() statement gets in finally block till that statement finally block executed and when System.exit() appears then control force fully come out of the finally block.

    try- catch- finally are the key words for using exception handling case. 
As normal explanotory

try {
     //code statements
     //exception thrown here
     //lines not reached if exception thrown
} catch (Exception e) {
    //lines reached only when exception is thrown
} finally {
    // always executed when the try block is exited
    //independent of an exception thrown or not
}


The finally block  prevent executing...


When you called System.exit(0);
If JVM exits.
Errors in the JVM

    Yes It will.
Only case it will not is JVM exits or crashes 
    
  Answer is simple YES.


INPUT:

try{
    int divideByZeroException = 5 / 0;
} catch (Exception e){
    System.out.println(""catch"");
    return;    // also tried with break; in switch-case, got same output
} finally {
    System.out.println(""finally"");
}


OUTPUT:

catch
finally

    I tried this,
It is single threaded.

public static void main(String args[]) throws Exception {
    Object obj = new Object();
    try {
        synchronized (obj) {
            obj.wait();
            System.out.println(""after wait()"");
        }
    } catch (Exception ignored) {
    } finally {
        System.out.println(""finally"");
    }
}


The main Thread will be on wait state forever, hence finally will never be called,

so console output will not print String: after wait() or finally

Agreed with @Stephen C, the above example is one of the 3rd case mention here:

Adding some more such infinite loop possibilities in following code:

// import java.util.concurrent.Semaphore;

public static void main(String[] args) {
    try {
        // Thread.sleep(Long.MAX_VALUE);
        // Thread.currentThread().join();
        // new Semaphore(0).acquire();
        // while (true){}
        System.out.println(""after sleep join semaphore exit infinite while loop"");
    } catch (Exception ignored) {
    } finally {
        System.out.println(""finally"");
    }
}


Case 2: If the JVM crashes first

import sun.misc.Unsafe;
import java.lang.reflect.Field;

public static void main(String args[]) {
    try {
        unsafeMethod();
        //Runtime.getRuntime().halt(123);
        System.out.println(""After Jvm Crash!"");
    } catch (Exception e) {
    } finally {
        System.out.println(""finally"");
    }
}

private static void unsafeMethod() throws NoSuchFieldException, IllegalAccessException {
    Field f = Unsafe.class.getDeclaredField(""theUnsafe"");
    f.setAccessible(true);
    Unsafe unsafe = (Unsafe) f.get(null);
    unsafe.putAddress(0, 0);
}


Ref: How do you crash a JVM?

Case 6: If finally block is going to be executed by daemon Thread and all other non-daemon Threads exit before finally is called.

public static void main(String args[]) {
    Runnable runnable = new Runnable() {
        @Override
        public void run() {
            try {
                printThreads(""Daemon Thread printing"");
                // just to ensure this thread will live longer than main thread
                Thread.sleep(10000);
            } catch (Exception e) {
            } finally {
                System.out.println(""finally"");
            }
        }
    };
    Thread daemonThread = new Thread(runnable);
    daemonThread.setDaemon(Boolean.TRUE);
    daemonThread.setName(""My Daemon Thread"");
    daemonThread.start();
    printThreads(""main Thread Printing"");
}

private static synchronized void printThreads(String str) {
    System.out.println(str);
    int threadCount = 0;
    Set<Thread> threadSet = Thread.getAllStackTraces().keySet();
    for (Thread t : threadSet) {
        if (t.getThreadGroup() == Thread.currentThread().getThreadGroup()) {
            System.out.println(""Thread :"" + t + "":"" + ""state:"" + t.getState());
            ++threadCount;
        }
    }
    System.out.println(""Thread count started by Main thread:"" + threadCount);
    System.out.println(""-------------------------------------------------"");
}


output: This does not print ""finally"" which implies ""Finally block"" in ""daemon thread"" did not execute  


main Thread Printing  
Thread :Thread[My Daemon Thread,5,main]:state:BLOCKED  
Thread :Thread[main,5,main]:state:RUNNABLE  
Thread :Thread[Monitor Ctrl-Break,5,main]:state:RUNNABLE   
Thread count started by Main thread:3  
-------------------------------------------------  
Daemon Thread printing  
Thread :Thread[My Daemon Thread,5,main]:state:RUNNABLE  
Thread :Thread[Monitor Ctrl-Break,5,main]:state:RUNNABLE  
Thread count started by Main thread:2  
-------------------------------------------------  

Process finished with exit code 0


    In addition to the point about return in finally replacing a return in the try block, the same is true of an exception.  A finally block that throws an exception will replace a return or exception thrown from within the try block.
    If an exception is thrown, finally runs. If an exception is not thrown, finally runs. If the exception is caught, finally runs. If the exception is not caught, finally runs.

Only time it does not run is when JVM exits.
    Because a finally block will always be called unless you call System.exit() (or the thread crashes).
    That's actually true in any language...finally will always execute before a return statement, no matter where that return is in the method body. If that wasn't the case, the finally block wouldn't have much meaning.
    Adding to @vibhash's answer as no other answer explains what happens in the case of a mutable object like the one below.

public static void main(String[] args) {
    System.out.println(test().toString());
}

public static StringBuffer test() {
    StringBuffer s = new StringBuffer();
    try {
        s.append(""sb"");
        return s;
    } finally {
        s.append(""updated "");
    }
}


Will output


sbupdated 


    Yes, it is written here


  If the JVM exits while the try or catch code is being executed, then the finally block may not execute. Likewise, if the thread executing the try or catch code is interrupted or killed, the finally block may not execute even though the application as a whole continues.

    ","[2594, 2921, 608, 413, 173, 127, 128, 262, 10, 42, 19, 10, 21, 9, 18, 7, 10, 8, 10, 8, 7, 6, 8, 10, 8, 7, 5, 10, 7, 8, 5]",567335,536,2008-09-15T17:43:54,2022-01-24 22:15:15Z,java 
Open a URL in a new tab (and not a new window),"
                
I'm trying to open a URL in a new tab, as opposed to a popup window.

I've seen related questions where the responses would look something like:

window.open(url,'_blank');
window.open(url);


But none of them worked for me, the browser still tried to open a popup window.
    This is a trick,
function openInNewTab(url) {
 window.open(url, '_blank').focus();
}

//or just
window.open(url, '_blank').focus();

In most cases, this should happen directly in the onclick handler for the link to prevent pop-up blockers, and the default ""new window"" behavior. You could do it this way, or by adding an event listener to your DOM object.
<div onclick=""openInNewTab('www.test.com');"">Something To Click On</div>

http://www.tutsplanet.com/open-url-new-tab-using-javascript/
    Nothing an author can do can choose to open in a new tab instead of a new window; it is a user preference. (Note that the default user preference in most browsers is for new tabs, so a trivial test on a browser where that preference hasn't been changed will not demonstrate this.)
CSS3 proposed target-new, but the specification was abandoned.
The reverse is not true; by specifying certain window features for the window in the third argument of window.open(), you can trigger a new window when the preference is for tabs.
    function openInNewTab(href) {
  Object.assign(document.createElement('a'), {
    target: '_blank',
    href: href,
  }).click();
}

It creates a virtual a element, gives it target=""_blank"" so it opens in a new tab, gives it proper url href and then clicks it.
and then you can use it like:
openInNewTab(""https://google.com""); 

Important note:
openInNewTab (as well as any other solution on this page) must be called during the so-called 'trusted event' callback - eg. during click event (not necessary in callback function directly, but during click action). Otherwise opening a new page will be blocked by the browser
If you call it manually at some random moment (e.g., inside an interval or after server response) - it might be blocked by the browser (which makes sense as it'd be a security risk and might lead to poor user experience)
    window.open() will not open in a new tab if it is not happening on the actual click event. In the example given the URL is being opened on the actual click event. This will work provided the user has appropriate settings in the browser.

<a class=""link"">Link</a>
<script  type=""text/javascript"">
     $(""a.link"").on(""click"",function(){
         window.open('www.yourdomain.com','_blank');
     });
</script>


Similarly, if you are trying to do an Ajax call within the click function and want to open a window on success, ensure you are doing the Ajax call with the async : false option set.
    I use the following and it works very well!

window.open(url, '_blank').focus();

    window.open Cannot Reliably Open Popups in a New Tab in All Browsers

Different browsers implement the behavior of window.open in different ways, especially with regard to a user's browser preferences. You cannot expect the same behavior for window.open to be true across all of Internet Explorer, Firefox, and Chrome, because of the different ways in which they handle a user's browser preferences.

For example, InternetExplorer (11) users can choose to open popups in a new window or a new tab, you cannot force InternetExplorer11 users to open popups in a certain way through window.open, as alluded to in Quentin's answer.

As for Firefox (29) users, using window.open(url, '_blank') depends on their browser's tab preferences, though you can still force them to open popups in a new window by specifying a width and height (see ""What About Chrome?"" section below).

Demonstration

Go to your browser's settings and configure it to open popups in a new window.

Internet Explorer (11)





Test Page

After setting up Internet Explorer (11) to open popups in a new window as demonstrated above, use the following test page to test window.open:

<!DOCTYPE html>
<html>
  <head>
    <title>Test</title>
  </head>

  <body>
    <button onclick=""window.open('https://stackoverflow.com/q/4907843/456814');"">
      <code>window.open(url)</code>
    </button>
    <button onclick=""window.open('https://stackoverflow.com/q/4907843/456814', '_blank');"">
      <code>window.open(url, '_blank')</code>
    </button>
  </body>
</html>


Observe that the popups are opened in a new window, not a new tab.

You can also test those snippets above in Firefox (29) with its tab preference set to new windows, and see the same results.

What About Chrome? It Implements window.open Differently from InternetExplorer (11) and Firefox (29).

I'm not 100% sure, but it looks like Chrome (version 34.0.1847.131 m) does not appear to have any settings that the user can use to choose whether or not to open popups in a new window or a new tab (like Firefox and InternetExplorer have). I checked the Chrome documentation for managing pop-ups, but it didn't mention anything about that sort of thing.

Also, once again, different browsers seem to implement the behavior of window.open differently. In Chrome and Firefox, specifying a width and height will force a popup, even when a user has set Firefox (29) to open new windows in a new tab (as mentioned in the answers to JavaScript open in a new window, not tab):

<!DOCTYPE html>
<html>
  <head>
    <title>Test</title>
  </head>

  <body>
    <button onclick=""window.open('https://stackoverflow.com/q/4907843/456814', 'test', 'width=400, height=400');"">
      <code>window.open(url)</code>
    </button>
  </body>
</html>


However, the same code snippet above will always open a new tab in InternetExplorer11 if users set tabs as their browser preferences, not even specifying a width and height will force a new window popup for them.

So the behavior of window.open in Chrome seems to be to open popups in a new tab when used in an onclick event, to open them in new windows when used from the browser console (as noted by other people), and to open them in new windows when specified with a width and a height.

Summary

Different browsers implement the behavior of window.open differently with regard to users' browser preferences. You cannot expect the same behavior for window.open to be true across all of Internet Explorer, Firefox, and Chrome, because of the different ways in which they handle a user's browser preferences.

Additional Reading


window.open documentation.

    Whether to open the URL in a new tab or a new window, is actually controlled by the user's browser preferences. There is no way to override it in JavaScript.
window.open() behaves differently depending on how it is being used. If it is called as a direct result of a user action, let us say a button click, it should work fine and open a new tab (or window):
const button = document.querySelector('#openTab');

// add click event listener
button.addEventListener('click', () => {
    // open a new tab
    const tab = window.open('https://attacomsian.com', '_blank');
});

However, if you try to open a new tab from an AJAX request callback, the browser will block it as it was not a direct user action.
To bypass the popup blocker and open a new tab from a callback, here is a little hack:
const button = document.querySelector('#openTab');

// add click event listener
button.addEventListener('click', () => {

    // open an empty window
    const tab = window.open('about:blank');

    // make an API call
    fetch('/api/validate')
        .then(res => res.json())
        .then(json => {

            // TODO: do something with JSON response

            // update the actual URL
            tab.location = 'https://attacomsian.com';
            tab.focus();
        })
        .catch(err => {
            // close the empty window
            tab.close();
        });
});

    If you use window.open(url, '_blank'), it will be blocked (popup blocker) on Chrome.

Try this:

//With JQuery

$('#myButton').click(function () {
    var redirectWindow = window.open('http://google.com', '_blank');
    redirectWindow.location;
});


With pure JavaScript,

document.querySelector('#myButton').onclick = function() {
    var redirectWindow = window.open('http://google.com', '_blank');
    redirectWindow.location;
};

    To elaborate Steven Spielberg's answer, I did this in such a case:

$('a').click(function() {
  $(this).attr('target', '_blank');
});


This way, just before the browser will follow the link I'm setting the target attribute, so it will make the link open in a new tab or window (depends on user's settings).

One line example in jQuery:

$('a').attr('target', '_blank').get(0).click();
// The `.get(0)` must be there to return the actual DOM element.
// Doing `.click()` on the jQuery object for it did not work.


This can also be accomplished just using native browser DOM APIs as well:

document.querySelector('a').setAttribute('target', '_blank');
document.querySelector('a').click();

    function openTab(url) {
  const link = document.createElement('a');
  link.href = url;
  link.target = '_blank';
  document.body.appendChild(link);
  link.click();
  link.remove();
}

    I think that you can't control this. If the user had setup their browser to open links in a new window, you can't force this to open links in a new tab.

JavaScript open in a new window, not tab
    Do not use target=""_blank""
Always use specific name for that window  in my case meaningfulName, in this case you save processor resource:
button.addEventListener('click', () => {
    window.open('https://google.com', 'meaningfulName')
})

On this way when you click for example 10 times on button, browser will always re-render it in one new tab, instead of opening it in 10 different tabs which will consume much more resources.
You can read more about this on MDN.
    JQuery

$('<a />',{'href': url, 'target': '_blank'}).get(0).click();


JS

Object.assign(document.createElement('a'), { target: '_blank', href: 'URL_HERE'}).click();

    Just omitting [strWindowFeatures] parameters will open a new tab, UNLESS the browser setting overrides (browser setting trumps JavaScript).

New window

var myWin = window.open(strUrl, strWindowName, [strWindowFeatures]);


New tab

var myWin = window.open(strUrl, strWindowName);


-- or --

var myWin = window.open(strUrl);

    This has nothing to do with browser settings if you are trying to open a new tab from a custom function.

In this page, open a JavaScript console and type:

document.getElementById(""nav-questions"").setAttribute(""target"", ""_blank"");
document.getElementById(""nav-questions"").click();


And it will try to open a popup regardless of your settings, because the 'click' comes from a custom action.

In order to behave like an actual 'mouse click' on a link, you need to follow @spirinvladimir's advice and really create it:

document.getElementById(""nav-questions"").setAttribute(""target"", ""_blank"");
document.getElementById(""nav-questions"").dispatchEvent((function(e){
  e.initMouseEvent(""click"", true, true, window, 0, 0, 0, 0, 0,
                    false, false, false, false, 0, null);
  return e
}(document.createEvent('MouseEvents'))));


Here is a complete example (do not try it on jsFiddle or similar online editors, as it will not let you redirect to external pages from there):

<!DOCTYPE html>
<html>
<head>
  <style>
    #firing_div {
      margin-top: 15px;
      width: 250px;
      border: 1px solid blue;
      text-align: center;
    }
  </style>
</head>
<body>
  <a id=""my_link"" href=""http://www.google.com""> Go to Google </a>
  <div id=""firing_div""> Click me to trigger custom click </div>
</body>
<script>
  function fire_custom_click() {
    alert(""firing click!"");
    document.getElementById(""my_link"").dispatchEvent((function(e){
      e.initMouseEvent(""click"", true, true, window, /* type, canBubble, cancelable, view */
            0, 0, 0, 0, 0,              /* detail, screenX, screenY, clientX, clientY */
            false, false, false, false, /* ctrlKey, altKey, shiftKey, metaKey */
            0, null);                   /* button, relatedTarget */
      return e
    }(document.createEvent('MouseEvents'))));
  }
  document.getElementById(""firing_div"").onclick = fire_custom_click;
</script>
</html>

    An interesting fact is that the new tab can not be opened if the action is not invoked by the user (clicking a button or something) or if it is asynchronous, for example, this will NOT open in new tab:

$.ajax({
    url: ""url"",
    type: ""POST"",
    success: function() {
        window.open('url', '_blank');              
    }
});


But this may open in a new tab, depending on browser settings:

$.ajax({
    url: ""url"",
    type: ""POST"",
    async: false,
    success: function() {
        window.open('url', '_blank');              
    }
});

    You can use a trick with form:

$(function () {
    $('#btn').click(function () {
        openNewTab(""http://stackoverflow.com"")
        return false;
    });
});

function openNewTab(link) {
    var frm = $('<form   method=""get"" action=""' + link + '"" target=""_blank""></form>')
    $(""body"").append(frm);
    frm.submit().remove();
}


jsFiddle demo
    (function(a){
document.body.appendChild(a);
a.setAttribute('href', location.href);
a.dispatchEvent((function(e){
    e.initMouseEvent(""click"", true, true, window, 0, 0, 0, 0, 0, true, false, false, false, 0, null);
    return e
}(document.createEvent('MouseEvents'))))}(document.createElement('a')))

    The window.open(url) will open url in new browser Tab. Belowe JS alternative to it

let a= document.createElement('a');
a.target= '_blank';
a.href= 'https://support.wwf.org.uk/';
a.click(); // we don't need to remove 'a' from DOM because we not add it


here is working example (stackoverflow snippets not allow to opening new tab)
    There is an answer to this question and it is not no.


  I found an easy work around:


Step 1: Create an invisible link:

<a id=""yourId"" href=""yourlink.html"" target=""_blank"" style=""display: none;""></a>

Step 2: Click on that link programmatically:

document.getElementById(""yourId"").click();

Here you go! Works a charm for me.
    

I researched a lot of information about how to open new tab and stay on the same tab. I have found one small trick to do it.
Lets assume you have url which you need to open - newUrl and old url - currentUrl, which you need to stay on after new tab opened.
JS code will look something like next:

// init urls
let newUrl = 'http://example.com';
let currentUrl = window.location.href;
// open window with url of current page, you will be automatically moved 
// by browser to a new opened tab. It will look like your page is reloaded
// and you will stay on same page but with new page opened
window.open(currentUrl , '_blank');
// on your current tab will be opened new url
location.href = newUrl;

    Or you could just create a link element and click it...

var evLink = document.createElement('a');
evLink.href = 'http://' + strUrl;
evLink.target = '_blank';
document.body.appendChild(evLink);
evLink.click();
// Now delete it
evLink.parentNode.removeChild(evLink);


This shouldn't be blocked by any popup blockers... Hopefully.
    This might be a hack, but in Firefox if you specify a third parameter, 'fullscreen=yes', it opens a fresh new window.

For example,

<a href=""#"" onclick=""window.open('MyPDF.pdf', '_blank', 'fullscreen=yes'); return false;"">MyPDF</a>


It seems to actually override the browser settings.
    How about creating an <a> with _blank as target attribute value and the url as href, with style display:hidden with a a children element? Then add to the DOM and then trigger the click event on a children element.

UPDATE

That doesn't work. The browser prevents the default behaviour. It could be triggered programmatically, but it doesn't follow the default behaviour.

Check and see for yourself: http://jsfiddle.net/4S4ET/
    Somehow a website can do it. (I don't have the time to extract it from this mess, but this is the code)

if (!Array.prototype.indexOf)
    Array.prototype.indexOf = function(searchElement, fromIndex) {
        if (this === undefined || this === null)
            throw new TypeError('""this"" is null or not defined');
        var length = this.length >>> 0;
        fromIndex = +fromIndex || 0;
        if (Math.abs(fromIndex) === Infinity)
            fromIndex = 0;
        if (fromIndex < 0) {
            fromIndex += length;
            if (fromIndex < 0)
                fromIndex = 0
        }
        for (; fromIndex < length; fromIndex++)
            if (this[fromIndex] === searchElement)
                return fromIndex;
        return -1
    };
(function Popunder(options) {
    var _parent, popunder, posX, posY, cookieName, cookie, browser, numberOfTimes, expires = -1,
        wrapping, url = """",
        size, frequency, mobilePopupDisabled = options.mobilePopupDisabled;
    if (this instanceof Popunder === false)
        return new Popunder(options);
    try {
        _parent = top != self && typeof top.document.location.toString() === ""string"" ? top : self
    } catch (e) {
        _parent = self
    }
    cookieName = ""adk2_popunder"";
    popunder = null;
    browser = function() {
        var n = navigator.userAgent.toLowerCase(),
            b = {
                webkit: /webkit/.test(n),
                mozilla: /mozilla/.test(n) && !/(compatible|webkit)/.test(n),
                chrome: /chrome/.test(n),
                msie: /msie/.test(n) && !/opera/.test(n),
                firefox: /firefox/.test(n),
                safari: /safari/.test(n) && !/chrome/.test(n),
                opera: /opera/.test(n)
            };
        b.version = b.safari ? (n.match(/.+(?:ri)[\/: ]([\d.]+)/) || [])[1] : (n.match(/.+(?:ox|me|ra|ie)[\/:]([\d.]+)/) || [])[1];
        return b
    }();
    initOptions(options);

    function initOptions(options) {
        options = options || {};
        if (options.wrapping)
            wrapping = options.wrapping;
        else {
            options.serverdomain = options.serverdomain || ""ads.adk2.com"";
            options.size = options.size || ""800x600"";
            options.ci = ""3"";
            var arr = [],
                excluded = [""serverdomain"", ""numOfTimes"", ""duration"", ""period""];
            for (var p in options)
                options.hasOwnProperty(p) && options[p].toString() && excluded.indexOf(p) === -1 && arr.push(p + ""="" + encodeURIComponent(options[p]));
            url = ""http://"" + options.serverdomain + ""/player.html?rt=popunder&"" + arr.join(""&"")
        }
        if (options.size) {
            size = options.size.split(""x"");
            options.width = size[0];
            options.height = size[1]
        }
        if (options.frequency) {
            frequency = /([0-9]+)\/([0-9]+)(\w)/.exec(options.frequency);
            options.numOfTimes = +frequency[1];
            options.duration = +frequency[2];
            options.period = ({
                m: ""minute"",
                h: ""hour"",
                d: ""day""
            })[frequency[3].toLowerCase()]
        }
        if (options.period)
            switch (options.period.toLowerCase()) {
                case ""minute"":
                    expires = options.duration * 60 * 1e3;
                    break;
                case ""hour"":
                    expires = options.duration * 60 * 60 * 1e3;
                    break;
                case ""day"":
                    expires = options.duration * 24 * 60 * 60 * 1e3
            }
        posX = typeof options.left != ""undefined"" ? options.left.toString() : window.screenX;
        posY = typeof options.top != ""undefined"" ? options.top.toString() : window.screenY;
        numberOfTimes = options.numOfTimes
    }

    function getCookie(name) {
        try {
            var parts = document.cookie.split(name + ""="");
            if (parts.length == 2)
                return unescape(parts.pop().split("";"").shift()).split(""|"")
        } catch (err) {}
    }

    function setCookie(value, expiresDate) {
        expiresDate = cookie[1] || expiresDate.toGMTString();
        document.cookie = cookieName + ""="" + escape(value + ""|"" + expiresDate) + "";expires="" + expiresDate + "";path=/""
    }

    function addEvent(listenerEvent) {
        if (document.addEventListener)
            document.addEventListener(""click"", listenerEvent, false);
        else
            document.attachEvent(""onclick"", listenerEvent)
    }

    function removeEvent(listenerEvent) {
        if (document.removeEventListener)
            document.removeEventListener(""click"", listenerEvent, false);
        else
            document.detachEvent(""onclick"", listenerEvent)
    }

    function isCapped() {
        cookie = getCookie(cookieName) || [];
        return !!numberOfTimes && +numberOfTimes <= +cookie[0]
    }

    function pop() {
        var features = ""type=fullWindow, fullscreen, scrollbars=yes"",
            listenerEvent = function() {
                var now, next;
                if (/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent))
                    if (mobilePopupDisabled)
                        return;
                if (isCapped())
                    return;
                if (browser.chrome && parseInt(browser.version.split(""."")[0], 10) > 30 && adParams.openNewTab) {
                    now = new Date;
                    next = new Date(now.setTime(now.getTime() + expires));
                    setCookie((+cookie[0] || 0) + 1, next);
                    removeEvent(listenerEvent);
                    window.open(""javascript:window.focus()"", ""_self"", """");
                    simulateClick(url);
                    popunder = null
                } else
                    popunder = _parent.window.open(url, Math.random().toString(36).substring(7), features);
                if (wrapping) {
                    popunder.document.write(""<html><head></head><body>"" + unescape(wrapping || """") + ""</body></html>"");
                    popunder.document.body.style.margin = 0
                }
                if (popunder) {
                    now = new Date;
                    next = new Date(now.setTime(now.getTime() + expires));
                    setCookie((+cookie[0] || 0) + 1, next);
                    moveUnder();
                    removeEvent(listenerEvent)
                }
            };
        addEvent(listenerEvent)
    }
    var simulateClick = function(url) {
        var a = document.createElement(""a""),
            u = !url ? ""data:text/html,<script>window.close();<\/script>;"" : url,
            evt = document.createEvent(""MouseEvents"");
        a.href = u;
        document.body.appendChild(a);
        evt.initMouseEvent(""click"", true, true, window, 0, 0, 0, 0, 0, true, false, false, true, 0, null);
        a.dispatchEvent(evt);
        a.parentNode.removeChild(a)
    };

    function moveUnder() {
        try {
            popunder.blur();
            popunder.opener.window.focus();
            window.self.window.focus();
            window.focus();
            if (browser.firefox)
                openCloseWindow();
            else if (browser.webkit)
                openCloseTab();
            else
                browser.msie && setTimeout(function() {
                    popunder.blur();
                    popunder.opener.window.focus();
                    window.self.window.focus();
                    window.focus()
                }, 1e3)
        } catch (e) {}
    }

    function openCloseWindow() {
        var tmp = popunder.window.open(""about:blank"");
        tmp.focus();
        tmp.close();
        setTimeout(function() {
            try {
                tmp = popunder.window.open(""about:blank"");
                tmp.focus();
                tmp.close()
            } catch (e) {}
        }, 1)
    }

    function openCloseTab() {
        var ghost = document.createElement(""a""),
            clk;
        document.getElementsByTagName(""body"")[0].appendChild(ghost);
        clk = document.createEvent(""MouseEvents"");
        clk.initMouseEvent(""click"", false, true, window, 0, 0, 0, 0, 0, true, false, false, true, 0, null);
        ghost.dispatchEvent(clk);
        ghost.parentNode.removeChild(ghost);
        window.open(""about:blank"", ""PopHelper"").close()
    }
    pop()
})(adParams)

    There are lots of answer copies suggesting using ""_blank"" as the target, however I found this didn't work. As Prakash notes, it is up to the browser. However, you can make certain suggestions to the browser, such as to whether the window should have a location bar.
If you suggest enough ""tab-like things"" you might get a tab, as per Nico's answer to this more specific question for chrome:
window.open('http://www.stackoverflow.com', '_blank', 'toolbar=yes, location=yes, status=yes, menubar=yes, scrollbars=yes');

Disclaimer: This is not a panacea. It is still up to the user and browser. Now at least you've specified one more preference for what you'd like your window to look like.
    Opening a new tab from within a Firefox (Mozilla) extension goes like this:

gBrowser.selectedTab = gBrowser.addTab(""http://example.com"");

    This way is similar to the above solution but implemented differently  

.social_icon -> some class with CSS  

 <div class=""social_icon"" id=""SOME_ID"" data-url=""SOME_URL""></div>


 $('.social_icon').click(function(){

        var url = $(this).attr('data-url');
        var win = window.open(url, '_blank');  ///similar to above solution
        win.focus();
   });

    this work for me, just prevent the event, add the url to an <a> tag then trigger the click event on that tag.

Js
$('.myBtn').on('click', function(event) {
        event.preventDefault();
        $(this).attr('href',""http://someurl.com"");
        $(this).trigger('click');
});
HTML
<a href=""#"" class=""myBtn"" target=""_blank"">Go</a>

    I'm going to agree somewhat with the person who wrote (paraphrased here): ""For a link in an existing web page, the browser will always open the link in a new tab if the new page is part of the same web site as the existing web page.""  For me, at least, this ""general rule"" works in Chrome, Firefox, Opera, IE, Safari, SeaMonkey, and Konqueror.

Anyway, there is a less complicated way to take advantage of what the other person presented.  Assuming we are talking about your own web site (""thissite.com"" below), where you want to control what the browser does, then, below, you want ""specialpage.htm"" to be EMPTY, no HTML at all in it (saves time sending data from the server!).

 var wnd, URL;  //global variables

 //specifying ""_blank"" in window.open() is SUPPOSED to keep the new page from replacing the existing page
 wnd = window.open(""http://www.thissite.com/specialpage.htm"", ""_blank""); //get reference to just-opened page
 //if the ""general rule"" above is true, a new tab should have been opened.
 URL = ""http://www.someothersite.com/desiredpage.htm"";  //ultimate destination
 setTimeout(gotoURL(),200);  //wait 1/5 of a second; give browser time to create tab/window for empty page


 function gotoURL()
 { wnd.open(URL, ""_self"");  //replace the blank page, in the tab, with the desired page
   wnd.focus();             //when browser not set to automatically show newly-opened page, this MAY work
 }

    ","[2590, 2243, 1098, 299, 417, 74, 264, 16, 93, 72, 13, 22, 4, 5, 15, 12, 19, 12, 12, 0, 3, 2, 3, 0, 0, -9, -1, -2, -2, -2, -5]",3294789,371,2011-02-05T15:52:13,2021-11-29 07:19:06Z,javascript 
How can I transition height: 0; to height: auto; using CSS?,"
                
I am trying to make a <ul> slide down using CSS transitions.

The <ul> starts off at height: 0;. On hover, the height is set to height:auto;. However, this is causing it to simply appear, not transition,

If I do it from height: 40px; to height: auto;, then it will slide up to height: 0;, and then suddenly jump to the correct height.

How else could I do this without using JavaScript?

#child0 {
  height: 0;
  overflow: hidden;
  background-color: #dedede;
  -moz-transition: height 1s ease;
  -webkit-transition: height 1s ease;
  -o-transition: height 1s ease;
  transition: height 1s ease;
}
#parent0:hover #child0 {
  height: auto;
}
#child40 {
  height: 40px;
  overflow: hidden;
  background-color: #dedede;
  -moz-transition: height 1s ease;
  -webkit-transition: height 1s ease;
  -o-transition: height 1s ease;
  transition: height 1s ease;
}
#parent40:hover #child40 {
  height: auto;
}
h1 {
  font-weight: bold;
}The only difference between the two snippets of CSS is one has height: 0, the other height: 40.
<hr>
<div id=""parent0"">
  <h1>Hover me (height: 0)</h1>
  <div id=""child0"">Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>
  </div>
</div>
<hr>
<div id=""parent40"">
  <h1>Hover me (height: 40)</h1>
  <div id=""child40"">Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>
  </div>
</div>

    Use max-height in the transition and not height. And set a value on max-height to something bigger than your box will ever get.

See JSFiddle demo provided by Chris Jordan in another answer here.

#menu #list {
    max-height: 0;
    transition: max-height 0.15s ease-out;
    overflow: hidden;
    background: #d5d5d5;
}

#menu:hover #list {
    max-height: 500px;
    transition: max-height 0.25s ease-in;
}<div id=""menu"">
    <a>hover me</a>
    <ul id=""list"">
        <!-- Create a bunch, or not a bunch, of li's to see the timing. -->
        <li>item</li>
        <li>item</li>
        <li>item</li>
        <li>item</li>
        <li>item</li>
    </ul>
</div>

    You should use scaleY instead.
ul {
  background-color: #eee;
  transform: scaleY(0);    
  transform-origin: top;
  transition: transform 0.26s ease;
}
p:hover ~ ul {
  transform: scaleY(1);
}<p>Hover This</p>
<ul>
  <li>Coffee</li>
  <li>Tea</li>
  <li>Milk</li>
</ul>

I've made a vendor prefixed version of the above code on jsfiddle,  and changed your jsfiddle to use scaleY instead of height.
Edit
Some people do not like how scaleY transforms the content. If that is a problem then I suggest using clip instead.
ul {
  clip: rect(auto, auto, 0, auto);
  position: absolute;
  margin: -1rem 0;
  padding: .5rem;

  color: white;

  background-color: rgba(0, 0, 0, 0.8);

  transition-property: clip;
  transition-duration: 0.5s;
  transition-timing-function: cubic-bezier(0.175, 0.885, 0.32, 1.275);
}
h3:hover ~ ul,
h3:active ~ ul,
ul:hover {
  clip: rect(auto, auto, 10rem, auto);
}<h3>Hover here</h3>
<ul>
  <li>This list</li>
  <li>is clipped.</li>
  <li>A clip transition</li>
  <li>will show it</li>
</ul>
<p>
  Some text...
</p>

    This is a CSS-only solution with the following properties:

There is no delay at the beginning, and the transition doesn't stop early. In both directions (expanding and collapsing), if you specify a transition duration of 300ms in your CSS, then the transition takes 300ms, period.
It's transitioning the actual height (unlike transform: scaleY(0)), so it does the right thing if there's content after the collapsible element.
While (like in other solutions) there are magic numbers (like ""pick a length that is higher than your box is ever going to be""), it's not fatal if your assumption ends up being wrong. The transition may not look amazing in that case, but before and after the transition, this is not a problem: In the expanded (height: auto) state, the whole content always has the correct height (unlike e.g. if you pick a max-height that turns out to be too low). And in the collapsed state, the height is zero as it should.

Demo
Here's a demo with three collapsible elements, all of different heights, that all use the same CSS. You might want to click ""full page"" after clicking ""run snippet"". Note that the JavaScript only toggles the collapsed CSS class, there's no measuring involved. (You could do this exact demo without any JavaScript at all by using a checkbox or :target). Also note that the part of the CSS that's responsible for the transition is pretty short, and the HTML only requires a single additional wrapper element.
$(function () {
  $("".toggler"").click(function () {
    $(this).next().toggleClass(""collapsed"");
    $(this).toggleClass(""toggled""); // this just rotates the expander arrow
  });
});.collapsible-wrapper {
  display: flex;
  overflow: hidden;
}
.collapsible-wrapper:after {
  content: '';
  height: 50px;
  transition: height 0.3s linear, max-height 0s 0.3s linear;
  max-height: 0px;
}
.collapsible {
  transition: margin-bottom 0.3s cubic-bezier(0, 0, 0, 1);
  margin-bottom: 0;
  max-height: 1000000px;
}
.collapsible-wrapper.collapsed > .collapsible {
  margin-bottom: -2000px;
  transition: margin-bottom 0.3s cubic-bezier(1, 0, 1, 1),
              visibility 0s 0.3s, max-height 0s 0.3s;
  visibility: hidden;
  max-height: 0;
}
.collapsible-wrapper.collapsed:after
{
  height: 0;
  transition: height 0.3s linear;
  max-height: 50px;
}

/* END of the collapsible implementation; the stuff below
   is just styling for this demo */

#container {
  display: flex;
  align-items: flex-start;
  max-width: 1000px;
  margin: 0 auto;
}  


.menu {
  border: 1px solid #ccc;
  box-shadow: 0 1px 3px rgba(0,0,0,0.5);
  margin: 20px;

  
}

.menu-item {
  display: block;
  background: linear-gradient(to bottom, #fff 0%,#eee 100%);
  margin: 0;
  padding: 1em;
  line-height: 1.3;
}
.collapsible .menu-item {
  border-left: 2px solid #888;
  border-right: 2px solid #888;
  background: linear-gradient(to bottom, #eee 0%,#ddd 100%);
}
.menu-item.toggler {
  background: linear-gradient(to bottom, #aaa 0%,#888 100%);
  color: white;
  cursor: pointer;
}
.menu-item.toggler:before {
  content: '';
  display: block;
  border-left: 8px solid white;
  border-top: 8px solid transparent;
  border-bottom: 8px solid transparent;
  width: 0;
  height: 0;
  float: right;
  transition: transform 0.3s ease-out;
}
.menu-item.toggler.toggled:before {
  transform: rotate(90deg);
}

body { font-family: sans-serif; font-size: 14px; }

*, *:after {
  box-sizing: border-box;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>

<div id=""container"">
  <div class=""menu"">
    <div class=""menu-item"">Something involving a holodeck</div>
    <div class=""menu-item"">Send an away team</div>
    <div class=""menu-item toggler"">Advanced solutions</div>
    <div class=""collapsible-wrapper collapsed"">
      <div class=""collapsible"">
        <div class=""menu-item"">Separate saucer</div>
        <div class=""menu-item"">Send an away team that includes the captain (despite Riker's protest)</div>
        <div class=""menu-item"">Ask Worf</div>
        <div class=""menu-item"">Something involving Wesley, the 19th century, and a holodeck</div>
        <div class=""menu-item"">Ask Q for help</div>
      </div>
    </div>
    <div class=""menu-item"">Sweet-talk the alien aggressor</div>
    <div class=""menu-item"">Re-route power from auxiliary systems</div>
  </div>

  <div class=""menu"">
    <div class=""menu-item"">Something involving a holodeck</div>
    <div class=""menu-item"">Send an away team</div>
    <div class=""menu-item toggler"">Advanced solutions</div>
    <div class=""collapsible-wrapper collapsed"">
      <div class=""collapsible"">
        <div class=""menu-item"">Separate saucer</div>
        <div class=""menu-item"">Send an away team that includes the captain (despite Riker's protest)</div>
      </div>
    </div>
    <div class=""menu-item"">Sweet-talk the alien aggressor</div>
    <div class=""menu-item"">Re-route power from auxiliary systems</div>
  </div>

  <div class=""menu"">
    <div class=""menu-item"">Something involving a holodeck</div>
    <div class=""menu-item"">Send an away team</div>
    <div class=""menu-item toggler"">Advanced solutions</div>
    <div class=""collapsible-wrapper collapsed"">
      <div class=""collapsible"">
        <div class=""menu-item"">Separate saucer</div>
        <div class=""menu-item"">Send an away team that includes the captain (despite Riker's protest)</div>
        <div class=""menu-item"">Ask Worf</div>
        <div class=""menu-item"">Something involving Wesley, the 19th century, and a holodeck</div>
        <div class=""menu-item"">Ask Q for help</div>
        <div class=""menu-item"">Separate saucer</div>
        <div class=""menu-item"">Send an away team that includes the captain (despite Riker's protest)</div>
        <div class=""menu-item"">Ask Worf</div>
        <div class=""menu-item"">Something involving Wesley, the 19th century, and a holodeck</div>
        <div class=""menu-item"">Ask Q for help</div>
      </div>
    </div>
    <div class=""menu-item"">Sweet-talk the alien aggressor</div>
    <div class=""menu-item"">Re-route power from auxiliary systems</div>
  </div>

</div>

How does it work?
There are in fact two transitions involved in making this happen. One of them transitions the margin-bottom from 0px (in the expanded state) to -2000px in the collapsed state (similar to this answer). The 2000 here is the first magic number, it's based on the assumption that your box won't be higher than this (2000 pixels seems like a reasonable choice).
Using the margin-bottom transition alone by itself has two issues:

If you actually have a box that's higher than 2000 pixels, then a margin-bottom: -2000px won't hide everything -- there'll be visible stuff even in the collapsed case. This is a minor fix that we'll do later.
If the actual box is, say, 1000 pixels high, and your transition is 300ms long, then the visible transition is already over after about 150ms (or, in the opposite direction, starts 150ms late).

Fixing this second issue is where the second transition comes in, and this transition conceptually targets the wrapper's minimum height (""conceptually"" because we're not actually using the min-height property for this; more on that later).
Here's an animation that shows how combining the bottom margin transition with the minimum height transition, both of equal duration, gives us a combined transition from full height to zero height that has the same duration.

The left bar shows how the negative bottom margin pushes the bottom upwards, reducing the visible height. The middle bar shows how the minimum height ensures that in the collapsing case, the transition doesn't end early, and in the expanding case, the transition doesn't start late. The right bar shows how the combination of the two causes the box to transition from full height to zero height in the correct amount of time.
For my demo I've settled on 50px as the upper minimum height value. This is the second magic number, and it should be lower than the box' height would ever be. 50px seems reasonable as well; it seems unlikely that you'd very often want to make an element collapsible that isn't even 50 pixels high in the first place.
As you can see in the animation, the resulting transition is continuous, but it is not differentiable -- at the moment when the minimum height is equal to the full height adjusted by the bottom margin, there is a sudden change in speed. This is very noticeable in the animation because it uses a linear timing function for both transitions, and because the whole transition is very slow. In the actual case (my demo at the top), the transition only takes 300ms, and the bottom margin transition is not linear. I've played around with a lot of different timing functions for both transitions, and the ones I ended up with felt like they worked best for the widest variety of cases.
Two problems remain to fix:

the point from above, where boxes of more than 2000 pixels height aren't completely hidden in the collapsed state,
and the reverse problem, where in the non-hidden case, boxes of less than 50 pixels height are too high even when the transition isn't running, because the minimum height keeps them at 50 pixels.

We solve the first problem by giving the container element a max-height: 0 in the collapsed case, with a 0s 0.3s transition. This means that it's not really a transition, but the max-height is applied with a delay; it only applies once the transition is over. For this to work correctly, we also need to pick a numerical max-height for the opposite, non-collapsed, state. But unlike in the 2000px case, where picking too large of a number affects the quality of the transition, in this case, it really doesn't matter. So we can just pick a number that is so high that we know that no height will ever come close to this. I picked a million pixels. If you feel you may need to support content of a height of more than a million pixels, then 1) I'm sorry, and 2) just add a couple of zeros.
The second problem is the reason why we're not actually using min-height for the minimum height transition. Instead, there is an ::after pseudo-element in the container with a height that transitions from 50px to zero. This has the same effect as a min-height: It won't let the container shrink below whatever height the pseudo-element currently has. But because we're using height, not min-height, we can now use max-height (once again applied with a delay) to set the pseudo-element's actual height to zero once the transition is over, ensuring that at least outside the transition, even small elements have the correct height. Because min-height is stronger than max-height, this wouldn't work if we used the container's min-height instead of the pseudo-element's height. Just like the max-height in the previous paragraph, this max-height also needs a value for the opposite end of the transition. But in this case we can just pick the 50px.
Tested in Chrome (Win, Mac, Android, iOS), Firefox (Win, Mac, Android), Edge, IE11 (except for a flexbox layout issue with my demo that I didn't bother debugging), and Safari (Mac, iOS). Speaking of flexbox, it should be possible to make this work without using any flexbox; in fact I think you could make almost everything work in IE7  except for the fact that you won't have CSS transitions, making it a rather pointless exercise.
    You can't currently animate on height when one of the heights involved is auto, you have to set two explicit heights.
    The solution that I've always used was to first fade out, then shrink the font-size, padding and margin values. It doesn't look the same as a wipe, but it works without a static height or max-height.

Working example:

/* final display */
#menu #list {
    margin: .5em 1em;
    padding: 1em;
}

/* hide */
#menu:not(:hover) #list {
    font-size: 0;
    margin: 0;
    opacity: 0;
    padding: 0;
    /* fade out, then shrink */
    transition: opacity .25s,
                font-size .5s .25s,
                margin .5s .25s,
                padding .5s .25s;
}

/* reveal */
#menu:hover #list {
    /* unshrink, then fade in */
    transition: font-size .25s,
                margin .25s,
                padding .25s,
                opacity .5s .25s;
}<div id=""menu"">
    <b>hover me</b>
    <ul id=""list"">
        <li>item</li>
        <li>item</li>
        <li>item</li>
        <li>item</li>
        <li>item</li>
    </ul>
</div>

<p>Another paragraph...</p>

    You can, with a little bit of non-semantic jiggery-pokery. My usual approach is to animate the height of an outer DIV which has a single child which is a style-less DIV used only for measuring the content height.

function growDiv() {
  var growDiv = document.getElementById('grow');
  if (growDiv.clientHeight) {
    growDiv.style.height = 0;
  } else {
    var wrapper = document.querySelector('.measuringWrapper');
    growDiv.style.height = wrapper.clientHeight + ""px"";
  }
}#grow {
  -moz-transition: height .5s;
  -ms-transition: height .5s;
  -o-transition: height .5s;
  -webkit-transition: height .5s;
  transition: height .5s;
  height: 0;
  overflow: hidden;
  outline: 1px solid red;
}<input type=""button"" onclick=""growDiv()"" value=""grow"">
<div id='grow'>
  <div class='measuringWrapper'>
    <div>
      The contents of my div.
    </div>
    <div>
      The contents of my div.
    </div>
    <div>
      The contents of my div.
    </div>
    <div>
      The contents of my div.
    </div>
    <div>
      The contents of my div.
    </div>
    <div>
      The contents of my div.
    </div>
  </div>
</div>


One would like to just be able to dispense with the .measuringWrapper and just set the DIV's height to auto and have that animate, but that doesn't seem to work (the height gets set, but no animation occurs).

function growDiv() {
  var growDiv = document.getElementById('grow');
  if (growDiv.clientHeight) {
    growDiv.style.height = 0;
  } else {
    growDiv.style.height = 'auto';
  }
}#grow {
  -moz-transition: height .5s;
  -ms-transition: height .5s;
  -o-transition: height .5s;
  -webkit-transition: height .5s;
  transition: height .5s;
  height: 0;
  overflow: hidden;
  outline: 1px solid red;
}<input type=""button"" onclick=""growDiv()"" value=""grow"">
<div id='grow'>
  <div>
    The contents of my div.
  </div>
  <div>
    The contents of my div.
  </div>
  <div>
    The contents of my div.
  </div>
  <div>
    The contents of my div.
  </div>
  <div>
    The contents of my div.
  </div>
  <div>
    The contents of my div.
  </div>
</div>


My interpretation is that an explicit height is needed for the animation to run. You can't get an animation on height when either height (the start or end height) is auto.
    According to MDN Web Docs, auto values have been intentionally excluded from the CSS transitions spec, so instead of height: auto, use height: 100%, top, or the flex property in grid and flex layouts.
Expanding/collapsing an overlay
.grid-container {
  display: grid;
  position: absolute;
}

.content {
  background: aqua;
  height: 0;
  overflow: hidden;
  transition: 1s;
}

span:hover + .grid-container .content {
  height: 100%;
}<span>Hover over me!</span>

<div class=""grid-container"">

  <div class=""content"">Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</div>

</div>

<p>Rest of the page content...</p>

Expanding/collapsing a sliding overlay
.grid-container {
  display: grid;
  position: absolute;
  overflow: hidden;
  pointer-events: none; /* to enable interaction with elements below the container */
}

.content {
  background: aqua;
  pointer-events: auto;
  position: relative;
  top: -100%;
  transition: 1s;
}

span:hover + .grid-container .content {
  top: 0;
}<span>Hover over me!</span>

<div class=""grid-container"">

  <div class=""content"">Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</div>

</div>

<p>Rest of the page content...</p>

Expanding/collapsing in the document flow
html {
  display: grid;
}

body {
  display: flex;
  flex-direction: column;
}

.content {
  background: aqua;
  flex-basis: 0;
  overflow: hidden;
  transition: 1s;
}

span:hover + .content {
  flex: 1;
}<span>Hover over me!</span>

<div class=""content"">Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</div>

<p>Rest of the page content...</p>

    There was little mention of the Element.prototype.scrollHeight property which can be useful here and still may be used with a pure CSS transition, although scripting support would obviously be required. The property always contains the ""full"" height of an element, regardless of whether and how its content overflows as a result of collapsed height (e.g. height: 0).
As such, for a height: 0 (effectively fully collapsed) element, its ""normal"" or ""full"" height is still readily available through its scrollHeight value (invariably a pixel length).
For such an element, assuming it already has the transition set up like (using ul as per original question):
ul {
    height: 0;
    transition: height 1s; /* An example transition. */
}

We can trigger desired animated ""expansion"" of height, using CSS only, with something like the following (here assuming ul variable refers to the list):
ul.style.height = ul.scrollHeight + ""px"";

That's it. If you need to collapse the list, either of the two following statements will do:
ul.style.height = 0;
ul.style.removeProperty(""height"");

My particular use case revolved around animating lists of unknown and often considerable lengths, so I was not comfortable dealing with ""large enough"" height or max-height specification and risking cut-off content or content that you suddenly need to scroll (if overflow: auto, for example). Additionally, the easing and timing is broken with max-height-based solutions, because the used height may reach its maximum value a lot sooner than it would take for max-height to reach 9999px. And as screen resolutions grow, pixel lengths like 9999px leave a bad taste in my mouth. This particular solution solves the problem in an elegant manner, in my opinion.
Finally, here is hoping that future revisions of CSS address authors' need to do these kind of things even more elegantly -- revisit the notion of ""computed"" vs ""used"" and ""resolved"" values, and consider whether transitions should apply to computed values, including transitions with width and height (which currently get a bit of a special treatment).
    The accepted answer works for most cases, but it doesn't work well when your div can vary greatly in height  the animation speed is not dependent on the actual height of the content, and it can look choppy.

You can still perform the actual animation with CSS, but you need to use JavaScript to compute the height of the items, instead of trying to use auto. No jQuery is required, although you may have to modify this a bit if you want compatibility (works in the latest version of Chrome :)).

window.toggleExpand = function(element) {
    if (!element.style.height || element.style.height == '0px') { 
        element.style.height = Array.prototype.reduce.call(element.childNodes, function(p, c) {return p + (c.offsetHeight || 0);}, 0) + 'px';
    } else {
        element.style.height = '0px';
    }
}#menu #list {
    height: 0px;
    transition: height 0.3s ease;
    background: #d5d5d5;
    overflow: hidden;
}<div id=""menu"">
    <input value=""Toggle list"" type=""button"" onclick=""toggleExpand(document.getElementById('list'));"">
    <ul id=""list"">
        <!-- Works well with dynamically-sized content. -->
        <li>item</li>
        <li><div style=""height: 100px; width: 100px; background: red;""></div></li>
        <li>item</li>
        <li>item</li>
        <li>item</li>
    </ul>
</div>

    Use max-height with different transition easing and delay for each state. 

HTML:

<a href=""#"" id=""trigger"">Hover</a>
<ul id=""toggled"">
    <li>One</li>
    <li>Two</li>
    <li>Three</li>
<ul>


CSS:

#toggled{
    max-height: 0px;
    transition: max-height .8s cubic-bezier(0, 1, 0, 1) -.1s;
}

#trigger:hover + #toggled{
    max-height: 9999px;
    transition-timing-function: cubic-bezier(0.5, 0, 1, 0); 
    transition-delay: 0s;
}


See example: http://jsfiddle.net/0hnjehjc/1/
    One sentence solution: Use padding transition. It's enough for most of cases such as accordion, and even better because it's fast due to that the padding value is often not big.

If you want the animation process to be better, just raise the padding value.

.parent{ border-top: #999 1px solid;}
h1{ margin: .5rem; font-size: 1.3rem}
.children {
  height: 0;
  overflow: hidden;
  background-color: #dedede;
  transition: padding .2s ease-in-out, opacity .2s ease-in-out;
  padding: 0 .5rem;
  opacity: 0;
}
.children::before, .children::after{ content: """";display: block;}
.children::before{ margin-top: -2rem;}
.children::after{ margin-bottom: -2rem;}
.parent:hover .children {
  height: auto;
  opacity: 1;
  padding: 2.5rem .5rem;/* 0.5 + abs(-2), make sure it's less than expected min-height */
}<div class=""parent"">
  <h1>Hover me</h1>
  <div class=""children"">Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>Some content
    <br>
  </div>
</div>
<div class=""parent"">
  <h1>Hover me(long content)</h1>
  <div class=""children"">Some content
    <br>Some content<br>Some content
    <br>Some content<br>Some content
    <br>Some content<br>Some content
    <br>Some content<br>Some content
    <br>Some content<br>Some content
    <br>
  </div>
</div>
<div class=""parent"">
  <h1>Hover me(short content)</h1>
  <div class=""children"">Some content
    <br>Some content
    <br>Some content
    <br>
  </div>
</div>

    A visual workaround to animating height using CSS3 transitions is to animate the padding instead. 

You don't quite get the full wipe effect, but playing around with the transition-duration and padding values should get you close enough. If you don't want to explicitly set height/max-height, this should be what you're looking for.

div {
    height: 0;
    overflow: hidden;
    padding: 0 18px;
    -webkit-transition: all .5s ease;
       -moz-transition: all .5s ease;
            transition: all .5s ease;
}
div.animated {
    height: auto;
    padding: 24px 18px;
}


http://jsfiddle.net/catharsis/n5XfG/17/ (riffed off stephband's above jsFiddle)
    No hard coded values.

No JavaScript.

No approximations.

The trick is to use a hidden & duplicated div to get the browser to understand what 100% means. 

This method is suitable whenever you're able to duplicate the DOM of the element you wish to animate. 

.outer {
  border: dashed red 1px;
  position: relative;
}

.dummy {
  visibility: hidden;
}

.real {
  position: absolute;
  background: yellow;
  height: 0;
  transition: height 0.5s;
  overflow: hidden;
}

.outer:hover>.real {
  height: 100%;
}Hover over the box below:
<div class=""outer"">
  <!-- The actual element that you'd like to animate -->
  <div class=""real"">
unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable
content unpredictable content unpredictable content unpredictable content
  </div>
  <!-- An exact copy of the element you'd like to animate. -->
  <div class=""dummy"" aria-hidden=""true"">
unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable content unpredictable
content unpredictable content unpredictable content unpredictable content
  </div>
</div>

    As I post this there are over 30 answers already, but I feel my answer improves on the already accepted answer by jake.

I was not content with the issue that arises from simply using max-height and CSS3 transitions, since as many commenters noted, you have to set your max-height value very close to the actual height or you'll get a delay. See this JSFiddle for an example of that problem.

To get around this (while still using no JavaScript), I added another HTML element that transitions the transform: translateY CSS value.

This means both max-height and translateY are used: max-height allows the element to push down elements below it, while translateY gives the ""instant"" effect we want. The issue with max-height still exists, but its effect is lessened.
This means you can set a much larger height for your max-height value and worry about it less.

The overall benefit is that on the transition back in (the collapse), the user sees the translateY animation immediately, so it doesn't really matter how long the max-height takes.

Solution as Fiddle

body {
  font-family: sans-serif;
}

.toggle {
  position: relative;
  border: 2px solid #333;
  border-radius: 3px;
  margin: 5px;
  width: 200px;
}

.toggle-header {
  margin: 0;
  padding: 10px;
  background-color: #333;
  color: white;
  text-align: center;
  cursor: pointer;
}

.toggle-height {
  background-color: tomato;
  overflow: hidden;
  transition: max-height .6s ease;
  max-height: 0;
}

.toggle:hover .toggle-height {
  max-height: 1000px;
}

.toggle-transform {
  padding: 5px;
  color: white;
  transition: transform .4s ease;
  transform: translateY(-100%);
}

.toggle:hover .toggle-transform {
  transform: translateY(0);
}<div class=""toggle"">
  <div class=""toggle-header"">
    Toggle!
  </div>
  <div class=""toggle-height"">
    <div class=""toggle-transform"">
      <p>Content!</p>
      <p>Content!</p>
      <p>Content!</p>
      <p>Content!</p>
    </div>
  </div>
</div>

<div class=""toggle"">
  <div class=""toggle-header"">
    Toggle!
  </div>
  <div class=""toggle-height"">
    <div class=""toggle-transform"">
      <p>Content!</p>
      <p>Content!</p>
      <p>Content!</p>
      <p>Content!</p>
    </div>
  </div>
</div>

    My workaround is to transition max-height to the exact content height for a nice smooth animation, then use a transitionEnd callback to set max-height to 9999px so the content can resize freely.
var content = $('#content');
content.inner = $('#content .inner'); // inner div needed to get size of content when closed

// css transition callback
content.on('transitionEnd webkitTransitionEnd transitionend oTransitionEnd msTransitionEnd', function(e){
    if(content.hasClass('open')){
        content.css('max-height', 9999); // try setting this to 'none'... I dare you!
    }
});

$('#toggle').on('click', function(e){
    content.toggleClass('open closed');
    content.contentHeight = content.outerHeight();
    
    if(content.hasClass('closed')){
        
        // disable transitions & set max-height to content height
        content.removeClass('transitions').css('max-height', content.contentHeight);
        setTimeout(function(){
            
            // enable & start transition
            content.addClass('transitions').css({
                'max-height': 0,
                'opacity': 0
            });
            
        }, 10); // 10ms timeout is the secret ingredient for disabling/enabling transitions
        // chrome only needs 1ms but FF needs ~10ms or it chokes on the first animation for some reason
        
    }else if(content.hasClass('open')){  
        
        content.contentHeight += content.inner.outerHeight(); // if closed, add inner height to content height
        content.css({
            'max-height': content.contentHeight,
            'opacity': 1
        });
        
    }
});.transitions {
    transition: all 0.5s ease-in-out;
    -webkit-transition: all 0.5s ease-in-out;
    -moz-transition: all 0.5s ease-in-out;
}

body {
    font-family:Arial;
    line-height: 3ex;
}
code {
    display: inline-block;
    background: #fafafa;
    padding: 0 1ex;
}
#toggle {
    display:block;
    padding:10px;
    margin:10px auto;
    text-align:center;
    width:30ex;
}
#content {
    overflow:hidden;
    margin:10px;
    border:1px solid #666;
    background:#efefef;
    opacity:1;
}
#content .inner {
    padding:10px;
    overflow:auto;
}<script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js""></script>
<div id=""content"" class=""open"">
    <div class=""inner"">
        <h3>Smooth CSS Transitions Between <code>height: 0</code> and <code>height: auto</code></h3>
        <p>A clever workaround is to use <code>max-height</code> instead of <code>height</code>, and set it to something bigger than your content. Problem is the browser uses this value to calculate transition duration. So if you set it to <code>max-height: 1000px</code> but the content is only 100px high, the animation will be 10x too fast.</p>
        <p>Another option is to measure the content height with JS and transition to that fixed value, but then you have to keep track of the content and manually resize it if it changes.</p>
        <p>This solution is a hybrid of the two - transition to the measured content height, then set it to <code>max-height: 9999px</code> after the transition for fluid content sizing.</p>
    </div>
</div>

<br />

<button id=""toggle"">Challenge Accepted!</button>

    This solution uses a few techniques:

padding-bottom:100% 'hack' where percentages are defined in terms of the current width of the element. More info on this technique.
float shrink-wrapping, (necessitating an extra div to apply the float clearing hack)
non-semantic use of https://caniuse.com/#feat=css-writing-mode and some transformations to undo it (this allows use of the padding hack above in a vertical context)

The upshot though is that we get performant transitioning using CSS only, and a single transition function to smoothly achieve the transition; the holy grail!
Of course, there's a downside! I can't work out how to control the width at which content gets cut off (overflow:hidden); because of the padding-bottom hack, the width and height are intimately related. There may be a way though, so will come back to it.
https://jsfiddle.net/EoghanM/n1rp3zb4/28/
body {
  padding: 1em;
}

.trigger {
  font-weight: bold;
}

/* .expander is there for float clearing purposes only */
.expander::after {
  content: '';
  display: table;
  clear: both;
}

.outer {
  float: left; /* purpose: shrink to fit content */
  border: 1px solid green;
  overflow: hidden;
}

.inner {
  transition: padding-bottom 0.3s ease-in-out;  /* or whatever crazy transition function you can come up with! */
  padding-bottom: 0%;  /* percentage padding is defined in terms of width. The width at this level is equal to the height of the content */
  height: 0;

  /* unfortunately, change of writing mode has other bad effects like orientation of cursor */
  writing-mode: vertical-rl;
  cursor: default; /* don't want the vertical-text (sideways I-beam) */
  transform: rotate(-90deg) translateX(-100%);  /* undo writing mode */
  transform-origin: 0 0;
  margin: 0;  /* left/right margins here will add to height */
}

.inner > div { white-space: nowrap; }

.expander:hover .inner,  /* to keep open when expanded */
.trigger:hover+.expander .inner {
  padding-bottom: 100%;
}<div class=""trigger"">HoverMe</div>
<div class=""expander"">
  <div class=""outer"">
    <div class=""inner"">
      <div>First Item</div>
      <div>Content</div>
      <div>Content</div>
      <div>Content</div>
      <div>Long Content can't be wider than outer height unfortunately</div>
      <div>Last Item</div>
    </div>
  </div>
</div>
<div>
  after content</div>
</div>

    Alternate CSS-only solution with line-height, padding, opacity and margin:
body {
  background-color: linen;
}

main {
  background-color: white;
}

[id^=""toggle_""] ~ .content {
  line-height: 0;
  opacity: 0;
  padding: 0 .5rem;
  transition: .2s ease-out;
}

[id^=""toggle_""] ~ .content > p {
  margin: 0;
    transition: .2s ease-out;
}

[id^=""toggle_""]:checked ~ .content   {
  opacity: 1;
  padding: .5rem;
  line-height: 1.5;
}

[id^=""toggle_""]:checked ~ .content p  {
    margin-bottom: .75rem;
}

[id^=""toggle_""] + label {
  display: flex;
  justify-content: space-between;
  padding: 0.5em 1em;
  background: lightsteelblue;
  border-bottom: 1px solid gray;
  cursor: pointer;
}

[id^=""toggle_""] + label:before {
  content: ""Show"";
}

[id^=""toggle_""]:checked + label:before {
  content: ""Hide"";
}

[id^=""toggle_""] + label:after {
  content: ""\25BC"";
}

[id^=""toggle_""]:checked + label:after {
  content: ""\25B2"";
}<main>
    <div>
        <input type=""checkbox"" id=""toggle_1"" hidden>
        <label for=""toggle_1"" hidden></label>
        <div class=""content"">
            <p>
                Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dolor neque, commodo quis leo ut, auctor tincidunt mauris. Nunc fringilla tincidunt metus, non gravida lorem condimentum non. Duis ornare purus nisl, at porta arcu eleifend eget. Integer lorem ante, porta vulputate dui ut, blandit tempor tellus. Proin facilisis bibendum diam, sit amet rutrum est feugiat ut. Mauris rhoncus convallis arcu in condimentum. Donec volutpat dui eu mollis vulputate. Nunc commodo lobortis nunc at ultrices. Suspendisse in lobortis diam. Suspendisse eget vestibulum ex.
            </p>
        </div>
    </div>
    <div>
        <input type=""checkbox"" id=""toggle_2"" hidden>
        <label for=""toggle_2"" hidden></label>
        <div class=""content"">
            <p>
                Maecenas laoreet nunc sit amet nulla ultrices auctor. Vivamus sed nisi vitae nibh condimentum pulvinar eu vel lorem. Sed pretium viverra eros ut facilisis. In ut fringilla magna. Sed a tempor libero. Donec sapien libero, lacinia sed aliquet ut, imperdiet finibus tellus. Nunc tellus lectus, rhoncus in posuere quis, tempus sit amet enim. Morbi et erat ac velit fringilla dignissim. Donec commodo, est id accumsan cursus, diam dui hendrerit nisi, vel hendrerit purus dolor ut risus. Phasellus mattis egestas ipsum sed ullamcorper. In diam ligula, rhoncus vel enim et, imperdiet porta justo. Curabitur vulputate hendrerit nisl, et ultricies diam. Maecenas ac leo a diam cursus ornare nec eu quam.
            </p>
            <p>Sed non vulputate purus, sed consectetur odio. Sed non nibh fringilla, imperdiet odio nec, efficitur ex. Suspendisse ut dignissim enim. Maecenas felis augue, tempor sit amet sem fringilla, accumsan fringilla nibh. Quisque posuere lacus tortor, quis malesuada magna elementum a. Nullam id purus in ante molestie tincidunt. Morbi luctus orci eu egestas dignissim. Sed tincidunt, libero quis scelerisque bibendum, ligula nisi gravida libero, id lacinia nulla leo in elit.
            </p>
            <p>Aenean aliquam risus id consectetur sagittis. Aliquam aliquam nisl eu augue accumsan, vel maximus lorem viverra. Aliquam ipsum dolor, tempor et justo ac, fermentum mattis dui. Etiam at posuere ligula. Vestibulum tortor metus, viverra vitae mi non, laoreet iaculis purus. Praesent vel semper nibh. Curabitur a congue lacus. In et pellentesque lorem. Morbi posuere felis non diam vulputate, non vulputate ex vehicula. Vivamus ultricies, massa id sagittis consequat, sem mauris tincidunt nunc, eu vehicula augue quam ut mauris.
            </p>
        </div>
    </div>
        <div>
        <input type=""checkbox"" id=""toggle_3"" hidden>
        <label for=""toggle_3"" hidden></label>
        <div class=""content"">
            <p>
                Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dolor neque, commodo quis leo ut, auctor tincidunt mauris. Nunc fringilla tincidunt metus, non gravida lorem condimentum non. Duis ornare purus nisl, at porta arcu eleifend eget. Integer lorem ante, porta vulputate dui ut, blandit tempor tellus. Proin facilisis bibendum diam, sit amet rutrum est feugiat ut. Mauris rhoncus convallis arcu in condimentum. Donec volutpat dui eu mollis vulputate. Nunc commodo lobortis nunc at ultrices. Suspendisse in lobortis diam. Suspendisse eget vestibulum ex.
            </p>
            <p>Sed non vulputate purus, sed consectetur odio. Sed non nibh fringilla, imperdiet odio nec, efficitur ex. Suspendisse ut dignissim enim. Maecenas felis augue, tempor sit amet sem fringilla, accumsan fringilla nibh. Quisque posuere lacus tortor, quis malesuada magna elementum a. Nullam id purus in ante molestie tincidunt. Morbi luctus orci eu egestas dignissim. Sed tincidunt, libero quis scelerisque bibendum, ligula nisi gravida libero, id lacinia nulla leo in elit.
            </p>
        </div>
    </div>
</main>

    Ok, so I think I came up with a super simple answer...
no max-height, uses relative positioning, works on li elements, & is pure CSS.
I have not tested in anything but Firefox, though judging by the CSS, it should work on all browsers.

FIDDLE: http://jsfiddle.net/n5XfG/2596/

CSS

.wrap { overflow:hidden; }

.inner {
            margin-top:-100%;
    -webkit-transition:margin-top 500ms;
            transition:margin-top 500ms;
}

.inner.open { margin-top:0px; }


HTML

<div class=""wrap"">
    <div class=""inner"">Some Cool Content</div>
</div>

    Flexbox Solution

Pros:


simple
no JS
smooth transition


Cons:


element needs to be put in a fixed height flex container


The way it works is by always having flex-basis: auto on the element with content, and transitioning flex-grow and flex-shrink instead. 

Edit: Improved JS Fiddle inspired by the Xbox One interface. 

* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
  transition: 0.25s;
  font-family: monospace;
}

body {
  margin: 10px 0 0 10px;
}

.box {
  width: 150px;
  height: 150px;
  margin: 0 2px 10px 0;
  background: #2d333b;
  border: solid 10px #20262e;
  overflow: hidden;
  display: inline-flex;
  flex-direction: column;
}

.space {
  flex-basis: 100%;
  flex-grow: 1;
  flex-shrink: 0;    
}

p {
  flex-basis: auto;
  flex-grow: 0;
  flex-shrink: 1;
  background: #20262e;
  padding: 10px;
  width: 100%;
  text-align: left;
  color: white;
}

.box:hover .space {
  flex-grow: 0;
  flex-shrink: 1;
}
  
.box:hover p {
  flex-grow: 1;
  flex-shrink: 0;    
}<div class=""box"">
  <div class=""space""></div>
  <p>
    Super Metroid Prime Fusion
  </p>
</div>
<div class=""box"">
  <div class=""space""></div>
  <p>
    Resident Evil 2 Remake
  </p>
</div>
<div class=""box"">
  <div class=""space""></div>
  <p>
    Yolo The Game
  </p>
</div>
<div class=""box"">
  <div class=""space""></div>
  <p>
    Final Fantasy 7 Remake + All Additional DLC + Golden Tophat
  </p>
</div>
<div class=""box"">
  <div class=""space""></div>
  <p>
    DerpVille
  </p>
</div>


JS Fiddle
    I understand the question asks for a solution without JavaScript. But for those interested here's my solution using just a little bit of JS.
ok, so the element's css whose height will change by default is set to height: 0; and when open height: auto;. It also has transition: height .25s ease-out;. But of course the problem is that it won't transition to or from height: auto;
So what i've done is when opening or closing set the height to the scrollHeight property of the element. This new inline style will have higher specificity and override both height: auto; and height: 0; and the transition runs.
When opening i add a transitionend event listener which will run just once then remove the inline style setting it back to height: auto; which will allow the element to resize if necessary, as in this more complex example with sub menus https://codepen.io/ninjabonsai/pen/GzYyVe
When closing i remove the inline style right after the next event loop cycle by using setTimeout with no delay. This means height: auto; is temporarily overridden which allows the transition back to height 0;
const showHideElement = (element, open) => {
  element.style.height = element.scrollHeight + 'px';
  element.classList.toggle('open', open);

  if (open) {
    element.addEventListener('transitionend', () => {
      element.style.removeProperty('height');
    }, {
      once: true
    });
  } else {
    window.setTimeout(() => {
      element.style.removeProperty('height');
    });
  }
}

const menu = document.body.querySelector('#menu');
const list = document.body.querySelector('#menu > ul')

menu.addEventListener('mouseenter', () => showHideElement(list, true));
menu.addEventListener('mouseleave', () => showHideElement(list, false));#menu > ul {
  height: 0;
  overflow: hidden;
  background-color: #999;
  transition: height .25s ease-out;
}

#menu > ul.open {
  height: auto;
}<div id=""menu"">
  <a>hover me</a>
  <ul>
    <li>item</li>
    <li>item</li>
    <li>item</li>
    <li>item</li>
    <li>item</li>
  </ul>
</div>

    A lot of answers, some better than other, most using JS. I believe I figured this out in two use-cases that are easy to understand, although one of them only works in Firefox for now.
This works in every browser if you want to toggle an overlay:
.demo01 {
  overflow: hidden;
  position: absolute;
  pointer-events: none;
}
.demo01__content {
  background: lightgray;
  padding: 1rem;
  pointer-events: all;
  transform: translateY(-100%);
  transition: transform 1s, visibility 1s;
  visibility: hidden;
}
:checked ~ .demo01 .demo01__content {
  transform: translateY(0);
  visibility: visible;
}<input type=""checkbox"" />  Toggle
<div>Something before </div>
<div class=""demo01"">
  <div class=""demo01__content"">
    This content should<br />
    toggle! 
  </div>
</div>
<div>Something after </div>

This (for now) works only in Firefox for toggling in the document flow:
.demo02 {
  display: grid;
  grid-template-rows: 0fr;
  overflow: hidden;
  transition: grid-template-rows 1s;
}
.demo02__content {
  align-self: end;
  min-height: 0;
  background: lightgray;
  transition: visibility 1s;
  visibility: hidden;
}
.demo02__padding {
  padding: 1rem;
}
:checked ~ .demo02 {
  grid-template-rows: 1fr;
}
:checked ~ .demo02 .demo02__content {
  visibility: visible;
}<input type=""checkbox"" />  Toggle
<div>Something before </div>
<div class=""demo02"">
  <div class=""demo02__content"">
    <div class=""demo02__padding"">
      This content should<br />
      toggle! 
    </div>
  </div>
</div>
<div>Something after </div>

I wrote a blog post about these techniques, with links to the Chromium bug that needs fixing for this to be 100% usable.
    Flexible Height CSS Only Solution
I've stumbled upon a quirky solution using flex behavior. It works in at least Chrome and Firefox.

First, the height transition only works between 0 and 100%, two
numeric values. Since ""auto"" is not a numeric value, fractional
increments don't exist between 0 and ""auto"". 100% is a flexible
value, so no specific height is required.

Second, both the outer container and the inner container of the hidden content must be set to display: flex with flex-direction: column.

Third, the outer container must have a height property. It doesn't matter what the height is since the flex behavior is prioritized over the height, so I use a height of 0.


.outer-container { height: 0; display: flex; flex-direction: column; }

.inner-container { display: flex; flex-direction: column; }

.hidden-content { 
    height: 0; 
    opacity: 0; 
    transition: height 1s 0.5s ease-in-out, opacity 0.5s ease-in-out;
    /* transition out: first fade out opacity, then shrink height after a delay equal to the opacity duration */
    }

.trigger:hover + .inner-container > .hidden-content { 
    height: 100%; 
    opacity: 1; 
    transition: height 1s ease-in-out, opacity 0.5s 1s ease-in-out;
    /* transition in: first expand height, then fade in opacity after a delay equal to the height duration */
}<div class=""outer-container"">
  <a href=""#"" class=""trigger"">Hover to Reveal Inner Container's Hidden Content</a>
  <div class=""inner-container"">
    <div class=""hidden-content"">This is hidden content. When triggered by hover, its height transitions from 0 to 100%, which pushes other content in the same container down gradually.</div>
    <div>Within the same container, this other content is pushed down gradually as the hidden content's height transitions from 0 to 100%.</div>
  </div>
</div>

Press the Run Code Snippet button to see the transition in action.
It's CSS only, with no specific height.
    You could do this by creating a reverse (collapse) animation with clip-path.

#child0 {
    display: none;
}
#parent0:hover #child0 {
    display: block;
    animation: height-animation;
    animation-duration: 200ms;
    animation-timing-function: linear;
    animation-fill-mode: backwards;
    animation-iteration-count: 1;
    animation-delay: 200ms;
}
@keyframes height-animation {
    0% {
        clip-path: polygon(0% 0%, 100% 0.00%, 100% 0%, 0% 0%);
    }
    100% {
        clip-path: polygon(0% 0%, 100% 0.00%, 100% 100%, 0% 100%);
    }
}<div id=""parent0"">
    <h1>Hover me (height: 0)</h1>
    <div id=""child0"">Some content
        <br>Some content
        <br>Some content
        <br>Some content
        <br>Some content
        <br>Some content
        <br>
    </div>
</div>

    You can transition from height:0 to height:auto providing that you also provide min-height and max-height.

div.stretchy{
    transition: 1s linear;
}

div.stretchy.hidden{
    height: 0;
}

div.stretchy.visible{
    height: auto;
    min-height:40px;
    max-height:400px;
}

    I've recently been transitioning the max-height on the li elements rather than the wrapping ul.

The reasoning is that the delay for small max-heights is far less noticeable (if at all) compared to large max-heights, and I can also set my max-height value relative to the font-size of the li rather than some arbitrary huge number by using ems or rems.

If my font size is 1rem, I'll set my max-height to something like 3rem (to accommodate wrapped text). You can see an example here:

http://codepen.io/mindfullsilence/pen/DtzjE
    Expanding on @jake's answer, the transition will go all the way to the max height value, causing an extremely fast animation - if you set the transitions for both :hover and off you can then control the crazy speed a little bit more.

So the li:hover is when the mouse enters the state and then the transition on the non-hovered property will be the mouse leave.

Hopefully this will be of some help.

e.g: 

.sidemenu li ul {
   max-height: 0px;
   -webkit-transition: all .3s ease;
   -moz-transition: all .3s ease;
   -o-transition: all .3s ease;
   -ms-transition: all .3s ease;
   transition: all .3s ease;
}
.sidemenu li:hover ul {
    max-height: 500px;
    -webkit-transition: all 1s ease;
   -moz-transition: all 1s ease;
   -o-transition: all 1s ease;
   -ms-transition: all 1s ease;
   transition: all 1s ease;
}
/* Adjust speeds to the possible height of the list */


Here's a fiddle: http://jsfiddle.net/BukwJ/
    I combined both max-height and negative margin to achive this animation.
I used max-height: 2000px, but you can push that number to much higher value if needed.
I animate the max-height on the expand and the margin on collapse.
The js part is just the click, can be replaced with :hover or checkbox for pure css solution.
There are only 2 problems i can see so far,

The transition-timing is limited. (i added only 2 timings)
If you click again while the dropdown is collapsing, it will jump.

Here's the result
[...document.querySelectorAll('.ab')].forEach(wrapper => {
    wrapper.addEventListener('click', function () {
        this.classList.toggle('active');
    });
});* {
  margin: 0;
  box-sizing: border-box;
}

.c {
  overflow: hidden;
}

.items {
  width: 100%;
  visibility: hidden;
  max-height: 0;
  margin-bottom: -2000px;
  -webkit-transition: margin 0.6s cubic-bezier(1, 0, 1, 1), max-height 0s 0.6s linear, visibility 0s 0.6s linear;
  transition: margin 0.6s cubic-bezier(1, 0, 1, 1), max-height 0s 0.6s linear, visibility 0s 0.6s linear;
}
.items > * {
  padding: 1rem;
  background-color: #ddd;
  -webkit-transition: background-color 0.6s ease;
  transition: background-color 0.6s ease;
}
.items > *:hover {
  background-color: #eee;
}

.ab {
  padding: 1rem;
  cursor: pointer;
  background: #eee;
}
.ab.active + .c .items {
  max-height: 2000px;
  margin-bottom: 0;
  visibility: visible;
  -webkit-transition: max-height 0.6s cubic-bezier(1, 0, 1, 1);
  transition: max-height 0.6s cubic-bezier(1, 0, 1, 1);
}

.dropdown {
  margin-right: 1rem;
}

.wrapper {
  display: -webkit-box;
  display: flex;
}<div class=""wrapper"">
    <div class=""dropdown"">
        <div class=""ab"">just text</div>
        <div class=""ab"">just text</div>
        <div class=""ab"">dropdown</div>
        <div class=""c"">
            <div class=""items"">
                <p>items</p>
                <p>items</p>
                <p>items asl;dk l;kasl;d sa;lk</p>
                <p>items sal;kd</p>
                <p>items</p>
            </div>
        </div>
        <div class=""ab"">just text</div>
        <div class=""ab"">just text</div>
    </div>
    
    <div class=""dropdown"">
        <div class=""ab"">dropdown</div>
        <div class=""c"">
            <div class=""items"">
                <p>items</p>
                <p>items</p>
                <p>items</p>
                <p>items</p>
                <p>items</p>
                <p>items</p>
                <p>items</p>
                <p>items</p>
                <p>items</p>
                <p>items</p>
                <p>items</p>
            </div>
        </div>
        <div class=""ab"">text</div>
    </div>
    
    <div class=""dropdown"">
        <div class=""ab"">placeholder</div>
        <div class=""ab"">dropdown</div>
        <div class=""c"">
            <div class=""items"">
                <p>items</p>
                <p>items</p>
            </div>
        </div>
        <div class=""ab"">placeholder</div>
        <div class=""ab"">placeholder</div>
        <div class=""ab"">placeholder</div>
    </div>
</div>
<h1>text to be pushed</h1>

    EDIT: Scroll down for updated answer 
I was making a drop down list and saw this Post ... many different answers but I decide to share my drop down list too, ... It's not perfect but at least it will using only css for drop down! I've been using transform:translateY(y) to transform the list to the view ...
You can see more in the test 
http://jsfiddle.net/BVEpc/4/ 
I've placed div behind every li because my drop down list are coming from up and to show them properly this was needed, my div code is:

#menu div {
    transition: 0.5s 1s;
    z-index:-1;
    -webkit-transform:translateY(-100%);
    -webkit-transform-origin: top;
}


and hover is : 

#menu > li:hover div {
    transition: 0.5s;
    -webkit-transform:translateY(0);
}


and because ul height is set to the content it can get over your body content that's why I did this for ul:

 #menu ul {
    transition: 0s 1.5s;
    visibility:hidden;
    overflow:hidden;
}


and hover:

#menu > li:hover ul {
     transition:none;
     visibility:visible;
}


the second time after transition is delay and it will get hidden after my drop down list has been closed animately ... 
Hope later someone get benefit of this one.

EDIT: I just can't believe ppl actually using this prototype! this drop down menu is only for one sub menu and that's all!!
I've updated a better one that can have two sub menu for both ltr and rtl direction with IE 8 support. 
Fiddle for LTR 
Fiddle for RTL 
hopefully someone find this useful in future.
    I have not read everything in detail but I have had this problem recently and I did what follows:
div.class{
   min-height:1%;
   max-height:200px;
   -webkit-transition: all 0.5s ease;
   -moz-transition: all 0.5s ease;
   -o-transition: all 0.5s ease;
   -webkit-transition: all 0.5s ease;
   transition: all 0.5s ease;
   overflow:hidden;
}

div.class:hover{
   min-height:100%;
   max-height:3000px;
}

This allows you to have a div that at first shows content up to 200px height and on hover it's size becomes at least as high as the whole content of the div. The Div does not become 3000px but 3000px is the limit that I am imposing. Make sure to have the transition on the non :hover, otherwise you might get some strange rendering. In this way the :hover inherits from the non :hover.
Transition does not work form px to % or to auto. You need to use same unit of measure.
    I just animated the <li> element instead of the whole container:

<style>
.menu {
    border: solid;
}
.menu ul li {
    height: 0px;
    transition: height 0.3s;
    overflow: hidden;
}
button:hover ~ .wrapper .menu ul li,
button:focus ~ .wrapper .menu ul li,
.menu:hover ul li {
    height: 20px;
}
</style>


<button>Button</button>
<div class=""wrapper"">
    <div class=""menu"">
        <ul>
            <li>menuitem</li>
            <li>menuitem</li>
            <li>menuitem</li>
            <li>menuitem</li>
            <li>menuitem</li>
            <li>menuitem</li>
        </ul>
    </div>
</div>


you can add ul: margin 0;  to have 0 height.
    ","[2588, 3317, 410, 139, 250, 163, 107, 21, 50, 57, 39, 9, 57, 35, 26, 51, 8, 5, 21, 11, 6, 3, 2, 5, 12, 6, 8, 2, 16, 4, 1]",1447762,534,2010-08-18T02:50:35,2022-02-22 15:30:04Z,css 
Add table row in jQuery,"
                
What is the best method in jQuery to add an additional row to a table as the last row?

Is this acceptable?

$('#myTable').append('<tr><td>my data</td><td>more data</td></tr>');


Are there limitations to what you can add to a table like this (such as inputs, selects, number of rows)?
    The approach you suggest is not guaranteed to give you the result you're looking for - what if you had a tbody for example:

<table id=""myTable"">
  <tbody>
    <tr>...</tr>
    <tr>...</tr>
  </tbody>
</table>


You would end up with the following:

<table id=""myTable"">
  <tbody>
    <tr>...</tr>
    <tr>...</tr>
  </tbody>
  <tr>...</tr>
</table>


I would therefore recommend this approach instead:

$('#myTable tr:last').after('<tr>...</tr><tr>...</tr>');


You can include anything within the after() method as long as it's valid HTML, including multiple rows as per the example above.

Update: Revisiting this answer following recent activity with this question. eyelidlessness makes a good comment that there will always be a tbody in the DOM; this is true, but only if there is at least one row. If you have no rows, there will be no tbody unless you have specified one yourself.

DaRKoN_ suggests appending to the tbody rather than adding content after the last tr. This gets around the issue of having no rows, but still isn't bulletproof as you could theoretically have multiple tbody elements and the row would get added to each of them.

Weighing everything up, I'm not sure there is a single one-line solution that accounts for every single possible scenario. You will need to make sure the jQuery code tallies with your markup.

I think the safest solution is probably to ensure your table always includes at least one tbody in your markup, even if it has no rows. On this basis, you can use the following which will work however many rows you have (and also account for multiple tbody elements):

$('#myTable > tbody:last-child').append('<tr>...</tr><tr>...</tr>');

    jQuery has a built-in facility to manipulate DOM elements on the fly.

You can add anything to your table like this:

$(""#tableID"").find('tbody')
    .append($('<tr>')
        .append($('<td>')
            .append($('<img>')
                .attr('src', 'img.png')
                .text('Image cell')
            )
        )
    );


The $('<some-tag>') thing in jQuery is a tag object that can have several attr attributes that can be set and get, as well as text, which represents the text between the tag here: <tag>text</tag>.

This is some pretty weird indenting, but it's easier for you to see what's going on in this example.
    So things have changed ever since @Luke Bennett answered this question. Here is an update.

jQuery since version 1.4(?) automatically detects if the element you are trying to insert (using any of the append(), prepend(), before(), or after() methods) is a <tr> and inserts it into the first <tbody> in your table or wraps it into a new <tbody> if one doesn't exist. 

So yes your example code is acceptable and will work fine with jQuery 1.4+. ;)

$('#myTable').append('<tr><td>my data</td><td>more data</td></tr>');

    I know you have asked for a jQuery method. I looked a lot and find that we can do it in a better way than using JavaScript directly by the following function.
tableObject.insertRow(index)

index is an integer that specifies the position of the row to insert (starts at 0). The value of -1 can also be used; which result in that the new row will be inserted at the last position.
This parameter is required in Firefox and Opera, but it is optional in Internet Explorer, Chrome and Safari.
If this parameter is omitted, insertRow() inserts a new row at the last position in InternetExplorer and at the first position in Chrome and Safari.
It will work for every acceptable structure of HTML table.
The following example will insert a row in last (-1 is used as index):
<html>
    <head>
        <script type=""text/javascript"">
        function displayResult()
        {
            document.getElementById(""myTable"").insertRow(-1).innerHTML = '<td>1</td><td>2</td>';
        }
        </script>
    </head>

    <body>       
        <table id=""myTable"" border=""1"">
            <tr>
                <td>cell 1</td>
                <td>cell 2</td>
            </tr>
            <tr>
                <td>cell 3</td>
                <td>cell 4</td>
            </tr>
        </table>
        <br />
        <button type=""button"" onclick=""displayResult()"">Insert new row</button>            
    </body>
</html>

I hope it helps.
    What if you had a <tbody> and a <tfoot>? 

Such as:

<table>
    <tbody>
        <tr><td>Foo</td></tr>
    </tbody>
    <tfoot>
        <tr><td>footer information</td></tr>
    </tfoot>
</table>


Then it would insert your new row in the footer - not to the body.

Hence the best solution is to include a <tbody> tag and use .append, rather than .after.

$(""#myTable > tbody"").append(""<tr><td>row content</td></tr>"");

    I solved it this way.

Using jquery

$('#tab').append($('<tr>')
    .append($('<td>').append(""text1""))
    .append($('<td>').append(""text2""))
    .append($('<td>').append(""text3""))
    .append($('<td>').append(""text4""))
  )




Snippet

$('#tab').append($('<tr>')
  .append($('<td>').append(""text1""))
  .append($('<td>').append(""text2""))
  .append($('<td>').append(""text3""))
  .append($('<td>').append(""text4""))
)<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>

<table id=""tab"">
  <tr>
    <th>Firstname</th>
    <th>Lastname</th> 
    <th>Age</th>
    <th>City</th>
  </tr>
  <tr>
    <td>Jill</td>
    <td>Smith</td> 
    <td>50</td>
    <td>New York</td>
  </tr>
</table>

    In my opinion the fastest and clear way is
//Try to get tbody first with jquery children. works faster!
var tbody = $('#myTable').children('tbody');

//Then if no tbody just select your table 
var table = tbody.length ? tbody : $('#myTable');

//Add row
table.append('<tr><td>hello></td></tr>');

here is demo  Fiddle
Also I can recommend a small function to make more html changes
//Compose template string
String.prototype.compose = (function (){
var re = /\{{(.+?)\}}/g;
return function (o){
        return this.replace(re, function (_, k){
            return typeof o[k] != 'undefined' ? o[k] : '';
        });
    }
}());

If you use my string composer you can do this like
var tbody = $('#myTable').children('tbody');
var table = tbody.length ? tbody : $('#myTable');
var row = '<tr>'+
    '<td>{{id}}</td>'+
    '<td>{{name}}</td>'+
    '<td>{{phone}}</td>'+
'</tr>';


//Add row
table.append(row.compose({
    'id': 3,
    'name': 'Lee',
    'phone': '123 456 789'
}));

Here is demo
Fiddle
    This can be done easily using the ""last()"" function of jQuery.

$(""#tableId"").last().append(""<tr><td>New row</td></tr>"");

    I recommend

$('#myTable > tbody:first').append('<tr>...</tr><tr>...</tr>'); 


as opposed to 

$('#myTable > tbody:last').append('<tr>...</tr><tr>...</tr>'); 


The first and last keywords work on the first or last tag to be started, not closed. Therefore, this plays nicer with nested tables, if you don't want the nested table to be changed, but instead add to the overall table. At least, this is what I found.

<table id=myTable>
  <tbody id=first>
    <tr><td>
      <table id=myNestedTable>
        <tbody id=last>
        </tbody>
      </table>
    </td></tr>
  </tbody>
</table>

    )Daryl:
You can append it to the tbody using the appendTo method like this:
$(() => {
   $(""<tr><td>my data</td><td>more data</td></tr>"").appendTo(""tbody"");
});

You'll probably want to use the latest JQuery and ECMAScript. Then you can use a back-end language to add your data to the table. You can also wrap it in a variable like so:
$(() => {
  var t_data = $('<tr><td>my data</td><td>more data</td></tr>');
  t_data.appendTo('tbody');
});

    The answers above are very helpful, but when student refer this link to add data from form they often require a sample. 

I want to contribute an sample get input from from and use .after() to insert tr to table using string interpolation.

function add(){
  let studentname = $(""input[name='studentname']"").val();
  let studentmark = $(""input[name='studentmark']"").val();

  $('#student tr:last').after(`<tr><td>${studentname}</td><td>${studentmark}</td></tr>`);
}


function add(){
let studentname = $(""input[name='studentname']"").val();
let studentmark = $(""input[name='studentmark']"").val();

$('#student tr:last').after(`<tr><td>${studentname}</td><td>${studentmark}</td></tr>`);
}<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<!DOCTYPE html>
<html>
<head>
<style>
table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #dddddd;
}
</style>
</head>
<body>
<form>
<input type='text' name='studentname' />
<input type='text' name='studentmark' />
<input type='button' onclick=""add()"" value=""Add new"" />
</form>
<table id='student'>
  <thead>
    <th>Name</th>
    <th>Mark</th>
  </thead>
</table>
</body>
</html>

    Pure JS is quite short in your case 

myTable.firstChild.innerHTML += '<tr><td>my data</td><td>more data</td></tr>'


function add() {
  myTable.firstChild.innerHTML+=`<tr><td>date</td><td>${+new Date}</td></tr>`
}td {border: 1px solid black;}<button onclick=""add()"">Add</button><br>
<table id=""myTable""><tbody></tbody> </table>


(if we remove <tbody> and firstChild it will also works but wrap every row with <tbody>)
    Try this : very simple way

$('<tr><td>3</td></tr><tr><td>4</td></tr>').appendTo(""#myTable tbody"");<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js""></script>
<table id=""myTable"">
  <tbody>
    <tr><td>1</td></tr>
    <tr><td>2</td></tr>
  </tbody>
</table>

    This is my solution

$('#myTable').append('<tr><td>'+data+'</td><td>'+other data+'</td>...</tr>');

    You can use this great jQuery add table row function. It works great with tables that have <tbody> and that don't. Also it takes into the consideration the colspan of your last table row.

Here is an example usage:

// One table
addTableRow($('#myTable'));
// add table row to number of tables
addTableRow($('.myTables'));

    I'm using this way when there is not any row in the table, as well as, each row is quite complicated.

style.css:

...
#templateRow {
  display:none;
}
...


xxx.html

...
<tr id=""templateRow""> ... </tr>
...

$(""#templateRow"").clone().removeAttr(""id"").appendTo( $(""#templateRow"").parent() );

...

        // Create a row and append to table
    var row = $('<tr />', {})
        .appendTo(""#table_id"");

    // Add columns to the row. <td> properties can be given in the JSON
    $('<td />', {
        'text': 'column1'
    }).appendTo(row);

    $('<td />', {
        'text': 'column2',
        'style': 'min-width:100px;'
    }).appendTo(row);

    if you have another variable you can access in <td> tag like that try.

This way I hope it would be helpful

var table = $('#yourTableId');
var text  = 'My Data in td';
var image = 'your/image.jpg'; 
var tr = (
  '<tr>' +
    '<td>'+ text +'</td>'+
    '<td>'+ text +'</td>'+
    '<td>'+
      '<img src=""' + image + '"" alt=""yourImage"">'+
    '</td>'+
  '</tr>'
);

$('#yourTableId').append(tr);

    <table id=""myTable"">
  <tbody>
    <tr>...</tr>
    <tr>...</tr>
  </tbody>
  <tr>...</tr>
</table>


Write with a javascript function

document.getElementById(""myTable"").insertRow(-1).innerHTML = '<tr>...</tr><tr>...</tr>';

    I have tried the most upvoted one, but it did not work for me, but below works well.
$('#mytable tr').last().after('<tr><td></td></tr>');

Which will work even there is a tobdy there.
    For the best solution posted here, if there's a nested table on the last row, the new row will be added to the nested table instead of the main table. A quick solution (considering tables with/without tbody and tables with nested tables):

function add_new_row(table, rowcontent) {
    if ($(table).length > 0) {
        if ($(table + ' > tbody').length == 0) $(table).append('<tbody />');
        ($(table + ' > tr').length > 0) ? $(table).children('tbody:last').children('tr:last').append(rowcontent): $(table).children('tbody:last').append(rowcontent);
    }
}


Usage example:

add_new_row('#myTable','<tr><td>my new row</td></tr>');

    Add tabe row using JQuery:

if you want to add row after last of table's row child, you can try this

$('#myTable tr:last').after('<tr>...</tr><tr>...</tr>');


if you want to add row 1st of table's row child, you can try this

$('#myTable tr').after('<tr>...</tr><tr>...</tr>');

    I found this AddRow plugin quite useful for managing table rows. Though, Luke's solution would be the best fit if you just need to add a new row.
    If you want to add row before the <tr> first child.

$(""#myTable > tbody"").prepend(""<tr><td>my data</td><td>more data</td></tr>"");


If you want to add row after the <tr> last child.

$(""#myTable > tbody"").append(""<tr><td>my data</td><td>more data</td></tr>"");

    I was having some related issues, trying to insert a table row after the clicked row. All is fine except the .after() call does not work for the last row.

$('#traffic tbody').find('tr.trafficBody).filter(':nth-child(' + (column + 1) + ')').after(insertedhtml);


I landed up with a very untidy solution:

create the table as follows (id for each row):

<tr id=""row1""> ... </tr>
<tr id=""row2""> ... </tr>
<tr id=""row3""> ... </tr>


etc ...

and then :

$('#traffic tbody').find('tr.trafficBody' + idx).after(html);

    My solution:

//Adds a new table row
$.fn.addNewRow = function (rowId) {
    $(this).find('tbody').append('<tr id=""' + rowId + '""> </tr>');
};


usage:

$('#Table').addNewRow(id1);

    Neil's answer is by far the best one. However things get messy really fast. My suggestion would be to use variables to store elements and append them to the DOM hierarchy.

HTML

<table id=""tableID"">
    <tbody>
    </tbody>
</table>


JAVASCRIPT

// Reference to the table body
var body = $(""#tableID"").find('tbody');

// Create a new row element
var row = $('<tr>');

// Create a new column element
var column = $('<td>');

// Create a new image element
var image = $('<img>');
image.attr('src', 'img.png');
image.text('Image cell');

// Append the image to the column element
column.append(image);
// Append the column to the row element
row.append(column);
// Append the row to the table body
body.append(row);

    Here is some hacketi hack code. I wanted to maintain a row template in an HTML page. Table rows 0...n are rendered at request time, and this example has one hardcoded row and a simplified template row. The template table is hidden, and the row tag must be within a valid table or browsers may drop it from the DOM tree. Adding a row uses counter+1 identifier, and the current value is maintained in the data attribute. It guarantees each row gets unique URL parameters.

I have run tests on InternetExplorer8, InternetExplorer9, Firefox, Chrome, Opera, Nokia Lumia 800, Nokia C7 (with Symbian 3), Android stock and Firefox beta browsers.

<table id=""properties"">
<tbody>
  <tr>
    <th>Name</th>
    <th>Value</th>
    <th>&nbsp;</th>
  </tr>
  <tr>
    <td nowrap>key1</td>
    <td><input type=""text"" name=""property_key1"" value=""value1"" size=""70""/></td>
    <td class=""data_item_options"">
       <a class=""buttonicon"" href=""javascript:deleteRow()"" title=""Delete row"" onClick=""deleteRow(this); return false;""></a>
    </td>
  </tr>
</tbody>
</table>

<table id=""properties_rowtemplate"" style=""display:none"" data-counter=""0"">
<tr>
 <td><input type=""text"" name=""newproperty_name_\${counter}"" value="""" size=""35""/></td>
 <td><input type=""text"" name=""newproperty_value_\${counter}"" value="""" size=""70""/></td>
 <td><a class=""buttonicon"" href=""javascript:deleteRow()"" title=""Delete row"" onClick=""deleteRow(this); return false;""></a></td>
</tr>
</table>
<a class=""action"" href=""javascript:addRow()"" onclick=""addRow('properties'); return false"" title=""Add new row"">Add row</a><br/>
<br/>

- - - - 
// add row to html table, read html from row template
function addRow(sTableId) {
    // find destination and template tables, find first <tr>
    // in template. Wrap inner html around <tr> tags.
    // Keep track of counter to give unique field names.
    var table  = $(""#""+sTableId);
    var template = $(""#""+sTableId+""_rowtemplate"");
    var htmlCode = ""<tr>""+template.find(""tr:first"").html()+""</tr>"";
    var id = parseInt(template.data(""counter""),10)+1;
    template.data(""counter"", id);
    htmlCode = htmlCode.replace(/\${counter}/g, id);
    table.find(""tbody:last"").append(htmlCode);
}

// delete <TR> row, childElem is any element inside row
function deleteRow(childElem) {
    var row = $(childElem).closest(""tr""); // find <tr> parent
    row.remove();
}


PS: I give all credits to the jQuery team; they deserve everything. JavaScript programming without jQuery - I don't even want think about that nightmare.
    <tr id=""tablerow""></tr>

$('#tablerow').append('<tr>...</tr><tr>...</tr>');

    I Guess i have done in my project , here it is:

html

<div class=""container"">
    <div class = ""row"">
    <div class = ""span9"">
        <div class = ""well"">
          <%= form_for (@replication) do |f| %>
    <table>
    <tr>
      <td>
          <%= f.label :SR_NO %>
      </td>
      <td>
          <%= f.text_field :sr_no , :id => ""txt_RegionName"" %>
      </td>
    </tr>
    <tr>
      <td>
        <%= f.label :Particular %>
      </td>
      <td>
        <%= f.text_area :particular , :id => ""txt_Region"" %>
      </td>
    </tr>
    <tr>
      <td>
        <%= f.label :Unit %>
      </td>
      <td>
        <%= f.text_field :unit ,:id => ""txt_Regio"" %>
      </td>
      </tr>
      <tr>

      <td> 
        <%= f.label :Required_Quantity %>
      </td>
      <td>
        <%= f.text_field :quantity ,:id => ""txt_Regi"" %>
      </td>
    </tr>
    <tr>
    <td></td>
    <td>
    <table>
    <tr><td>
    <input type=""button""  name=""add"" id=""btn_AddToList"" value=""add"" class=""btn btn-primary"" />
    </td><td><input type=""button""  name=""Done"" id=""btn_AddToList1"" value=""Done"" class=""btn btn-success"" />
    </td></tr>
    </table>
    </td>
    </tr>
    </table>
    <% end %>
    <table id=""lst_Regions"" style=""width: 500px;"" border= ""2"" class=""table table-striped table-bordered table-condensed"">
    <tr>
    <td>SR_NO</td>
    <td>Item Name</td>
    <td>Particular</td>
    <td>Cost</td>
    </tr>
    </table>
    <input type=""button"" id= ""submit"" value=""Submit Repication""  class=""btn btn-success"" />
    </div>
    </div>
    </div>
</div>


js

$(document).ready(function() {     
    $('#submit').prop('disabled', true);
    $('#btn_AddToList').click(function () {
     $('#submit').prop('disabled', true);
    var val = $('#txt_RegionName').val();
    var val2 = $('#txt_Region').val();
    var val3 = $('#txt_Regio').val();
    var val4 = $('#txt_Regi').val();
    $('#lst_Regions').append('<tr><td>' + val + '</td>' + '<td>' + val2 + '</td>' + '<td>' + val3 + '</td>' + '<td>' + val4 + '</td></tr>');
    $('#txt_RegionName').val('').focus();
    $('#txt_Region').val('');
        $('#txt_Regio').val('');
        $('#txt_Regi').val('');
    $('#btn_AddToList1').click(function () {
         $('#submit').prop('disabled', false).addclass('btn btn-warning');
    });
      });
});

    ","[2587, 2264, 814, 329, 79, 178, 21, 36, 27, 38, 2, 5, 2, 5, 8, 11, 18, 11, 7, 9, 1, 14, 2, 9, 3, 12, 10, 9, 7, 7, 7]",1898861,509,2008-10-04T21:33:10,2022-04-20 05:26:56Z,javascript 
How do I vertically center text with CSS? [duplicate],"
                    
            
        
            
                
                    
                        This question already has answers here:
                        
                    
                
            
                    
                        How to center an element horizontally and vertically
                            
                                (26 answers)
                            
                    
                    
                        How do I vertically align text in a div?
                            
                                (33 answers)
                            
                    
                Closed 3 years ago.
        

    

I have a <div> element which contains text and I want to align the contents of this <div> vertically center.
Here is my <div> style:
#box {
  height: 170px;
  width: 270px;
  background: #000;
  font-size: 48px;
  color: #FFF;
  text-align: center;
}<div id=""box"">
  Lorem ipsum dolor sit
</div>

What is the best way to achieve this goal?
    You can try this basic approach:

div {
  height: 100px;
  line-height: 100px;
  text-align: center;
  border: 2px dashed #f69c55;
}<div>
  Hello World!
</div>


It only works for a single line of text though, because we set the line's height to the same height as the containing box element.



A more versatile approach

This is another way to align text vertically. This solution will work for a single line and multiple lines of text, but it still requires a fixed height container:

div {
  height: 100px;
  line-height: 100px;
  text-align: center;
  border: 2px dashed #f69c55;
}
span {
  display: inline-block;
  vertical-align: middle;
  line-height: normal;
}<div>
  <span>Hello World!</span>
</div>


The CSS just sizes the <div>, vertically center aligns the <span> by setting the <div>'s line-height equal to its height, and makes the <span> an inline-block with vertical-align: middle. Then it sets the line-height back to normal for the <span>, so its contents will flow naturally inside the block.



Simulating table display

And here is another option, which may not work on older browsers that don't support display: table and display: table-cell (basically just Internet Explorer 7). Using CSS we simulate table behavior (since tables support vertical alignment), and the HTML is the same as the second example:

div {
  display: table;
  height: 100px;
  width: 100%;
  text-align: center;
  border: 2px dashed #f69c55;
}
span {
  display: table-cell;
  vertical-align: middle;
}<div>
  <span>Hello World!</span>
</div>




Using absolute positioning

This technique uses an absolutely positioned element setting top, bottom, left and right to 0. It is described in more detail in an article in Smashing Magazine, Absolute Horizontal And Vertical Centering In CSS.

div {
  display: flex;
  justify-content: center;
  align-items: center;
  height: 100px;
  width: 100%;
  border: 2px dashed #f69c55;
}<div>
  <span>Hello World!</span>
</div>

    Another way (not mentioned here yet) is with Flexbox.
Just add the following code to the container element:
display: flex;
justify-content: center; /* align horizontal */
align-items: center; /* align vertical */

Flexbox demo 1
.box {
  height: 150px;
  width: 400px;
  background: #000;
  font-size: 24px;
  font-style: oblique;
  color: #FFF;
  text-align: center;
  padding: 0 20px;
  margin: 20px;
  display: flex;
  justify-content: center;
  /* align horizontal */
  align-items: center;
  /* align vertical */
}<div class=""box"">
  Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh
</div>

Alternatively, instead of aligning the content via the container, flexbox can also center a flex item with an auto margin when there is only one flex-item in the flex container (like the example given in the question above).
So to center the flex item both horizontally and vertically just set it with margin:auto
Flexbox Demo 2
.box {
  height: 150px;
  width: 400px;
  background: #000;
  font-size: 24px;
  font-style: oblique;
  color: #FFF;
  text-align: center;
  padding: 0 20px;
  margin: 20px;
  display: flex;
}
.box span {
  margin: auto;
}<div class=""box"">
  <span>margin:auto on a flex item centers it both horizontally and vertically</span> 
</div>

NB: All the above applies to centering items while laying them out in horizontal rows. This is also the default behavior, because by default the value for flex-direction is row. If, however flex-items need to be laid out in vertical columns, then flex-direction: column should be set on the container to set the main-axis as column and additionally the justify-content and align-items properties now work the other way around with justify-content: center centering vertically and  align-items: center centering horizontally)
flex-direction: column demo
.box {
  height: 150px;
  width: 400px;
  background: #000;
  font-size: 18px;
  font-style: oblique;
  color: #FFF;
  display: flex;
  flex-direction: column;
  justify-content: center;
  /* vertically aligns items */
  align-items: center;
  /* horizontally aligns items */
}
p {
  margin: 5px;
  }<div class=""box"">
  <p>
    When flex-direction is column...
  </p>
  <p>
    ""justify-content: center"" - vertically aligns
  </p>
  <p>
    ""align-items: center"" - horizontally aligns
  </p>
</div>

A good place to start with Flexbox to see some of its features and get syntax for maximum browser support is flexyboxes
Also, browser support nowadays is very good: caniuse
For cross-browser compatibility for display: flex and align-items, you can use the following:
display: -webkit-box;
display: -webkit-flex;
display: -moz-box;
display: -ms-flexbox;
display: flex;
-webkit-flex-align: center;
-ms-flex-align: center;
-webkit-align-items: center;
align-items: center;

    Here is another option using flexbox.
#container {
  display: flex;
  height: 200px;
  background: orange;
}

.child {
  margin: auto;
}<div id=""container"">
  <div class=""child"">
    <span>Lorem ipsum dolor sit amet consectetur adipisicing elit. Molestiae, nemo.</span>
  </div>
</div>

Result

Here is a great article about centering in css. Check it out.
    You can easily do this by adding the following piece of CSS code:

display: table-cell;
vertical-align: middle;


That means your CSS finally looks like:

#box {
  height: 90px;
  width: 270px;
  background: #000;
  font-size: 48px;
  font-style: oblique;
  color: #FFF;
  text-align: center;
  margin-top: 20px;
  margin-left: 5px;
  display: table-cell;
  vertical-align: middle;
}<div id=""box"">
  Some text
</div>

    For reference and to add a simpler answer:

Pure CSS:

.vertical-align {
    position: relative;
    top: 50%;
    -webkit-transform: translateY(-50%);
    -ms-transform: translateY(-50%);
    transform: translateY(-50%);
}


Or as a SASS/SCSS Mixin:

@mixin vertical-align {
    position: relative;
    top: 50%;
    -webkit-transform: translateY(-50%);
    -ms-transform: translateY(-50%);
    transform: translateY(-50%);
}


Use by:

.class-to-center {
    @include vertical-align;
}


By Sebastian Ekstrm's blog post Vertical align anything with just 3 lines of CSS:

This method can cause elements to be blurry due to the element being placed on a half pixel. A solution for this is to set its parent element to preserve-3d. Like following:

.parent-element {
    -webkit-transform-style: preserve-3d;
    -moz-transform-style: preserve-3d;
    transform-style: preserve-3d;
}


We live in 2015+ and Flex Box is supported by every major modern browser.

It will be the way websites are made from here on out.

Learn it!
    There is a tiny magic with CSS3 flexboxes:
/* Important */
section {
    display: flex;
    display: -webkit-flex;
}
section p {
    /* Key Part */
    margin: auto;
}


/* Unimportant, coloring and UI */
section {
    height: 200px;
    width: 60%;
    margin: auto;
    border-radius: 20px;
    border: 3px solid orange;
    background-color: gold;
}
section p {
    text-align: center;
    font-family: Cantarell, Calibri;
    font-size: 15px;
    background-color: yellow;
    border-radius: 20px;
    padding: 15px;
}<section>
    <p>
        I'm a centered box!<br/>
        Flexboxes are great!
    </p>
</section>

Tip: Replace the line above marked as ""Key Part"" with one of these lines, if you want to center the text:

Only vertically:
margin: auto 0;


Only horizontally:
margin: 0 auto;



As I noticed, this trick works with grids (i.e. display: grid), also.
    All credit goes to this link owner @Sebastian Ekstrm Link; please go through this. See it in action codepen. By reading the above article I also created a demo fiddle.

With just three lines of CSS (excluding vendor prefixes) we can do it with the help of a transform: translateY vertically centers whatever we want, even if we dont know its height.

The CSS property transform is usually used for rotating and scaling elements, but with its translateY function we can now vertically align elements. Usually this must be done with absolute positioning or setting line-heights, but these require you to either know the height of the element or only works on single-line text, etc.

So, to do this we write:

.element {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
} 


Thats all you need. It is a similar technique to the absolute-position method, but with the upside that we dont have to set any height on the element or position-property on the parent. It works straight out of the box, even in InternetExplorer9!

To make it even more simple, we can write it as a mixin with its vendor prefixes.
    I saw the previous answers, and they will work only for that width of screen (not responsive). For the responsive you have to use flex.

Example:

div { display:flex; align-items:center; }

    Flexible approach

div {
  width: 250px;
  min-height: 50px;
  line-height: 50px;
  text-align: center;
  border: 1px solid #123456;
  margin-bottom: 5px;
}
span {
  display: inline-block;
  vertical-align: middle;
  line-height: normal;
}<div>
  <span>Lorem ipsum dolor sit amet, consectetur adipiscing elit.<br />
    Lorem ipsum dolor sit amet, consectetur adipiscing elit.<br />
    Lorem ipsum dolor sit amet, consectetur adipiscing elit.</span>
</div>
<div>
  <span>Lorem ipsum dolor sit amet, consectetur adipiscing elit.</span>
</div>
<div>
  <span>Lorem ipsum dolor sit amet.</span>
</div>
<div>

    You can use the following code snippet as the reference. It is working like a charm for me:
html,
body {
  height: 100%;
  margin: 0;
  padding: 0;
  width: 100%;
}

body {
  display: table;
}

.centered-text {
  text-align: center;
  display: table-cell;
  vertical-align: middle;
}<div class=""centered-text"">
  <h1>Yes, it's my landing page</h1>
  <h2>Under construction, coming soon!!!</h2>
</div>

The output of the above code snippet is as follow:

    The solution accepted as the answer is perfect to use line-height the same as the height of div, but this solution does not work perfectly when text is wrapped OR is in two lines.

Try this one if text is wrapped or is on multiple lines inside a div.

#box
{
  display: table-cell;
  vertical-align: middle;
}


For more reference, see:


Vertically Center Multi-Lined Text
6 Methods For Vertical Centering With CSS

    Try this solution:

.EXTENDER {
    position: absolute;
    top: 0px;
    left: 0px;
    bottom: 0px;
    right: 0px;
    width: 100%;
    height: 100%;
    overflow-y: hidden;
    overflow-x: hidden;
}

.PADDER-CENTER {
    width: 100%;
    height: 100%;
    display: -webkit-box;
    display: -moz-box;
    display: -ms-flexbox;
    display: -webkit-flex;
    display: flex;
    -webkit-box-pack: center;
    -moz-box-pack: center;
    -ms-flex-pack: center;
    -webkit-justify-content: center;
    justify-content: center;
    -webkit-box-align: center;
    -moz-box-align: center;
    -ms-flex-align: center;
    -webkit-align-items: center;
    align-items: center;
}<div class=""EXTENDER"">
  <div class=""PADDER-CENTER"">
    <div contentEditable=""true"">Edit this text...</div>
  </div>
</div>


Built using CSS+.
    The following code will put the div in the middle of the screen regardless of screen size or div size:

.center-screen {
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  text-align: center;
  min-height: 100vh;
} <html>
 <head>
 </head>
 <body>
 <div class=""center-screen"">
 I'm in the center
 </div>
 </body>
 </html>


See more details about flex here. 
    You can also use below properties.

display: flex; 
align-content: center; 
justify-content : center;

    Another way:

Don't set the height attribute of the div, but instead use padding: to achieve the effect. Similarly to line-height, it only works if you have one line of text.  Although this way, if you have more content, the text will still be centered, but the div itself will be slightly larger.

So instead of going with:

div {
  height: 120px;
  line-height: 120px;
}


You can say:

div {
   padding: 60px 0; // Maybe 60 minus font-size divided by two, if you want to be exact
}


This will set the top and bottom padding of the div to 60px, and the left and right padding to zero, making the div 120 pixels (plus the height of your font) high, and placing the text vertically centered in the div.
    A very simple & most powerful solution to vertically align center:

.outer-div {
  height: 200px;
  width: 200px;
  text-align: center;
  border: 1px solid #000;
}

.inner {
  position: relative;
  top: 50%;
  transform: translateY(-50%);
  color: red;
}<div class=""outer-div"">
  <span class=""inner"">No data available</span>
</div>

    For a single line of text (or a single character) you can use this technique:

It can be used when #box has a non-fixed, relative height in %.

<div id=""box""></div>

#box::before {
    display: block;
    content: """";
    height: 50%;
}

#box::after {
    vertical-align: top;
    line-height: 0;
    content: ""TextContent"";
}


See a live demo at JsBin (easier to edit CSS) or JsFiddle (easier to change height of result frame).

If you want to place inner text in HTML, not in CSS, then you need to wrap text content in additional inline element and edit #box::after to match it. (And, of course, content: property should be removed.)

For example,
<div id=""box""><span>TextContent</span></div>.
In this case, #box::after should be replaced with #box span.

For InternetExplorer8 support you must replace :: with :.
    Wherever you want vertically center style means you can try display:table-cell and vertical-align:middle.

Example:

#box
{
  display: table-cell;
  vertical-align: middle;
  height: 90px;
  width: 270px;
  background: #000;
  font-size: 48px;
  font-style: oblique;
  color: #FFF;
  text-align: center;
  margin-top: 20px;
  margin-left: 5px;
}<div Id=""box"">
  Lorem ipsum dolor sit amet, consectetur adipiscing elit.
</div>

    Even better idea for this. You can do like this too 

body,
html {
  height: 100%;
}

.parent {
  white-space: nowrap;
  height: 100%;
  text-align: center;
}

.parent:after {
  display: inline-block;
  vertical-align: middle;
  height: 100%;
  content: '';
}

.centered {
  display: inline-block;
  vertical-align: middle;
  white-space: normal;
}<div class=""parent"">
  <div class=""centered"">
    <p>Lorem ipsum dolor sit amet.</p>
  </div>
</div>

    I'm not sure anyone has gone the writing-mode route, but I think it solves the problem cleanly and has broad support:

.vertical {
  //border: 1px solid green;
  writing-mode: vertical-lr;
  text-align: center;
  height: 100%;
  width: 100%;
}
.horizontal {
  //border: 1px solid blue;
  display: inline-block;
  writing-mode: horizontal-tb;
  width: 100%;
  text-align: center;
}
.content {
  text-align: left;
  display: inline-block;
  border: 1px solid #e0e0e0;
  padding: .5em 1em;
  border-radius: 1em;
}<div class=""vertical"">
  <div class=""horizontal"">
    <div class=""content"">
      I'm centered in the vertical and horizontal thing
    </div>
  </div>
</div>


This will, of course, work with any dimensions you need (besides 100% of the parent). If you uncomment the border lines, it'll be helpful to familiarize yourself.

JSFiddle demo for you to fiddle.

Caniuse support: 85.22% + 6.26% = 91.48% (even InternetExplorer is in!)
    The simple and versatile way is (as Michielvoo's
table approach):

[ctrv]{
    display:table !important;
}

[ctrv] > *{ /* adressing direct discendents */
      display: table-cell;
      vertical-align: middle;
      // text-align: center; /* optional */
}


Using this attribute (or a equivalent class) on a parent tag works even for many children to align:

<parent ctrv>  <ch1/>  <ch2/>   </parent>

    You can use the positioning method in CSS:

Check the result here.... 

HTML:

<div class=""relativediv"">
  <p>
    Make me vertical align as center
  </p>
</div>


CSS:

.relativediv{position:relative;border:1px solid #ddd;height:300px;width:300px}
.relativediv p{position:absolute:top:50%;transfrom:translateY(-50%);}


Hope you use this method too.
    Try the transform property:

 #box {
  height: 90px;
  width: 270px;
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
} <div Id=""box"">
    Lorem ipsum dolor sit amet, consectetur adipiscing elit.
</div>

    Try the following code:

display: table-cell;
vertical-align: middle;


div {
  height: 80%;
  width: 100%;
  text-align: center;
  display: table-cell;
  vertical-align: middle;
  background: #4CAF50;
  color: #fff;
  font-size: 50px;
  font-style: italic;
}<div>
  Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s
</div>

    I needed a row of clickable elephants, vertically centered, but without using a table to get around some InternetExplorer9 weirdness.

I eventually found the nicest CSS (for my needs) and it's great with Firefox, Chrome, and InternetExplorer11. Sadly InternetExplorer9 is still laughing at me...

div {
  border: 1px dotted blue;
  display: inline;
  line-height: 100px;
  height: 100px;
}

span {
  border: 1px solid red;
  display: inline-block;
  line-height: normal;
  vertical-align: middle;
}

.out {
  border: 3px solid silver;
  display: inline-block;
}<div class=""out"" onclick=""alert(1)"">
  <div> <span><img src=""http://www.birdfolk.co.uk/littleredsolo.png""/></span> </div>
  <div> <span>A lovely clickable option.</span> </div>
</div>

<div class=""out"" onclick=""alert(2)"">
  <div> <span><img src=""http://www.birdfolk.co.uk/bang2/Ship01.png""/></span> </div>
  <div> <span>Something charming to click on.</span> </div>
</div>


Obviously you don't need the borders, but they can help you see how it works.
    I would just like to extend the answer from Michielvoo in order to release need for line-height and breathing of div height. It is basically just a simplified version like this:

div {
  width: 250px;
  /* height: 100px;
  line-height: 100px; */
  text-align: center;
  border: 1px solid #123456;
  background-color: #bbbbff;
  padding: 10px;
  margin: 10px;
}

span {
  display: inline-block;
  vertical-align: middle;
  line-height: normal;
}<div>
  <span>All grown-ups were once children... but only few of them remember it</span>
</div>

<div>
  <span>And now here is my secret, a very simple secret: It is only with the heart that one can see rightly; what is essential is invisible to the eye.</span>
</div>


NOTE: commented out part of cssis needed for fixed-height of enclosing div.
    Absolute Positioning and Stretching

As with the method above this one begins by setting positioning on the parent and child elements as relative and absolute respectively. From there things differ.

In the code below Ive once again used this method to center the child both horizontally and vertically, though you can use the method for vertical centering only.

HTML

<div id=""parent"">
    <div id=""child"">Content here</div>
</div>


CSS

#parent {position: relative;}
#child {
    position: absolute;
    top: 0;
    bottom: 0;
    left: 0;
    right: 0;
    width: 50%;
    height: 30%;
    margin: auto;
}


The idea with this method is to try to get the child element to stretch to all four edges by setting the top, bottom, right, and left vales to 0. Because our child element is smaller than our parent elements it cant reach all four edges.

Setting auto as the margin on all four sides however causes opposite margins to be equal and displays our child div in the center of the parent div.

Unfortunately the above wont work in InternetExplorer7 and below, and like the previous method the content inside the child div can grow too large, causing it to be hidden.
    .text{
   background: #ccc;
   position: relative;
   float: left;
   text-align: center;
   width: 400px;
   line-height: 80px;
   font-size: 24px;
   color: #000;
   float: left;
 }

    .box {  
  width: 100%;
  background: #000;
  font-size: 48px;
  color: #FFF;
  text-align: center;
}

.height {
  line-height: 170px;
  height: 170px;
}

.transform { 
  height: 170px;
  position: relative;
}

.transform p {
  margin: 0;
  position: absolute;
  top: 50%;
  left: 50%;
  -ms-transform: translate(-50%, -50%);
  transform: translate(-50%, -50%);
}<h4>Using Height</h4>
<div class=""box height"">
  Lorem ipsum dolor sit
</div>

<hr />

<h4>Using Transform</h4>
<div class=""box transform"">
  <p>Lorem ipsum dolor sit</p>
</div>

    <!DOCTYPE html>
<html>
  <head>
    <style>
      .main{
        height:450px;
        background:#f8f8f8;
        display: -ms-flexbox;
        display: -webkit-flex;
        display: flex;
        -ms-flex-align: center;
        -webkit-box-align: center;
        align-items: center;
        justify-content: center;
        width: 100%;
      }
    </style>
  </head>
  <body>
    <div class=""main"">
      <h1>Hello</h1>
    </div>
  </body>
</html>

    ","[2586, 3163, 1459, 28, 148, 128, 54, 70, 24, 26, 16, 20, 15, 4, 13, 12, 4, 5, 1, 5, 6, 4, 2, 4, 3, 2, 3, 1, -3, 0, 0]",3897592,645,2012-01-14T21:25:10,2021-09-27 00:10:42Z,html css 
Scroll to an element with jQuery,"
                
I have this input element:
<input type=""text"" class=""textfield"" value="""" id=""subject"" name=""subject"">

Then I have some other elements, like other text inputs, textareas, etc.
When the user clicks on that input with #subject, the page should scroll to the last element of the page with a nice animation. It should be a scroll to bottom and not to top.
The last item of the page is a submit button with #submit:
<input type=""submit"" class=""submit"" id=""submit"" name=""submit"" value=""Ok, Done."">

The animation should not be too fast and should be fluid.
I am running the latest jQuery version. I prefer to not install any plugin but to use the default jQuery features to achieve this.
    Assuming you have a button with the id button, try this example:
$(""#button"").click(function() {
    $([document.documentElement, document.body]).animate({
        scrollTop: $(""#elementtoScrollToID"").offset().top
    }, 2000);
});

I got the code from the article Smoothly scroll to an element without a jQuery plugin. And I have tested it on the example below.
<html>
    <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js""></script>
    <script>
        $(document).ready(function (){
            $(""#click"").click(function (){
                $('html, body').animate({
                    scrollTop: $(""#div1"").offset().top
                }, 2000);
            });
        });
    </script>
    <div id=""div1"" style=""height: 1000px; width 100px"">
        Test
    </div>
    <br/>
    <div id=""div2"" style=""height: 1000px; width 100px"">
        Test 2
    </div>
    <button id=""click"">Click me</button>
</html>

    If you are not much interested in the smooth scroll effect and just interested in scrolling to a particular element, you don't require some jQuery function for this. Javascript has got your case covered:

https://developer.mozilla.org/en-US/docs/Web/API/element.scrollIntoView

So all you need to do is: $(""selector"").get(0).scrollIntoView();  

.get(0) is used because we want to retrieve the JavaScript's DOM element and not the JQuery's DOM element.
    
                    
            
        
            
                    
                        
                    
                
                    
                        Locked. There are disputes about this answers content being resolved at this time. It is not currently accepting new interactions.
                        
                    
                
            
        

    

This is achievable without jQuery:
document.getElementById(""element-id"").scrollIntoView();

      

jQuery .scrollTo():  View - Demo, API, Source 

I wrote this lightweight plugin to make page/element scrolling much easier. It's flexible where you could pass in a target element or specified value. Perhaps this could be part of jQuery's next official release, what do you think?



Examples Usage:

$('body').scrollTo('#target'); // Scroll screen to target element

$('body').scrollTo(500); // Scroll screen 500 pixels down

$('#scrollable').scrollTo(100); // Scroll individual element 100 pixels down




Options:

scrollTarget: A element, string, or number which indicates desired scroll position.

offsetTop: A number that defines additional spacing above scroll target.

duration: A string or number determining how long the animation will run.

easing: A string indicating which easing function to use for the transition.

complete: A function to call once the animation is complete.
    This is the way I do it.

document.querySelector('scrollHere').scrollIntoView({ behavior: 'smooth' })


Works in any browser.

It can easily be wrapped into a function

function scrollTo(selector) {
    document.querySelector(selector).scrollIntoView({ behavior: 'smooth' })
}


Here is a working example
$("".btn"").click(function() {
  document.getElementById(""scrollHere"").scrollIntoView( {behavior: ""smooth"" })
}).btn {margin-bottom: 500px;}
.middle {display: block; margin-bottom: 500px; color: red;}<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>

<button class=""btn"">Scroll down</button>

<h1 class=""middle"">You see?</h1>

<div id=""scrollHere"">Arrived at your destination</div>


Docs
    Using this simple script

if($(window.location.hash).length > 0){
        $('html, body').animate({ scrollTop: $(window.location.hash).offset().top}, 1000);
}


Would make in sort that if a hash tag is found in the url, the scrollTo animate to the ID. If not hash tag found, then ignore the script.
    The solution by Steve and Peter works very well.

But in some cases, you may have to convert the value to an integer. Strangely, the returned value from $(""..."").offset().top is sometimes in float.
Use: parseInt($(""...."").offset().top)

For example:

$(""#button"").click(function() {
    $('html, body').animate({
        scrollTop: parseInt($(""#elementtoScrollToID"").offset().top)
    }, 2000);
});

    jQuery(document).ready(function($) {
  $('a[href^=""#""]').bind('click.smoothscroll',function (e) {
    e.preventDefault();
    var target = this.hash,
        $target = $(target);

    $('html, body').stop().animate( {
      'scrollTop': $target.offset().top-40
    }, 900, 'swing', function () {
      window.location.hash = target;
    } );
  } );
} );<script src=""https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js""></script>


<ul role=""tablist"">
  <li class=""active"" id=""p1""><a href=""#pane1"" role=""tab"">Section 1</a></li>
  <li id=""p2""><a href=""#pane2"" role=""tab"">Section 2</a></li>
  <li id=""p3""><a href=""#pane3"" role=""tab"">Section 3</a></li>
</ul>

<div id=""pane1""></div>
<div id=""pane2""></div>
<div id=""pane3""></div>

    After finding the way to get my code work, I think I should make thing a bit clear:
For using:
$('html, body').animate({
   scrollTop: $(""#div1"").offset().top
}, 2000);

you need to be on top of the page since $(""#div1"").offset().top will return different numbers for different positions you scroll to. If you already scrolled out of the top, you need to specify the exact pageY value (see pageY definition here: https://javascript.info/coordinates).
So now, the problem is to calculate the pageY value of one element. Below is an example in case the scroll container is the body:
function getPageY(id) {
    let elem = document.getElementById(id);
    let box = elem.getBoundingClientRect();
    var body = document.getElementsByTagName(""BODY"")[0];
    return box.top + body.scrollTop; // for window scroll: box.top + window.scrollY;
}

The above function returns the same number even if you scrolled somewhere. Now, to scroll back to that element:
$(""html, body"").animate({ scrollTop: getPageY('div1') }, ""slow"");

    Easy way to achieve the scroll of page to target div id

var targetOffset = $('#divID').offset().top;
$('html, body').animate({scrollTop: targetOffset}, 1000);

    A compact version of ""animate"" solution.

$.fn.scrollTo = function (speed) {
    if (typeof(speed) === 'undefined')
        speed = 1000;

    $('html, body').animate({
        scrollTop: parseInt($(this).offset().top)
    }, speed);
};


Basic usage: $('#your_element').scrollTo();
    With this solution you do not need any plugin and there's no setup required besides placing the script before your closing </body> tag.
$(""a[href^='#']"").on(""click"", function(e) {
  $(""html, body"").animate({
    scrollTop: $($(this).attr(""href"")).offset().top
  }, 1000);
  return false;
});

if ($(window.location.hash).length > 1) {
  $(""html, body"").animate({
    scrollTop: $(window.location.hash).offset().top
  }, 1000);
}

On load, if there is a hash in the address, we scroll to it.
And - whenever you click an a link with an href hash e.g. #top, we scroll to it.
##Edit 2020
If you want a pure JavaScript solution: you could perhaps instead use something like:
var _scrollToElement = function (selector) {
  try {
    document.querySelector(selector).scrollIntoView({ behavior: 'smooth' });
  } catch (e) {
    console.warn(e);
  }
}

var _scrollToHashesInHrefs = function () {
  document.querySelectorAll(""a[href^='#']"").forEach(function (el) {
    el.addEventListener('click', function (e) {
      _scrollToElement(el.getAttribute('href'));
      return false;
    })
  })
  if (window.location.hash) {
    _scrollToElement(window.location.hash);
  }
}

_scrollToHashesInHrefs();

    If you want to scroll within an overflow container (instead of $('html, body') answered above), working also with absolute positioning, this is the way to do :

var elem = $('#myElement'),
    container = $('#myScrollableContainer'),
    pos = elem.position().top + container.scrollTop() - container.position().top;

container.animate({
  scrollTop: pos
}

    This is my approach abstracting the ID's and href's, using a generic class selector

$(function() {
  // Generic selector to be used anywhere
  $("".js-scroll-to"").click(function(e) {

    // Get the href dynamically
    var destination = $(this).attr('href');

    // Prevent href=# link from changing the URL hash (optional)
    e.preventDefault();

    // Animate scroll to destination
    $('html, body').animate({
      scrollTop: $(destination).offset().top
    }, 500);
  });
});<!-- example of a fixed nav menu -->
<ul class=""nav"">
  <li>
    <a href=""#section-1"" class=""nav-item js-scroll-to"">Item 1</a>
  </li>
  <li>
    <a href=""#section-2"" class=""nav-item js-scroll-to"">Item 2</a>
  </li>
  <li>
    <a href=""#section-3"" class=""nav-item js-scroll-to"">Item 3</a>
  </li>
</ul>

    Animations:

// slide to top of the page
$('.up').click(function () {
    $(""html, body"").animate({
        scrollTop: 0
    }, 600);
    return false;
});

// slide page to anchor
$('.menutop b').click(function(){
    //event.preventDefault();
    $('html, body').animate({
        scrollTop: $( $(this).attr('href') ).offset().top
    }, 600);
    return false;
});

// Scroll to class, div
$(""#button"").click(function() {
    $('html, body').animate({
        scrollTop: $(""#target-element"").offset().top
    }, 1000);
});

// div background animate
$(window).scroll(function () {

    var x = $(this).scrollTop();

    // freezze div background
    $('.banner0').css('background-position', '0px ' + x +'px');

    // from left to right
    $('.banner0').css('background-position', x+'px ' +'0px');

    // from right to left
    $('.banner0').css('background-position', -x+'px ' +'0px');

    // from bottom to top
    $('#skills').css('background-position', '0px ' + -x + 'px');

    // move background from top to bottom
    $('.skills1').css('background-position', '0% ' + parseInt(-x / 1) + 'px' + ', 0% ' + parseInt(-x / 1) + 'px, center top');

    // Show hide mtop menu  
    if ( x > 100 ) {
    $( "".menu"" ).addClass( 'menushow' );
    $( "".menu"" ).fadeIn(""slow"");
    $( "".menu"" ).animate({opacity: 0.75}, 500);
    } else {
    $( "".menu"" ).removeClass( 'menushow' );
    $( "".menu"" ).animate({opacity: 1}, 500);
    }

});

// progres bar animation simple
$('.bar1').each(function(i) {
  var width = $(this).data('width');  
  $(this).animate({'width' : width + '%' }, 900, function(){
    // Animation complete
  });  
});

    $('html, body').animate(...) does not work for me in the iPhone, Android, Chrome, or Safari browsers.
I had to target the root content element of the page.

$('#cotnent').animate(...)

Here is what I have ended up with:
if (navigator.userAgent.match(/(iPod|iPhone|iPad|Android)/)) {
    $('#content').animate({
    scrollTop: $(""#elementtoScrollToID"").offset().top
   }, 'slow');
}
else{
    $('html, body').animate({
    scrollTop: $(""#elementtoScrollToID"").offset().top
    }, 'slow');
}

All body content wired up with a #content div
<html>
    ....
    <body>
        <div id=""content"">
        ...
        </div>
    </body>
</html>

    If you are only handling scrolling to an input element, you can use focus().  For example, if you wanted to scroll to the first visible input:

$(':input:visible').first().focus();


Or the first visible input in an container with class .error:

$('.error :input:visible').first().focus();


Thanks to Tricia Ball for pointing this out!
    Very simple and easy to use custom jQuery plugin. Just add the attribute scroll= to your clickable element and set its value to the selector you want to scroll to. 

Like so: <a scroll=""#product"">Click me</a>. It can be used on any element. 

(function($){
    $.fn.animateScroll = function(){
        console.log($('[scroll]'));
        $('[scroll]').click(function(){
            selector = $($(this).attr('scroll'));
            console.log(selector);
            console.log(selector.offset().top);
            $('html body').animate(
                {scrollTop: (selector.offset().top)}, //- $(window).scrollTop()
                1000
            );
        });
    }
})(jQuery);

// RUN
jQuery(document).ready(function($) {
    $().animateScroll();
});

// IN HTML EXAMPLE
// RUN ONCLICK ON OBJECT WITH ATTRIBUTE SCROLL="".SELECTOR""
// <a scroll=""#product"">Click To Scroll</a>

    To show the full element (if it's possible with the current window size):

var element       = $(""#some_element"");
var elementHeight = element.height();
var windowHeight  = $(window).height();

var offset = Math.min(elementHeight, windowHeight) + element.offset().top;
$('html, body').animate({ scrollTop: offset }, 500);

    $('html, body').animate({scrollTop: 
  Math.min( 
    $(to).offset().top-margintop, //margintop is the margin above the target
    $('body')[0].scrollHeight-$('body').height()) //if the target is at the bottom
}, 2000);

    You just need:
$(""selector"").get(0).scrollTo(0, 0)

    ONELINER

subject.onclick = e=> window.scroll({ top: submit.offsetTop, behavior: 'smooth'});


subject.onclick = e=> window.scroll({top: submit.offsetTop, behavior: 'smooth'});.box,.foot{display: flex;background:#fdf;padding:500px 0} .foot{padding:250px}<input type=""text"" class=""textfield"" value=""click here"" id=""subject"" name=""subject"">

<div class=""box"">
  Some content
  <textarea></textarea>
</div>

<input type=""submit"" class=""submit"" id=""submit"" name=""submit"" value=""Ok, Done."">

<div class=""foot"">Some footer</div>

    Updated answer as of 2019:

$('body').animate({
    scrollTop: $('#subject').offset().top - $('body').offset().top + $('body').scrollTop()
}, 'fast');

    I wrote a general purpose function that scrolls to either a jQuery object, a CSS selector, or a numeric value.

Example usage:

// scroll to ""#target-element"":
$.scrollTo(""#target-element"");

// scroll to 80 pixels above first element with class "".invalid"":
$.scrollTo("".invalid"", -80);

// scroll a container with id ""#my-container"" to 300 pixels from its top:
$.scrollTo(300, 0, ""slow"", ""#my-container"");


The function's code:

/**
* Scrolls the container to the target position minus the offset
*
* @param target    - the destination to scroll to, can be a jQuery object
*                    jQuery selector, or numeric position
* @param offset    - the offset in pixels from the target position, e.g.
*                    pass -80 to scroll to 80 pixels above the target
* @param speed     - the scroll speed in milliseconds, or one of the
*                    strings ""fast"" or ""slow"". default: 500
* @param container - a jQuery object or selector for the container to
*                    be scrolled. default: ""html, body""
*/
jQuery.scrollTo = function (target, offset, speed, container) {

    if (isNaN(target)) {

        if (!(target instanceof jQuery))
            target = $(target);

        target = parseInt(target.offset().top);
    }

    container = container || ""html, body"";
    if (!(container instanceof jQuery))
        container = $(container);

    speed = speed || 500;
    offset = offset || 0;

    container.animate({
        scrollTop: target + offset
    }, speed);
};

    This is Atharva's answer from: https://developer.mozilla.org/en-US/docs/Web/API/element.scrollIntoView.
Just wanted to add if your document is in an iframe, you can choose an element in the parent frame to scroll into view:

 $('#element-in-parent-frame', window.parent.document).get(0).scrollIntoView();

    In most cases, it would be best to use a plugin. Seriously. I'm going to tout mine here. Of course there are others, too. But please check if they really avoid the pitfalls for which you'd want a plugin in the first place - not all of them do.

I have written about the reasons for using a plugin elsewhere. In a nutshell, the one liner underpinning most answers here 

$('html, body').animate( { scrollTop: $target.offset().top }, duration );


is bad UX.


The animation doesn't respond to user actions. It carries on even if the user clicks, taps, or tries to scroll.
If the starting point of the animation is close to the target element, the animation is painfully slow. 
If the target element is placed near the bottom of the page, it can't be scrolled to the top of the window. The scroll animation stops abruptly then, in mid motion.


To handle these issues (and a bunch of others), you can use a plugin of mine, jQuery.scrollable. The call then becomes

$( window ).scrollTo( targetPosition );


and that's it. Of course, there are more options.

With regard to the target position, $target.offset().top does the job in most cases. But please be aware that the returned value doesn't take a border on the html element into account (see this demo). If you need the target position to be accurate under any circumstances, it is better to use

targetPosition = $( window ).scrollTop() + $target[0].getBoundingClientRect().top;


That works even if a border on the html element is set.
    I set up a module scroll-element npm install scroll-element. It works like this:

import { scrollToElement, scrollWindowToElement } from 'scroll-element'

/* scroll the window to your target element, duration and offset optional */
let targetElement = document.getElementById('my-item')
scrollWindowToElement(targetElement)

/* scroll the overflow container element to your target element, duration and offset optional */
let containerElement = document.getElementById('my-container')
let targetElement = document.getElementById('my-item')
scrollToElement(containerElement, targetElement)


Written with help from the following SO posts:


offset-top-of-an-element-without-jquery
scrolltop-animation-without-jquery


Here is the code:

export const scrollToElement = function(containerElement, targetElement, duration, offset) {
  if (duration == null) { duration = 1000 }
  if (offset == null) { offset = 0 }

  let targetOffsetTop = getElementOffset(targetElement).top
  let containerOffsetTop = getElementOffset(containerElement).top
  let scrollTarget = targetOffsetTop + ( containerElement.scrollTop - containerOffsetTop)
  scrollTarget += offset
  scroll(containerElement, scrollTarget, duration)
}

export const scrollWindowToElement = function(targetElement, duration, offset) {
  if (duration == null) { duration = 1000 }
  if (offset == null) { offset = 0 }

  let scrollTarget = getElementOffset(targetElement).top
  scrollTarget += offset
  scrollWindow(scrollTarget, duration)
}

function scroll(containerElement, scrollTarget, duration) {
  let scrollStep = scrollTarget / (duration / 15)
  let interval = setInterval(() => {
    if ( containerElement.scrollTop < scrollTarget ) {
      containerElement.scrollTop += scrollStep
    } else {
      clearInterval(interval)
    }
  },15)
}

function scrollWindow(scrollTarget, duration) {
  let scrollStep = scrollTarget / (duration / 15)
  let interval = setInterval(() => {
    if ( window.scrollY < scrollTarget ) {
      window.scrollBy( 0, scrollStep )
    } else {
      clearInterval(interval)
    }
  },15)
}

function getElementOffset(element) {
  let de = document.documentElement
  let box = element.getBoundingClientRect()
  let top = box.top + window.pageYOffset - de.clientTop
  let left = box.left + window.pageXOffset - de.clientLeft
  return { top: top, left: left }
}

    
  When the user clicks on that input with #subject, the page should
  scroll to the last element of the page with a nice animation. It
  should be a scroll to bottom and not to top.
  
  The last item of the page is a submit button with #submit


$('#subject').click(function()
{
    $('#submit').focus();
    $('#subject').focus();
});


This will first scroll down to #submit then restore the cursor back to the input that was clicked, which mimics a scroll down, and works on most browsers. It also doesn't require jQuery as it can be written in pure JavaScript.

Can this fashion of using focus function mimic animation in a better way, through chaining focus calls. I haven't tested this theory, but it would look something like this:

<style>
  #F > *
  {
    width: 100%;
  }
</style>

<form id=""F"" >
  <div id=""child_1""> .. </div>
  <div id=""child_2""> .. </div>
  ..
  <div id=""child_K""> .. </div>
</form>

<script>
  $('#child_N').click(function()
  {
    $('#child_N').focus();
    $('#child_N+1').focus();
    ..
    $('#child_K').focus();

    $('#child_N').focus();
  });
</script>

    This worked for me:

var targetOffset = $('#elementToScrollTo').offset().top;
$('#DivParent').animate({scrollTop: targetOffset}, 2500);

    var scrollTo = function($parent, $element) {
    var topDiff = $element.position().top - $parent.position().top;

    $parent.animate({
        scrollTop : topDiff
    }, 100);
};

    ","[2584, 4386, 438, 90, 558, 41, 58, 39, 43, 8, 11, 24, 20, 10, 7, 9, 5, 18, 7, 3, 4, 2, 3, 3, 3, 4, 8, 3, 3, 2, 4]",2937656,632,2011-07-13T09:49:44,2021-09-01 06:42:15Z,javascript 
Set a default parameter value for a JavaScript function,"
                
I would like a JavaScript function to have optional arguments which I set a default on, which get used if the value isn't defined (and ignored if the value is passed). In Ruby you can do it like this:

def read_file(file, delete_after = false)
  # code
end


Does this work in JavaScript?

function read_file(file, delete_after = false) {
  // Code
}

    From ES6/ES2015, default parameters are in the language specification.

function read_file(file, delete_after = false) {
  // Code
}


just works.

Reference: Default Parameters - MDN


  Default function parameters allow formal parameters to be initialized with default values if no value or undefined is passed.


You can also simulate default named parameters via destructuring:

// the `= {}` below lets you call the function without any parameters
function myFor({ start = 5, end = 1, step = -1 } = {}) { // (A)
    // Use the variables `start`, `end` and `step` here
    
}


Pre ES2015,

There are a lot of ways, but this is my preferred method  it lets you pass in anything you want, including false or null. (typeof null == ""object"")

function foo(a, b) {
  a = typeof a !== 'undefined' ? a : 42;
  b = typeof b !== 'undefined' ? b : 'default_b';
  ...
}

    function read_file(file, delete_after) {
    delete_after = delete_after || ""my default here"";
    //rest of code
}


This assigns to delete_after the value of delete_after if it is not a falsey value otherwise it assigns the string ""my default here"". For more detail, check out Doug Crockford's survey of the language and check out the section on Operators.

This approach does not work if you want to pass in a falsey value i.e. false, null, undefined, 0 or """". If you require falsey values to be passed in you would need to use the method in Tom Ritter's answer.

When dealing with a number of parameters to a function, it is often useful to allow the consumer to pass the parameter arguments in an object and then merge these values with an object that contains the default values for the function

function read_file(values) {
    values = merge({ 
        delete_after : ""my default here""
    }, values || {});

    // rest of code
}

// simple implementation based on $.extend() from jQuery
function merge() {
    var obj, name, copy,
        target = arguments[0] || {},
        i = 1,
        length = arguments.length;

    for (; i < length; i++) {
        if ((obj = arguments[i]) != null) {
            for (name in obj) {
                copy = obj[name];

                if (target === copy) {
                    continue;
                }
                else if (copy !== undefined) {
                    target[name] = copy;
                }
            }
        }
    }

    return target;
};


to use

// will use the default delete_after value
read_file({ file: ""my file"" }); 

// will override default delete_after value
read_file({ file: ""my file"", delete_after: ""my value"" }); 

    I find something simple like this to be much more concise and readable personally.

function pick(arg, def) {
   return (typeof arg == 'undefined' ? def : arg);
}

function myFunc(x) {
  x = pick(x, 'my default');
} 

    In ECMAScript 6 you will actually be able to write exactly what you have:

function read_file(file, delete_after = false) {
  // Code
}


This will set delete_after to false if it s not present or undefined. You can use ES6 features like this one today with transpilers such as Babel.

See the MDN article for more information. 
    Default Parameter Values

With ES6, you can do perhaps one of the most common idioms in JavaScript relates to setting a default  value for a function parameter. The way weve done this for years should look quite  familiar:

function foo(x,y) {
 x = x || 11;
 y = y || 31;
 console.log( x + y );
}
foo(); // 42
foo( 5, 6 ); // 11
foo( 5 ); // 36
foo( null, 6 ); // 17


This pattern is most used, but is dangerous when we pass values like 

foo(0, 42)
foo( 0, 42 ); // 53 <-- Oops, not 42


Why? Because the 0 is falsy, and so the x || 11 results in 11, not the directly passed  in 0. To fix this gotcha, some people will instead write the check more verbosely like this:

function foo(x,y) {
 x = (x !== undefined) ? x : 11;
 y = (y !== undefined) ? y : 31;
 console.log( x + y );
}
foo( 0, 42 ); // 42
foo( undefined, 6 ); // 17


we can now examine a nice helpful syntax added as of ES6 to  streamline the assignment of default values to missing arguments:

function foo(x = 11, y = 31) {
 console.log( x + y );
}

foo(); // 42
foo( 5, 6 ); // 11
foo( 0, 42 ); // 42
foo( 5 ); // 36
foo( 5, undefined ); // 36 <-- `undefined` is missing
foo( 5, null ); // 5 <-- null coerces to `0`
foo( undefined, 6 ); // 17 <-- `undefined` is missing
foo( null, 6 ); // 6 <-- null coerces to `0`


x = 11 in a function declaration is more like x !== undefined ? x : 11 than the  much more common idiom x || 11

Default Value Expressions

Function default values can be more than just simple values like 31; they can be any  valid expression, even a function call:

function bar(val) {
 console.log( ""bar called!"" );
 return y + val;
}
function foo(x = y + 3, z = bar( x )) {
 console.log( x, z );
}
var y = 5;
foo(); // ""bar called""
 // 8 13
foo( 10 ); // ""bar called""
 // 10 15
y = 6;
foo( undefined, 10 ); // 9 10


As you can see, the default value expressions are lazily evaluated, meaning theyre  only run if and when theyre needed  that is, when a parameters argument is omitted or is undefined.

A default value expression can  even be an inline function expression call  commonly referred to as an Immediately  Invoked Function Expression (IIFE):

function foo( x =
 (function(v){ return v + 11; })( 31 )
) {
 console.log( x );
}
foo(); // 42

    that solution is work for me in js:

function read_file(file, delete_after) {
    delete_after = delete_after || false;
    // Code
}

    I would highly recommend extreme caution when using default parameter values in javascript.  It often creates bugs when used in conjunction with higher order functions like forEach, map, and reduce.  For example, consider this line of code:

['1', '2', '3'].map(parseInt); // [1, NaN, NaN]


parseInt has an optional second parameter function parseInt(s, [radix=10]) but map calls parseInt with three arguments: (element, index, and array).

I suggest you separate your required parameters form your optional/default valued arguments.  If your function takes 1,2, or 3 required parameters for which no default value makes sense, make them positional parameters to the function, any optional parameters should follow as named attributes of a single object. If your function takes 4 or more, perhaps it makes more sense to supply all arguments via attributes of a single object parameter.

In your case I would suggest you write your deleteFile function like this: (edited per instead's comments)...

// unsafe
function read_file(fileName, deleteAfter=false) {
    if (deleteAfter) {
        console.log(`Reading and then deleting ${fileName}`);
    } else {
        console.log(`Just reading ${fileName}`);
    }
}

// better
function readFile(fileName, options) {
  const deleteAfter = !!(options && options.deleteAfter === true);
  read_file(fileName, deleteAfter);
}

console.log('unsafe...');
['log1.txt', 'log2.txt', 'log3.txt'].map(read_file);

console.log('better...');
['log1.txt', 'log2.txt', 'log3.txt'].map(readFile);


Running the above snippet illustrates the dangers lurking behind default argument values for unused parameters.
    If you are using ES6+ you can set default parameters in the following manner:

function test (foo = 1, bar = 2) {
  console.log(foo, bar);
}

test(5); // foo gets overwritten, bar remains default parameter


If you need ES5 syntax you can do it in the following manner:

function test(foo, bar) {
  foo = foo || 2;
  bar = bar || 0;
  
  console.log(foo, bar);
}

test(5); // foo gets overwritten, bar remains default parameter


In the above syntax the OR operator is used. The OR operator always returns the first value if this can be converted to true if not it returns the righthandside value. When the function is called with no corresponding argument the parameter variable (bar in  our example) is set to undefined by the JS engine. undefined Is then converted to false and thus does the OR operator return the value 0.
    Yes - proof:
function read_file(file, delete_after = false) {
  // Code
  console.log({file,delete_after});
}



// TEST
read_file(""A"");
read_file(""B"",true);
read_file(""C"",false);

    Sounds of Future
In future, you will be able to ""spread"" one object to another (currently as of 2019 NOT supported by Edge!) - demonstration how to use that for nice default options regardless of order:
function test(options) {
    var options = {
       // defaults
       url: 'defaultURL',
       some: 'somethingDefault',
       // override with input options
       ...options
    };
    
    var body = document.getElementsByTagName('body')[0];
    body.innerHTML += '<br>' + options.url + ' : ' + options.some;
}
test();
test({});
test({url:'myURL'});
test({some:'somethingOfMine'});
test({url:'overrideURL', some:'andSomething'});
test({url:'overrideURL', some:'andSomething', extra:'noProblem'});

MDN reference: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax
...meanwhile what Edge DOES support is Object.assign() (IE does not, but I really hope we can leave IE behind :) )
Similarly you could do
    function test(options) {
        var options = Object.assign({
           // defaults
           url: 'defaultURL',
           some: 'somethingDefault',
        }, options); // override with input options
        
        var body = document.getElementsByTagName('body')[0];
        body.innerHTML += '<br>' + options.url + ' : ' + options.some;
    }
    test();
    test({});
    test({url:'myURL'});
    test({some:'somethingOfMine'});
    test({url:'overrideURL', some:'andSomething'});
    test({url:'overrideURL', some:'andSomething', extra:'noProblem'});

EDIT: Due to comments regarding const options - the problem with using constant options in the rest of the function is actually not that you can't do that, is just that you can't use the constant variable in its own declaration - you would have to adjust the input naming to something like
function test(input_options){
   const options = {
     // defaults
     someKey:    'someDefaultValue',
     anotherKey: 'anotherDefaultValue',

     // merge-in input options
     ...input_options
   };

   // from now on use options with no problem
}

    As an update...with ECMAScript 6 you can FINALLY set default values in function parameter declarations like so:

function f (x, y = 7, z = 42) {
  return x + y + z
}

f(1) === 50


As referenced by - http://es6-features.org/#DefaultParameterValues
    Just use an explicit comparison with undefined.

function read_file(file, delete_after)
{
    if(delete_after === undefined) { delete_after = false; }
}

    Use this if you want to use latest ECMA6 syntax:                      

function myFunction(someValue = ""This is DEFAULT!"") {
  console.log(""someValue --> "", someValue);
}

myFunction(""Not A default value"") // calling the function without default value
myFunction()  // calling the function with default value
                                    

It is called default function parameters. It allows formal parameters to be initialized with default values if no value or undefined is passed.
NOTE: It wont work with Internet Explorer or older browsers.               

For maximum possible compatibility use this:                       

function myFunction(someValue) {
  someValue = (someValue === undefined) ? ""This is DEFAULT!"" : someValue;
  console.log(""someValue --> "", someValue);
}

myFunction(""Not A default value"") // calling the function without default value
myFunction()  // calling the function with default value


Both functions have exact same behavior as each of these example rely on the fact that the parameter variable will be undefined if no parameter value was passed when calling that function.
    Just to showcase my skills too (lol), above function can written even without having named arguments as below:

ES5 and above

function foo() {
    a = typeof arguments[0] !== 'undefined' ? a : 42;
    b = typeof arguments[1] !== 'undefined' ? b : 'default_b';
    ...
}


ES6 and above

function foo(...rest) {
    a = typeof rest[0] !== 'undefined' ? a : 42;
    b = typeof rest[1] !== 'undefined' ? b : 'default_b';
    ...
}

    The answer is yes. In fact, there are many languages who support default parameters. Python is one of them: 

def(a, enter=""Hello""):
   print(a+enter)


Even though this is Python 3 code due to the parentheses, default parameters in functions also work in JS.

For example, and in your case: 

function read_file(file, deleteAfter=false){
  console.log(deleteAfter);
}

read_file(""test.txt"");


But sometimes you don't really need default parameters.

You can just define the variable right after the start of the function, like this:

function read_file(file){
  var deleteAfter = false;
  console.log(deleteAfter);
}

read_file(""test.txt"");


In both of my examples, it returns the same thing. But sometimes they actually could be useful, like in very advanced projects.

So, in conclusion, default parameter values can be used in JS. But it is almost the same thing as defining a variable right after the start of the function. However, sometimes they are still very useful. As you have may noticed, default parameter values take 1 less line of code than the standard way which is defining the parameter right after the start of the function. 

EDIT: And this is super important! This will not work in IE. See documentation. So with IE you have to use the ""define variable at top of function"" method. Default parameters won't work in IE.
    being a long time C++ developer (Rookie to web development :)), when I first came across this situation, I did the parameter assignment in the function definition, like it is mentioned in the question, as follows. 

function myfunc(a,b=10)


But beware that it doesn't work consistently across browsers. For me it worked on chrome on my desktop, but did not work on chrome on android.
Safer option, as many have mentioned above is -

    function myfunc(a,b)
    {
    if (typeof(b)==='undefined') b = 10;
......
    }


Intention for this answer is not to repeat the same solutions, what others have already mentioned, but to inform that parameter assignment in the function definition may work on some browsers, but don't rely on it.
    function helloWorld(name, symbol = '!!!') {
    name = name || 'worlds';
    console.log('hello ' + name + symbol);
}

helloWorld(); // hello worlds!!!

helloWorld('john'); // hello john!!!

helloWorld('john', '(>.<)'); // hello john(>.<)

helloWorld('john', undefined); // hello john!!!

helloWorld(undefined, undefined); // hello worlds!!!

    ES6: As already mentioned in most answers, in ES6, you can simply initialise a parameter along with a value.



ES5: Most of the given answers aren't good enough for me because there are occasions where I may have to pass falsey values such as 0, null and undefined to a function. To determine if a parameter is undefined because that's the value I passed instead of undefined due to not have been defined at all I do this:

function foo (param1, param2) {
   param1 = arguments.length >= 1 ? param1 : ""default1"";
   param2 = arguments.length >= 2 ? param2 : ""default2"";
}

    As per the syntax 

function [name]([param1[ = defaultValue1 ][, ..., paramN[ = defaultValueN ]]]) {
   statements
}


you can define the default value of formal parameters.
and also check undefined value by using typeof function.
    To anyone interested in having there code work in Microsoft Edge, do not use defaults in function parameters. 

function read_file(file, delete_after = false) {
    #code
}


In that example Edge will throw an error ""Expecting ')'""    

To get around this use

function read_file(file, delete_after) {
  if(delete_after == undefined)
  {
    delete_after = false;
  }
  #code
}


As of Aug 08 2016 this is still an issue
    function throwIfNoValue() {
throw new Error('Missing argument');
}
function foo(argValue = throwIfNoValue()) {
return argValue ;
}


Here foo() is a function which has a parameter named argValue. If we dont pass anything in the function call here, then the function throwIfNoValue() will be called and the returned result will be assigned to the only argument argValue. This is how a function call can be used as a default parameter. Which makes the code more simplified and readable.

This example has been taken from here
    If for some reason you are not on ES6 and are using lodash here is a concise way to default function parameters via _.defaultTo method:

var fn = function(a, b) {
  a = _.defaultTo(a, 'Hi')
  b = _.defaultTo(b, 'Mom!')

  console.log(a, b)
}

fn()                 // Hi Mom!
fn(undefined, null)  // Hi Mom!
fn(NaN, NaN)         // Hi Mom!
fn(1)                // 1 ""Mom!""
fn(null, 2)          // Hi 2
fn(false, false)     // false false
fn(0, 2)             // 0 2<script src=""https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.11/lodash.min.js""></script>


Which will set the default if the current value is NaN, null, or undefined
    I've noticed a few answers mentioning that using default params isn't portable to other browsers, but it's only fair to point out that you can use transpilers like Babel to convert your code into ES5 syntax for browsers that have limited support for modern JS features.
So this:
function read_file(file, delete_after = false) {
  // Code
}

would be transpiled as this (try it out in the Babel REPL -> https://babeljs.io/repl/):
""use strict"";

function read_file(file) {

  var delete_after =
    arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;
  
  //Code...

}

Of course, if you have no intention of using transpilation, then setting default params in the body of the function like others have demonstrated is perfectly fine as well.
    Yeah this is referred to as a default parameter

Default function parameters allow formal parameters to be initialized with default values if no value or undefined is passed.

Syntax:

function [name]([param1[ = defaultValue1 ][, ..., paramN[ = defaultValueN ]]]) {
   statements
}


Description:

Parameters of functions default to undefined However, in situations it might be useful to set a different default value.  This is where default parameters can help. 

In the past, the general strategy for setting defaults was to test parameter values in the body of the function and assign a value if they are undefined.  If no value is provided in the call, its value would be undefined.  You would have to set a conditional check to make sure the parameter is not undefined

With default parameters in ES2015, the check in the function body is no longer necessary.  Now you can simply put a default value in the function head.

Example of the differences:

// OLD METHOD
function multiply(a, b) {
  b = (typeof b !== 'undefined') ?  b : 1;
  return a * b;
}

multiply(5, 2); // 10
multiply(5, 1); // 5
multiply(5);    // 5


// NEW METHOD
function multiply(a, b = 1) {
  return a * b;
}

multiply(5, 2); // 10
multiply(5, 1); // 5
multiply(5);    // 5


Different Syntax Examples:

Padding undefined vs other falsy values:

Even if the value is set explicitly when calling, the value of the num argument is the default one.

function test(num = 1) {
  console.log(typeof num);
}

test();          // 'number' (num is set to 1)
test(undefined); // 'number' (num is set to 1 too)

// test with other falsy values:
test('');        // 'string' (num is set to '')
test(null);      // 'object' (num is set to null)


Evaluated at call time:

The default argument gets evaluated at call time, so unlike some other languages, a new object is created each time the function is called.

function append(value, array = []) {
  array.push(value);
  return array;
}

append(1); //[1]
append(2); //[2], not [1, 2]


// This even applies to functions and variables
function callSomething(thing = something()) {
 return thing;
}

function something() {
  return 'sth';
}

callSomething();  //sth


Default parameters are available to later default parameters:

Params already encountered are available to later default parameters

function singularAutoPlural(singular, plural = singular + 's',
                        rallyingCry = plural + ' ATTACK!!!') {
  return [singular, plural, rallyingCry];
}

//[""Gecko"",""Geckos"", ""Geckos ATTACK!!!""]
singularAutoPlural('Gecko');

//[""Fox"",""Foxes"", ""Foxes ATTACK!!!""]
singularAutoPlural('Fox', 'Foxes');

//[""Deer"", ""Deer"", ""Deer ... change.""]
singularAutoPlural('Deer', 'Deer', 'Deer peaceably and respectfully \ petition the government for positive change.')


Functions defined inside function body:

Introduced in Gecko 33 (Firefox 33 / Thunderbird 33 / SeaMonkey 2.30). Functions declared in the function body cannot be referred inside default parameters and throw a ReferenceError (currently a TypeError in SpiderMonkey, see bug 1022967). Default parameters are always executed first, function declarations inside the function body evaluate afterwards.

// Doesn't work! Throws ReferenceError.
function f(a = go()) {
  function go() { return ':P'; }
}


Parameters without defaults after default parameters:

Prior to Gecko 26 (Firefox 26 / Thunderbird 26 / SeaMonkey 2.23 / Firefox OS 1.2), the following code resulted in a SyntaxError. This has been fixed in bug 777060 and works as expected in later versions. Parameters are still set left-to-right, overwriting default parameters even if there are later parameters without defaults.

function f(x = 1, y) {
  return [x, y];
}

f(); // [1, undefined]
f(2); // [2, undefined]


Destructured paramet with default value assignment:

You can use default value assignment with the destructuring assignment notation

function f([x, y] = [1, 2], {z: z} = {z: 3}) {
  return x + y + z;
}

f(); // 6

    Yes, using default parameters is fully supported in ES6:

function read_file(file, delete_after = false) {
  // Code
}


or 

const read_file = (file, delete_after = false) => {
    // Code
}


but prior in ES5 you could easily do this:

function read_file(file, delete_after) {
  var df = delete_after || false;
  // Code
}


Which means if the value is there, use the value, otherwise, use the second value after || operation which does the same thing...

Note: also there is a big difference between those if you pass a value to ES6 one even the value be falsy, that will be replaced with new value, something like null or """"... but ES5 one only will be replaced if only the passed value is truthy, that's because the way || working...
    Yes, This will work in Javascript. You can also do that:

function func(a=10,b=20)
{
    alert (a+' and '+b);
}

func(); // Result: 10 and 20

func(12); // Result: 12 and 20

func(22,25); // Result: 22 and 25

    def read_file(file, delete_after = false)
  # code
end


Following code may work in this situation including ECMAScript 6 (ES6) as well as earlier versions.    

function read_file(file, delete_after) {
    if(delete_after == undefined)
        delete_after = false;//default value

    console.log('delete_after =',delete_after);
}
read_file('text1.txt',true);
read_file('text2.txt');


as default value in languages works when the function's parameter value is skipped when calling, in JavaScript it is assigned to undefined. This approach doesn't look attractive programmatically but have backward compatibility.
    Just a different approach to set default params is to use object map of arguments, instead of arguments directly.
For example, 

const defaultConfig = {
 category: 'Animals',
 legs: 4
};

function checkOrganism(props) {
 const category = props.category || defaultConfig.category;
 const legs = props.legs || defaultConfig.legs;
}


This way, it's easy to extend the arguments and not worry about argument length mismatch.
    export const getfilesize = (bytes, decimals = 2) => {
    if (bytes === 0){ 
        return '0 Bytes';
    }else{
        const k = 1024;
        const dm = decimals < 0 ? 0 : decimals;
        const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'];
        const i = Math.floor(Math.log(bytes) / Math.log(k));
        return parseFloat((bytes / Math.pow(k, i)).toFixed(dm)) + ' ' + sizes[i];

    }
}

    ","[2564, 3487, 621, 155, 66, 30, 16, 12, 6, 1, 3, 10, 11, 5, 2, 0, 9, 5, 5, 5, 8, 4, 3, 1, 1, 3, -3, 0, 0, 0]",1386229,542,2009-05-21T20:07:17,2021-07-26 12:12:59Z,javascript 
"What's the difference between dependencies, devDependencies and peerDependencies in npm package.json file?","
                
This documentation answers my question very poorly. I didn't understand those explanations. Can someone say in simpler words? Maybe with examples if it's hard to choose simple words?

EDIT also added peerDependencies, which is closely related and might cause confusion.
    Summary of important behavior differences:

dependencies are installed on both:

npm install from a directory that contains package.json
npm install $package on any other directory


devDependencies are:

also installed on npm install on a directory that contains package.json, unless you pass the --production flag (go upvote Gayan Charith's answer), or if the NODE_ENV=production environment variable is set
not installed on npm install ""$package"" on any other directory, unless you give it the --dev option.
are not installed transitively.


peerDependencies:

before 3.0: are always installed if missing, and raise an error if multiple incompatible versions of the dependency would be used by different dependencies.
expected to start on 3.0 (untested): give a warning if missing on npm install, and you have to solve the dependency yourself manually. When running, if the dependency is missing, you get an error (mentioned by @nextgentech) This explains it nicely: https://flaviocopes.com/npm-peer-dependencies/
in version 7 peerDependencies are automatically installed unless an upstream dependency conflict is present that cannot be automatically resolved


Transitivity (mentioned by Ben Hutchison):

dependencies are installed transitively: if A requires B, and B requires C, then C gets installed, otherwise, B could not work, and neither would A.

devDependencies is not installed transitively. E.g. we don't need to test B to test A, so B's testing dependencies can be left out.




Related options not discussed here:

bundledDependencies which is discussed on the following question: Advantages of bundledDependencies over normal dependencies in npm
optionalDependencies (mentioned by Aidan Feldman)

devDependencies
dependencies are required to run, devDependencies only to develop, e.g.: unit tests, CoffeeScript to JavaScript transpilation, minification, ...
If you are going to develop a package, you download it (e.g. via git clone), go to its root which contains package.json, and run:
npm install

Since you have the actual source, it is clear that you want to develop it, so by default, both dependencies (since you must, of course, run to develop) and devDependency dependencies are also installed.
If however, you are only an end user who just wants to install a package to use it, you will do from any directory:
npm install ""$package""

In that case, you normally don't want the development dependencies, so you just get what is needed to use the package: dependencies.
If you really want to install development packages in that case, you can set the dev configuration option to true, possibly from the command line as:
npm install ""$package"" --dev

The option is false by default since this is a much less common case.
peerDependencies
(Tested before 3.0)
Source: https://nodejs.org/en/blog/npm/peer-dependencies/
With regular dependencies, you can have multiple versions of the dependency: it's simply installed inside the node_modules of the dependency.
E.g. if dependency1 and dependency2 both depend on dependency3 at different versions the project tree will look like:
root/node_modules/
                 |
                 +- dependency1/node_modules/
                 |                          |
                 |                          +- dependency3 v1.0/
                 |
                 |
                 +- dependency2/node_modules/
                                            |
                                            +- dependency3 v2.0/

Plugins, however, are packages that normally don't require the other package, which is called the host in this context. Instead:

plugins are required by the host
plugins offer a standard interface that the host expects to find
only the host will be called directly by the user, so there must be a single version of it.

E.g. if dependency1 and dependency2 peer depend on dependency3, the project tree will look like:
root/node_modules/
                 |
                 +- dependency1/
                 |
                 +- dependency2/
                 |
                 +- dependency3 v1.0/

This happens even though you never mention dependency3 in your package.json file.
I think this is an instance of the Inversion of Control design pattern.
A prototypical example of peer dependencies is Grunt, the host, and its plugins.
For example, on a Grunt plugin like https://github.com/gruntjs/grunt-contrib-uglify, you will see that:

grunt is a peer-dependency
the only require('grunt') is under tests/: it's not actually used by the program.

Then, when the user will use a plugin, he will implicitly require the plugin from the Gruntfile by adding a grunt.loadNpmTasks('grunt-contrib-uglify') line, but it's grunt that the user will call directly.
This would not work then if each plugin required a different Grunt version.
Manual
I think the documentation answers the question quite well, maybe you are just not familiar enough with node / other package managers. I probably only understand it because I know a bit about Ruby bundler.
The key line is:

These things will be installed when doing npm link or npm install from the root of a package and can be managed like any other npm configuration parameter. See npm-config(7) for more on the topic.

And then under npm-config(7) find dev:
Default: false
Type: Boolean

Install dev-dependencies along with packages.

    If you do not want to install devDependencies you can use npm install --production 
    dependencies
Dependencies that your project needs to run, like a library that provides functions that you call from your code.
They are installed transitively (if A depends on B depends on C, npm install on A will install B and C).
Example: lodash: your project calls some lodash functions.
devDependencies
Dependencies you only need during development or releasing, like compilers that take your code and compile it into javascript, test frameworks or documentation generators.
They are not installed transitively (if A depends on B dev-depends on C, npm install on A will install B only).
Example: grunt: your project uses grunt to build itself.
peerDependencies
Dependencies that your project hooks into, or modifies, in the parent project, usually a plugin for some other library or tool. It is just intended to be a check, making sure that the parent project (project that will depend on your project) has a dependency on the project you hook into. So if you make a plugin C that adds functionality to library B, then someone making a project A will need to have a dependency on B if they have a dependency on C.
They are not installed (unless npm < 3), they are only checked for.
Example: grunt: your project adds functionality to grunt and can only be used on projects that use grunt.
This documentation explains peer dependencies really well: https://nodejs.org/en/blog/npm/peer-dependencies/
Also, the npm documentation has been improved over time, and now has better explanations of the different types of dependencies: https://github.com/npm/cli/blob/latest/docs/content/configuring-npm/package-json.md#devdependencies
    As an example, mocha would normally be a devDependency, since testing isn't necessary in production, while express would be a dependency. 
    To save a package to package.json as dev dependencies: 

npm install ""$package"" --save-dev


When you run npm install it will install both devDependencies and dependencies. To avoid install devDependencies run:

npm install --production

    I found a simple explanation. 

Short Answer:

dependencies
""...are those that your project really needs to be able to work in production.""

devDependencies
""...are those that you need during development.""

peerDependencies
""if you want to create and publish your own library so that it can be used as a dependency""

More details in this post:
https://code-trotter.com/web/dependencies-vs-devdependencies-vs-peerdependencies
    peerDependencies didn't quite make sense for me until I read this snippet from a blog post on the topic Ciro mentioned above:


  What [plugins] need is a way of expressing these dependencies between plugins and their host package. Some way of saying, I only work when plugged in to version 1.2.x of my host package, so if you install me, be sure that its alongside a compatible host. We call this relationship a peer dependency.


The plugin does expect a specific version of the host...

peerDependencies are for plugins, libraries that require a ""host"" library to perform their function, but may have been written at a time before the latest version of the host was released. 

That is, if I write PluginX v1 for HostLibraryX v3 and walk away, there's no guarantee PluginX v1 will work when HostLibraryX v4 (or even HostLibraryX v3.0.1) is released.

... but the plugin doesn't depend on the host...

From the point of view of the plugin, it only adds functions to the host library. I don't really ""need"" the host to add a dependency to a plugin, and plugins often don't literally depend on their host. If you don't have the host, the plugin harmlessly does nothing.

This means dependencies isn't really the right concept for plugins. 

Even worse, if my host was treated like a dependency, we'd end up in this situation that the same blog post mentions (edited a little to use this answer's made up host & plugin):


  But now, [if we treat the contemporary version of HostLibraryX as a dependency for PluginX,] running npm install results in the unexpected dependency graph of

 HostLibraryX@4.0.0
 PluginX@1.0.0
   HostLibraryX@3.0.0

  
  Ill leave the subtle failures that come from the plugin using a different [HostLibraryX] API than the main application to your imagination.


... and the host obviously doesn't depend on the plugin...

... that's the whole point of plugins. Now if the host was nice enough to include dependency information for all of its plugins, that'd solve the problem, but that'd also introduce a huge new cultural problem: plugin management! 

The whole point of plugins is that they can pair up anonymously. In a perfect world, having the host manage 'em all would be neat & tidy, but we're not going to require libraries herd cats.

If we're not hierarchically dependent, maybe we're intradependent peers...

Instead, we have the concept of being peers. Neither host nor plugin sits in the other's dependency bucket. Both live at the same level of the dependency graph.



... but this is not an automatable relationship. <<< Moneyball!!!

If I'm PluginX v1 and expect a peer of (that is, have a peerDependency of) HostLibraryX v3, I'll say so. If you've auto-upgraded to the latest HostLibraryX v4 (note that's version 4) AND have Plugin v1 installed, you need to know, right? 

npm can't manage this situation for me --


  ""Hey, I see you're using PluginX v1! I'm automatically downgrading HostLibraryX from v4 to v3, kk?"" 


... or...


  ""Hey I see you're using PluginX v1. That expects HostLibraryX v3, which you've left in the dust during your last update. To be safe, I'm automatically uninstalling Plugin v1!!1!


How about no, npm?! 

So npm doesn't. It alerts you to the situation, and lets you figure out if HostLibraryX v4 is a suitable peer for Plugin v1.



Coda

Good peerDependency management in plugins will make this concept work more intuitively in practice. From the blog post, yet again...


  One piece of advice: peer dependency requirements, unlike those for regular dependencies, should be lenient. You should not lock your peer dependencies down to specific patch versions. It would be really annoying if one Chai plugin peer-depended on Chai 1.4.1, while another depended on Chai 1.5.0, simply because the authors were lazy and didnt spend the time figuring out the actual minimum version of Chai they are compatible with.

    There are some modules and packages only necessary for development, which are not needed in production. Like it says it in the documentation:

If someone is planning on downloading and using your module in their program, then they probably don't want or need to download and build the external test or documentation framework that you use. In this case, it's best to list these additional items in a devDependencies hash.

    A simple explanation that made it more clear to me is:

When you deploy your app, modules in dependencies need to be installed or your app won't work. Modules in devDependencies don't need to be installed on the production server since you're not developing on that machine.
link
    I'd like to add to the answer my view on these dependencies explanations


dependencies are used for direct usage in your codebase, things that usually end up in the production code, or chunks of code
devDependencies are used for the build process, tools that help you manage how the end code will end up, third party test modules, (ex. webpack stuff)

    In short


Dependencies -  npm install <package> --save-prod installs packages required by your application in production environment. 
DevDependencies -  npm install <package> --save-dev installs
    packages required only for local development and testing
Just typing npm install installs all packages mentioned in the
package.json


so if you are working on your local computer just type npm install and continue :)
    Dependencies vs dev dependencies

Dev dependencies are modules which are only required during development whereas dependencies are required at runtime. If you are deploying your application, dependencies has to be installed, or else your app simply will not work. Libraries that you call from your code that enables the program to run can be considered as dependencies.

Eg- React , React - dom

Dev dependency modules need not be installed in the production server since you are not gonna develop in that machine .compilers that covert your code to javascript , test frameworks and document generators can be considered as dev-dependencies since they are only required during development .

Eg- ESLint , Babel , webpack

@FYI,

mod-a
  dev-dependents:
    - mod-b
  dependents:
    - mod-c

mod-d
  dev-dependents:
    - mod-e
  dependents:
    - mod-a

----

npm install mod-d

installed modules:
  - mod-d
  - mod-a
  - mod-c

----

checkout the mod-d code repository

npm install

installed modules:
  - mod-a
  - mod-c
  - mod-e


If you are publishing to npm, then it is important that you use the correct flag for the correct modules. If it is something that your npm module needs to function, then use the ""--save"" flag to save the module as a dependency. If it is something that your module doesn't need to function but it is needed for testing, then use the ""--save-dev"" flag.

# For dependent modules
npm install dependent-module --save

# For dev-dependent modules
npm install development-module --save-dev

    dependencies: packages that your project/package needs to work in production.
devDependencies: packages that your project/package needs to work while development but are not needed on production (eg: testing packages)
peerDependencies: packages that your project/package needs to work in tandem with (colaborating with them) or as a base, useful mainly when you are developing a plugin/component to let know with which version of the main package your plugin/component is supposed to work with (eg: React 16)
    When trying to distribute an npm package you should avoid using dependencies. Instead you need to consider adding it into peerDependencies.
Update
Most of the time dependencies are just a bunch of libraries that describes your ecosystem. Unless, you're really using a specific version of a library you should instead let the user choose whether or not to install that library and which version to choose by adding it into the peerDependencies.
    Dependencies
These are the packages that your package needs to run, so they will be installed when people run
 npm install PACKAGE-NAME

An example would be if you used jQuery in your project. If someone doesn't have jQuery installed, then it wouldn't work. To save as a dependency, use
 npm install --save

Dev-Dependencies
These are the dependencies that you use in development, but isn't needed when people are using it, so when people run npm install, it won't install them since the are not necessary. For example, if you use mocha to test, people don't need mocha to run, so npm install doesn't install it. To save as a dev dependency, use
npm install PACKAGE --save-dev

Peer Dependencies
These can be used if you want to create and publish your own library so that it can be used as a dependency. For example, if you want your package to be used as a dependency in another project, then these will also be installed when someone installs the project which has your project as a dependency. Most of the time you won't use peer dependencies.
    The difference between these two is that devDependencies are modules that are only required during development, while dependencies are modules that are also required at runtime.
To save a dependency as a devDependency on installation we need to do an #npm install --save-dev, instead of just an #npm install --save
A nice shorthand for installing a devDependency that I like to use is #npm i -D. The shorthand for saving a regular dependency is -S instead of -D.
Some good examples of dependencies that would be required at runtime include React, Redux, Express, and Axios.
Some good examples of when to install devDependencies would be Nodemon, Babel, ESLint, and testing frameworks like Chai, Mocha, Enzyme, etc
    dependencies are required to run, devDependencies only to develop
    ","[2564, 2895, 593, 124, 142, 77, 19, 24, 45, 22, 18, 12, 7, 2, 2, 2, 1, 0]",754052,833,2013-09-18T14:57:06,2022-04-21 12:30:47Z,
How do I get the last element of a list?,"
                
How do I get the last element of a list?
    some_list[-1] is the shortest and most Pythonic.
In fact, you can do much more with this syntax. The some_list[-n] syntax gets the nth-to-last element. So some_list[-1] gets the last element, some_list[-2] gets the second to last, etc, all the way down to some_list[-len(some_list)], which gives you the first element.
You can also set list elements in this way. For instance:
>>> some_list = [1, 2, 3]
>>> some_list[-1] = 5 # Set the last element
>>> some_list[-2] = 3 # Set the second to last element
>>> some_list
[1, 3, 5]

Note that getting a list item by index will raise an IndexError if the expected item doesn't exist. This means that some_list[-1] will raise an exception if some_list is empty, because an empty list can't have a last element.
    If your str() or list() objects might end up being empty as so: astr = '' or alist = [], then you might want to use alist[-1:] instead of alist[-1] for object ""sameness"".

The significance of this is:

alist = []
alist[-1]   # will generate an IndexError exception whereas 
alist[-1:]  # will return an empty list
astr = ''
astr[-1]    # will generate an IndexError exception whereas
astr[-1:]   # will return an empty str


Where the distinction being made is that returning an empty list object or empty str object is more ""last element""-like then an exception object.
    You can also do:
last_elem = alist.pop()

It depends on what you want to do with your list because the pop() method will delete the last element.
    
  In Python, how do you get the last element of a list?


To just get the last element, 


without modifying the list, and 
assuming you know the list has a last element (i.e. it is nonempty) 


pass -1 to the subscript notation:

>>> a_list = ['zero', 'one', 'two', 'three']
>>> a_list[-1]
'three'


Explanation

Indexes and slices can take negative integers as arguments.

I have modified an example from the documentation to indicate which item in a sequence each index references, in this case, in the string ""Python"", -1 references the last element, the character, 'n':

 +---+---+---+---+---+---+
 | P | y | t | h | o | n |
 +---+---+---+---+---+---+
   0   1   2   3   4   5 
  -6  -5  -4  -3  -2  -1

>>> p = 'Python'
>>> p[-1]
'n'


Assignment via iterable unpacking

This method may unnecessarily materialize a second list for the purposes of just getting the last element, but for the sake of completeness (and since it supports any iterable - not just lists):

>>> *head, last = a_list
>>> last
'three'


The variable name, head is bound to the unnecessary newly created list:

>>> head
['zero', 'one', 'two']


If you intend to do nothing with that list, this would be more apropos:

*_, last = a_list


Or, really, if you know it's a list (or at least accepts subscript notation):

last = a_list[-1]


In a function

A commenter said:


  I wish Python had a function for first() and last() like Lisp does... it would get rid of a lot of unnecessary lambda functions.


These would be quite simple to define:

def last(a_list):
    return a_list[-1]

def first(a_list):
    return a_list[0]


Or use operator.itemgetter:

>>> import operator
>>> last = operator.itemgetter(-1)
>>> first = operator.itemgetter(0)


In either case:

>>> last(a_list)
'three'
>>> first(a_list)
'zero'


Special cases

If you're doing something more complicated, you may find it more performant to get the last element in slightly different ways.

If you're new to programming, you should avoid this section, because it couples otherwise semantically different parts of algorithms together. If you change your algorithm in one place, it may have an unintended impact on another line of code. 

I try to provide caveats and conditions as completely as I can, but I may have missed something. Please comment if you think I'm leaving a caveat out.

Slicing

A slice of a list returns a new list - so we can slice from -1 to the end if we are going to want the element in a new list:

>>> a_slice = a_list[-1:]
>>> a_slice
['three']


This has the upside of not failing if the list is empty:

>>> empty_list = []
>>> tail = empty_list[-1:]
>>> if tail:
...     do_something(tail)


Whereas attempting to access by index raises an IndexError which would need to be handled:

>>> empty_list[-1]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
IndexError: list index out of range


But again, slicing for this purpose should only be done if you need:


a new list created
and the new list to be empty if the prior list was empty.


for loops

As a feature of Python, there is no inner scoping in a for loop.

If you're performing a complete iteration over the list already, the last element will still be referenced by the variable name assigned in the loop:

>>> def do_something(arg): pass
>>> for item in a_list:
...     do_something(item)
...     
>>> item
'three'


This is not semantically the last thing in the list. This is semantically the last thing that the name, item, was bound to.

>>> def do_something(arg): raise Exception
>>> for item in a_list:
...     do_something(item)
...
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""<stdin>"", line 1, in do_something
Exception
>>> item
'zero'


Thus this should only be used to get the last element if you 


are already looping, and 
you know the loop will finish (not break or exit due to errors), otherwise it will point to the last element referenced by the loop.


Getting and removing it

We can also mutate our original list by removing and returning the last element:

>>> a_list.pop(-1)
'three'
>>> a_list
['zero', 'one', 'two']


But now the original list is modified. 

(-1 is actually the default argument, so list.pop can be used without an index argument):

>>> a_list.pop()
'two'


Only do this if


you know the list has elements in it, or are prepared to handle the exception if it is empty, and
you do intend to remove the last element from the list, treating it like a stack. 


These are valid use-cases, but not very common.

Saving the rest of the reverse for later:

I don't know why you'd do it, but for completeness, since reversed returns an iterator (which supports the iterator protocol) you can pass its result to next:

>>> next(reversed([1,2,3]))
3


So it's like doing the reverse of this:

>>> next(iter([1,2,3]))
1


But I can't think of a good reason to do this, unless you'll need the rest of the reverse iterator later, which would probably look more like this:

reverse_iterator = reversed([1,2,3])
last_element = next(reverse_iterator)

use_later = list(reverse_iterator)


and now:

>>> use_later
[2, 1]
>>> last_element
3

    The simplest way to display last element in python is

>>> list[-1:] # returns indexed value
    [3]
>>> list[-1]  # returns value
    3


there are many other method to achieve such a goal but these are short and sweet to use.
    To prevent IndexError: list index out of range, use this syntax:

mylist = [1, 2, 3, 4]

# With None as default value:
value = mylist and mylist[-1]

# With specified default value (option 1):
value = mylist and mylist[-1] or 'default'

# With specified default value (option 2):
value = mylist[-1] if mylist else 'default'

    Here is the solution for your query.
a=[""first"",""second from last"",""last""] # A sample list
print(a[0]) #prints the first item in the list because the index of the list always starts from 0.
print(a[-1]) #prints the last item in the list.
print(a[-2]) #prints the last second item in the list.

Output:
>>> first
>>> last
>>> second from last

    Strange that nobody posted this yet:
>>> l = [1, 2, 3]
>>> *x, last_elem = l
>>> last_elem
3
>>> 

Just unpack.
    Accessing the last element from the list in Python:
1: Access the last element with negative indexing -1
>> data = ['s','t','a','c','k','o','v','e','r','f','l','o','w']
>> data[-1]
'w'

2. Access the last element with pop() method
>> data = ['s','t','a','c','k','o','v','e','r','f','l','o','w']
>> data.pop()
'w'

However, pop method will remove the last element from the list.
    lst[-1] is the best approach, but with general iterables, consider more_itertools.last:

Code

import more_itertools as mit


mit.last([0, 1, 2, 3])
# 3

mit.last(iter([1, 2, 3]))
# 3

mit.last([], ""some default"")
# 'some default'

    You can also use the length to get the last element:
last_elem = arr[len(arr) - 1]

If the list is empty, you'll get an IndexError exception, but you also get that with arr[-1].
    You can also use the code below, if you do not want to get IndexError when the list is empty.

next(reversed(some_list), None)

    Pythonic Way
So lets consider that we have a list a = [1,2,3,4], in Python List can be manipulated to give us part of it or a element of it, using the following command one can easily get the last element.
print(a[-1])

    Another method:

some_list.reverse() 
some_list[0]

    list[-1] will retrieve the last element of the list without changing the list.
list.pop() will retrieve the last element of the list, but it will mutate/change the original list. Usually, mutating the original list is not recommended.

Alternatively, if, for some reason, you're looking for something less pythonic, you could use list[len(list)-1], assuming the list is not empty.
    To avoid ""IndexError: list index out of range"", you can use this piece of code.
list_values = [12, 112, 443]

def getLastElement(lst):
    if len(lst) == 0:
        return 0
    else:
        return lst[-1]

print(getLastElement(list_values))

    Ok, but what about common in almost every language way items[len(items) - 1]? This is IMO the easiest way to get last element, because it does not require anything pythonic knowledge.
    You will just need to take the and put [-1] index. For example:
list=[0,1,2]
last_index=list[-1]
print(last_index)

You will get 2 as the output.
    If you do my_list[-1] this returns the last element of the list. Negative sequence indexes represent positions from the end of the array. Negative indexing means beginning from the end, -1 refers to the last item, -2 refers to the second-last item, etc.
    Couldn't find any answer mentioning this. So adding.
You could try some_list[~0] also.
That's the tilde symbol
    You could use it with next and iter with [::-1]:
>>> a = [1, 2, 3]
>>> next(iter(a[::-1]))
3
>>> 

    array=[1,2,3,4,5,6,7]
last_element= array[len(array)-1]
last_element

Another simple solution
    ","[2560, 3731, 302, 128, 79, 94, 23, 6, 4, 3, 12, 2, 8, 3, 12, 8, 2, 6, 1, 1, 0, 0, 0]",2796468,264,2009-05-30T19:28:53,2022-03-29 10:02:52Z,python 
Homebrew install specific version of formula?,"
                
How do I install a specific version of a formula in homebrew?  For example, postgresql-8.4.4 instead of the latest 9.0.
    TLDR: brew install postgresql@8.4.4 See answer below for more details.



*(Ive re-edited my answer to give a more thorough workflow for installing/using older software versions with homebrew. Feel free to add a note if you found the old version better.)

Lets start with the simplest case:

1) Check, whether the version is already installed (but not activated)

When homebrew installs a new formula, it puts it in a versioned directory like /usr/local/Cellar/postgresql/9.3.1. Only symbolic links to this folder are then installed globally. In principle, this makes it pretty easy to switch between two installed versions. (*)

If you have been using homebrew for longer and never removed older versions (using, for example brew cleanup), chances are that some older version of your program may still be around. If you want to simply activate that previous version, brew switch is the easiest way to do this.

Check with brew info postgresql (or brew switch postgresql <TAB>) whether the older version is installed:

$ brew info postgresql
postgresql: stable 9.3.2 (bottled)
http://www.postgresql.org/
Conflicts with: postgres-xc
/usr/local/Cellar/postgresql/9.1.5 (2755 files, 37M)
  Built from source
/usr/local/Cellar/postgresql/9.3.2 (2924 files, 39M) *
  Poured from bottle
From: https://github.com/Homebrew/homebrew/commits/master/Library/Formula/postgresql.rb
#  and some more


We see that some older version is already installed. We may activate it using brew switch:

$ brew switch postgresql 9.1.5
Cleaning /usr/local/Cellar/postgresql/9.1.5
Cleaning /usr/local/Cellar/postgresql/9.3.2
384 links created for /usr/local/Cellar/postgresql/9.1.5


Lets double-check what is activated:

$ brew info postgresql
postgresql: stable 9.3.2 (bottled)
http://www.postgresql.org/
Conflicts with: postgres-xc
/usr/local/Cellar/postgresql/9.1.5 (2755 files, 37M) *
  Built from source
/usr/local/Cellar/postgresql/9.3.2 (2924 files, 39M)
  Poured from bottle
From: https://github.com/Homebrew/homebrew/commits/master/Library/Formula/postgresql.rb
#  and some more


Note that the star * has moved to the newly activated version

(*) Please note that brew switch only works as long as all dependencies of the older version are still around. In some cases, a rebuild of the older version may become necessary. Therefore, using brew switch is mostly useful when one wants to switch between two versions not too far apart.

2) Check, whether the version is available as a tap

Especially for larger software projects, it is very probably that there is a high enough demand for several (potentially API incompatible) major versions of a certain piece of software. As of March 2012, Homebrew 0.9 provides a mechanism for this: brew tap & the homebrew versions repository.

That versions repository may include backports of older versions for several formulae. (Mostly only the large and famous ones, but of course theyll also have several formulae for postgresql.)

brew search postgresql will show you where to look:

$ brew search postgresql
postgresql
homebrew/versions/postgresql8    homebrew/versions/postgresql91
homebrew/versions/postgresql9    homebrew/versions/postgresql92


We can simply install it by typing

$ brew install homebrew/versions/postgresql8
Cloning into '/usr/local/Library/Taps/homebrew-versions'...
remote: Counting objects: 1563, done.
remote: Compressing objects: 100% (943/943), done.
remote: Total 1563 (delta 864), reused 1272 (delta 620)
Receiving objects: 100% (1563/1563), 422.83 KiB | 339.00 KiB/s, done.
Resolving deltas: 100% (864/864), done.
Checking connectivity... done.
Tapped 125 formula
==> Downloading http://ftp.postgresql.org/pub/source/v8.4.19/postgresql-8.4.19.tar.bz2
# 


Note that this has automatically tapped the homebrew/versions tap. (Check with brew tap, remove with brew untap homebrew/versions.) The following would have been equivalent:

$ brew tap homebrew/versions
$ brew install postgresql8


As long as the backported version formulae stay up-to-date, this approach is probably the best way to deal with older software.

3) Try some formula from the past

The following approaches are listed mostly for completeness. Both try to resurrect some undead formula from the brew repository. Due to changed dependencies, API changes in the formula spec or simply a change in the download URL, things may or may not work.

Since the whole formula directory is a git repository, one can install specific versions using plain git commands. However, we need to find a way to get to a commit where the old version was available.

a) historic times

Between August 2011 and October 2014, homebrew had a brew versions command, which spat out all available versions with their respective SHA hashes. As of October 2014, you have to do a brew tap homebrew/boneyard before you can use it. As the name of the tap suggests, you should probably only do this as a last resort.

E.g.

$ brew versions postgresql
Warning: brew-versions is unsupported and may be removed soon.
Please use the homebrew-versions tap instead:
  https://github.com/Homebrew/homebrew-versions
9.3.2    git checkout 3c86d2b Library/Formula/postgresql.rb
9.3.1    git checkout a267a3e Library/Formula/postgresql.rb
9.3.0    git checkout ae59e09 Library/Formula/postgresql.rb
9.2.4    git checkout e3ac215 Library/Formula/postgresql.rb
9.2.3    git checkout c80b37c Library/Formula/postgresql.rb
9.2.2    git checkout 9076baa Library/Formula/postgresql.rb
9.2.1    git checkout 5825f62 Library/Formula/postgresql.rb
9.2.0    git checkout 2f6cbc6 Library/Formula/postgresql.rb
9.1.5    git checkout 6b8d25f Library/Formula/postgresql.rb
9.1.4    git checkout c40c7bf Library/Formula/postgresql.rb
9.1.3    git checkout 05c7954 Library/Formula/postgresql.rb
9.1.2    git checkout dfcc838 Library/Formula/postgresql.rb
9.1.1    git checkout 4ef8fb0 Library/Formula/postgresql.rb
9.0.4    git checkout 2accac4 Library/Formula/postgresql.rb
9.0.3    git checkout b782d9d Library/Formula/postgresql.rb


As you can see, it advises against using it. Homebrew spits out all versions it can find with its internal heuristic and shows you a way to retrieve the old formulae. Lets try it.

# First, go to the homebrew base directory
$ cd $( brew --prefix )
# Checkout some old formula
$ git checkout 6b8d25f Library/Formula/postgresql.rb
$ brew install postgresql
#  installing


Now that the older postgresql version is installed, we can re-install the latest formula in order to keep our repository clean:

$ git checkout -- Library/Formula/postgresql.rb


brew switch is your friend to change between the old and the new.

b) prehistoric times

For special needs, we may also try our own digging through the homebrew repo.

$ cd Library/Taps/homebrew/homebrew-core && git log -S'8.4.4' -- Formula/postgresql.rb


git log -S looks for all commits in which the string '8.4.4' was either added or removed in the file Library/Taps/homebrew/homebrew-core/Formula/postgresql.rb. We get two commits as a result.

commit 7dc7ccef9e1ab7d2fc351d7935c96a0e0b031552
Author: Aku Kotkavuo
Date:   Sun Sep 19 18:03:41 2010 +0300

    Update PostgreSQL to 9.0.0.

    Signed-off-by: Adam Vandenberg

commit fa992c6a82eebdc4cc36a0c0d2837f4c02f3f422
Author: David Hppner
Date:   Sun May 16 12:35:18 2010 +0200

    postgresql: update version to 8.4.4


Obviously, fa992c6a82eebdc4cc36a0c0d2837f4c02f3f422 is the commit were interested in. As this commit is pretty old, well try to downgrade the complete homebrew installation (that way, the formula API is more or less guaranteed to be valid):

$ git checkout -b postgresql-8.4.4 fa992c6a82eebdc4cc36a0c0d2837f4c02f3f422
$ brew install postgresql
$ git checkout master
$ git branch -d postgresql-8.4.4


You may skip the last command to keep the reference in your git repository.

One note: When checking out the older commit, you temporarily downgrade your homebrew installation. So, you should be careful as some commands in homebrew might be different to the most recent version.

4) Manually write a formula

Its not too hard and you may then upload it to your own repository. Used to be Homebrew-Versions, but that is now discontinued.

A.) Bonus: Pinning

If you want to keep a certain version of, say postgresql, around and stop it from being updated when you do the natural brew update; brew upgrade procedure, you can pin a formula:

$ brew pin postgresql


Pinned formulae are listed in /usr/local/Library/PinnedKegs/ and once you want to bring in the latest changes and updates, you can unpin it again:

$ brew unpin postgresql

    Simple Workflow
Now that Homebrew/versions has been deprecated, Homebrew/core supports a few versions of formulae with a new naming format.
To install a specific version, e.g. postgresql 9.5 you simply run:
$ brew install postgresql@9.5

To list the available versions run a search with @:
$ brew search postgresql@
==> Searching local taps...
postgresql@10.1      postgresql@9.4        postgresql@9.5        postgresql@9.6

    Solution
brew extract --version=8.4p1  openssh homebrew/cask
brew install openssh@8.4p1

The newest openssl@8.5p1 has bugs after I run brew upgrade, so I backed to the previous version successfully via the above way.
    Along the lines of @halfcube's suggestion, this works really well:


Find the library you're looking for at 
https://github.com/Homebrew/homebrew-core/tree/master/Formula
Click it: https://github.com/Homebrew/homebrew-core/blob/master/Formula/postgresql.rb
Click the ""history"" button to look at old commits: https://github.com/Homebrew/homebrew-core/commits/master/Formula/postgresql.rb
Click the one you want: ""postgresql: update version to 8.4.4"", https://github.com/Homebrew/homebrew-core/blob/8cf29889111b44fd797c01db3cf406b0b14e858c/Formula/postgresql.rb
Click the ""raw"" link: https://raw.githubusercontent.com/Homebrew/homebrew-core/8cf29889111b44fd797c01db3cf406b0b14e858c/Formula/postgresql.rb
brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/8cf29889111b44fd797c01db3cf406b0b14e858c/Formula/postgresql.rb

    Most of the other answers are obsolete by now. Unfortunately Homebrew still doesnt have a builtin way of installing an outdated version, unless that version exists as a separate formula (e.g. python@2, postgresql@9.4).

Luckily, for other formulas theres a much easier way than the convoluted mess that used to be necessary. Here are the full instructions:


Search for the correct version in the logs:

brew log formula
# Scroll down/up with j/k or the arrow keys
# or use eg. /4\.4\.23 to search a specific version

# This syntax only works on pre-2.0 Homebrew versions
brew log --format=format:%H\ %s -F --grep=version formula


This will show a list of commit hashes. Take one that is appropriate (mostly it should be pretty obvious, and usually is the most recent (i.e. top) one.
Find the URL at which the formula resides in the upstream repository:

brew info formula | grep ^From:

Fix the URL:


Replace github.com with raw.githubusercontent.com
Replace blob/master with the commit hash we found in the first step.

Install the desired version by replacing master in the previously found URL by the commit hash, e.g.:

brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/hash/Formula/formula.rb



(The last step may necessitate running brew unlink formula before.)



If you have copied a commit hash you want to use, you can use something like this example to install that version, replacing the value and bash with your commit hash and your desired formula.

BREW_VERSION_SHA=32353d2286f850fd965e0a48bcf692b83a6e9a41
BREW_FORMULA_NAME=bash
brew info $BREW_FORMULA_NAME \
| sed -n \
    -e '/^From: /s///' \
    -e 's/github.com/raw.githubusercontent.com/' \
    -e 's%blob/%%' \
    -e ""s/master/$BREW_VERSION_SHA/p"" \
| xargs brew install


This example is installing bash 4.4.23 instead of bash 5, though if you performed a brew upgrade afterward then bash 5 would get installed over top, unless you first executed brew pin bash. Instead to make this smoother WITHOUT pinning, you should first install the latest with brew install bash, then brew unlink bash, then install the older version you want per the script above, and then use brew switch bash 4.4.23 to set up the symlinks to the older version. Now a brew upgrade shouldn't affect your version of Bash. You can brew switch bash to get a list of the versions available to switch to.



Alternative using a custom local-only tap

Another way of achieving the same goal appears to be:

brew tap-new username/repo-name
# extract with a version seems to run a grep under the hood
brew extract --version='4.4.23' bash username/repo-name
brew install bash@4.4.23
# Note this ""fails"" when trying to grab a bottle for the package and seems to have
# some odd doubling of the version in that output, but this isn't fatal.


This creates a formula@version in your custom tap that you can install per the above example. The downside is that you probably still need to brew unlink bash and then brew link bash@4.4.23 in order to use your specific version of Bash or any other formula.
    Update: 1/15/2015


Find the commit history of the desired software and version. e.g. I need to switch from docker version 1.4.1 to 1.3.3: 
https://github.com/Homebrew/homebrew-core/commits/master/Formula/docker.rb
View the file with this button:  
Click the Raw button: 
Copy the URL (docker.rb url in this example) from address bar
brew install <url> (may have to brew unlink first, e.g. brew unlink docker)
brew switch docker 1.3.3
Switch back to docker 1.4.1 brew switch docker 1.4.1


From this gist

brew update
brew versions FORMULA
cd `brew --prefix`
git checkout HASH Library/Formula/FORMULA.rb  # use output of ""brew versions""
brew install FORMULA
brew switch FORMULA VERSION
git checkout -- Library/Formula/FORMULA.rb    # reset formula

## Example: Using Subversion 1.6.17
#
# $ brew versions subversion
# 1.7.3    git checkout f8bf2f3 /usr/local/Library/Formula/subversion.rb
# 1.7.2    git checkout d89bf83 /usr/local/Library/Formula/subversion.rb
# 1.6.17   git checkout 6e2d550 /usr/local/Library/Formula/subversion.rb
# 1.6.16   git checkout 83ed494 /usr/local/Library/Formula/subversion.rb
# 1.6.15   git checkout 809a18a /usr/local/Library/Formula/subversion.rb
# 1.6.13   git checkout 7871a99 /usr/local/Library/Formula/subversion.rb
# 1.6.12   git checkout c99b3ac /usr/local/Library/Formula/subversion.rb
# 1.6.6    git checkout 8774131 /usr/local/Library/Formula/subversion.rb
# 1.6.5    git checkout a82e823 /usr/local/Library/Formula/subversion.rb
# 1.6.3    git checkout 6b6d369 /usr/local/Library/Formula/subversion.rb
# $ cd `brew --prefix`
# $ git checkout 6e2d550 /usr/local/Library/Formula/subversion.rb
# $ brew install subversion
# $ brew switch subversion 1.6.17
# $ git checkout -- Library/Formula/subversion.rb

    Homebrew changed recently. Things that used to work do not work anymore. The easiest way I found to work (January 2021), was to:

Find the .rb file for my software (first go to Formulas, find the one I need and then click ""History""; for CMake, this is at https://github.com/Homebrew/homebrew-core/commits/master/Formula/cmake.rb)

Pick the desired version among the revisions, e.g. 3.18.4, click three dots in the top right corner of the .rb file diff (...) and then click Raw. Copy the URL.


Unlink the old version brew unlink cmake
Installing directly from the git URL does not work anymore (brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/2bf16397f163187ae5ac8be41ca7af25b5b2e2cc/Formula/cmake.rb will fail)

Instead, download it and install from a local file curl -O https://raw.githubusercontent.com/Homebrew/homebrew-core/2bf16397f163187ae5ac8be41ca7af25b5b2e2cc/Formula/cmake.rb && brew install ./cmake.rb



Voila! You can delete the downloaded .rb file now.
    Official method ( judging from the response to https://github.com/Homebrew/brew/issues/6028 )

Unfortunately Homebrew still doesnt have an obvious builtin way of installing an older version.

Luckily, for most formulas theres a much easier way than the convoluted mess that used to be necessary. Here are the full instructions using bash as an example:

brew tap-new $USER/local-tap
# extract with a version seems to run a `git log --grep` under the hood
brew extract --version=4.4.23 bash $USER/local-tap
# Install your new version from the tap
brew install bash@4.4.23
# Note this ""fails"" trying to grab a bottle for the package and seems to have
# some odd doubling of the version in that output, but this isn't fatal.


This creates the formula@version in your custom tap that you can install per the above example. An important note is that you probably need to brew unlink bash if you had previously installed the default/latest version of the formula and then brew link bash@4.4.23 in order to use your specific version of Bash (or any other formula where you have latest and an older version installed).

A potential downside to this method is you can't easily switch back and forth between the versions because according to brew it is a ""different formula"".

If you want to be able to use brew switch $FORMULA $VERSION you should use the next method.



Scripted Method (Recommended)

This example shows installing the older bash 4.4.23, a useful example since the bash formula currently installs bash 5.


First install the latest version of the formula with brew install bash
then brew unlink bash
then install the older version you want per the snippets below
finally use brew switch bash 4.4.23 to set up the symlinks to your version


If you performed a brew upgrade after installing an older version without installing the latest first, then the latest would get installed clobbering your older version, unless you first executed brew pin bash.

The steps here AVOID pinning because it is easy to forget about and you might pin to a version that becomes insecure in the future (see Shellshock/etc). With this setup a brew upgrade shouldn't affect your version of Bash and you can always run brew switch bash to get a list of the versions available to switch to.

Copy and paste and edit the export lines from the code snippet below to update with your desired version and formula name, then copy and paste the rest as-is and it will use those variables to do the magic.

# This search syntax works with newer Homebrew
export BREW_FORMULA_SEARCH_VERSION=4.4.23 BREW_FORMULA_NAME=bash
# This will print any/all commits that match the version and formula name
git -C $(brew --repo homebrew/core) log \
--format=format:%H\ %s -F --all-match \
--grep=$BREW_FORMULA_SEARCH_VERSION --grep=$BREW_FORMULA_NAME


When you are certain the version exists in the formula, you can use the below:

# Gets only the latest Git commit SHA for the script further down
export BREW_FORMULA_VERSION_SHA=$(git -C $(brew --repo homebrew/core) log \
 --format=format:%H\ %s -F --all-match \
--grep=$BREW_FORMULA_SEARCH_VERSION --grep=$BREW_FORMULA_NAME | \
head -1 | awk '{print $1}')


Once you have exported the commit hash you want to use, you can use this to install that version of the package.

brew info $BREW_FORMULA_NAME \
| sed -n \
    -e '/^From: /s///' \
    -e 's/github.com/raw.githubusercontent.com/' \
    -e 's%blob/%%' \
    -e ""s/master/$BREW_FORMULA_VERSION_SHA/p"" \
| xargs brew install


Follow the directions in the formula output to put it into your PATH or set it as your default shell.
    UPDATE: This method is deprecated and no longer works.
This method results in error: Installation of mysql from a GitHub commit URL is unsupported! brew extract mysql to a stable tap on GitHub instead. (UsageError)
$ brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/c77882756a832ac1d87e7396c114158e5619016c/Formula/mysql.rb
Updating Homebrew...
==> Auto-updated Homebrew!
Updated 2 taps (homebrew/core and homebrew/cask).

...

Traceback (most recent call last):
    9: from /usr/local/Homebrew/Library/Homebrew/brew.rb:122:in `<main>'
    8: from /usr/local/Homebrew/Library/Homebrew/cmd/install.rb:132:in `install'
    7: from /usr/local/Homebrew/Library/Homebrew/cli/parser.rb:302:in `parse'
    6: from /usr/local/Homebrew/Library/Homebrew/cli/parser.rb:651:in `formulae'
    5: from /usr/local/Homebrew/Library/Homebrew/cli/parser.rb:651:in `map'
    4: from /usr/local/Homebrew/Library/Homebrew/cli/parser.rb:655:in `block in formulae'
    3: from /usr/local/Homebrew/Library/Homebrew/formulary.rb:351:in `factory'
    2: from /usr/local/Homebrew/Library/Homebrew/formulary.rb:138:in `get_formula'
    1: from /usr/local/Homebrew/Library/Homebrew/formulary.rb:142:in `klass'
/usr/local/Homebrew/Library/Homebrew/formulary.rb:227:in `load_file': Invalid usage: Installation of mysql from a GitHub commit URL is unsupported! `brew extract mysql` to a stable tap on GitHub instead. (UsageError)
    12: from /usr/local/Homebrew/Library/Homebrew/brew.rb:155:in `<main>'
    11: from /usr/local/Homebrew/Library/Homebrew/brew.rb:157:in `rescue in <main>'
    10: from /usr/local/Homebrew/Library/Homebrew/help.rb:64:in `help'
     9: from /usr/local/Homebrew/Library/Homebrew/help.rb:83:in `command_help'
     8: from /usr/local/Homebrew/Library/Homebrew/help.rb:103:in `parser_help'
     7: from /usr/local/Homebrew/Library/Homebrew/cli/parser.rb:302:in `parse'
     6: from /usr/local/Homebrew/Library/Homebrew/cli/parser.rb:651:in `formulae'
     5: from /usr/local/Homebrew/Library/Homebrew/cli/parser.rb:651:in `map'
     4: from /usr/local/Homebrew/Library/Homebrew/cli/parser.rb:655:in `block in formulae'
     3: from /usr/local/Homebrew/Library/Homebrew/formulary.rb:351:in `factory'
     2: from /usr/local/Homebrew/Library/Homebrew/formulary.rb:138:in `get_formula'
     1: from /usr/local/Homebrew/Library/Homebrew/formulary.rb:142:in `klass'
/usr/local/Homebrew/Library/Homebrew/formulary.rb:227:in `load_file': Invalid usage: Installation of mysql from a GitHub commit URL is unsupported! `brew extract mysql` to a stable tap on GitHub instead. (UsageError)

I tried to install with the recommended command, but it doesn't work in this particular instance of MySQL 5.7.10.  You may have better luck with a more recent Formula.
$ brew extract --version=5.7.10 mysql homebrew/cask
==> Searching repository history
==> Writing formula for mysql from revision 0fa511b to:
/usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask/Formula/mysql@5.7.10.rb

$ 

$ brew install /usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask/Formula/mysql@5.7.10.rb
Updating Homebrew...
==> Auto-updated Homebrew!
Updated 1 tap (homebrew/core).
==> Updated Formulae
Updated 1 formula.
Error: undefined method `core_tap?' for nil:NilClass

Error: Failed to load cask: /usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask/Formula/mysql@5.7.10.rb
Cask 'mysql@5.7.10' is unreadable: wrong constant name #<Class:0x00007f9b9498cad8>
Warning: Treating /usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask/Formula/mysql@5.7.10.rb as a formula.
==> Installing mysql@5.7.10 from homebrew/cask
==> Downloading https://homebrew.bintray.com/bottles/cmake-3.19.4.big_sur.bottle.tar.gz
==> Downloading from https://d29vzk4ow07wi7.cloudfront.net/278f2ad1caf664019ff7b4a7fc5493999c06adf503637447af13a617d45cf484?response-content-disposition=attachment%3Bfilenam
######################################################################## 100.0%
==> Downloading https://downloads.sourceforge.net/project/boost/boost/1.59.0/boost_1_59_0.tar.bz2
==> Downloading from https://phoenixnap.dl.sourceforge.net/project/boost/boost/1.59.0/boost_1_59_0.tar.bz2
######################################################################## 100.0%
==> Downloading https://cdn.mysql.com/Downloads/MySQL-5.7/mysql-5.7.10.tar.gz

curl: (22) The requested URL returned error: 404 Not Found
Error: Failed to download resource ""mysql@5.7.10""
Download failed: https://cdn.mysql.com/Downloads/MySQL-5.7/mysql-5.7.10.tar.gz

You could modify the Formula at the path above (written in ruby) to attempt to achieve your desired result (e.g., an installation of MySQL 5.7.10 on a recent macOS version).

You can use the strategy of identifying the formula and a particular commit in the history of the formula that matches the version of the package you'd like to install.

Go to https://github.com/Homebrew/homebrew-core

Press t on your keyboard to activate the file finder.

Identify a formula that looks most relevant, perhaps: Formula/mysql.rb, bringing you to a forumla file location: https://github.com/Homebrew/homebrew-core/blob/master/Formula/mysql.rb.

Look at the revision history by clicking on the History button, which is located at https://github.com/Homebrew/homebrew-core/commits/master/Formula/mysql.rb.  If you're interested in MySQL 5.7.10, you might want to click the latest revision prior to 5.7.11, which navigates to a GitHub commit:


https://github.com/Homebrew/homebrew-core/commit/c77882756a832ac1d87e7396c114158e5619016c#Formula/mysql.rb
NOTE: You may have to view the commit history in your console per GitHub's suggestion if the commit history does not load in your browser.  Replace the commit SHA above in the URL if you're interested in seeing that commit on GitHub.  Alternatively, skip to step 7, below.

Click the ""View"" button to view the source for the mysql.rb file after the commit was applied.

Then click the ""Raw"" button to view the raw source.

Copy the URL. Alternatively, build the URL yourself with the mysql.rb file name to identify your formula and the particular version of that formula (identified by the commmit SHA in the URL below).


https://raw.githubusercontent.com/Homebrew/homebrew-core/c77882756a832ac1d87e7396c114158e5619016c/Formula/mysql.rb

Install it with $ brew install [URL from step 7]
 $ brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/c77882756a832ac1d87e7396c114158e5619016c/Formula/mysql.rb



    Upgraded Postgres by accident?
My case:

postgresql was upgraded from 11 to 12 accidentally (after running brew upgrade without arguments)
I want to keep Postgres 11.

Solution:

Stop the DB:

brew services stop postgresql


Install Postgres 11:

brew install postgresql@11


Enable it:

brew link postgresql@11 --force


(Optional) Rename DB data directory from postgres to postgres@11:

cd /usr/local/var
ls -lh
mv postgresql@11 postgresql@11-fresh-backup
mv postgres postgresql@11


Start the DB:

brew services start postgresql@11

If you have any errors, check /usr/local/var/log/postgresql@11.log (notice the @11).
     This answer won't work anymore after December 2020:
brew switch got disabled in HomeBrew 2.7.0 (deprecated in 2.6.0)

There's now a much easier way to install an older version of a formula that you'd previously installed. Simply use
brew switch [formula] [version]

For instance, I alternate regularly between Node.js 0.4.12 and 0.6.5:
brew switch node 0.4.12
brew switch node 0.6.5

Since brew switch just changes the symlinks, it's very fast. See further documentation on the Homebrew Wiki under External Commands.
    The other answers here are great, but if you need to install an older version of the package and ensure that the package name is modified, you'll need a different approach. This is important when using scripts (in my case, PHP build scripts) which use brew --prefix package_name to determine what directory to use for compilation.
If you are using brew extract a version is added to the end of the package name which will break the brew --prefix lookup.
Here's how to install an older package version while maintaining the original package name:
# uninstall the newer version of the package that you accidentally installed
brew uninstall --ignore-dependencies icu4c

# `extract` the version you'd like to install into a custom tap
brew tap-new $USER/local-tap
brew extract --version=68.2 icu4c $USER/local-tap

# jump into the new tap you created
cd $(brew --repository $USER/local-tap)/Formula

# rename the formula
mv icu4c@68.2.rb icu4c.rb

# change the name of the formula by removing ""AT682"" from the `class` definition
# the exact text you'll need to remove will be different
# depending on the version you extracted
nano icu4c.rb

# then, install this specific formula directly
brew install $(brew --repository $USER/local-tap)/Formula/icu4c.rb

I wrote more about this here.
    I've discovered a better alternative solution then the other complex solutions. 

brew install https://raw.github.com/Homebrew/homebrew-versions/master/postgresql8.rb


This will download and install PostgreSQL 8.4.8



I found this solution by starting to follow the steps of searching the repo and a comment in the repo .

After a little research found that someone has a collection of rare formulars to brew up with.



If your looking for MySQL 5.1.x, give this a try.

brew install https://raw.github.com/Homebrew/homebrew-versions/master/mysql51.rb

    Here my simple answer for it - was really annoyed that there is no built in solution for that so I've built my own lazy ""script"". Feel free to contribute 
# Please define variables
packageName=<packageName>
packageVersion=<packageVersion>

# Create a new tab
brew tap-new local/$packageName

# Extract into local tap
brew extract --version=$packageVersion $packageName local/$packageName

# Verify packages is present
brew search $packageName@

# Run brew install@version as usual
brew install local/$packageName/$packageName@$packageVersion

https://gist.github.com/ArgonQQ/cff4834dab6b254cc2140bb1454b47ef
    Edit: 2021, this answer is no longer functional due to the github install being deprecated. (Thanks Tim Smith for update).
Install an old brew package version (Flyway 4.2.0 example)
Find your local homebrew git dir or clone Homebrew/homebrew-core locally
cd /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/
OR
git clone git@github.com:Homebrew/homebrew-core.git
List all available versions
git log master -- Formula/flyway.rb
Copy the commit ID for the version you want and install it directly
brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/793abfa325531415184e1549836c982b39e89299/Formula/flyway.rb 
    An updated answer since that adds to what @lance-pollard already posted as working answer.

How to Install specific version of a Formula (formula used in this example is terraform):


Find your formula file, e.g: https://github.com/Homebrew/homebrew-core/blob/master/Formula/terraform.rb
Get the commit version from githubs history with https://github.com/Homebrew/homebrew-core/commits/master/Formula/terraform.rb or git log master -- Formula/terraform.rb if you have cloned the repo locally.
Get the raw git URL with the commit version of your formula: If the formula link in github.com is https://github.com/Homebrew/homebrew-core/blob/e4ca4d2c41d4c1412994f9f1cb14993be5b2c59a/Formula/terraform.rb, your raw URL will be: https://raw.githubusercontent.com/Homebrew/homebrew-core/e4ca4d2c41d4c1412994f9f1cb14993be5b2c59a/Formula/terraform.rb
Install it with: brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/e4ca4d2c41d4c1412994f9f1cb14993be5b2c59a/Formula/terraform.rb

    I've tried most of the solutions here and they are outdated. I had to combine some ideas from here with my own work. As a result I've created a script to help me do the heavy lifting which you can find here

Usage:

brewv.sh formula_name desired_version

    Currently the old ways of installing specific formula versions have been deprecated. So it seems like we have to use brew edit [formula]. E.g. say we want to install an the 62.1 version of icu4c (needed e.g. for postgresql 10.5). Then you'd have to 

> brew edit icu4c
# drops you to editor


Here you'd have to alter the url, version and sha256 (perhaps also mirror) to the corresponding 62.1 strings. 

url ""https://ssl.icu-project.org/files/icu4c/62.1/icu4c-62_1-src.tgz""
mirror ""https://github.com/unicode-org/icu/releases/download/release-62-1/icu4c-62_1-src.tgz""
version ""62.1""
sha256 ""3dd9868d666350dda66a6e305eecde9d479fb70b30d5b55d78a1deffb97d5aa3""


then run brew reinstall icu4c to finally download the 62.1 version.
    I created a tool to ease the process prescribed in this answer.

To find a package pkg with version a.b.c, run:

$ brew-install-specific pkg@a.b.c


This will list commits on the pkg homebrew formula that mention the given version along with their GitHub urls.

Matching versions:
1. pkg: update a.b.c bottle.
   https://github.com/Homebrew/homebrew-core/commit/<COMMIT-SHA>
2. pkg: release a.b.c-beta
   https://github.com/Homebrew/homebrew-core/commit/<COMMIT-SHA>
3. pkg a.b.c
   https://github.com/Homebrew/homebrew-core/commit/<COMMIT-SHA>

Select index: 


Verify the commit from the given URL, and enter the index of the selected commit.

Select index: 2
Run:
  brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/<COMMIT-SHA>/Formula/pkg.rb


Copy and run the given command to install.
    The problem with homebrew/versions is that someone has to have that specific version of software listed in the repository for you to be able to use it.  Also, since brew versions is no longer supported, another solution is required.  For solutions that indicate using brew switch, this will only work if you haven't done a brew cleanup since the version needs to exist on your computer.

I had a problem with wanting to install a specific older version of docker-machine which wasn't listed in homebrew/versions.  I solved this using the below, which should also work for any brew installed software.  The example below will use docker-machine as the package I want to downgrade from version 0.5.0 to 0.4.1.


Go to your homebrew Formula directory.
You can determine this by running brew info [any package name]. 
For example, brew info docker-machine gives me a line that shows me
a path - /usr/local/Cellar/docker-machine/0.5.0.  This tells me that on my machine, homebrew is installed at /usr/localand my Formula
directory is located by default at /usr/local/Library/Formula
Locate the specific formula file (.rb) for your package.
Since I want to downgrade docker-machine, I can see a docker-machine.rb file.
Get the version history for this formula file . 
Enter git log docker-machine.rb.  This will list out the complete commit history for this file.  You will see output like this:



    ...more 

    commit 20c7abc13d2edd67c8c1d30c407bd5e31229cacc
    Author: BrewTestBot 
    Date:   Thu Nov 5 16:14:18 2015 +0000

        docker-machine: update 0.5.0 bottle.

    commit 8f615708184884e501bf5c16482c95eff6aea637
    Author: Vincent Lesierse 
    Date:   Tue Oct 27 22:25:30 2015 +0100

        docker-machine 0.5.0

        Updated docker-machine to 0.5.0

        Closes #45403.

        Signed-off-by: Dominyk Tiller 

    commit 5970e1af9b13dcbeffd281ae57c9ab90316ba423
    Author: BrewTestBot 
    Date:   Mon Sep 21 14:04:04 2015 +0100

        docker-machine: update 0.4.1 bottle.

    commit 18fcbd36d22fa0c19406d699308fafb44e4c8dcd
    Author: BrewTestBot 
    Date:   Sun Aug 16 09:05:56 2015 +0100

        docker-machine: update 0.4.1 bottle.

    ...more


The tricky part is to find the latest commit for the specific version you want.  In the above, I can tell the latest 0.4.1 version was committed with this commit tag : commit 5970e1af9b13dcbeffd281ae57c9ab90316ba423.  The commits above this point start using version 0.5.0 (git log entries are listed from latest to earliest date).


Get a previous version of the formula file.
Using the commit tag from step #3 (you can use the first 6 chars), you can get an older version of the formula file using the following:

git checkout 5970e1 docker-machine.rb
Uninstall your current package version.
Just run the normal brew commands to uninstall the current version of your package.
Ex. brew uninstall docker-machine
Install the older package version
Now, you can just run the normal brew install command and it will install the formula that you have checkout out.
Ex. brew install docker-machine


You may need to re-link by using the brew link docker-machine if necessary.

If at any time you want to revert back to the latest version of a specific package, go into the Formula directory and issue the following commands on your formula file (.rb)

git reset HEAD docker-machine.rb
git checkout -- docker-machine.rb


Then you can brew uninstall docker-machine and brew install docker-machine to get the latest version and keep it that way going forward.
    Here is how I downgrade KOPS (which does not support versioning)
# brew has a git repo on your localhost
cd /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core

git remote -v
origin  https://github.com/Homebrew/homebrew-core (fetch)
origin  https://github.com/Homebrew/homebrew-core (push)

# find the version of kops.rb you need
git log Formula/kops.rb

# checkout old commit
# kops: update 1.18.1 bottle.
git checkout 2f0ede7f27dfc074d5b5493894f3468f27cc73f0 -- Formula/kops.rb

brew unlink kops
brew install kops

# now we have old version installed
ls -1 /usr/local/Cellar/kops/
1.18.1
1.18.2

which kops
/usr/local/bin/kops
ls -l /usr/local/bin/kops
/usr/local/bin/kops -> ../Cellar/kops/1.18.1/bin/kops
kops version
Version 1.18.1

# revert to the newest version
brew uninstall kops
git checkout -f
brew link kops
kops version
Version 1.18.2

    brew versions and brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/<COMMIT-HASH>/Formula/<Formula>.rb not supported now.
You can try like this:
$ brew extract --version 5.6.2 gradle vitahlin/core
$ brew install gradle@5.6.2

    None of these really worked for my case (Python), so I'll add my 2 cents:

cd `brew --prefix`
git log Library/Formula/python.rb


Output looks like this:


commit 9ff2d8ca791ed1bd149fb8be063db0ed6a67a6de
Author: Dominyk Tiller <dominyktiller@gmail.com>
Date:   Thu Jun 30 17:42:18 2016 +0100

    python: clarify pour_bottle reason

commit cb3b29b824a264895434214e191d0d7ef4d51c85
Author: BrewTestBot <brew-test-bot@googlegroups.com>
Date:   Wed Jun 29 14:18:40 2016 +0100

    python: update 2.7.12 bottle.

commit 45bb1e220341894bbb7de6fd3f6df20987dc14f0
Author: Rakesh <rakkesh@users.noreply.github.com>
Date:   Wed Jun 29 10:02:26 2016 +0530

    python 2.7.12

    Closes #2452.

    Signed-off-by: Tim D. Smith <git@tim-smith.us>

commit cf5da0547cd261f79d69e7ff62fdfbd2c2d646e9
Author: BrewTestBot <brew-test-bot@googlegroups.com>
Date:   Fri Jun 17 20:14:36 2016 +0100

    python: update 2.7.11 bottle.

...



I want version 2.7.11 so my hash is cf5da0547cd261f79d69e7ff62fdfbd2c2d646e9 (or cf5da05 for short).  Next, I check out that version and install the formula python:

git checkout cf5da05
brew install python


Finally, clean up:

git checkout master

    Based on the workflow described by @tschundeee and @Debilskis update 1, I automated the procedure and added cleanup in this script.

Download it, put it in your path and brewv <formula_name> <wanted_version>. For the specific OP, it would be:

cd path/to/downloaded/script/
./brewv postgresql 8.4.4


:)
    I just used Homebrew to go back to Maven 2.2.1 since the simple brew install maven installed Maven 3.0.3.

First you have to leave the maven dir there so 


$ brew unlink maven


Use the brew tap command


$ brew tap homebrew/versions
Cloning into '/usr/local/Library/Taps/homebrew-versions'...
remote: Counting objects: 590, done.
remote: Compressing objects: 100% (265/265), done.
remote: Total 590 (delta 362), reused 549 (delta 325)
Receiving objects: 100% (590/590), 117.49 KiB | 79 KiB/s, done.
Resolving deltas: 100% (362/362), done.
Tapped 50 formula


Now you can install the maven2 formula:  


$ brew install maven2
==> Downloading http://www.apache.org/dist/maven/maven-2/2.2.1/binaries/apache-maven-2.2.1-bin.tar.gz
######################################################################## 100.0%
/usr/local/Cellar/maven2/2.2.1: 10 files, 3.1M, built in 6 seconds



$ mvn --version
Apache Maven 2.2.1 (r801777; 2009-08-06 12:16:01-0700)
Java version: 1.6.0_37
Java home: /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home
Default locale: en_US, platform encoding: MacRoman
OS name: ""mac os x"" version: ""10.7.4"" arch: ""x86_64"" Family: ""mac"" 


Edit:
You can also just brew switch maven 2.2.1 to switch to a different version. 

Edit:
The Apache Maven project reorganized their repo.  Updated this answer to account for this change.
    If you can't find your version with brew search <formula>, you can also try going over the commit logs for your formula to find the version you want: 

here is an example for installing an older version of nginx via brew:


ngxnx formula commit log
see nginx: update 1.6.3 bottle eba75b9a1a474b9fc4df30bd0a32637fa31ec049


From there, we can install 1.6.3 with the sha and raw git url:

brew install https://raw.githubusercontent.com/Homebrew/homebrew/eba75b9a1a474b9fc4df30bd0a32637fa31ec049/Library/Formula/nginx.rb
    On the newest version of homebrew (0.9.5 as of this writing) there will be a specific recipe for the version of the homebrew keg you want to install. Example:

$ brew search mongodb
mongodb    mongodb24  mongodb26


Then just do brew install mongodb26 like normal.

In the case that you had already installed the latest version, make sure to unlink the latest version and link the desired version: brew unlink mongodb && brew link mongodb26.
    I just copied an older release of elasticsearch into the /usr/local/Cellar/elasticsearch directory.

$ mkdir /usr/local/Cellar/elasticsearch/5.4.3/bin
$ cp elasticsearch /usr/local/Cellar/elasticsearch/5.4.3/bin
$ brew switch elasticsearch 5.4.3


That's it.
Maybe it's useful for anyone.
    it could be done very easy for last version of brew.

brew tap homebrew/versions
brew install subversion17 # for svn 1.7 branch instead of last available
brew install postgresql8  # for postgresql 8 (which you ask)

    Update on the Library/Formula/postgresql.rb line 8 to

http://ftp2.uk.postgresql.org/sites/ftp.postgresql.org/source/v8.4.6/postgresql-8.4.6.tar.bz2


And MD5 on line 9 to

fcc3daaf2292fa6bf1185ec45e512db6


Save and exit.

brew install postgres
initdb /usr/local/var/postgres


Now in this stage you might face the postgresql could not create shared memory segment error, to work around that update the  /etc/sysctl.conf like this:

kern.sysv.shmall=65536
kern.sysv.shmmax=16777216


Try initdb /usr/local/var/postgres again, and it should run smooth.

To run postgresql on start

launchctl load -w /usr/local/Cellar/postgresql/8.4.6/org.postgresql.postgres.plist


Hope that helps :)
    ","[2558, 2832, 821, 64, 139, 61, 257, 22, 40, 88, 27, 459, 7, 110, 3, 11, 18, 10, 6, 2, 11, 1, 1, 6, 22, 29, 6, 12, 1, 6, 1]",1545936,1049,2010-10-21T12:58:19,2022-05-02 11:29:26Z,
How can I recursively find all files in current and subfolders based on wildcard matching?,"
                
How can I recursively find all files in current and subfolders based on wildcard matching?
    Use find for that:

find . -name ""foo*""


find needs a starting point, and the . (dot) points to the current directory.
    Piping find into grep is often more convenient; it gives you the full power of regular expressions for arbitrary wildcard matching.

For example, to find all files with case insensitive string ""foo"" in the filename:

~$ find . -print | grep -i foo

    fd

In case, find is too slow, try fd utility - a simple and fast alternative to find written in Rust.

Syntax:

fd PATTERN


Demo:



Homepage: https://github.com/sharkdp/fd
    find will find all files that match a pattern:

find . -name ""*foo""


However, if you want a picture:

tree -P ""*foo""


Hope this helps!
    Use

find path/to/dir -name ""*.ext1"" -o -name ""*.ext2""


Explanation


The first parameter is the directory you want to search.
By default find does recursion.
The -o stands for -or. So above means search for this wildcard OR this one. If you have only one pattern then no need for -o.
The quotes around the wildcard pattern are required.

    find -L . -name ""foo*""


In a few cases, I have needed the -L parameter to handle symbolic directory links.  By default symbolic links are ignored.  In those cases it was quite confusing as I would change directory to a sub-directory and see the file matching the pattern but find would not return the filename.  Using -L solves that issue.  The symbolic link options for find are -P -L -H
    If your shell supports a new globbing option (can be enabled by: shopt -s globstar), you can use:

echo **/*foo*


to find any files or folders recursively. This is supported by Bash 4, zsh and similar shells.



Personally I've got this shell function defined:

f() { find . -name ""*$1*""; }


Note: Above line can be pasted directly to shell or added into your user's ~/.bashrc file.

Then I can look for any files by typing:

f some_name


Alternatively you can use a fd utility with a simple syntax, e.g. fd pattern.
    find <directory_path>  -type f -name ""<wildcard-match>""


In the wildcard-match you can provide the string you wish to match e.g. *.c (for all c files) 
    I am surprised to see that locate is not used heavily when we are to go recursively.

I would first do a locate ""$PWD"" to get the list of files in the current folder of interest, and then run greps on them as I please.

locate ""$PWD"" | grep -P <pattern>


Of course, this is assuming that the updatedb is done and the index is updated periodically. This is much faster way to find files than to run a find and asking it go down the tree.  Mentioning this for completeness.  Nothing against using find, if the tree is not very heavy.
    for file search
find / -xdev -name settings.xml   --> whole computer
find ./ -xdev -name settings.xml  --> current directory & its sub directory
for files with extension type

find . -type f -name ""*.iso""

    You can use:



# find . -type f  -name 'text_for_search'


If you want use REGX use -iname

# find . -type f  -iname 'text_for_search'

    Try with fd command if installed. Install instruction
find all file starts with 'name'
fd ""name*""

This command ignores all .hidden and .gitignoreed files.
To include .gitignoreed files, add -I option as below
fd -I ""name*""

To include hidden files, add -H option as below
fd -H ""name*""

    If you want to search special file with wildcard, you can used following code:

find . -type f -name ""*.conf""


Suppose, you want to search every .conf files from here:  

. means search started from here (current place)
-type means type of search item that here is file (f).
-name means you want to search files with *.conf names.
    Following command will list down all the files having exact name ""pattern"" (for example) in current and its sub folders. 

find ./ -name ""pattern""
    Default way to search for recursive file, and available in most cases is

find . -name ""filepattern""


It starts recursive traversing for filename  or pattern from within current directory where you are positioned. With find command, you can use wildcards, and various switches, to see full list of options, type

man find


or if man pages aren't available at your system

find --help


However, there are more modern and faster tools then find, which are traversing your whole filesystem and indexing your files, one such common tool is locate or slocate/mlocate, you should check manual of your OS on how to install it, and once it's installed it needs to initiate database, if install script don't do it for you, it can be done manually by typing

sudo updatedb


And, to use it to look for some particular file type

locate filename


Or, to look for filename or patter from within current directory, you can type:

 pwd | xargs -n 1 -I {} locate ""filepattern""


It will look through its database of files and quickly print out path names that match pattern that you have typed.
To see full list of locate's options, type:
locate --help or man locate

Additionally you can configure locate to update it's database on scheduled times via cron job, so sample cron which updates db at 1AM would look like:

0 1 * * * updatedb


These cron jobs need to be configured by root, since updatedb needs root privilege to traverse whole filesystem.
    This will search all the related files in current and sub directories, calculating their line count separately as well as totally:

find . -name ""*.wanted"" | xargs wc -l

    Below command helps to search for any files 
1) Irrespective of case
2) Result Excluding folders without permission
3) Searching from the root or from the path you like. Change / with the path you prefer. 
Syntax : 
find  -iname ''   2>&1 | grep -v ""Permission denied""
Example


  find / -iname 'C*.xml'   2>&1 | grep -v ""Permission denied""


find / -iname '*C*.xml'   2>&1 | grep -v ""Permission denied""

    With Python>3.5, using glob, . pointing to your current folder and looking for .txt files:
 python -c ""import glob;[print(x) for x in glob.glob('./**/*txt', recursive=True)]""

For older versions of Python, you can install glob2
    ","[2553, 3660, 276, 86, 194, 41, 65, 56, 37, 13, 14, 18, 6, 9, 12, 12, 6, 7, -8]",3484884,510,2011-05-05T23:01:34,2022-01-18 17:43:15Z,
How do I determine whether an array contains a particular value in Java?,"
                
I have a String[] with values like so:

public static final String[] VALUES = new String[] {""AB"",""BC"",""CD"",""AE""};


Given String s, is there a good way of testing whether VALUES contains s?
    Arrays.asList(yourArray).contains(yourValue)


Warning: this doesn't work for arrays of primitives (see the comments).



Since java-8 you can now use Streams.

String[] values = {""AB"",""BC"",""CD"",""AE""};
boolean contains = Arrays.stream(values).anyMatch(""s""::equals);


To check whether an array of int, double or long contains a value use IntStream, DoubleStream or LongStream respectively.

Example

int[] a = {1,2,3,4};
boolean contains = IntStream.of(a).anyMatch(x -> x == 4);

    Concise update for Java SE 9
Reference arrays are bad. For this case we are after a set. Since Java SE 9 we have Set.of.
private static final Set<String> VALUES = Set.of(
    ""AB"",""BC"",""CD"",""AE""
);

""Given String s, is there a good way of testing whether VALUES contains s?""
VALUES.contains(s)

O(1).
The right type, immutable, O(1) and concise. Beautiful.*
Original answer details
Just to clear the code up to start with. We have (corrected):
public static final String[] VALUES = new String[] {""AB"",""BC"",""CD"",""AE""};

This is a mutable static which FindBugs will tell you is very naughty. Do not modify statics and do not allow other code to do so also. At an absolute minimum, the field should be private:
private static final String[] VALUES = new String[] {""AB"",""BC"",""CD"",""AE""};

(Note, you can actually drop the new String[]; bit.)
Reference arrays are still bad and we want a set:
private static final Set<String> VALUES = new HashSet<String>(Arrays.asList(
     new String[] {""AB"",""BC"",""CD"",""AE""}
));

(Paranoid people, such as myself, may feel more at ease if this was wrapped in Collections.unmodifiableSet - it could then even be made public.)
(*To be a little more on brand, the collections API is predictably still missing immutable collection types and the syntax is still far too verbose, for my tastes.)
    You can use ArrayUtils.contains from Apache Commons Lang

public static boolean contains(Object[] array, Object objectToFind)

Note that this method returns false if the passed array is null.

There are also methods available for primitive arrays of all kinds.

Example:

String[] fieldsToInclude = { ""id"", ""name"", ""location"" };

if ( ArrayUtils.contains( fieldsToInclude, ""id"" ) ) {
    // Do some stuff.
}

    Four Different Ways to Check If an Array Contains a Value


Using List:

public static boolean useList(String[] arr, String targetValue) {
    return Arrays.asList(arr).contains(targetValue);
}

Using Set:

public static boolean useSet(String[] arr, String targetValue) {
    Set<String> set = new HashSet<String>(Arrays.asList(arr));
    return set.contains(targetValue);
}

Using a simple loop:

public static boolean useLoop(String[] arr, String targetValue) {
    for (String s: arr) {
        if (s.equals(targetValue))
            return true;
    }
    return false;
}

Using Arrays.binarySearch():

The code below is wrong, it is listed here for completeness. binarySearch() can ONLY be used on sorted arrays. You will find the result is weird below. This is the best option when array is sorted.

public static boolean binarySearch(String[] arr, String targetValue) {  
    return Arrays.binarySearch(arr, targetValue) >= 0;
}



Quick Example:

String testValue=""test"";
String newValueNotInList=""newValue"";
String[] valueArray = { ""this"", ""is"", ""java"" , ""test"" };
Arrays.asList(valueArray).contains(testValue); // returns true
Arrays.asList(valueArray).contains(newValueNotInList); // returns false

    Just simply implement it by hand:

public static <T> boolean contains(final T[] array, final T v) {
    for (final T e : array)
        if (e == v || v != null && v.equals(e))
            return true;

    return false;
}


Improvement:

The v != null condition is constant inside the method. It always evaluates to the same Boolean value during the method call. So if the input array is big, it is more efficient to evaluate this condition only once, and we can use a simplified/faster condition inside the for loop based on the result. The improved contains() method:

public static <T> boolean contains2(final T[] array, final T v) {
    if (v == null) {
        for (final T e : array)
            if (e == null)
                return true;
    } 
    else {
        for (final T e : array)
            if (e == v || v.equals(e))
                return true;
    }

    return false;
}

    Instead of using the quick array initialisation syntax too, you could just initialise it as a List straight away in a similar manner using the Arrays.asList method, e.g.:

public static final List<String> STRINGS = Arrays.asList(""firstString"", ""secondString"" ...., ""lastString"");


Then you can do (like above): 

STRINGS.contains(""the string you want to find"");

    With Java 8 you can create a stream and check if any entries in the stream matches ""s"":

String[] values = {""AB"",""BC"",""CD"",""AE""};
boolean sInArray = Arrays.stream(values).anyMatch(""s""::equals);


Or as a generic method:

public static <T> boolean arrayContains(T[] array, T value) {
    return Arrays.stream(array).anyMatch(value::equals);
}

    If the array is not sorted, you will have to iterate over everything and make a call to equals on each.

If the array is sorted, you can do a binary search, there's one in the Arrays class.

Generally speaking, if you are going to do a lot of membership checks, you may want to store everything in a Set, not in an array. 
    In Java 8 use Streams.
List<String> myList =
        Arrays.asList(""a1"", ""a2"", ""b1"", ""c2"", ""c1"");

myList.stream()
        .filter(s -> s.startsWith(""c""))
        .map(String::toUpperCase)
        .sorted()
        .forEach(System.out::println);

    For what it's worth I ran a test comparing the 3 suggestions for speed. I generated random integers, converted them to a String and added them to an array. I then searched for the highest possible number/string, which would be a worst case scenario for the asList().contains().
When using a 10K array size the results were:
Sort & Search   : 15
Binary Search   : 0
asList.contains : 0

When using a 100K array the results were:
Sort & Search   : 156
Binary Search   : 0
asList.contains : 32

So if the array is created in sorted order the binary search is the fastest, otherwise the asList().contains would be the way to go. If you have many searches, then it may be worthwhile to sort the array so you can use the binary search. It all depends on your application.
I would think those are the results most people would expect. Here is the test code:
import java.util.*;

public class Test {
    public static void main(String args[]) {
        long start = 0;
        int size = 100000;
        String[] strings = new String[size];
        Random random = new Random();

        for (int i = 0; i < size; i++)
            strings[i] = """" + random.nextInt(size);

        start = System.currentTimeMillis();
        Arrays.sort(strings);
        System.out.println(Arrays.binarySearch(strings, """" + (size - 1)));
        System.out.println(""Sort & Search : ""
                + (System.currentTimeMillis() - start));

        start = System.currentTimeMillis();
        System.out.println(Arrays.binarySearch(strings, """" + (size - 1)));
        System.out.println(""Search        : ""
                + (System.currentTimeMillis() - start));

        start = System.currentTimeMillis();
        System.out.println(Arrays.asList(strings).contains("""" + (size - 1)));
        System.out.println(""Contains      : ""
                + (System.currentTimeMillis() - start));
    }
}

    the shortest solution
the array VALUES may contain duplicates
since Java 9
List.of(VALUES).contains(s);

    Developers often do:
Set<String> set = new HashSet<String>(Arrays.asList(arr));
return set.contains(targetValue);

The above code works, but there is no need to convert a list to set first. Converting a list to a set requires extra time. It can as simple as:
Arrays.asList(arr).contains(targetValue);

or
for (String s : arr) {
    if (s.equals(targetValue))
        return true;
}

return false;

The first one is more readable than the second one.
    ObStupidAnswer (but I think there's a lesson in here somewhere):

enum Values {
    AB, BC, CD, AE
}

try {
    Values.valueOf(s);
    return true;
} catch (IllegalArgumentException exc) {
    return false;
}

    You can use the Arrays class to perform a binary search for the value. If your array is not sorted, you will have to use the sort functions in the same class to sort the array, then search through it.
    Use the following (the contains() method is ArrayUtils.in() in this code):
ObjectUtils.java
public class ObjectUtils {
    /**
     * A null safe method to detect if two objects are equal.
     * @param object1
     * @param object2
     * @return true if either both objects are null, or equal, else returns false.
     */
    public static boolean equals(Object object1, Object object2) {
        return object1 == null ? object2 == null : object1.equals(object2);
    }
}

ArrayUtils.java
public class ArrayUtils {
    /**
     * Find the index of of an object is in given array,
     * starting from given inclusive index.
     * @param ts    Array to be searched in.
     * @param t     Object to be searched.
     * @param start The index from where the search must start.
     * @return Index of the given object in the array if it is there, else -1.
     */
    public static <T> int indexOf(final T[] ts, final T t, int start) {
        for (int i = start; i < ts.length; ++i)
            if (ObjectUtils.equals(ts[i], t))
                return i;
        return -1;
    }

    /**
     * Find the index of of an object is in given array, starting from 0;
     * @param ts Array to be searched in.
     * @param t  Object to be searched.
     * @return indexOf(ts, t, 0)
     */
    public static <T> int indexOf(final T[] ts, final T t) {
        return indexOf(ts, t, 0);
    }

    /**
     * Detect if the given object is in the given array.
     * @param ts Array to be searched in.
     * @param t  Object to be searched.
     * @return If indexOf(ts, t) is greater than -1.
     */
    public static <T> boolean in(final T[] ts, final T t) {
        return indexOf(ts, t) > -1;
    }
}

As you can see in the code above, that there are other utility methods ObjectUtils.equals() and ArrayUtils.indexOf(), that were used at other places as well.
    Actually, if you use HashSet<String> as Tom Hawtin proposed you don't need to worry about sorting, and your speed is the same as with binary search on a presorted array, probably even faster.

It all depends on how your code is set up, obviously, but from where I stand, the order would be:

On an unsorted array:


HashSet
asList
sort & binary


On a sorted array:


HashSet
Binary
asList


So either way, HashSet for the win.
    If you don't want it to be case sensitive 

Arrays.stream(VALUES).anyMatch(s::equalsIgnoreCase);

    Check this
String[] VALUES = new String[]{""AB"", ""BC"", ""CD"", ""AE""};
String s;

for (int i = 0; i < VALUES.length; i++) {
    if (VALUES[i].equals(s)) {
        // do your stuff
    } else {
        //do your stuff
    }
}

    Try using Java 8 predicate test method
Here is a full example of it.
import java.util.Arrays;
import java.util.List;
import java.util.function.Predicate;

public class Test {
    public static final List<String> VALUES =
            Arrays.asList(""AA"", ""AB"", ""BC"", ""CD"", ""AE"");

    public static void main(String args[]) {
        Predicate<String> containsLetterA = VALUES -> VALUES.contains(""AB"");
        for (String i : VALUES) {
            System.out.println(containsLetterA.test(i));
        }
    }
}

http://mytechnologythought.blogspot.com/2019/10/java-8-predicate-test-method-example.html
https://github.com/VipulGulhane1/java8/blob/master/Test.java
    One possible solution:

import java.util.Arrays;
import java.util.List;

public class ArrayContainsElement {
  public static final List<String> VALUES = Arrays.asList(""AB"", ""BC"", ""CD"", ""AE"");

  public static void main(String args[]) {

      if (VALUES.contains(""AB"")) {
          System.out.println(""Contains"");
      } else {
          System.out.println(""Not contains"");
      }
  }
}

    As I'm dealing with low level Java using primitive types byte and byte[], the best so far I got is from bytes-java https://github.com/patrickfav/bytes-java seems a fine piece of work
    If you have the google collections library, Tom's answer can be simplified a lot by using ImmutableSet (http://google-collections.googlecode.com/svn/trunk/javadoc/com/google/common/collect/ImmutableSet.html)

This really removes a lot of clutter from the initialization proposed

private static final Set<String> VALUES =  ImmutableSet.of(""AB"",""BC"",""CD"",""AE"");

    Try this:

ArrayList<Integer> arrlist = new ArrayList<Integer>(8);

// use add() method to add elements in the list
arrlist.add(20);
arrlist.add(25);
arrlist.add(10);
arrlist.add(15);

boolean retval = arrlist.contains(10);
if (retval == true) {
    System.out.println(""10 is contained in the list"");
}
else {
    System.out.println(""10 is not contained in the list"");
}

    
For arrays of limited length use the following (as given by camickr).  This is slow for repeated checks, especially for longer arrays (linear search).  

 Arrays.asList(...).contains(...)

For fast performance if you repeatedly check against a larger set of elements


An array is the wrong structure.  Use a TreeSet and add each element to it.  It sorts elements and has a fast exist() method (binary search).
If the elements implement Comparable & you want the TreeSet sorted accordingly:

ElementClass.compareTo() method must be compatable with ElementClass.equals(): see Triads not showing up to fight? (Java Set missing an item) 

TreeSet myElements = new TreeSet();

// Do this for each element (implementing *Comparable*)
myElements.add(nextElement);

// *Alternatively*, if an array is forceably provided from other code:
myElements.addAll(Arrays.asList(myArray));

Otherwise, use your own Comparator:

class MyComparator implements Comparator<ElementClass> {
     int compareTo(ElementClass element1; ElementClass element2) {
          // Your comparison of elements
          // Should be consistent with object equality
     }

     boolean equals(Object otherComparator) {
          // Your equality of comparators
     }
}


// construct TreeSet with the comparator
TreeSet myElements = new TreeSet(new MyComparator());

// Do this for each element (implementing *Comparable*)
myElements.add(nextElement);

The payoff: check existence of some element:

// Fast binary search through sorted elements (performance ~ log(size)):
boolean containsElement = myElements.exists(someElement);



    Create a boolean initially set to false. Run a loop to check every value in the array and compare to the value you are checking against. If you ever get a match, set boolean to true and stop the looping. Then assert that the boolean is true.
    Arrays.asList() -> then calling the contains() method will always work, but a search algorithm is much better since you don't need to create a lightweight list wrapper around the array, which is what Arrays.asList() does.

public boolean findString(String[] strings, String desired){
   for (String str : strings){
       if (desired.equals(str)) {
           return true;
       }
   }
   return false; //if we get here there is no desired String, return false.
}

    Use Array.BinarySearch(array,obj) for finding the given object in array or not.

Example:

if (Array.BinarySearch(str, i) > -1)`  true --exists


false --not exists
    Using a simple loop is the most efficient way of doing this. 

boolean useLoop(String[] arr, String targetValue) {
    for(String s: arr){
        if(s.equals(targetValue))
            return true;
    }
    return false;
}


Courtesy to Programcreek
    You can check it by two methods
A) By converting the array into string and then check the required string by .contains method
String a = Arrays.toString(VALUES);
System.out.println(a.contains(""AB""));
System.out.println(a.contains(""BC""));
System.out.println(a.contains(""CD""));
System.out.println(a.contains(""AE""));

B) This is a more efficent method
Scanner s = new Scanner(System.in);

String u = s.next();
boolean d = true;
for (int i = 0; i < VAL.length; i++) {
    if (VAL[i].equals(u) == d)
        System.out.println(VAL[i] + "" "" + u + VAL[i].equals(u));
}

    ","[2550, 3232, 406, 228, 89, 165, 41, 39, 74, 10, 52, 3, 10, 19, 29, 6, 14, 4, 4, 2, 10, 1, 11, 4, 5, 1, 3, 2, 6, -2]",2391478,430,2009-07-15T00:03:21,2021-06-02 05:04:29Z,java 
How do I split a string on a delimiter in Bash?,"
                
I have this string stored in a variable:

IN=""bla@some.com;john@home.com""


Now I would like to split the strings by ; delimiter so that I have:

ADDR1=""bla@some.com""
ADDR2=""john@home.com""


I don't necessarily need the ADDR1 and ADDR2 variables. If they are elements of an array that's even better.



After suggestions from the answers below, I ended up with the following which is what I was after:

#!/usr/bin/env bash

IN=""bla@some.com;john@home.com""

mails=$(echo $IN | tr "";"" ""\n"")

for addr in $mails
do
    echo ""> [$addr]""
done


Output:

> [bla@some.com]
> [john@home.com]


There was a solution involving setting Internal_field_separator (IFS) to ;. I am not sure what happened with that answer, how do you reset IFS back to default?

RE: IFS solution, I tried this and it works, I keep the old IFS and then restore it:

IN=""bla@some.com;john@home.com""

OIFS=$IFS
IFS=';'
mails2=$IN
for x in $mails2
do
    echo ""> [$x]""
done

IFS=$OIFS


BTW, when I tried 

mails2=($IN)


I only got the first string when printing it in loop, without brackets around $IN it works.
    Taken from Bash shell script split array:
IN=""bla@some.com;john@home.com""
arrIN=(${IN//;/ })
echo ${arrIN[1]}                  # Output: john@home.com

Explanation:
This construction replaces all occurrences of ';' (the initial // means global replace) in the string IN with ' ' (a single space), then interprets the space-delimited string as an array (that's what the surrounding parentheses do).
The syntax used inside of the curly braces to replace each ';' character with a ' ' character is called Parameter Expansion.
There are some common gotchas:

If the original string has spaces, you will need to use IFS:


IFS=':'; arrIN=($IN); unset IFS;


If the original string has spaces and the delimiter is a new line, you can set IFS with:


IFS=$'\n'; arrIN=($IN); unset IFS;

    You can set the internal field separator (IFS) variable, and then let it parse into an array. When this happens in a command, then the assignment to IFS only takes place to that single command's environment (to read ). It then parses the input according to the IFS variable value into an array, which we can then iterate over.
This example will parse one line of items separated by ;, pushing it into an array:
IFS=';' read -ra ADDR <<< ""$IN""
for i in ""${ADDR[@]}""; do
  # process ""$i""
done

This other example is for processing the whole content of $IN, each time one line of input separated by ;:
while IFS=';' read -ra ADDR; do
  for i in ""${ADDR[@]}""; do
    # process ""$i""
  done
done <<< ""$IN""

    I've seen a couple of answers referencing the cut command, but they've all been deleted.  It's a little odd that nobody has elaborated on that, because I think it's one of the more useful commands for doing this type of thing, especially for parsing delimited log files.

In the case of splitting this specific example into a bash script array, tr is probably more efficient, but cut can be used, and is more effective if you want to pull specific fields from the middle.

Example:

$ echo ""bla@some.com;john@home.com"" | cut -d "";"" -f 1
bla@some.com
$ echo ""bla@some.com;john@home.com"" | cut -d "";"" -f 2
john@home.com


You can obviously put that into a loop, and iterate the -f parameter to pull each field independently.

This gets more useful when you have a delimited log file with rows like this:

2015-04-27|12345|some action|an attribute|meta data


cut is very handy to be able to cat this file and select a particular field for further processing.
    Compatible answer
There are a lot of different ways to do this in bash.
However, it's important to first note that bash has many special features (so-called bashisms) that won't work in any other shell.
In particular, arrays, associative arrays, and pattern substitution, which are used in the solutions in this post as well as others in the thread, are bashisms and may not work under other shells that many people use.
For instance: on my Debian GNU/Linux, there is a standard shell called dash; I know many people who like to use another shell called ksh; and there is also a special tool called busybox with his own shell interpreter (ash).
Requested string
The string to be split in the above question is:
IN=""bla@some.com;john@home.com""

I will use a modified version of this string to ensure that my solution is robust to strings containing whitespace, which could break other solutions:
IN=""bla@some.com;john@home.com;Full Name <fulnam@other.org>""

Split string based on delimiter in bash (version >=4.2)
In pure bash, we can create an array with elements split by a temporary value for IFS (the input field separator). The IFS, among other things, tells bash which character(s) it should treat as a delimiter between elements when defining an array:
IN=""bla@some.com;john@home.com;Full Name <fulnam@other.org>""

# save original IFS value so we can restore it later
oIFS=""$IFS""
IFS="";""
declare -a fields=($IN)
IFS=""$oIFS""
unset oIFS

In newer versions of bash, prefixing a command with an IFS definition changes the IFS for that command only and resets it to the previous value immediately afterwards. This means we can do the above in just one line:
IFS=\; read -a fields <<<""$IN""
# after this command, the IFS resets back to its previous value (here, the default):
set | grep ^IFS=
# IFS=$' \t\n'

We can see that the string IN has been stored into an array named fields, split on the semicolons:
set | grep ^fields=\\\|^IN=
# fields=([0]=""bla@some.com"" [1]=""john@home.com"" [2]=""Full Name <fulnam@other.org>"")
# IN='bla@some.com;john@home.com;Full Name <fulnam@other.org>'

(We can also display the contents of these variables using declare -p:)
declare -p IN fields
# declare -- IN=""bla@some.com;john@home.com;Full Name <fulnam@other.org>""
# declare -a fields=([0]=""bla@some.com"" [1]=""john@home.com"" [2]=""Full Name <fulnam@other.org>"")

Note that read is the quickest way to do the split because there are no forks or external resources called.
Once the array is defined, you can use a simple loop to process each field (or, rather, each element in the array you've now defined):
# `""${fields[@]}""` expands to return every element of `fields` array as a separate argument
for x in ""${fields[@]}"" ;do
    echo ""> [$x]""
    done
# > [bla@some.com]
# > [john@home.com]
# > [Full Name <fulnam@other.org>]

Or you could drop each field from the array after processing using a shifting approach, which I like:
while [ ""$fields"" ] ;do
    echo ""> [$fields]""
    # slice the array 
    fields=(""${fields[@]:1}"")
    done
# > [bla@some.com]
# > [john@home.com]
# > [Full Name <fulnam@other.org>]

And if you just want a simple printout of the array, you don't even need to loop over it:
printf ""> [%s]\n"" ""${fields[@]}""
# > [bla@some.com]
# > [john@home.com]
# > [Full Name <fulnam@other.org>]

Update: recent bash >= 4.4
In newer versions of bash, you can also play with the command mapfile:
mapfile -td \; fields < <(printf ""%s\0"" ""$IN"")

This syntax preserve special chars, newlines and empty fields!
If you don't want to include empty fields, you could do the following:
mapfile -td \; fields <<<""$IN""
fields=(""${fields[@]%$'\n'}"")   # drop '\n' added by '<<<'

With mapfile, you can also skip declaring an array and implicitly ""loop"" over the delimited elements, calling a function on each:
myPubliMail() {
    printf ""Seq: %6d: Sending mail to '%s'..."" $1 ""$2""
    # mail -s ""This is not a spam..."" ""$2"" </path/to/body
    printf ""\e[3D, done.\n""
}

mapfile < <(printf ""%s\0"" ""$IN"") -td \; -c 1 -C myPubliMail

(Note: the \0 at end of the format string is useless if you don't care about empty fields at end of the string or they're not present.)
mapfile < <(echo -n ""$IN"") -td \; -c 1 -C myPubliMail

# Seq:      0: Sending mail to 'bla@some.com', done.
# Seq:      1: Sending mail to 'john@home.com', done.
# Seq:      2: Sending mail to 'Full Name <fulnam@other.org>', done.

Or you could use <<<, and in the function body include some processing to drop the newline it adds:
myPubliMail() {
    local seq=$1 dest=""${2%$'\n'}""
    printf ""Seq: %6d: Sending mail to '%s'..."" $seq ""$dest""
    # mail -s ""This is not a spam..."" ""$dest"" </path/to/body
    printf ""\e[3D, done.\n""
}

mapfile <<<""$IN"" -td \; -c 1 -C myPubliMail

# Renders the same output:
# Seq:      0: Sending mail to 'bla@some.com', done.
# Seq:      1: Sending mail to 'john@home.com', done.
# Seq:      2: Sending mail to 'Full Name <fulnam@other.org>', done.


Split string based on delimiter in shell
If you can't use bash, or if you want to write something that can be used in many different shells, you often can't use bashisms -- and this includes the arrays we've been using in the solutions above.
However, we don't need to use arrays to loop over ""elements"" of a string. There is a syntax used in many shells for deleting substrings of a string from the first or last occurrence of a pattern. Note that * is a wildcard that stands for zero or more characters:
(The lack of this approach in any solution posted so far is the main reason I'm writing this answer ;)
${var#*SubStr}  # drops substring from start of string up to first occurrence of `SubStr`
${var##*SubStr} # drops substring from start of string up to last occurrence of `SubStr`
${var%SubStr*}  # drops substring from last occurrence of `SubStr` to end of string
${var%%SubStr*} # drops substring from first occurrence of `SubStr` to end of string

As explained by Score_Under:

# and % delete the shortest possible matching substring from the start and end of the string respectively, and
## and %% delete the longest possible matching substring.

Using the above syntax, we can create an approach where we extract substring ""elements"" from the string by deleting the substrings up to or after the delimiter.
The codeblock below works well in bash (including Mac OS's bash), dash, ksh, and busybox's ash:
(Thanks to Adam Katz's comment, making this loop a lot simplier!)
IN=""bla@some.com;john@home.com;Full Name <fulnam@other.org>""
while [ ""$IN"" != ""$iter"" ] ;do
    # extract the substring from start of string up to delimiter.
    iter=${IN%%;*}
    # delete this first ""element"" AND next separator, from $IN.
    IN=""${IN#$iter;}""
    # Print (or doing anything with) the first ""element"".
    echo ""> [$iter]""
done
# > [bla@some.com]
# > [john@home.com]
# > [Full Name <fulnam@other.org>]

Have fun!
    If you don't mind processing them immediately, I like to do this:
for i in $(echo $IN | tr "";"" ""\n"")
do
  # process
done

You could use this kind of loop to initialize an array, but there's probably an easier way to do it.
    I think AWK is the best and efficient command to resolve your problem. AWK is included by default in almost every Linux distribution.

echo ""bla@some.com;john@home.com"" | awk -F';' '{print $1,$2}'


will give

bla@some.com john@home.com


Of course your can store each email address by redefining the awk print field.
    This worked for me:

string=""1;2""
echo $string | cut -d';' -f1 # output is 1
echo $string | cut -d';' -f2 # output is 2

    echo ""bla@some.com;john@home.com"" | sed -e 's/;/\n/g'
bla@some.com
john@home.com

    How about this approach:

IN=""bla@some.com;john@home.com"" 
set -- ""$IN"" 
IFS="";""; declare -a Array=($*) 
echo ""${Array[@]}"" 
echo ""${Array[0]}"" 
echo ""${Array[1]}"" 


Source
    Without setting the IFS

If you just have one colon you can do that:

a=""foo:bar""
b=${a%:*}
c=${a##*:}


you will get:

b = foo
c = bar

    How about this one liner, if you're not using arrays:

IFS=';' read ADDR1 ADDR2 <<<$IN

    In Bash, a bullet proof way, that will work even if your variable contains newlines:
IFS=';' read -d '' -ra array < <(printf '%s;\0' ""$in"")

Look:
$ in=$'one;two three;*;there is\na newline\nin this field'
$ IFS=';' read -d '' -ra array < <(printf '%s;\0' ""$in"")
$ declare -p array
declare -a array='([0]=""one"" [1]=""two three"" [2]=""*"" [3]=""there is
a newline
in this field"")'

The trick for this to work is to use the -d option of read (delimiter) with an empty delimiter, so that read is forced to read everything it's fed. And we feed read with exactly the content of the variable in, with no trailing newline thanks to printf. Note that's we're also putting the delimiter in printf to ensure that the string passed to read has a trailing delimiter. Without it, read would trim potential trailing empty fields:
$ in='one;two;three;'    # there's an empty field
$ IFS=';' read -d '' -ra array < <(printf '%s;\0' ""$in"")
$ declare -p array
declare -a array='([0]=""one"" [1]=""two"" [2]=""three"" [3]="""")'

the trailing empty field is preserved.

Update for Bash4.4
Since Bash 4.4, the builtin mapfile (aka readarray) supports the -d option to specify a delimiter. Hence another canonical way is:
mapfile -d ';' -t array < <(printf '%s;' ""$in"")

    This also works:

IN=""bla@some.com;john@home.com""
echo ADD1=`echo $IN | cut -d \; -f 1`
echo ADD2=`echo $IN | cut -d \; -f 2`


Be careful, this solution is not always correct. In case you pass ""bla@some.com"" only, it will assign it to both ADD1 and ADD2.
    There is a simple and smart way like this:

echo ""add:sfff"" | xargs -d: -i  echo {}


But you must use gnu xargs, BSD xargs cant support -d delim.  If you use apple mac like me.  You can install gnu xargs :

brew install findutils


then

echo ""add:sfff"" | gxargs -d: -i  echo {}

    A different take on Darron's answer, this is how I do it:

IN=""bla@some.com;john@home.com""
read ADDR1 ADDR2 <<<$(IFS="";""; echo $IN)

    The following Bash/zsh function splits its first argument on the delimiter given by the second argument:

split() {
    local string=""$1""
    local delimiter=""$2""
    if [ -n ""$string"" ]; then
        local part
        while read -d ""$delimiter"" part; do
            echo $part
        done <<< ""$string""
        echo $part
    fi
}


For instance, the command

$ split 'a;b;c' ';'


yields

a
b
c


This output may, for instance, be piped to other commands. Example:

$ split 'a;b;c' ';' | cat -n
1   a
2   b
3   c


Compared to the other solutions given, this one has the following advantages:


IFS is not overriden: Due to dynamic scoping of even local variables, overriding IFS over a loop causes the new value to leak into function calls performed from within the loop.
Arrays are not used: Reading a string into an array using read requires the flag -a in Bash and -A in zsh.


If desired, the function may be put into a script as follows:

#!/usr/bin/env bash

split() {
    # ...
}

split ""$@""

    Here is a clean 3-liner:

in=""foo@bar;bizz@buzz;fizz@buzz;buzz@woof""
IFS=';' list=($in)
for item in ""${list[@]}""; do echo $item; done


where IFS delimit words based on the separator and () is used to create an array. Then [@] is used to return each item as a separate word.

If you've any code after that, you also need to restore $IFS, e.g. unset IFS.
    you can apply awk to many situations

echo ""bla@some.com;john@home.com""|awk -F';' '{printf ""%s\n%s\n"", $1, $2}'


also you can use this

echo ""bla@some.com;john@home.com""|awk -F';' '{print $1,$2}' OFS=""\n""

    If no space, Why not this?

IN=""bla@some.com;john@home.com""
arr=(`echo $IN | tr ';' ' '`)

echo ${arr[0]}
echo ${arr[1]}

    A one-liner to split a string separated by ';' into an array is:

IN=""bla@some.com;john@home.com""
ADDRS=( $(IFS="";"" echo ""$IN"") )
echo ${ADDRS[0]}
echo ${ADDRS[1]}


This only sets IFS in a subshell, so you don't have to worry about saving and restoring its value.
    There are some cool answers here (errator esp.), but for something analogous to split in other languages -- which is what I took the original question to mean -- I settled on this:

IN=""bla@some.com;john@home.com""
declare -a a=""(${IN/;/ })"";


Now ${a[0]}, ${a[1]}, etc, are as you would expect. Use ${#a[*]} for number of terms. Or to iterate, of course: 

for i in ${a[*]}; do echo $i; done


IMPORTANT NOTE: 

This works in cases where there are no spaces to worry about, which solved my problem, but may not solve yours. Go with the $IFS solution(s) in that case. 
    IN='bla@some.com;john@home.com;Charlie Brown <cbrown@acme.com;!""#$%&/()[]{}*? are no problem;simple is beautiful :-)'
set -f
oldifs=""$IFS""
IFS=';'; arrayIN=($IN)
IFS=""$oldifs""
for i in ""${arrayIN[@]}""; do
echo ""$i""
done
set +f


Output:

bla@some.com
john@home.com
Charlie Brown <cbrown@acme.com
!""#$%&/()[]{}*? are no problem
simple is beautiful :-)


Explanation: Simple assignment using parenthesis () converts semicolon separated list into an array provided you have correct IFS while doing that. Standard FOR loop handles individual items in that array as usual.
Notice that the list given for IN variable must be ""hard"" quoted, that is, with single ticks. 

IFS must be saved and restored since Bash does not treat an assignment the same way as a command. An alternate workaround is to wrap the assignment inside a function and call that function with a modified IFS. In that case separate saving/restoring of IFS is not needed. Thanks for ""Bize"" for pointing that out.
    Use the set built-in to load up the $@ array:

IN=""bla@some.com;john@home.com""
IFS=';'; set $IN; IFS=$' \t\n'


Then, let the party begin:

echo $#
for a; do echo $a; done
ADDR1=$1 ADDR2=$2

    In Android shell, most of the proposed methods just do not work:

$ IFS=':' read -ra ADDR <<<""$PATH""                             
/system/bin/sh: can't create temporary file /sqlite_stmt_journals/mksh.EbNoR10629: No such file or directory


What does work is:

$ for i in ${PATH//:/ }; do echo $i; done
/sbin
/vendor/bin
/system/sbin
/system/bin
/system/xbin


where // means global replacement.
    Apart from the fantastic answers that were already provided, if it is just a matter of printing out the data you may consider using awk:

awk -F"";"" '{for (i=1;i<=NF;i++) printf(""> [%s]\n"", $i)}' <<< ""$IN""


This sets the field separator to ;, so that it can loop through the fields with a for loop and print accordingly.

Test

$ IN=""bla@some.com;john@home.com""
$ awk -F"";"" '{for (i=1;i<=NF;i++) printf(""> [%s]\n"", $i)}' <<< ""$IN""
> [bla@some.com]
> [john@home.com]


With another input:

$ awk -F"";"" '{for (i=1;i<=NF;i++) printf(""> [%s]\n"", $i)}' <<< ""a;b;c   d;e_;f""
> [a]
> [b]
> [c   d]
> [e_]
> [f]

    Two bourne-ish alternatives where neither require bash arrays:

Case 1: Keep it nice and simple: Use a NewLine as the Record-Separator... eg.

IN=""bla@some.com
john@home.com""

while read i; do
  # process ""$i"" ... eg.
    echo ""[email:$i]""
done <<< ""$IN""


Note: in this first case no sub-process is forked to assist with list manipulation.

Idea: Maybe it is worth using NL extensively internally, and only converting to a different RS when generating the final result externally.

Case 2: Using a "";"" as a record separator... eg.

NL=""
"" IRS="";"" ORS="";""

conv_IRS() {
  exec tr ""$1"" ""$NL""
}

conv_ORS() {
  exec tr ""$NL"" ""$1""
}

IN=""bla@some.com;john@home.com""
IN=""$(conv_IRS "";"" <<< ""$IN"")""

while read i; do
  # process ""$i"" ... eg.
    echo -n ""[email:$i]$ORS""
done <<< ""$IN""


In both cases a sub-list can be composed within the loop is persistent after the loop has completed.  This is useful when manipulating lists in memory, instead storing lists in files. {p.s. keep calm and carry on B-) }
    IN=""bla@some.com;john@home.com""
IFS=';'
read -a IN_arr <<< ""${IN}""
for entry in ""${IN_arr[@]}""
do
    echo $entry
done


Output

bla@some.com
john@home.com


System : Ubuntu 12.04.1
    This is the simplest way to do it.

spo='one;two;three'
OIFS=$IFS
IFS=';'
spo_array=($spo)
IFS=$OIFS
echo ${spo_array[*]}

    Here's my answer!
DELIMITER_VAL='='

read -d '' F_ABOUT_DISTRO_R <<""EOF""
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=14.04
DISTRIB_CODENAME=trusty
DISTRIB_DESCRIPTION=""Ubuntu 14.04.4 LTS""
NAME=""Ubuntu""
VERSION=""14.04.4 LTS, Trusty Tahr""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 14.04.4 LTS""
VERSION_ID=""14.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
EOF

SPLIT_NOW=$(awk -F$DELIMITER_VAL '{for(i=1;i<=NF;i++){printf ""%s\n"", $i}}' <<<""${F_ABOUT_DISTRO_R}"")
while read -r line; do
   SPLIT+=(""$line"")
done <<< ""$SPLIT_NOW""
for i in ""${SPLIT[@]}""; do
    echo ""$i""
done

Why this approach is ""the best"" for me?
Because of two reasons:

You do not need to escape the delimiter;
You will not have problem with blank spaces. The value will be properly separated in the array.

    Edit:
I'm sorry, I had read somewhere on SO that perl is required by POSIX, so I thought it would be legitimate to use it.
But on unix.stackexchange.com, several users state perl is NOT part of the POSIX specification.
My solution: A function that uses perl's split to do the work.
With detailed comments:
#!/bin/bash

# This function is a wrapper for Perl's split.\
# \
# Since we cannot return an array like in Perl,
# it takes the name of the resulting array as last
# argument.\
# \
# See https://perldoc.perl.org/functions/split for usage info
# and examples.\
# \
# If you provide a Perl regexp that contains e. g. an escaped token like \b,
# space(s) and/or capture group(s), it must be quoted, and e. g. /\b/ must
# be single-quoted.\
# Thus, it's best to generally single-quote a Perl regexp.
function split # Args: <Element separator regexp> <string> <array name>
{
    (($# != 3)) && echo ""${FUNCNAME[0]}: Wrong number of arguments, returning."" && return 1

    local elementSepRE=$1
    local string=$2
    local -n array=$3

    local element i=0

    # Attention! read does Word Splitting on each line!
    # I must admit I didn't know that so far.
    # This removes leading and trailing spaces, exactly
    # what we don't want.
    # Thus, we set IFS locally to newline only.
    local IFS=$'\n'

    while read element; do
        # As opposed to array+=($element),
        # this preserves leading and trailing spaces.
        array[i++]=$element
    done <<<$(_perl_split)
}

# This function calls Perl's split function and prints the elements of the
# resulting array on separate lines.\
# It uses the caller's $elementSepRE and $string.
function _perl_split
{
    # A heredoc is a great way of embedding a Perl script.
    # N.B.: - Shell variables get expanded.
    #         - Thus:
    #           - They must be quoted.
    #           - Perl scalar variables must be escaped.
    #       - The backslash of \n must be escaped to protect it.
    #       - Instead of redirecting a single heredoc to perl, we may
    #         use multiple heredocs with cat within a command group and
    #         pipe the result to perl.
    #         This enables us to conditionally add certain lines of code.

    {
        cat <<-END
            my \$elementSepRE=q($elementSepRE);
        END

        # If $elementSepRE is a literal Perl regexp, qr must be applied
        # to it in order to use it.
        # N.B.: We cannot write this condition in Perl because when perl
        # compiles the script, all statements are checked for validity,
        # no matter if they will actually be executed or not.
        # And if $elementSepRE was e. g. == ', the line below  although
        # not to be executed  would give an error because of an unterminated
        # single-quoted string.
        [[ $elementSepRE =~ ^m?/ && $elementSepRE =~ /[msixpodualn]*$ ]] && cat <<-END
            \$elementSepRE=qr$elementSepRE;
        END

        cat <<-END
            my @array=split(\$elementSepRE, q($string));

            print(\$_ . ""\\n"") for (@array);
        END
    } | perl
}

And the same without comments for those who see at a glance what's going on ;)
#!/bin/bash

# This function is a wrapper for Perl's split.\
# \
# Since we cannot return an array like in Perl,
# it takes the name of the resulting array as last
# argument.\
# \
# See https://perldoc.perl.org/functions/split for usage info
# and examples.\
# \
# If you provide a Perl regexp that contains e. g. an escaped token like \b,
# space(s) and/or capture group(s), it must be quoted, and e. g. /\b/ must
# be single-quoted.\
# Thus, it's best to generally single-quote a Perl regexp.
function split # Args: <Element separator regexp> <string> <array name>
{
    (($# != 3)) && echo ""${FUNCNAME[0]}: Wrong number of arguments, returning."" && return 1

    local elementSepRE=$1
    local string=$2
    local -n array=$3

    local element i=0

    local IFS=$'\n'

    while read element; do
        array[i++]=$element
    done <<<$(_perl_split)
}

function _perl_split
{
    {
        cat <<-END
            my \$elementSepRE=q($elementSepRE);
        END

        [[ $elementSepRE =~ ^m?/ && $elementSepRE =~ /[msixpodualn]*$ ]] && cat <<-END
            \$elementSepRE=qr$elementSepRE;
        END

        cat <<-END
            my @array=split(\$elementSepRE, q($string));

            print(\$_ . ""\\n"") for (@array);
        END
    } | perl
}

    ","[2542, 1350, 1513, 298, 297, 328, 138, 167, 82, 100, 33, 37, 39, 71, 11, 38, 14, 23, 11, 5, 1, 4, 2, 2, 2, 2, 2, 3, 4, 1, -1]",2897718,688,2009-05-28T02:03:43,2022-03-10 07:42:30Z,bash bash bash 
Renaming column names in Pandas,"
                
How do I change the column labels of a pandas DataFrame from:
['$a', '$b', '$c', '$d', '$e']

to
['a', 'b', 'c', 'd', 'e'].

    RENAME SPECIFIC COLUMNS
Use the df.rename() function and refer the columns to be renamed. Not all the columns have to be renamed:
df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})
# Or rename the existing DataFrame (rather than creating a copy) 
df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)

Minimal Code Example
df = pd.DataFrame('x', index=range(3), columns=list('abcde'))
df

   a  b  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x

The following methods all work and produce the same output:
df2 = df.rename({'a': 'X', 'b': 'Y'}, axis=1)  # new method
df2 = df.rename({'a': 'X', 'b': 'Y'}, axis='columns')
df2 = df.rename(columns={'a': 'X', 'b': 'Y'})  # old method  

df2

   X  Y  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x

Remember to assign the result back, as the modification is not-inplace. Alternatively, specify inplace=True:
df.rename({'a': 'X', 'b': 'Y'}, axis=1, inplace=True)
df

   X  Y  c  d  e
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x
 

From v0.25, you can also specify errors='raise' to raise errors if an invalid column-to-rename is specified. See v0.25 rename() docs.

REASSIGN COLUMN HEADERS
Use df.set_axis() with axis=1 and inplace=False (to return a copy).
df2 = df.set_axis(['V', 'W', 'X', 'Y', 'Z'], axis=1, inplace=False)
df2

   V  W  X  Y  Z
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x

This returns a copy, but you can modify the DataFrame in-place by setting inplace=True (this is the default behaviour for versions <=0.24 but is likely to change in the future).
You can also assign headers directly:
df.columns = ['V', 'W', 'X', 'Y', 'Z']
df

   V  W  X  Y  Z
0  x  x  x  x  x
1  x  x  x  x  x
2  x  x  x  x  x

    Just assign it to the .columns attribute:
>>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})
>>> df
   $a  $b
0   1  10
1   2  20

>>> df.columns = ['a', 'b']
>>> df
   a   b
0  1  10
1  2  20

    Renaming columns in Pandas is an easy task.
df.rename(columns={'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}, inplace=True)

    The rename method can take a function, for example:

In [11]: df.columns
Out[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)

In [12]: df.rename(columns=lambda x: x[1:], inplace=True)

In [13]: df.columns
Out[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)

    As documented in Working with text data:
df.columns = df.columns.str.replace('$', '')

    Pandas 0.21+ Answer

There have been some significant updates to column renaming in version 0.21. 


The rename method has added the axis parameter which may be set to columns or 1. This update makes this method match the rest of the pandas API. It still has the index and columns parameters but you are no longer forced to use them. 
The set_axis method with the inplace set to False enables you to rename all the index or column labels with a list.


Examples for Pandas 0.21+

Construct sample DataFrame:

df = pd.DataFrame({'$a':[1,2], '$b': [3,4], 
                   '$c':[5,6], '$d':[7,8], 
                   '$e':[9,10]})

   $a  $b  $c  $d  $e
0   1   3   5   7   9
1   2   4   6   8  10


Using rename with axis='columns' or axis=1

df.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis='columns')


or 

df.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis=1)


Both result in the following:

   a  b  c  d   e
0  1  3  5  7   9
1  2  4  6  8  10


It is still possible to use the old method signature:

df.rename(columns={'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'})


The rename function also accepts functions that will be applied to each column name.

df.rename(lambda x: x[1:], axis='columns')


or

df.rename(lambda x: x[1:], axis=1)




Using set_axis with a list and inplace=False

You can supply a list to the set_axis method that is equal in length to the number of columns (or index). Currently, inplace defaults to True, but inplace will be defaulted to False in future releases.

df.set_axis(['a', 'b', 'c', 'd', 'e'], axis='columns', inplace=False)


or

df.set_axis(['a', 'b', 'c', 'd', 'e'], axis=1, inplace=False)




Why not use df.columns = ['a', 'b', 'c', 'd', 'e']?

There is nothing wrong with assigning columns directly like this. It is a perfectly good solution. 

The advantage of using set_axis is that it can be used as part of a method chain and that it returns a new copy of the DataFrame. Without it, you would have to store your intermediate steps of the chain to another variable before reassigning the columns.

# new for pandas 0.21+
df.some_method1()
  .some_method2()
  .set_axis()
  .some_method3()

# old way
df1 = df.some_method1()
        .some_method2()
df1.columns = columns
df1.some_method3()

    Suppose your dataset name is df, and df has.
df = ['$a', '$b', '$c', '$d', '$e']`

So, to rename these, we would simply do.
df.columns = ['a','b','c','d','e']

    Many of pandas functions have an inplace parameter. When setting it True, the transformation applies directly to the dataframe that you are calling it on. For example:
df = pd.DataFrame({'$a':[1,2], '$b': [3,4]})
df.rename(columns={'$a': 'a'}, inplace=True)
df.columns

>>> Index(['a', '$b'], dtype='object')

Alternatively, there are cases where you want to preserve the original dataframe. I have often seen people fall into this case if creating the dataframe is an expensive task. For example, if creating the dataframe required querying a snowflake database. In this case, just make sure the the inplace parameter is set to False.
df = pd.DataFrame({'$a':[1,2], '$b': [3,4]})
df2 = df.rename(columns={'$a': 'a'}, inplace=False)
df.columns
    
>>> Index(['$a', '$b'], dtype='object')

df2.columns

>>> Index(['a', '$b'], dtype='object')

If these types of transformations are something that you do often, you could also look into a number of different pandas GUI tools. I'm the creator of one called Mito. Its a spreadsheet that automatically converts your edits to python code.
    If you already have a list for the new column names, you can try this:
new_cols = ['a', 'b', 'c', 'd', 'e']
new_names_map = {df.columns[i]:new_cols[i] for i in range(len(new_cols))}

df.rename(new_names_map, axis=1, inplace=True)

    Since you only want to remove the $ sign in all column names, you could just do:

df = df.rename(columns=lambda x: x.replace('$', ''))


OR

df.rename(columns=lambda x: x.replace('$', ''), inplace=True)

    Let's understand renaming by a small example...

Renaming columns using mapping:
 df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]}) # Creating a df with column name A and B
 df.rename({""A"": ""new_a"", ""B"": ""new_b""}, axis='columns', inplace =True) # Renaming column A with 'new_a' and B with 'new_b'

 Output:

    new_a  new_b
 0  1       4
 1  2       5
 2  3       6


Renaming index/Row_Name using mapping:
 df.rename({0: ""x"", 1: ""y"", 2: ""z""}, axis='index', inplace =True) # Row name are getting replaced by 'x', 'y', and 'z'.

 Output:

        new_a  new_b
     x  1       4
     y  2       5
     z  3       6



    # This way it will work
import pandas as pd

# Define a dictionary 
rankings = {'test': ['a'],
        'odi': ['E'],
        't20': ['P']}

# Convert the dictionary into DataFrame
rankings_pd = pd.DataFrame(rankings)

# Before renaming the columns
print(rankings_pd)

rankings_pd.rename(columns = {'test':'TEST'}, inplace = True)

    Use:
old_names = ['$a', '$b', '$c', '$d', '$e'] 
new_names = ['a', 'b', 'c', 'd', 'e']
df.rename(columns=dict(zip(old_names, new_names)), inplace=True)

This way you can manually edit the new_names as you wish. It works great when you need to rename only a few columns to correct misspellings, accents, remove special characters, etc.
    df.columns = ['a', 'b', 'c', 'd', 'e']


It will replace the existing names with the names you provide, in the order you provide.
    Let's say this is your dataframe.



You can rename the columns using two methods.


Using dataframe.columns=[#list]

df.columns=['a','b','c','d','e']




The limitation of this method is that if one column has to be changed, full column list has to be passed. Also, this method is not applicable on index labels.
For example, if you passed this:

df.columns = ['a','b','c','d']


This will throw an error. Length mismatch: Expected axis has 5 elements, new values have 4 elements.
Another method is the Pandas rename() method which is used to rename any index, column or row

df = df.rename(columns={'$a':'a'})





Similarly, you can change any rows or columns.
    Column names vs Names of Series
I would like to explain a bit what happens behind the scenes.
Dataframes are a set of Series.
Series in turn are an extension of a numpy.array.
numpy.arrays have a property .name.
This is the name of the series. It is seldom that Pandas respects this attribute, but it lingers in places and can be used to hack some Pandas behaviors.
Naming the list of columns
A lot of answers here talks about the df.columns attribute being a list when in fact it is a Series. This means it has a .name attribute.
This is what happens if you decide to fill in the name of the columns Series:
df.columns = ['column_one', 'column_two']
df.columns.names = ['name of the list of columns']
df.index.names = ['name of the index']

name of the list of columns     column_one  column_two
name of the index
0                                    4           1
1                                    5           2
2                                    6           3

Note that the name of the index always comes one column lower.
Artefacts that linger
The .name attribute lingers on sometimes. If you set df.columns = ['one', 'two'] then the df.one.name will be 'one'.
If you set df.one.name = 'three' then df.columns will still give you ['one', 'two'], and df.one.name will give you 'three'.
BUT
pd.DataFrame(df.one) will return
    three
0       1
1       2
2       3

Because Pandas reuses the .name of the already defined Series.
Multi-level column names
Pandas has ways of doing multi-layered column names. There is not so much magic involved, but I wanted to cover this in my answer too since I don't see anyone picking up on this here.
    |one            |
    |one      |two  |
0   |  4      |  1  |
1   |  5      |  2  |
2   |  6      |  3  |

This is easily achievable by setting columns to lists, like this:
df.columns = [['one', 'one'], ['one', 'two']]

    One line or Pipeline solutions
I'll focus on two things:

OP clearly states

I have the edited column names stored it in a list, but I don't know how to replace the column names.

I do not want to solve the problem of how to replace '$' or strip the first character off of each column header.  OP has already done this step.  Instead I want to focus on replacing the existing columns object with a new one given a list of replacement column names.

df.columns = new where new is the list of new columns names is as simple as it gets.  The drawback of this approach is that it requires editing the existing dataframe's columns attribute and it isn't done inline.  I'll show a few ways to perform this via pipelining without editing the existing dataframe.



Setup 1
To focus on the need to rename of replace column names with a pre-existing list, I'll create a new sample dataframe df with initial column names and unrelated new column names.
df = pd.DataFrame({'Jack': [1, 2], 'Mahesh': [3, 4], 'Xin': [5, 6]})
new = ['x098', 'y765', 'z432']

df

   Jack  Mahesh  Xin
0     1       3    5
1     2       4    6


Solution 1
pd.DataFrame.rename
It has been said already that if you had a dictionary mapping the old column names to new column names, you could use pd.DataFrame.rename.
d = {'Jack': 'x098', 'Mahesh': 'y765', 'Xin': 'z432'}
df.rename(columns=d)

   x098  y765  z432
0     1     3     5
1     2     4     6

However, you can easily create that dictionary and include it in the call to rename.  The following takes advantage of the fact that when iterating over df, we iterate over each column name.
# Given just a list of new column names
df.rename(columns=dict(zip(df, new)))

   x098  y765  z432
0     1     3     5
1     2     4     6

This works great if your original column names are unique.  But if they are not, then this breaks down.

Setup 2
Non-unique columns
df = pd.DataFrame(
    [[1, 3, 5], [2, 4, 6]],
    columns=['Mahesh', 'Mahesh', 'Xin']
)
new = ['x098', 'y765', 'z432']

df

   Mahesh  Mahesh  Xin
0       1       3    5
1       2       4    6


Solution 2
pd.concat using the keys argument
First, notice what happens when we attempt to use solution 1:
df.rename(columns=dict(zip(df, new)))

   y765  y765  z432
0     1     3     5
1     2     4     6

We didn't map the new list as the column names.  We ended up repeating y765.  Instead, we can use the keys argument of the pd.concat function while iterating through the columns of df.
pd.concat([c for _, c in df.items()], axis=1, keys=new) 

   x098  y765  z432
0     1     3     5
1     2     4     6


Solution 3
Reconstruct.  This should only be used if you have a single dtype for all columns.  Otherwise, you'll end up with dtype object for all columns and converting them back requires more dictionary work.
Single dtype
pd.DataFrame(df.values, df.index, new)

   x098  y765  z432
0     1     3     5
1     2     4     6

Mixed dtype
pd.DataFrame(df.values, df.index, new).astype(dict(zip(new, df.dtypes)))

   x098  y765  z432
0     1     3     5
1     2     4     6


Solution 4
This is a gimmicky trick with transpose and set_index.  pd.DataFrame.set_index allows us to set an index inline, but there is no corresponding set_columns.  So we can transpose, then set_index, and transpose back.  However, the same single dtype versus mixed dtype caveat from solution 3 applies here.
Single dtype
df.T.set_index(np.asarray(new)).T

   x098  y765  z432
0     1     3     5
1     2     4     6

Mixed dtype
df.T.set_index(np.asarray(new)).T.astype(dict(zip(new, df.dtypes)))

   x098  y765  z432
0     1     3     5
1     2     4     6


Solution 5
Use a lambda in pd.DataFrame.rename that cycles through each element of new.
In this solution, we pass a lambda that takes x but then ignores it.  It also takes a y but doesn't expect it.  Instead, an iterator is given as a default value and I can then use that to cycle through one at a time without regard to what the value of x is.
df.rename(columns=lambda x, y=iter(new): next(y))

   x098  y765  z432
0     1     3     5
1     2     4     6

And as pointed out to me by the folks in sopython chat, if I add a * in between x and y, I can protect my y variable.  Though, in this context I don't believe it needs protecting.  It is still worth mentioning.
df.rename(columns=lambda x, *, y=iter(new): next(y))

   x098  y765  z432
0     1     3     5
1     2     4     6

    If you just want to remove the '$' sign then use the below code
df.columns = pd.Series(df.columns.str.replace(""$"", """"))

    df.rename(index=str, columns={'A':'a', 'B':'b'})

pandas.DataFrame.rename
    It is real simple. Just use:
df.columns = ['Name1', 'Name2', 'Name3'...]

And it will assign the column names by the order you put them in.
    Another way we could replace the original column labels is by stripping the unwanted characters (here '$') from the original column labels.
This could have been done by running a for loop over df.columns and appending the stripped columns to df.columns.
Instead, we can do this neatly in a single statement by using list comprehension like below:
df.columns = [col.strip('$') for col in df.columns]

(strip method in Python strips the given character from beginning and end of the string.)
    Another option is to rename using a regular expression:

import pandas as pd
import re

df = pd.DataFrame({'$a':[1,2], '$b':[3,4], '$c':[5,6]})

df = df.rename(columns=lambda x: re.sub('\$','',x))
>>> df
   a  b  c
0  1  3  5
1  2  4  6

    I needed to rename features for XGBoost, and it didn't like any of these:
import re
regex = r""[!\""#$%&'()*+,\-.\/:;<=>?@[\\\]^_`{|}~ ]+""
X_trn.columns = X_trn.columns.str.replace(regex, '_', regex=True)
X_tst.columns = X_tst.columns.str.replace(regex, '_', regex=True)

    In addition to the solution already provided, you can replace all the columns while you are reading the file. We can use names and header=0 to do that.

First, we create a list of the names that we like to use as our column names:

import pandas as pd

ufo_cols = ['city', 'color reported', 'shape reported', 'state', 'time']
ufo.columns = ufo_cols

ufo = pd.read_csv('link to the file you are using', names = ufo_cols, header = 0)


In this case, all the column names will be replaced with the names you have in your list.
    Assuming you can use a regular expression, this solution removes the need of manual encoding using a regular expression:
import pandas as pd
import re

srch = re.compile(r""\w+"")

data = pd.read_csv(""CSV_FILE.csv"")
cols = data.columns
new_cols = list(map(lambda v:v.group(), (list(map(srch.search, cols)))))
data.columns = new_cols

    df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]})


If your new list of columns is in the same order as the existing columns, the assignment is simple:

new_cols = ['a', 'b', 'c', 'd', 'e']
df.columns = new_cols
>>> df
   a  b  c  d  e
0  1  1  1  1  1


If you had a dictionary keyed on old column names to new column names, you could do the following:

d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}
df.columns = df.columns.map(lambda col: d[col])  # Or `.map(d.get)` as pointed out by @PiRSquared.
>>> df
   a  b  c  d  e
0  1  1  1  1  1


If you don't have a list or dictionary mapping, you could strip the leading $ symbol via a list comprehension:

df.columns = [col[1:] if col[0] == '$' else col for col in df]

    If you have to deal with loads of columns named by the providing system out of your control, I came up with the following approach that is a combination of a general approach and specific replacements in one go.
First create a dictionary from the dataframe column names using regular expressions in order to throw away certain appendixes of column names and then add specific replacements to the dictionary to name core columns as expected later in the receiving database.
This is then applied to the dataframe in one go.
dict = dict(zip(df.columns, df.columns.str.replace('(:S$|:C1$|:L$|:D$|\.Serial:L$)', '')))
dict['brand_timeseries:C1'] = 'BTS'
dict['respid:L'] = 'RespID'
dict['country:C1'] = 'CountryID'
dict['pim1:D'] = 'pim_actual'
df.rename(columns=dict, inplace=True)

    If you've got the dataframe, df.columns dumps everything into a list you can manipulate and then reassign into your dataframe as the names of columns...
columns = df.columns
columns = [row.replace(""$"", """") for row in columns]
df.rename(columns=dict(zip(columns, things)), inplace=True)
df.head() # To validate the output

Best way? I don't know. A way - yes.
A better way of evaluating all the main techniques put forward in the answers to the question is below using cProfile to gage memory and execution time. @kadee, @kaitlyn, and @eumiro had the functions with the fastest execution times - though these functions are so fast we're comparing the rounding of 0.000 and 0.001 seconds for all the answers. Moral: my answer above likely isn't the 'best' way.
import pandas as pd
import cProfile, pstats, re

old_names = ['$a', '$b', '$c', '$d', '$e']
new_names = ['a', 'b', 'c', 'd', 'e']
col_dict = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}

df = pd.DataFrame({'$a':[1, 2], '$b': [10, 20], '$c': ['bleep', 'blorp'], '$d': [1, 2], '$e': ['texa$', '']})

df.head()

def eumiro(df, nn):
    df.columns = nn
    # This direct renaming approach is duplicated in methodology in several other answers:
    return df

def lexual1(df):
    return df.rename(columns=col_dict)

def lexual2(df, col_dict):
    return df.rename(columns=col_dict, inplace=True)

def Panda_Master_Hayden(df):
    return df.rename(columns=lambda x: x[1:], inplace=True)

def paulo1(df):
    return df.rename(columns=lambda x: x.replace('$', ''))

def paulo2(df):
    return df.rename(columns=lambda x: x.replace('$', ''), inplace=True)

def migloo(df, on, nn):
    return df.rename(columns=dict(zip(on, nn)), inplace=True)

def kadee(df):
    return df.columns.str.replace('$', '')

def awo(df):
    columns = df.columns
    columns = [row.replace(""$"", """") for row in columns]
    return df.rename(columns=dict(zip(columns, '')), inplace=True)

def kaitlyn(df):
    df.columns = [col.strip('$') for col in df.columns]
    return df

print 'eumiro'
cProfile.run('eumiro(df, new_names)')
print 'lexual1'
cProfile.run('lexual1(df)')
print 'lexual2'
cProfile.run('lexual2(df, col_dict)')
print 'andy hayden'
cProfile.run('Panda_Master_Hayden(df)')
print 'paulo1'
cProfile.run('paulo1(df)')
print 'paulo2'
cProfile.run('paulo2(df)')
print 'migloo'
cProfile.run('migloo(df, old_names, new_names)')
print 'kadee'
cProfile.run('kadee(df)')
print 'awo'
cProfile.run('awo(df)')
print 'kaitlyn'
cProfile.run('kaitlyn(df)')

    Here's a nifty little function I like to use to cut down on typing:
def rename(data, oldnames, newname):
    if type(oldnames) == str: # Input can be a string or list of strings
        oldnames = [oldnames] # When renaming multiple columns
        newname = [newname] # Make sure you pass the corresponding list of new names
    i = 0
    for name in oldnames:
        oldvar = [c for c in data.columns if name in c]
        if len(oldvar) == 0:
            raise ValueError(""Sorry, couldn't find that column in the dataset"")
        if len(oldvar) > 1: # Doesn't have to be an exact match
            print(""Found multiple columns that matched "" + str(name) + "": "")
            for c in oldvar:
                print(str(oldvar.index(c)) + "": "" + str(c))
            ind = input('Please enter the index of the column you would like to rename: ')
            oldvar = oldvar[int(ind)]
        if len(oldvar) == 1:
            oldvar = oldvar[0]
        data = data.rename(columns = {oldvar : newname[i]})
        i += 1
    return data

Here is an example of how it works:
In [2]: df = pd.DataFrame(np.random.randint(0, 10, size=(10, 4)), columns = ['col1', 'col2', 'omg', 'idk'])
# First list = existing variables
# Second list = new names for those variables
In [3]: df = rename(df, ['col', 'omg'],['first', 'ohmy'])
Found multiple columns that matched col:
0: col1
1: col2

Please enter the index of the column you would like to rename: 0

In [4]: df.columns
Out[5]: Index(['first', 'col2', 'ohmy', 'idk'], dtype='object')

    Note that the approaches in previous answers do not work for a MultiIndex. For a MultiIndex, you need to do something like the following:
>>> df = pd.DataFrame({('$a','$x'):[1,2], ('$b','$y'): [3,4], ('e','f'):[5,6]})
>>> df
   $a $b  e
   $x $y  f
0  1  3  5
1  2  4  6
>>> rename = {('$a','$x'):('a','x'), ('$b','$y'):('b','y')}
>>> df.columns = pandas.MultiIndex.from_tuples([
        rename.get(item, item) for item in df.columns.tolist()])
>>> df
   a  b  e
   x  y  f
0  1  3  5
1  2  4  6

    ","[2535, 3860, 2416, 123, 479, 241, 188, 27, 20, 18, 148, 33, 13, 76, 90, 25, 41, 41, 6, 21, 17, 18, 12, 6, 8, 6, 20, 9, 20, 6, 10]",4735085,661,2012-07-05T14:21:15,2022-03-29 09:27:43Z,python 
How do I call one constructor from another in Java?,"
                
Is it possible to call a constructor from another (within the same class, not from a subclass)? If yes how? And what could be the best way to call another constructor (if there are several ways to do it)?
    Yes, it is possible:

public class Foo {
    private int x;

    public Foo() {
        this(1);
    }

    public Foo(int x) {
        this.x = x;
    }
}


To chain to a particular superclass constructor instead of one in the same class, use super instead of this. Note that you can only chain to one constructor, and it has to be the first statement in your constructor body.

See also this related question, which is about C# but where the same principles apply.
    Using this(args). The preferred pattern is to work from the smallest constructor to the largest.

public class Cons {

    public Cons() {
        // A no arguments constructor that sends default values to the largest
        this(madeUpArg1Value,madeUpArg2Value,madeUpArg3Value);
    }

    public Cons(int arg1, int arg2) {
       // An example of a partial constructor that uses the passed in arguments
        // and sends a hidden default value to the largest
        this(arg1,arg2, madeUpArg3Value);
    }

    // Largest constructor that does the work
    public Cons(int arg1, int arg2, int arg3) {
        this.arg1 = arg1;
        this.arg2 = arg2;
        this.arg3 = arg3;
    }
}


You can also use a more recently advocated approach of valueOf or just ""of"":

public class Cons {
    public static Cons newCons(int arg1,...) {
        // This function is commonly called valueOf, like Integer.valueOf(..)
        // More recently called ""of"", like EnumSet.of(..)
        Cons c = new Cons(...);
        c.setArg1(....);
        return c;
    }
} 


To call a super class, use super(someValue). The call to super must be the first call in the constructor or you will get a compiler error.
    [Note: I just want to add one aspect, which I did not see in the other answers: how to overcome limitations of the requirement that this() has to be on the first line).]

In Java another constructor of the same class can be called from a constructor via this(). Note however that this has to be on the first line.

public class MyClass {

  public MyClass(double argument1, double argument2) {
    this(argument1, argument2, 0.0);
  }

  public MyClass(double argument1, double argument2, double argument3) {
    this.argument1 = argument1;
    this.argument2 = argument2;
    this.argument3 = argument3;
  }
}


That this has to appear on the first line looks like a big limitation, but you can construct the arguments of other constructors via static methods. For example:

public class MyClass {

  public MyClass(double argument1, double argument2) {
    this(argument1, argument2, getDefaultArg3(argument1, argument2));
  }

  public MyClass(double argument1, double argument2, double argument3) {
    this.argument1 = argument1;
    this.argument2 = argument2;
    this.argument3 = argument3;
  }

  private static double getDefaultArg3(double argument1, double argument2) {
    double argument3 = 0;

    // Calculate argument3 here if you like.

    return argument3;

  }

}

    Using this keyword we can call one constructor in another constructor within same class.
Example :-
 public class Example {
   
      private String name;
   
      public Example() {
          this(""Mahesh"");
      }

      public Example(String name) {
          this.name = name;
      }

 }

    When I need to call another constructor from inside the code (not on the first line), I usually use a helper method like this:

class MyClass {
   int field;


   MyClass() {
      init(0);
   } 
   MyClass(int value) {
      if (value<0) {
          init(0);
      } 
      else { 
          init(value);
      }
   }
   void init(int x) {
      field = x;
   }
}


But most often I try to do it the other way around by calling the more complex constructors from the simpler ones on the first line, to the extent possible. For the above example

class MyClass {
   int field;

   MyClass(int value) {
      if (value<0)
         field = 0;
      else
         field = value;
   }
   MyClass() {
      this(0);
   }
}

    Within a constructor, you can use the this keyword to invoke another constructor in the same class. Doing so is called an explicit constructor invocation. 

Here's another Rectangle class, with a different implementation from the one in the Objects section.

public class Rectangle {
    private int x, y;
    private int width, height;

    public Rectangle() {
        this(1, 1);
    }
    public Rectangle(int width, int height) {
        this( 0,0,width, height);
    }
    public Rectangle(int x, int y, int width, int height) {
        this.x = x;
        this.y = y;
        this.width = width;
        this.height = height;
    }

}


This class contains a set of constructors. Each constructor initializes some or all of the rectangle's member variables.
    Yes, any number of constructors can be present in a class and they can be called by another constructor using this() [Please do not confuse this() constructor call with this keyword]. this() or this(args) should be the first line in the constructor.

Example:

Class Test {
    Test() {
        this(10); // calls the constructor with integer args, Test(int a)
    }
    Test(int a) {
        this(10.5); // call the constructor with double arg, Test(double a)
    }
    Test(double a) {
        System.out.println(""I am a double arg constructor"");
    }
}


This is known as constructor overloading.
Please note that for constructor, only overloading concept is applicable and not inheritance or overriding.
    Yes, you can call constructors from another constructor. For example:
public class Animal {
    private int animalType;

    public Animal() {
        this(1); //here this(1) internally make call to Animal(1);
    }

    public Animal(int animalType) {
        this.animalType = animalType;
    }
}

you can also read in details from
Constructor Chaining in Java
    As everybody already have said, you use this(), which is called an explicit constructor invocation.

However, keep in mind that within such an explicit constructor invocation statement you may not refer to


any instance variables or 
any instance methods or 
any inner classes declared in this class or any superclass, or 
this or 
super.


As stated in JLS (8.8.7.1).
    There are design patterns that cover the need for complex construction - if it can't be done succinctly, create a factory method or a factory class.

With the latest java and the addition of lambdas, it is easy to create a constructor which can accept any initialization code you desire.

class LambdaInitedClass {

   public LamdaInitedClass(Consumer<LambdaInitedClass> init) {
       init.accept(this);
   }
}


Call it with...

 new LambdaInitedClass(l -> { // init l any way you want });

    You can call another constructor via the this(...) keyword (when you need to call a constructor from the same class) or the super(...) keyword
(when you need to call a constructor from a superclass).

However, such a call must be the first statement of your constructor. To overcome this limitation, use this answer.
    Yes it is possible to call one constructor from another. But there is a rule to it. If a call is made from one constructor to another, then 


  that new constructor call must be the first statement in the current constructor


public class Product {
     private int productId;
     private String productName;
     private double productPrice;
     private String category;

    public Product(int id, String name) {
        this(id,name,1.0);
    }

    public Product(int id, String name, double price) {
        this(id,name,price,""DEFAULT"");
    }

    public Product(int id,String name,double price, String category){
        this.productId=id;
        this.productName=name;
        this.productPrice=price;
        this.category=category;
    }
}


So, something like below will not work.

public Product(int id, String name, double price) {
    System.out.println(""Calling constructor with price"");
    this(id,name,price,""DEFAULT"");
}


Also, in the case of inheritance, when sub-class's object is created, the super class constructor is first called.

public class SuperClass {
    public SuperClass() {
       System.out.println(""Inside super class constructor"");
    }
}
public class SubClass extends SuperClass {
    public SubClass () {
       //Even if we do not add, Java adds the call to super class's constructor like 
       // super();
       System.out.println(""Inside sub class constructor"");
    }
}


Thus, in this case also another constructor call is first declared before any other statements.
    Pretty simple 

public class SomeClass{

    private int number;
    private String someString;

    public SomeClass(){
        number = 0;
        someString = new String();
    }

    public SomeClass(int number){
        this(); //set the class to 0
        this.setNumber(number); 
    }

    public SomeClass(int number, String someString){
        this(number); //call public SomeClass( int number )
        this.setString(someString);
    }

    public void setNumber(int number){
        this.number = number;
    }
    public void setString(String someString){
        this.someString = someString;
    }
    //.... add some accessors
}


now here is some small extra credit:

public SomeOtherClass extends SomeClass {
    public SomeOtherClass(int number, String someString){
         super(number, someString); //calls public SomeClass(int number, String someString)
    }
    //.... Some other code.
}


Hope this helps.
    Calling constructor from another constructor

class MyConstructorDemo extends ConstructorDemo
{
    MyConstructorDemo()
    {
        this(""calling another constructor"");
    }
    MyConstructorDemo(String arg)
    {
        System.out.print(""This is passed String by another constructor :""+arg);
    }
}


Also you can call parent constructor by using super() call
    Yes it is possible to call one constructor from another with use of this()

class Example{
   private int a = 1;
   Example(){
        this(5); //here another constructor called based on constructor argument
        System.out.println(""number a is ""+a);   
   }
   Example(int b){
        System.out.println(""number b is ""+b);
   }

    I prefer this way:  

    class User {
        private long id;
        private String username;
        private int imageRes;

    public User() {
        init(defaultID,defaultUsername,defaultRes);
    }
    public User(String username) {
        init(defaultID,username, defaultRes());
    }

    public User(String username, int imageRes) {
        init(defaultID,username, imageRes);
    }

    public User(long id, String username, int imageRes) {
        init(id,username, imageRes);

    }

    private void init(long id, String username, int imageRes) {
        this.id=id;
        this.username = username;
        this.imageRes = imageRes;
    }
}

    The keyword this can be used to call a constructor from a constructor, when writing several constructor for a class, there are times when you'd like to call one constructor from another to avoid duplicate code.

Bellow is a link that I explain other topic about constructor and getters() and setters() and I used a class with two constructors. I hope the explanations and examples help you.

Setter methods or constructors
    You can a constructor from another constructor of same class by using ""this"" keyword.
Example -

class This1
{
    This1()
    {
        this(""Hello"");
        System.out.println(""Default constructor.."");
    }
    This1(int a)
    {
        this();
        System.out.println(""int as arg constructor..""); 
    }
    This1(String s)
    {
        System.out.println(""string as arg constructor.."");  
    }

    public static void main(String args[])
    {
        new This1(100);
    }
}


Output -
string as arg constructor..
Default constructor..
int as arg constructor..
    I will tell you an easy way

There are two types of constructors:


Default constructor
Parameterized constructor


I will explain in one Example

class ConstructorDemo 
{
      ConstructorDemo()//Default Constructor
      {
         System.out.println(""D.constructor "");
      }

      ConstructorDemo(int k)//Parameterized constructor
      {
         this();//-------------(1)
         System.out.println(""P.Constructor =""+k);       
      }

      public static void main(String[] args) 
      {
         //this(); error because ""must be first statement in constructor
         new ConstructorDemo();//-------(2)
         ConstructorDemo g=new ConstructorDemo(3);---(3)    
       }
   }                  


In the above example I showed 3 types of calling


this() call to this must be first statement in constructor
This is Name less Object. this automatically calls the default constructor.
3.This calls the Parameterized constructor.


Note:
this must be the first statement in the constructor.
    I know there are so many examples of this question but what I found I am putting here to share my Idea. there are two ways to chain constructor. In Same class you can use this keyword. in Inheritance, you need to use super keyword.

    import java.util.*;
    import java.lang.*;

    class Test
    {  
        public static void main(String args[])
        {
            Dog d = new Dog(); // Both Calling Same Constructor of Parent Class i.e. 0 args Constructor.
            Dog cs = new Dog(""Bite""); // Both Calling Same Constructor of Parent Class i.e. 0 args Constructor.

            // You need to Explicitly tell the java compiler to use Argument constructor so you need to use ""super"" key word
            System.out.println(""------------------------------"");
            Cat c = new Cat();
            Cat caty = new Cat(""10"");

            System.out.println(""------------------------------"");
            // Self s = new Self();
            Self ss = new Self(""self"");
        }
    }

    class Animal
    {
        String i;

        public Animal()
        {
            i = ""10"";
            System.out.println(""Animal Constructor :"" +i);
        }
        public Animal(String h)
        {
            i = ""20"";
            System.out.println(""Animal Constructor Habit :""+ i);
        }
    }

    class Dog extends Animal
    {
        public Dog()
        {
            System.out.println(""Dog Constructor"");
        }
        public Dog(String h)
        {
            System.out.println(""Dog Constructor with habit"");
        }
    }

    class Cat extends Animal
    {
        public Cat()
        {
            System.out.println(""Cat Constructor"");
        }
        public Cat(String i)
        {
            super(i); // Calling Super Class Paremetrize Constructor.
            System.out.println(""Cat Constructor with habit"");
        }
    }

    class Self
    {
        public Self()
        {
            System.out.println(""Self Constructor"");
        }
        public Self(String h)
        {
            this(); // Explicitly calling 0 args constructor. 
            System.out.println(""Slef Constructor with value"");
        }
    }

    It is called Telescoping Constructor anti-pattern or constructor chaining. Yes, you can definitely do. I see many examples above and I want to add by saying that if you know that you need only two or three constructor, it might be ok. But if you need more, please try to use different design pattern like Builder pattern. As for example: 

 public Omar(){};
 public Omar(a){};
 public Omar(a,b){};
 public Omar(a,b,c){};
 public Omar(a,b,c,d){};
 ...


You may need more. Builder pattern would be a great solution in this case. Here is an article, it might be helpful
https://medium.com/@modestofiguereo/design-patterns-2-the-builder-pattern-and-the-telescoping-constructor-anti-pattern-60a33de7522e
    Originally from an anser by Mirko Klemm, slightly modified to address the question:

Just for completeness: There is also the Instance initialization block that gets executed always and before any other constructor is called. It consists simply of a block of statements ""{ ... }"" somewhere in the body of your class definition.  You can even have more than one.  You can't call them, but they're like ""shared constructor"" code if you want to reuse some code across constructors, similar to calling methods.

So in your case

{ 
  System.out.println(""this is shared constructor code executed before the constructor"");
  field1 = 3;
}


There is also a ""static"" version of this to initialize static members: ""static { ... }""
    It is called constructor chaining. Constructor chaining is the process of calling one constructor from another constructor with respect to the current object. Constructor chaining can be done in two ways:
1.Within the same class: It can be done using this() keyword for constructors in the same class
2.From base class: by using the super() keyword to call a constructor from the base class.
    ","[2528, 3263, 282, 233, 12, 42, 30, 14, 4, 18, 7, 5, 12, 8, 8, 7, 1, 6, 8, 9, 4, 4, 1, 0]",1033967,307,2008-11-12T20:10:19,2022-04-28 09:58:24Z,java 
Extract filename and extension in Bash,"
                
I want to get the filename (without extension) and the extension separately.

The best solution I found so far is:

NAME=`echo ""$FILE"" | cut -d'.' -f1`
EXTENSION=`echo ""$FILE"" | cut -d'.' -f2`


This is wrong because it doesn't work if the file name contains multiple . characters. If, let's say, I have a.b.js, it will consider a and b.js, instead of a.b and js.

It can be easily done in Python with

file, ext = os.path.splitext(path)


but I'd prefer not to fire up a Python interpreter just for this, if possible.

Any better ideas?
    First, get file name without the path:

filename=$(basename -- ""$fullfile"")
extension=""${filename##*.}""
filename=""${filename%.*}""


Alternatively, you can focus on the last '/' of the path instead of the '.' which should work even if you have unpredictable file extensions:

filename=""${fullfile##*/}""


You may want to check the documentation :


On the web at section ""3.5.3 Shell Parameter Expansion""
In the bash manpage at section called ""Parameter Expansion""

    ~% FILE=""example.tar.gz""

~% echo ""${FILE%%.*}""
example

~% echo ""${FILE%.*}""
example.tar

~% echo ""${FILE#*.}""
tar.gz

~% echo ""${FILE##*.}""
gz


For more details, see shell parameter expansion in the Bash manual.
    Usually you already know the extension, so you might wish to use:

basename filename .extension


for example:

basename /path/to/dir/filename.txt .txt


and we get

filename

    You can use the magic of POSIX parameter expansion:

bash-3.2$ FILENAME=somefile.tar.gz
bash-3.2$ echo ""${FILENAME%%.*}""
somefile
bash-3.2$ echo ""${FILENAME%.*}""
somefile.tar




There's a caveat in that if your filename was of the form ./somefile.tar.gz then echo ${FILENAME%%.*} would greedily remove the longest match to the . and you'd have the empty string. 

(You can work around that with a temporary variable:

FULL_FILENAME=$FILENAME
FILENAME=${FULL_FILENAME##*/}
echo ${FILENAME%%.*}


)



This site explains more.

${variable%pattern}
  Trim the shortest match from the end
${variable##pattern}
  Trim the longest match from the beginning
${variable%%pattern}
  Trim the longest match from the end
${variable#pattern}
  Trim the shortest match from the beginning

    pax> echo a.b.js | sed 's/\.[^.]*$//'
a.b
pax> echo a.b.js | sed 's/^.*\.//'
js


works fine, so you can just use:

pax> FILE=a.b.js
pax> NAME=$(echo ""$FILE"" | sed 's/\.[^.]*$//')
pax> EXTENSION=$(echo ""$FILE"" | sed 's/^.*\.//')
pax> echo $NAME
a.b
pax> echo $EXTENSION
js


The commands, by the way, work as follows.

The command for NAME substitutes a ""."" character followed by any number of non-""."" characters up to the end of the line, with nothing (i.e., it removes everything from the final ""."" to the end of the line, inclusive). This is basically a non-greedy substitution using regex trickery.

The command for EXTENSION substitutes a any number of characters followed by a ""."" character at the start of the line, with nothing (i.e., it removes everything from the start of the line to the final dot, inclusive). This is a greedy substitution which is the default action.
    That doesn't seem to work if the file has no extension, or no filename.  Here is what I'm using; it only uses builtins and handles more (but not all) pathological filenames.

#!/bin/bash
for fullpath in ""$@""
do
    filename=""${fullpath##*/}""                      # Strip longest match of */ from start
    dir=""${fullpath:0:${#fullpath} - ${#filename}}"" # Substring from 0 thru pos of filename
    base=""${filename%.[^.]*}""                       # Strip shortest match of . plus at least one non-dot char from end
    ext=""${filename:${#base} + 1}""                  # Substring from len of base thru end
    if [[ -z ""$base"" && -n ""$ext"" ]]; then          # If we have an extension and no base, it's really the base
        base="".$ext""
        ext=""""
    fi

    echo -e ""$fullpath:\n\tdir  = \""$dir\""\n\tbase = \""$base\""\n\text  = \""$ext\""""
done


And here are some testcases:


$ basename-and-extension.sh / /home/me/ /home/me/file /home/me/file.tar /home/me/file.tar.gz /home/me/.hidden /home/me/.hidden.tar /home/me/.. .
/:
    dir  = ""/""
    base = """"
    ext  = """"
/home/me/:
    dir  = ""/home/me/""
    base = """"
    ext  = """"
/home/me/file:
    dir  = ""/home/me/""
    base = ""file""
    ext  = """"
/home/me/file.tar:
    dir  = ""/home/me/""
    base = ""file""
    ext  = ""tar""
/home/me/file.tar.gz:
    dir  = ""/home/me/""
    base = ""file.tar""
    ext  = ""gz""
/home/me/.hidden:
    dir  = ""/home/me/""
    base = "".hidden""
    ext  = """"
/home/me/.hidden.tar:
    dir  = ""/home/me/""
    base = "".hidden""
    ext  = ""tar""
/home/me/..:
    dir  = ""/home/me/""
    base = ""..""
    ext  = """"
.:
    dir  = """"
    base = "".""
    ext  = """"

    Mellen writes in a comment on a blog post:

Using Bash, theres also ${file%.*} to get the filename without the extension and ${file##*.} to get the extension alone. That is,

file=""thisfile.txt""
echo ""filename: ${file%.*}""
echo ""extension: ${file##*.}""


Outputs:

filename: thisfile
extension: txt

    No need to bother with awk or sed or even perl for this simple task. There is a pure-Bash, os.path.splitext()-compatible solution which only uses parameter expansions.

Reference Implementation

Documentation of os.path.splitext(path):


  Split the pathname path into a pair (root, ext) such that root + ext == path, and ext is empty or begins with a period and contains at most one period. Leading periods on the basename are ignored; splitext('.cshrc') returns ('.cshrc', '').


Python code:

root, ext = os.path.splitext(path)


Bash Implementation

Honoring leading periods

root=""${path%.*}""
ext=""${path#""$root""}""


Ignoring leading periods

root=""${path#.}"";root=""${path%""$root""}${root%.*}""
ext=""${path#""$root""}""


Tests

Here are test cases for the Ignoring leading periods implementation, which should match the Python reference implementation on every input.

|---------------|-----------|-------|
|path           |root       |ext    |
|---------------|-----------|-------|
|' .txt'        |' '        |'.txt' |
|' .txt.txt'    |' .txt'    |'.txt' |
|' txt'         |' txt'     |''     |
|'*.txt.txt'    |'*.txt'    |'.txt' |
|'.cshrc'       |'.cshrc'   |''     |
|'.txt'         |'.txt'     |''     |
|'?.txt.txt'    |'?.txt'    |'.txt' |
|'\n.txt.txt'   |'\n.txt'   |'.txt' |
|'\t.txt.txt'   |'\t.txt'   |'.txt' |
|'a b.txt.txt'  |'a b.txt'  |'.txt' |
|'a*b.txt.txt'  |'a*b.txt'  |'.txt' |
|'a?b.txt.txt'  |'a?b.txt'  |'.txt' |
|'a\nb.txt.txt' |'a\nb.txt' |'.txt' |
|'a\tb.txt.txt' |'a\tb.txt' |'.txt' |
|'txt'          |'txt'      |''     |
|'txt.pdf'      |'txt'      |'.pdf' |
|'txt.tar.gz'   |'txt.tar'  |'.gz'  |
|'txt.txt'      |'txt'      |'.txt' |
|---------------|-----------|-------|


Test Results

All tests passed.
    This is the only one that worked for me:

path='folder/other_folder/file.js'

base=${path##*/}
echo ${base%.*}

>> file


This can also be used in string interpolation as well, but unfortunately you have to set base beforehand.
    Smallest and simplest solution (in single line) is:

$ file=/blaabla/bla/blah/foo.txt
echo $(basename ${file%.*}) # foo

    You can use basename.

Example:

$ basename foo-bar.tar.gz .tar.gz
foo-bar


You do need to provide basename with the extension that shall be removed, however if you are always executing tar with -z then you know the extension will be .tar.gz.

This should do what you want:

tar -zxvf $1
cd $(basename $1 .tar.gz)

    The accepted answer works well in typical cases, but fails in edge cases, namely:


For filenames without extension (called suffix in the remainder of this answer), extension=${filename##*.} returns the input filename rather than an empty string.
extension=${filename##*.} does not include the initial ., contrary to convention.


Blindly prepending . would not work for filenames without suffix.

filename=""${filename%.*}"" will be the empty string, if the input file name starts with . and contains no further . characters (e.g., .bash_profile) - contrary to convention. 


---------

Thus, the complexity of a robust solution that covers all edge cases calls for a function - see its definition below; it can return all components of a path.

Example call:

splitPath '/etc/bash.bashrc' dir fname fnameroot suffix
# -> $dir == '/etc'
# -> $fname == 'bash.bashrc'
# -> $fnameroot == 'bash'
# -> $suffix == '.bashrc'


Note that the arguments after the input path are freely chosen, positional variable names.
To skip variables not of interest that come before those that are, specify _ (to use throw-away variable $_) or ''; e.g., to extract filename root and extension only, use splitPath '/etc/bash.bashrc' _ _ fnameroot extension.



# SYNOPSIS
#   splitPath path varDirname [varBasename [varBasenameRoot [varSuffix]]] 
# DESCRIPTION
#   Splits the specified input path into its components and returns them by assigning
#   them to variables with the specified *names*.
#   Specify '' or throw-away variable _ to skip earlier variables, if necessary.
#   The filename suffix, if any, always starts with '.' - only the *last*
#   '.'-prefixed token is reported as the suffix.
#   As with `dirname`, varDirname will report '.' (current dir) for input paths
#   that are mere filenames, and '/' for the root dir.
#   As with `dirname` and `basename`, a trailing '/' in the input path is ignored.
#   A '.' as the very first char. of a filename is NOT considered the beginning
#   of a filename suffix.
# EXAMPLE
#   splitPath '/home/jdoe/readme.txt' parentpath fname fnameroot suffix
#   echo ""$parentpath"" # -> '/home/jdoe'
#   echo ""$fname"" # -> 'readme.txt'
#   echo ""$fnameroot"" # -> 'readme'
#   echo ""$suffix"" # -> '.txt'
#   ---
#   splitPath '/home/jdoe/readme.txt' _ _ fnameroot
#   echo ""$fnameroot"" # -> 'readme'  
splitPath() {
  local _sp_dirname= _sp_basename= _sp_basename_root= _sp_suffix=
    # simple argument validation
  (( $# >= 2 )) || { echo ""$FUNCNAME: ERROR: Specify an input path and at least 1 output variable name."" >&2; exit 2; }
    # extract dirname (parent path) and basename (filename)
  _sp_dirname=$(dirname ""$1"")
  _sp_basename=$(basename ""$1"")
    # determine suffix, if any
  _sp_suffix=$([[ $_sp_basename = *.* ]] && printf %s "".${_sp_basename##*.}"" || printf '')
    # determine basename root (filemane w/o suffix)
  if [[ ""$_sp_basename"" == ""$_sp_suffix"" ]]; then # does filename start with '.'?
      _sp_basename_root=$_sp_basename
      _sp_suffix=''
  else # strip suffix from filename
    _sp_basename_root=${_sp_basename%$_sp_suffix}
  fi
  # assign to output vars.
  [[ -n $2 ]] && printf -v ""$2"" ""$_sp_dirname""
  [[ -n $3 ]] && printf -v ""$3"" ""$_sp_basename""
  [[ -n $4 ]] && printf -v ""$4"" ""$_sp_basename_root""
  [[ -n $5 ]] && printf -v ""$5"" ""$_sp_suffix""
  return 0
}

test_paths=(
  '/etc/bash.bashrc'
  '/usr/bin/grep'
  '/Users/jdoe/.bash_profile'
  '/Library/Application Support/'
  'readme.new.txt'
)

for p in ""${test_paths[@]}""; do
  echo ----- ""$p""
  parentpath= fname= fnameroot= suffix=
  splitPath ""$p"" parentpath fname fnameroot suffix
  for n in parentpath fname fnameroot suffix; do
    echo ""$n=${!n}""
  done
done


Test code that exercises the function:

test_paths=(
  '/etc/bash.bashrc'
  '/usr/bin/grep'
  '/Users/jdoe/.bash_profile'
  '/Library/Application Support/'
  'readme.new.txt'
)

for p in ""${test_paths[@]}""; do
  echo ----- ""$p""
  parentpath= fname= fnameroot= suffix=
  splitPath ""$p"" parentpath fname fnameroot suffix
  for n in parentpath fname fnameroot suffix; do
    echo ""$n=${!n}""
  done
done


Expected output - note the edge cases:


a filename having no suffix
a filename starting with . (not considered the start of the suffix)
an input path ending in / (trailing / is ignored)
an input path that is a filename only (. is returned as the parent path)
a filename that has more than .-prefixed token (only the last is considered the suffix):


----- /etc/bash.bashrc
parentpath=/etc
fname=bash.bashrc
fnameroot=bash
suffix=.bashrc
----- /usr/bin/grep
parentpath=/usr/bin
fname=grep
fnameroot=grep
suffix=
----- /Users/jdoe/.bash_profile
parentpath=/Users/jdoe
fname=.bash_profile
fnameroot=.bash_profile
suffix=
----- /Library/Application Support/
parentpath=/Library
fname=Application Support
fnameroot=Application Support
suffix=
----- readme.new.txt
parentpath=.
fname=readme.new.txt
fnameroot=readme.new
suffix=.txt

    Simply use ${parameter%word}

In your case:

${FILE%.*}


If you want to test it, all following work, and just remove the extension:

FILE=abc.xyz; echo ${FILE%.*};
FILE=123.abc.xyz; echo ${FILE%.*};
FILE=abc; echo ${FILE%.*};

    I think that if you just need the name of the file, you can try this:

FULLPATH=/usr/share/X11/xorg.conf.d/50-synaptics.conf

# Remove all the prefix until the ""/"" character
FILENAME=${FULLPATH##*/}

# Remove all the prefix until the ""."" character
FILEEXTENSION=${FILENAME##*.}

# Remove a suffix, in our case, the filename. This will return the name of the directory that contains this file.
BASEDIRECTORY=${FULLPATH%$FILENAME}

echo ""path = $FULLPATH""
echo ""file name = $FILENAME""
echo ""file extension = $FILEEXTENSION""
echo ""base directory = $BASEDIRECTORY""


And that is all =D.
    $ F = ""text file.test.txt""  
$ echo ${F/*./}  
txt  


This caters for multiple dots and spaces in a filename, however if there is no extension it returns the filename itself. Easy to check for though; just test for the filename and extension being the same.

Naturally this method doesn't work for .tar.gz files. However that could be handled in a two step process. If the extension is gz then check again to see if there is also a tar extension.
    Magic file recognition

In addition to the lot of good answers on this StackOverflow question I would like to add:

Under Linux and other unixen, there is a magic command named file, that do filetype detection by analysing some first bytes of file. This is a very old tool, initialy used for print servers (if not created for... I'm not sure about that).

file myfile.txt
myfile.txt: UTF-8 Unicode text

file -b --mime-type myfile.txt
text/plain


Standards extensions could be found in /etc/mime.types (on my Debian GNU/Linux desktop. See man file and man mime.types. Perhaps you have to install the file utility and mime-support packages):

grep $( file -b --mime-type myfile.txt ) </etc/mime.types
text/plain      asc txt text pot brf srt


You could create a bash function for determining right extension.
There is a little (not perfect) sample:

file2ext() {
    local _mimetype=$(file -Lb --mime-type ""$1"") _line _basemimetype
    case ${_mimetype##*[/.-]} in
        gzip | bzip2 | xz | z )
            _mimetype=${_mimetype##*[/.-]}
            _mimetype=${_mimetype//ip}
            _basemimetype=$(file -zLb --mime-type ""$1"")
            ;;
        stream )
            _mimetype=($(file -Lb ""$1""))
            [ ""${_mimetype[1]}"" = ""compressed"" ] &&
                _basemimetype=$(file -b --mime-type - < <(
                        ${_mimetype,,} -d <""$1"")) ||
                _basemimetype=${_mimetype,,}
            _mimetype=${_mimetype,,}
            ;;
        executable )  _mimetype='' _basemimetype='' ;;
        dosexec )     _mimetype='' _basemimetype='exe' ;;
        shellscript ) _mimetype='' _basemimetype='sh' ;;
        * )
            _basemimetype=$_mimetype
            _mimetype=''
            ;;
    esac
    while read -a _line ;do
        if [ ""$_line"" == ""$_basemimetype"" ] ;then
            [ ""$_line[1]"" ] &&
                _basemimetype=${_line[1]} ||
                _basemimetype=${_basemimetype##*[/.-]}
            break
        fi
        done </etc/mime.types
    case ${_basemimetype##*[/.-]} in
        executable ) _basemimetype='' ;;
        shellscript ) _basemimetype='sh' ;;
        dosexec ) _basemimetype='exe' ;;
        * ) ;;
    esac
    [ ""$_mimetype"" ] && [ ""$_basemimetype"" != ""$_mimetype"" ] &&
      printf ${2+-v} $2 ""%s.%s"" ${_basemimetype##*[/.-]} ${_mimetype##*[/.-]} ||
      printf ${2+-v} $2 ""%s"" ${_basemimetype##*[/.-]}
}


This function could set a Bash variable that can be used later:

(This is inspired from @Petesh right answer):

filename=$(basename ""$fullfile"")
filename=""${filename%.*}""
file2ext ""$fullfile"" extension

echo ""$fullfile -> $filename . $extension""

    No previous answer used a bash regex
Here's a pure bash ERE solution that splits a path into:

The directory path, with its trailing / when present
The regex that discards the trailing / is so much longer that I didn't post it
The filename, excluding the (last) dot extension
The (last) dot extension, with its leading .

The code is meant to handle every possible case, you're welcome to try it.
#!/bin/bash

for path; do

####### the relevant part ######

[[ $path =~ ^(\.{1,2}|.*/\.{0,2})$|^(.*/)([^/]+)(\.[^/]*)$|^(.*/)(.+)$|^(.+)(\..*)$|^(.+)$ ]]

dirpath=""${BASH_REMATCH[1]}${BASH_REMATCH[2]}${BASH_REMATCH[5]}""
filename=""${BASH_REMATCH[3]}${BASH_REMATCH[6]}${BASH_REMATCH[7]}${BASH_REMATCH[9]}""
filext=""${BASH_REMATCH[4]}${BASH_REMATCH[8]}""

# dirpath should be non-null
[[ $dirpath ]] || dirpath='.'

################################

printf '%s=%q\n' \
    path     ""$path"" \
    dirpath  ""$dirpath"" \
    filename ""$filename"" \
    filext   ""$filext""

done

How does it work?
Basically, it ensures that only one sub-expression (delimited with | in the regex) is able to capture the input. Thanks to that, you can concatenate all the capture groups of the same type (for example, the ones related to the directory path) stored in BASH_REMATCH because at most one will be non-null.
Here are the results of an extended but not exhaustive set of examples:
+--------------------------------------------------------+
| input             dirpath        filename       filext |
+--------------------------------------------------------+
''                  .              ''             ''
.                   .              ''             ''
..                  ..             ''             ''
...                 .              ..             .
.file               .              .file          ''
.file.              .              .file          .
.file..             .              .file.         .
.file.Z             .              .file          .Z
.file.sh.Z          .              .file.sh       .Z
file                .              file           ''
file.               .              file           .
file..              .              file.          .
file.Z              .              file           .Z
file.sh.Z           .              file.sh        .Z
dir/                dir/           ''             ''
dir/.               dir/.          ''             ''
dir/...             dir/           ..             .
dir/.file           dir/           .file          ''
dir/.file.          dir/           .file          .
dir/.file..         dir/           .file.         .
dir/.file.Z         dir/           .file          .Z
dir/.file.x.Z       dir/           .file.x        .Z
dir/file            dir/           file           ''
dir/file.           dir/           file           .
dir/file..          dir/           file.          .
dir/file.Z          dir/           file           .Z
dir/file.x.Z        dir/           file.x         .Z
dir./.              dir./.         ''             ''
dir./...            dir./          ..             .
dir./.file          dir./          .file          ''
dir./.file.         dir./          .file          .
dir./.file..        dir./          .file.         .
dir./.file.Z        dir./          .file          .Z
dir./.file.sh.Z     dir./          .file.sh       .Z
dir./file           dir./          file           ''
dir./file.          dir./          file           .
dir./file..         dir./          file.          .
dir./file.Z         dir./          file           .Z
dir./file.x.Z       dir./          file.x         .Z
dir//               dir//          ''             ''
dir//.              dir//.         ''             ''
dir//...            dir//          ..             .
dir//.file          dir//          .file          ''
dir//.file.         dir//          .file          .
dir//.file..        dir//          .file.         .
dir//.file.Z        dir//          .file          .Z
dir//.file.x.Z      dir//          .file.x        .Z
dir//file           dir//          file           ''
dir//file.          dir//          file           .
dir//file..         dir//          file.          .
dir//file.Z         dir//          file           .Z
dir//file.x.Z       dir//          file.x         .Z
dir.//.             dir.//.        ''             ''
dir.//...           dir.//         ..             .
dir.//.file         dir.//         .file          ''
dir.//.file.        dir.//         .file          .
dir.//.file..       dir.//         .file.         .
dir.//.file.Z       dir.//         .file          .Z
dir.//.file.x.Z     dir.//         .file.x        .Z
dir.//file          dir.//         file           ''
dir.//file.         dir.//         file           .
dir.//file..        dir.//         file.          .
dir.//file.Z        dir.//         file           .Z
dir.//file.x.Z      dir.//         file.x         .Z
/                   /              ''             ''
/.                  /.             ''             ''
/..                 /..            ''             ''
/...                /              ..             .
/.file              /              .file          ''
/.file.             /              .file          .
/.file..            /              .file.         .
/.file.Z            /              .file          .Z
/.file.sh.Z         /              .file.sh       .Z
/file               /              file           ''
/file.              /              file           .
/file..             /              file.          .
/file.Z             /              file           .Z
/file.sh.Z          /              file.sh        .Z
/dir/               /dir/          ''             ''
/dir/.              /dir/.         ''             ''
/dir/...            /dir/          ..             .
/dir/.file          /dir/          .file          ''
/dir/.file.         /dir/          .file          .
/dir/.file..        /dir/          .file.         .
/dir/.file.Z        /dir/          .file          .Z
/dir/.file.x.Z      /dir/          .file.x        .Z
/dir/file           /dir/          file           ''
/dir/file.          /dir/          file           .
/dir/file..         /dir/          file.          .
/dir/file.Z         /dir/          file           .Z
/dir/file.x.Z       /dir/          file.x         .Z
/dir./.             /dir./.        ''             ''
/dir./...           /dir./         ..             .
/dir./.file         /dir./         .file          ''
/dir./.file.        /dir./         .file          .
/dir./.file..       /dir./         .file.         .
/dir./.file.Z       /dir./         .file          .Z
/dir./.file.sh.Z    /dir./         .file.sh       .Z
/dir./file          /dir./         file           ''
/dir./file.         /dir./         file           .
/dir./file..        /dir./         file.          .
/dir./file.Z        /dir./         file           .Z
/dir./file.x.Z      /dir./         file.x         .Z
/dir//              /dir//         ''             ''
/dir//.             /dir//.        ''             ''
/dir//...           /dir//         ..             .
/dir//.file         /dir//         .file          ''
/dir//.file.        /dir//         .file          .
/dir//.file..       /dir//         .file.         .
/dir//.file.Z       /dir//         .file          .Z
/dir//.file.x.Z     /dir//         .file.x        .Z
/dir//file          /dir//         file           ''
/dir//file.         /dir//         file           .
/dir//file..        /dir//         file.          .
/dir//file.Z        /dir//         file           .Z
/dir//file.x.Z      /dir//         file.x         .Z
/dir.//.            /dir.//.       ''             ''
/dir.//...          /dir.//        ..             .
/dir.//.file        /dir.//        .file          ''
/dir.//.file.       /dir.//        .file          .
/dir.//.file..      /dir.//        .file.         .
/dir.//.file.Z      /dir.//        .file          .Z
/dir.//.file.x.Z    /dir.//        .file.x        .Z
/dir.//file         /dir.//        file           ''
/dir.//file.        /dir.//        file           .
/dir.//file..       /dir.//        file.          .
/dir.//file.Z       /dir.//        file           .Z
/dir.//file.x.Z     /dir.//        file.x         .Z
//                  //             ''             ''
//.                 //.            ''             ''
//..                //..           ''             ''
//...               //             ..             .
//.file             //             .file          ''
//.file.            //             .file          .
//.file..           //             .file.         .
//.file.Z           //             .file          .Z
//.file.sh.Z        //             .file.sh       .Z
//file              //             file           ''
//file.             //             file           .
//file..            //             file.          .
//file.Z            //             file           .Z
//file.sh.Z         //             file.sh        .Z
//dir/              //dir/         ''             ''
//dir/.             //dir/.        ''             ''
//dir/...           //dir/         ..             .
//dir/.file         //dir/         .file          ''
//dir/.file.        //dir/         .file          .
//dir/.file..       //dir/         .file.         .
//dir/.file.Z       //dir/         .file          .Z
//dir/.file.x.Z     //dir/         .file.x        .Z
//dir/file          //dir/         file           ''
//dir/file.         //dir/         file           .
//dir/file..        //dir/         file.          .
//dir/file.Z        //dir/         file           .Z
//dir/file.x.Z      //dir/         file.x         .Z
//dir./.            //dir./.       ''             ''
//dir./...          //dir./        ..             .
//dir./.file        //dir./        .file          ''
//dir./.file.       //dir./        .file          .
//dir./.file..      //dir./        .file.         .
//dir./.file.Z      //dir./        .file          .Z
//dir./.file.sh.Z   //dir./        .file.sh       .Z
//dir./file         //dir./        file           ''
//dir./file.        //dir./        file           .
//dir./file..       //dir./        file.          .
//dir./file.Z       //dir./        file           .Z
//dir./file.x.Z     //dir./        file.x         .Z
//dir//             //dir//        ''             ''
//dir//.            //dir//.       ''             ''
//dir//...          //dir//        ..             .
//dir//.file        //dir//        .file          ''
//dir//.file.       //dir//        .file          .
//dir//.file..      //dir//        .file.         .
//dir//.file.Z      //dir//        .file          .Z
//dir//.file.x.Z    //dir//        .file.x        .Z
//dir//file         //dir//        file           ''
//dir//file.        //dir//        file           .
//dir//file..       //dir//        file.          .
//dir//file.Z       //dir//        file           .Z
//dir//file.x.Z     //dir//        file.x         .Z
//dir.//.           //dir.//.      ''             ''
//dir.//...         //dir.//       ..             .
//dir.//.file       //dir.//       .file          ''
//dir.//.file.      //dir.//       .file          .
//dir.//.file..     //dir.//       .file.         .
//dir.//.file.Z     //dir.//       .file          .Z
//dir.//.file.x.Z   //dir.//       .file.x        .Z
//dir.//file        //dir.//       file           ''
//dir.//file.       //dir.//       file           .
//dir.//file..      //dir.//       file.          .
//dir.//file.Z      //dir.//       file           .Z
//dir.//file.x.Z    //dir.//       file.x         .Z

As you can see, the behaviour is different from basename and dirname. For example basename dir/ outputs dir while the regex will give you an empty filename for it. Same for . and .., those are considered directories, not filenames.
I timed it with 10000 paths of 256 characters and it took about 1 second, while the equivalent POSIX shell solution is 2x slower and solutions based on wild forking (external calls inside the for loop) are at least 60x slower.
remark: It's not necessary to test paths that contain \n or other notorious characters because all characters are handled the same way by the regex engine of bash. The only characters that would be able to break the current logic are / and ., intermixed or multiplied in a currently unexpected way. When I first posted my answer I found a few border cases that I had to fix; I can't say that the regex is 100% bullet proof but it should be quite robust now.

As an aside, here's the pure POSIX shell solution that yields the same output:
#!/bin/sh

for path; do

####### the relevant part ######

fullname=${path##*/}

case $fullname in
. | ..)
    dirpath=""$path""
    filename=''
    filext=''
    ;;
*)
    dirpath=${path%""$fullname""}
    filename=${fullname#.}
    filename=""${fullname%""$filename""}${filename%.*}""
    filext=${fullname#""$filename""}
    ;;
esac

# dirpath should be non-null
dirpath=${dirpath:-.}

################################

printf '%s=%s\n' \
    path     ""$path"" \
    dirpath  ""$dirpath"" \
    filename ""$filename"" \
    filext   ""$filext""

done


postscript: There are a few points for which some people may disagree with the results given by the above codes:

The special case of dotfiles: The reason is that dotfiles are a UNIX concept.

The special case of . and ..: IMHO it seems obvious to treat them as directories, but most libraries don't do that and force the user to post-process the result.

No support for double-extensions: That's because you'd need a whole database for storing all the valid double-extensions, and above all, because a file extension doesn't mean anything in UNIX; for example you can call a tar archive my_tarred_files and that's completely fine, you'll be able to tar xf my_tarred_files without any problem.


    How to extract the filename and extension in fish:

function split-filename-extension --description ""Prints the filename and extension""
  for file in $argv
    if test -f $file
      set --local extension (echo $file | awk -F. '{print $NF}')
      set --local filename (basename $file .$extension)
      echo ""$filename $extension""
    else
      echo ""$file is not a valid file""
    end
  end
end


Caveats: Splits on the last dot, which works well for filenames with dots in them, but not well for extensions with dots in them. See example below.

Usage:

$ split-filename-extension foo-0.4.2.zip bar.tar.gz
foo-0.4.2 zip  # Looks good!
bar.tar gz  # Careful, you probably want .tar.gz as the extension.


There's probably better ways to do this. Feel free to edit my answer to improve it.



If there's a limited set of extensions you'll be dealing with and you know all of them, try this:

switch $file
  case *.tar
    echo (basename $file .tar) tar
  case *.tar.bz2
    echo (basename $file .tar.bz2) tar.bz2
  case *.tar.gz
    echo (basename $file .tar.gz) tar.gz
  # and so on
end


This does not have the caveat as the first example, but you do have to handle every case so it could be more tedious depending on how many extensions you can expect.
    IMHO the best solution has already been given (using shell parameter expansion) and are the best rated one at this time.

I however add this one which just use dumbs commands, which is not efficient and which noone serious should use ever :

FILENAME=$(echo $FILE | cut -d . -f 1-$(printf $FILE | tr . '\n' | wc -l))
EXTENSION=$(echo $FILE | tr . '\n' | tail -1)


Added just for fun :-)
    Here are some alternative suggestions (mostly in awk), including some advanced use cases, like extracting version numbers for software packages.  

f='/path/to/complex/file.1.0.1.tar.gz'

# Filename : 'file.1.0.x.tar.gz'
    echo ""$f"" | awk -F'/' '{print $NF}'

# Extension (last): 'gz'
    echo ""$f"" | awk -F'[.]' '{print $NF}'

# Extension (all) : '1.0.1.tar.gz'
    echo ""$f"" | awk '{sub(/[^.]*[.]/, """", $0)} 1'

# Extension (last-2): 'tar.gz'
    echo ""$f"" | awk -F'[.]' '{print $(NF-1)"".""$NF}'

# Basename : 'file'
    echo ""$f"" | awk '{gsub(/.*[/]|[.].*/, """", $0)} 1'

# Basename-extended : 'file.1.0.1.tar'
    echo ""$f"" | awk '{gsub(/.*[/]|[.]{1}[^.]+$/, """", $0)} 1'

# Path : '/path/to/complex/'
    echo ""$f"" | awk '{match($0, /.*[/]/, a); print a[0]}'
    # or 
    echo ""$f"" | grep -Eo '.*[/]'

# Folder (containing the file) : 'complex'
    echo ""$f"" | awk -F'/' '{$1=""""; print $(NF-1)}'

# Version : '1.0.1'
    # Defined as 'number.number' or 'number.number.number'
    echo ""$f"" | grep -Eo '[0-9]+[.]+[0-9]+[.]?[0-9]?'

    # Version - major : '1'
    echo ""$f"" | grep -Eo '[0-9]+[.]+[0-9]+[.]?[0-9]?' | cut -d. -f1

    # Version - minor : '0'
    echo ""$f"" | grep -Eo '[0-9]+[.]+[0-9]+[.]?[0-9]?' | cut -d. -f2

    # Version - patch : '1'
    echo ""$f"" | grep -Eo '[0-9]+[.]+[0-9]+[.]?[0-9]?' | cut -d. -f3

# All Components : ""path to complex file 1 0 1 tar gz""
    echo ""$f"" | awk -F'[/.]' '{$1=""""; print $0}'

# Is absolute : True (exit-code : 0)
    # Return true if it is an absolute path (starting with '/' or '~/'
    echo ""$f"" | grep -q '^[/]\|^~/'


All use cases are using the original full path as input, without depending on intermediate results.
    Here is code with AWK. It can be done more simply. But I am not good in AWK.

filename$ ls
abc.a.txt  a.b.c.txt  pp-kk.txt
filename$ find . -type f | awk -F/ '{print $2}' | rev | awk -F""."" '{$1="""";print}' | rev | awk 'gsub("" "",""."") ,sub("".$"", """")'
abc.a
a.b.c
pp-kk
filename$ find . -type f | awk -F/ '{print $2}' | awk -F""."" '{print $NF}'
txt
txt
txt

    A simple answer:

To expand on the POSIX variables answer, note that you can do more interesting patterns. So for the case detailed here, you could simply do this:

tar -zxvf $1
cd ${1%.tar.*}


That will cut off the last occurrence of .tar.<something>.

More generally, if you wanted to remove the last occurrence of .<something>.<something-else> then

${1.*.*}


should work fine.

The link the above answer appears to be dead. Here's a great explanation of a bunch of the string manipulation you can do directly in Bash, from TLDP.
    You could use the cut command to remove the last two extensions (the "".tar.gz"" part):

$ echo ""foo.tar.gz"" | cut -d'.' --complement -f2-
foo




As noted by Clayton Hughes in a comment, this will not work for the actual example in the question. So as an alternative I propose using sed with extended regular expressions, like this:

$ echo ""mpc-1.0.1.tar.gz"" | sed -r 's/\.[[:alnum:]]+\.[[:alnum:]]+$//'
mpc-1.0.1


It works by removing the last two (alpha-numeric) extensions unconditionally.

[Updated again after comment from Anders Lindahl]
    Based largely off of @mklement0's excellent, and chock-full of random, useful bashisms - as well as other answers to this / other questions / ""that darn internet""... I wrapped it all up in a little, slightly more comprehensible, reusable function for my (or your) .bash_profile that takes care of what (I consider) should be a more robust version of dirname/basename / what have you..

function path { SAVEIFS=$IFS; IFS=""""   # stash IFS for safe-keeping, etc.
    [[ $# != 2 ]] && echo ""usage: path <path> <dir|name|fullname|ext>"" && return    # demand 2 arguments
    [[ $1 =~ ^(.*/)?(.+)?$ ]] && {     # regex parse the path
        dir=${BASH_REMATCH[1]}
        file=${BASH_REMATCH[2]}
        ext=$([[ $file = *.* ]] && printf %s ${file##*.} || printf '')
        # edge cases for extensionless files and files like "".nesh_profile.coffee""
        [[ $file == $ext ]] && fnr=$file && ext='' || fnr=${file:0:$((${#file}-${#ext}))}
        case ""$2"" in
             dir) echo      ""${dir%/*}""; ;;
            name) echo      ""${fnr%.*}""; ;;
        fullname) echo ""${fnr%.*}.$ext""; ;;
             ext) echo           ""$ext""; ;;
        esac
    }
    IFS=$SAVEIFS
}     


Usage examples...

SOMEPATH=/path/to.some/.random\ file.gzip
path $SOMEPATH dir        # /path/to.some
path $SOMEPATH name       # .random file
path $SOMEPATH ext        # gzip
path $SOMEPATH fullname   # .random file.gzip                     
path gobbledygook         # usage: -bash <path> <dir|name|fullname|ext>

    You can force cut to display all fields and subsequent ones adding - to field number.

NAME=`basename ""$FILE""`
EXTENSION=`echo ""$NAME"" | cut -d'.' -f2-`


So if FILE is eth0.pcap.gz, the EXTENSION will be pcap.gz

Using the same logic, you can also fetch the file name using '-' with cut as follows :

NAME=`basename ""$FILE"" | cut -d'.' -f-1`


This works even for filenames that do not have any extension.  
    Building from Petesh answer, if only the filename is needed,
both path and extension can be stripped in a single line,

filename=$(basename ${fullname%.*})

    Here is the algorithm I used for finding the name and extension of a file when I wrote a Bash script to make names unique when names conflicted with respect to casing. 

#! /bin/bash 

#
# Finds 
# -- name and extension pairs
# -- null extension when there isn't an extension.
# -- Finds name of a hidden file without an extension
# 

declare -a fileNames=(
  '.Montreal' 
  '.Rome.txt' 
  'Loundon.txt' 
  'Paris' 
  'San Diego.txt'
  'San Francisco' 
  )

echo ""Script ${0} finding name and extension pairs.""
echo 

for theFileName in ""${fileNames[@]}""
do
     echo ""theFileName=${theFileName}""  

     # Get the proposed name by chopping off the extension
     name=""${theFileName%.*}""

     # get extension.  Set to null when there isn't an extension
     # Thanks to mklement0 in a comment above.
     extension=$([[ ""$theFileName"" == *.* ]] && echo "".${theFileName##*.}"" || echo '')

     # a hidden file without extenson?
     if [ ""${theFileName}"" = ""${extension}"" ] ; then
         # hidden file without extension.  Fixup.
         name=${theFileName}
         extension=""""
     fi

     echo ""  name=${name}""
     echo ""  extension=${extension}""
done 


The test run.

$ config/Name\&Extension.bash 
Script config/Name&Extension.bash finding name and extension pairs.

theFileName=.Montreal
  name=.Montreal
  extension=
theFileName=.Rome.txt
  name=.Rome
  extension=.txt
theFileName=Loundon.txt
  name=Loundon
  extension=.txt
theFileName=Paris
  name=Paris
  extension=
theFileName=San Diego.txt
  name=San Diego
  extension=.txt
theFileName=San Francisco
  name=San Francisco
  extension=
$ 


FYI: The complete transliteration program and more test cases can be found here:
https://www.dropbox.com/s/4c6m0f2e28a1vxf/avoid-clashes-code.zip?dl=0
    Ok so if I understand correctly, the problem here is how to get the name and the full extension of a file that has multiple extensions, e.g., stuff.tar.gz.

This works for me:

fullfile=""stuff.tar.gz""
fileExt=${fullfile#*.}
fileName=${fullfile%*.$fileExt}


This will give you stuff as filename and .tar.gz as extension. It works for any number of extensions, including 0. Hope this helps for anyone having the same problem =)
    If you also want to allow empty extensions, this is the shortest I could come up with:

echo 'hello.txt' | sed -r 's/.+\.(.+)|.*/\1/' # EXTENSION
echo 'hello.txt' | sed -r 's/(.+)\..+|(.*)/\1\2/' # FILENAME


1st line explained: It matches PATH.EXT or ANYTHING and replaces it with EXT. If ANYTHING was matched, the ext group is not captured.
    I use the following script

$ echo ""foo.tar.gz""|rev|cut -d""."" -f3-|rev
foo

    ","[2527, 4057, 938, 553, 183, 65, 87, 42, 32, 7, 22, 50, 25, 8, 18, 9, 11, 3, 7, 3, 24, 6, 4, 27, 5, 13, 5, 2, 10, 3, 7]",1944968,935,2009-06-08T14:00:29,2022-03-24 10:15:55Z,bash bash 
How can I remove a key from a Python dictionary?,"
                
Is there a one-line way of deleting a key from a dictionary without raising a KeyError?
if 'key' in my_dict:
    del my_dict['key']

    To delete a key regardless of whether it is in the dictionary, use the two-argument form of dict.pop():
my_dict.pop('key', None)

This will return my_dict[key] if key exists in the dictionary, and None otherwise. If the second parameter is not specified (i.e. my_dict.pop('key')) and key does not exist, a KeyError is raised.
To delete a key that is guaranteed to exist, you can also use:
del my_dict['key']

This will raise a KeyError if the key is not in the dictionary.
    Specifically to answer ""is there a one line way of doing this?""

if 'key' in my_dict: del my_dict['key']


...well, you asked ;-)

You should consider, though, that this way of deleting an object from a dict is not atomicit is possible that 'key' may be in my_dict during the if statement, but may be deleted before del is executed, in which case del will fail with a KeyError.  Given this, it would be safest to either use dict.pop or something along the lines of

try:
    del my_dict['key']
except KeyError:
    pass


which, of course, is definitely not a one-liner.
    It took me some time to figure out what exactly my_dict.pop(""key"", None) is doing. So I'll add this as an answer to save others googling time:

pop(key[, default])
If key is in the dictionary, remove it and return its value, else
return default. If default is not given and key is not in the
dictionary, a KeyError is raised.

Documentation
    You can use a dictionary comprehension to create a new dictionary with that key removed:

>>> my_dict = {k: v for k, v in my_dict.items() if k != 'key'}


You can delete by conditions. No error if key doesn't exist.
    del my_dict[key] is slightly faster than my_dict.pop(key) for removing a key from a dictionary when the key exists

>>> import timeit
>>> setup = ""d = {i: i for i in range(100000)}""

>>> timeit.timeit(""del d[3]"", setup=setup, number=1)
1.79e-06
>>> timeit.timeit(""d.pop(3)"", setup=setup, number=1)
2.09e-06
>>> timeit.timeit(""d2 = {key: val for key, val in d.items() if key != 3}"", setup=setup, number=1)
0.00786


But when the key doesn't exist if key in my_dict: del my_dict[key] is slightly faster than my_dict.pop(key, None). Both are at least three times faster than del in a try/except statement:

>>> timeit.timeit(""if 'missing key' in d: del d['missing key']"", setup=setup)
0.0229
>>> timeit.timeit(""d.pop('missing key', None)"", setup=setup)
0.0426
>>> try_except = """"""
... try:
...     del d['missing key']
... except KeyError:
...     pass
... """"""
>>> timeit.timeit(try_except, setup=setup)
0.133

    If you need to remove a lot of keys from a dictionary in one line of code, I think using map() is quite succinct and Pythonic readable:

myDict = {'a':1,'b':2,'c':3,'d':4}
map(myDict.pop, ['a','c']) # The list of keys to remove
>>> myDict
{'b': 2, 'd': 4}


And if you need to catch errors where you pop a value that isn't in the dictionary, use lambda inside map() like this:

map(lambda x: myDict.pop(x,None), ['a', 'c', 'e'])
[1, 3, None] # pop returns
>>> myDict
{'b': 2, 'd': 4}


or in python3, you must use a list comprehension instead:

[myDict.pop(x, None) for x in ['a', 'c', 'e']]


It works. And 'e' did not cause an error, even though myDict did not have an 'e' key.
    Using the ""del"" keyword:

del dict[key]

    We can delete a key from a Python dictionary by the some of the following approaches.
Using the del keyword; it's almost the same approach like you did though -
 myDict = {'one': 100, 'two': 200, 'three': 300 }
 print(myDict)  # {'one': 100, 'two': 200, 'three': 300}
 if myDict.get('one') : del myDict['one']
 print(myDict)  # {'two': 200, 'three': 300}

Or
We can do like the following:
But one should keep in mind that, in this process actually it won't delete any key from the dictionary rather than making a specific key excluded from that dictionary. In addition, I observed that it returned a dictionary which was not ordered the same as myDict.
myDict = {'one': 100, 'two': 200, 'three': 300, 'four': 400, 'five': 500}
{key:value for key, value in myDict.items() if key != 'one'}

If we run it in the shell, it'll execute something like {'five': 500, 'four': 400, 'three': 300, 'two': 200} - notice that it's not the same ordered as myDict. Again if we try to print myDict, then we can see all keys including which we excluded from the dictionary by this approach. However, we can make a new dictionary by assigning the following statement into a variable:
var = {key:value for key, value in myDict.items() if key != 'one'}

Now if we try to print it, then it'll follow the parent order:
print(var) # {'two': 200, 'three': 300, 'four': 400, 'five': 500}

Or
Using the pop() method.
myDict = {'one': 100, 'two': 200, 'three': 300}
print(myDict)

if myDict.get('one') : myDict.pop('one')
print(myDict)  # {'two': 200, 'three': 300}

The difference between del and pop is that, using pop() method, we can actually store the key's value if needed, like the following:
myDict = {'one': 100, 'two': 200, 'three': 300}
if myDict.get('one') : var = myDict.pop('one')
print(myDict) # {'two': 200, 'three': 300}
print(var)    # 100

Fork this gist for future reference, if you find this useful.
    You can use exception handling if you want to be very verbose:

try: 
    del dict[key]

except KeyError: pass


This is slower, however, than the pop() method, if the key doesn't exist.

my_dict.pop('key', None)


It won't matter for a few keys, but if you're doing this repeatedly, then the latter method is a better bet.

The fastest approach is this:

if 'key' in dict: 
    del myDict['key']


But this method is dangerous because if 'key' is removed in between the two lines, a KeyError will be raised.
    Another way is by using items() + dict comprehension.
items() coupled with dict comprehension can also help us achieve the task of key-value pair deletion, but it has the drawback of not being an in place dict technique. Actually a new dict if created except for the key we dont wish to include.
test_dict = {""sai"" : 22, ""kiran"" : 21, ""vinod"" : 21, ""sangam"" : 21}

# Printing dictionary before removal
print (""dictionary before performing remove is : "" + str(test_dict))

# Using items() + dict comprehension to remove a dict. pair
# removes  vinod
new_dict = {key:val for key, val in test_dict.items() if key != 'vinod'}

# Printing dictionary after removal
print (""dictionary after remove is : "" + str(new_dict))

Output:
dictionary before performing remove is : {'sai': 22, 'kiran': 21, 'vinod': 21, 'sangam': 21}
dictionary after remove is : {'sai': 22, 'kiran': 21, 'sangam': 21}

    Dictionary data type has  a method called dict_name.pop(item) and this can be used to delete a key:value pair from a dictionary.
a={9:4,2:3,4:2,1:3}
a.pop(9)
print(a)

This will give the output as:
{2: 3, 4: 2, 1: 3}

This way you can delete an item from a dictionary in one line.
    You could also use filter with lambda:
>>> d = {'a': 1, 'b': 2, 'c': 3}
>>> dict(filter(lambda x: x[0] != 'a', d.items()))
{'b': 2, 'c': 3}
>>>

    Single filter on key


return ""key"" and remove it from my_dict if ""key"" exists in my_dict
return None if ""key"" doesn't exist in my_dict



  this will change my_dict in place (mutable)


my_dict.pop('key', None)


Multiple filters on keys


  generate a new dict (immutable)


dic1 = {
    ""x"":1,
    ""y"": 2,
    ""z"": 3
}

def func1(item):
    return  item[0]!= ""x"" and item[0] != ""y""

print(
    dict(
        filter(
            lambda item: item[0] != ""x"" and item[0] != ""y"", 
            dic1.items()
            )
    )
)

    just create a copy of your dictionary .
newMy_dict = my_dict.copy()
if 'key' in newMy_dict :
    del newMy_dict['key']


This way , you can control exception.
    I prefer the immutable version

foo = {
    1:1,
    2:2,
    3:3
}
removeKeys = [1,2]
def woKeys(dct, keyIter):
    return {
        k:v
        for k,v in dct.items() if k not in keyIter
    }

>>> print(woKeys(foo, removeKeys))
{3: 3}
>>> print(foo)
{1: 1, 2: 2, 3: 3}

    Try this
if key in data:
   del data[key]

    ","[2527, 4038, 418, 181, 52, 74, 61, 19, 10, 8, 5, 5, 2, 3, 1, 3, 0]",2276356,302,2012-06-30T20:27:59,2022-04-19 17:39:10Z,python 
When to use margin vs padding in CSS [closed],"
                    
            
        
            
                
                    
                        Closed. This question is opinion-based. It is not currently accepting answers.
                        
                    
                
            
        
            
        
                
                    
                
            
                
                    Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                
                    Closed 3 years ago.

            
        
            
                    
                        Improve this question
                    
            

    

When writing CSS, is there a particular rule or guideline that should be used in deciding when to use margin and when to use padding?
    TL;DR: By default I use margin everywhere, except when I have a border or background and want to increase the space inside that visible box.

To me, the biggest difference between padding and margin is that vertical margins auto-collapse, and padding doesn't. 

Consider two elements one above the other each with padding of 1em. This padding is considered to be part of the element and is always preserved. 

So you will end up with the content of the first element, followed by the padding of the first element, followed by the padding of the second, followed by the content of the second element. 

Thus the content of the two elements will end up being 2em apart.

Now replace that padding with 1em margin. Margins are considered to be outside of the element, and margins of adjacent items will overlap. 

So in this example, you will end up with the content of the first element followed by 1em of combined margin followed by the content of the second element. So the content of the two elements is only 1em apart. 

This can be really useful when you know that you want to say 1em of spacing around an element, regardless of what element it is next to.

The other two big differences are that padding is included in the click region and background color/image, but not the margin.

div.box > div { height: 50px; width: 50px; border: 1px solid black; text-align: center; }
div.padding > div { padding-top: 20px; }
div.margin > div { margin-top: 20px; }<h3>Default</h3>
<div class=""box"">
  <div>A</div>
  <div>B</div>
  <div>C</div>
</div>

<h3>padding-top: 20px</h3>
<div class=""box padding"">
  <div>A</div>
  <div>B</div>
  <div>C</div>
</div>

<h3>margin-top: 20px; </h3>
<div class=""box margin"">
  <div>A</div>
  <div>B</div>
  <div>C</div>
</div>

    Margin is on the outside of block elements while padding is on the inside.


Use margin to separate the block from things outside it 
Use padding to move the contents away from the edges of the block.



    There are more technical explanations for your question, but if you want a way to think about margin and padding, this analogy might help.

Imagine block elements as picture frames hanging on a wall:


The photo is the content.
The matting is the padding.
The frame moulding is the border.
The wall is the viewport.
The space between two frames is the margin.


With this in mind, a good rule of thumb is to use margin when you want to space an element in relationship to other elements on the wall, and padding when you're adjusting the appearance of the element itself. Margin won't change the size of the element, but padding will make the element bigger1.



1 You can alter this behavior with the box-sizing attribute.
    The best I've seen explaining this with examples, diagrams, and even a 'try it yourself' view is here.  

The diagram below I think gives an instant visual understanding of the difference.



One thing to keep in mind is standards compliant browsers (IE quirks is an exception) render only the content portion to the given width, so keep track of this in layout calculations.  Also note that border box is seeing somewhat of a comeback with Bootstrap 3 supporting it.
    It's good to know the differences between margin and padding. Here are some differences:


Margin is outer space of an element, while padding is inner space of an element. 
Margin is the space outside the border of an element, while padding is the space inside the border of it.
Margin accepts the value of auto: margin: auto, but you can't set padding to auto.
Margin can be set to any number, but padding must be non-negative.
When you style an element, padding will also be affected (e.g. background color), but not margin.

    From https://www.w3schools.com/css/css_boxmodel.asp


  Explanation of the different parts:
  
  
  Content - The content of the box, where text and images appear
  Padding - Clears an area around the content. The padding is transparent
  Border - A border that goes around the padding and content
  Margin - Clears an area outside the border. The margin is transparent
  
  
  


Live example (play around by changing the values):
https://www.w3schools.com/css/tryit.asp?filename=trycss_boxmodel
    MARGIN vs PADDING :


Margin is used in an element to create distance between that element and other elements of page. Where padding is used to create distance between content and border of an element.
Margin is not part of an element where padding is part of element.


Please refer below image extracted from Margin Vs Padding - CSS Properties


    Here is some HTML that demonstrates how padding and margin affect clickability, and background filling.  An object receives clicks to its padding, but clicks on an objects margin'd area go to its parent.  

$("".outer"").click(function(e) {
  console.log(""outer"");
  e.stopPropagation();
});

$("".inner"").click(function(e) {
  console.log(""inner"");
  e.stopPropagation();
});.outer {
  padding: 10px;
  background: red;
}

.inner {
  margin: 10px;
  padding: 10px;
  background: blue;
  border: solid white 1px;
}<script src=""http://code.jquery.com/jquery-latest.js""></script>

<div class=""outer"">
  <div class=""inner"" style=""position:relative; height:0px; width:0px"">

  </div>
</div>

    Advanced Margin versus Padding Explained

It is inappropriate to use padding to space content in an element; you must utilize margin on the child element instead. Older browsers such as Internet Explorer misinterpreted the box model except when it came to using margin which works perfectly in Internet Explorer 4.

There are two exceptions when using padding is appropriate to use:


It is applied to an inline element which can not contain any child elements such as an input element.
You are compensating for a highly miscellaneous browser bug which a vendor *cough* Mozilla *cough* refuses to fix and are certain (to the degree that you hold regular exchanges with W3C and WHATWG editors) that you must have a working solution and this solution will not effect the styling of anything other then the bug you are compensating for.


When you have a 100% width element with padding: 50px; you effectively get width: calc(100% + 100px);. Since margin is not added to the width it will not cause unexpected layout problems when you use margin on child elements instead of padding directly on the element.

So if you're not doing one of those two things do not add padding to the element but to it's direct child/children element(s) to ensure you're going to get the expected behavior in all browsers.
    The thing about margins is that you don't need to worry about the element's width.

Like when you give something {padding: 10px;}, you'll have to reduce the width of the element by 20px to keep the 'fit' and not disturb other elements around it.

So I generally start off by using paddings to get everything 'packed' and then use margins for minor tweaks. 

Another thing to be aware of is that paddings are more consistent on different browsers and IE doesn't treat negative margins very well.
    I always use this principle:



This is the box model from the inspect element feature in Firefox. It works like an onion: 


Your content is in the middle.
Padding is space between your content and edge of the tag it is
inside.
The border and its specifications
The margin is the space around the tag.


So bigger margins will make more space around the box that contains your content.

Larger padding will increase the space between your content and the box of which it is inside.

Neither of them will increase or decrease the size of the box if it is set to a specific value.
    Margin

Margin is usually used to create a space between the element itself and its surround.

for example I use it when I'm building a navbar to make it sticks to the edges of the screen and for no white gap.

Padding

I usually use when I've an element inside a border, <div> or something similar, and I want to decrease its size but at the time I want to keep the distance or the margin between the other elements around it.  

So briefly, it's situational; it depends on what you are trying to do.
    The margin clears an area around an element (outside the border), but the padding clears an area around the content (inside the border) of an element.



it means that your element does not know about its outside margins, so if you are developing dynamic web controls, I recommend that to use padding vs margin if you can.

note that some times you have to use margin.
    First let's look at what are the differences and what each responsibility is:

1) Margin 
The CSS margin properties are used to generate space around elements.
The margin properties set the size of the white space outside the
border. With CSS, you have full control over the margins. There are
CSS properties for setting the margin for each side of an element
(top, right, bottom, and left).



2) Padding 
The CSS padding properties are used to generate space around content.
The padding clears an area around the content (inside the border) of
an element. With CSS, you have full control over the padding. There
are CSS properties for setting the padding for each side of an element
(top, right, bottom, and left).

So simply Margins are space around elements, while Padding are space around content which are part of the element.

This image from codemancers shows how margin and borders get togther and how border box and content-box make it different.
Also they define each section as below:


Content - this defines the content area of the box where the actual    content like text, images or maybe other elements reside.
Padding -    this clears the main content from its containing box.
Border - this    surrounds both content and padding.
Margin - this area defines a    transparent space that separates it from other elements.


    One thing to note is when auto collapsing margins annoy you (and you are not using background colours on your elements), something it's just easier to use padding.
    Margin is outside the box and padding is inside the box
    ","[2524, 1744, 1554, 126, 610, 42, 55, 90, 36, 18, 34, 9, 7, 30, 14, 18, -3]",937833,765,2010-02-03T03:20:12,2020-06-17 14:02:08Z,css 
How to mkdir only if a directory does not already exist?,"
                
I am writing a shell script to run under the KornShell (ksh) on AIX. I would like to use the mkdir command to create a directory. But the directory may already exist, in which case I do not want to do anything. So I want to either test to see that the directory does not exist, or suppress the ""File exists"" error that mkdir throws when it tries to create an existing directory. 

How can I best do this?
    Try mkdir -p:

mkdir -p foo


Note that this will also create any intermediate directories that don't exist; for instance,

mkdir -p foo/bar/baz


will create directories foo, foo/bar, and foo/bar/baz if they don't exist.

Some implementation like GNU mkdir include mkdir --parents as a more readable alias, but this is not specified in POSIX/Single Unix Specification and not available on many common platforms like macOS, various BSDs, and various commercial Unixes, so it should be avoided.

If you want an error when parent directories don't exist, and want to create the directory if it doesn't exist, then you can test for the existence of the directory first:

[ -d foo ] || mkdir foo

    This should work:

$ mkdir -p dir


or:

if [[ ! -e $dir ]]; then
    mkdir $dir
elif [[ ! -d $dir ]]; then
    echo ""$dir already exists but is not a directory"" 1>&2
fi


which will create the directory if it doesn't exist, but warn you if the name of the directory you're trying to create is already in use by something other than a directory.
    Use the -p flag.  

man mkdir
mkdir -p foo

    mkdir foo works even if the directory exists.
To make it work only if the directory named ""foo"" does not exist, try using the -p flag.

Example:

mkdir -p foo


This will create the directory named ""foo"" only if it does not exist. :)
    Defining complex directory trees with one command

mkdir -p project/{lib/ext,bin,src,doc/{html,info,pdf},demo/stat/a}

    Simple, silent and deadly:
mkdir -p /my/new/dir >/dev/null 2>&1

    If you don't want to show any error message:

[ -d newdir ] || mkdir newdir


If you want to show your own error message:

[ -d newdir ] && echo ""Directory Exists"" || mkdir newdir

    You can either use an if statement to check if the directory exists or not. If it does not exits, then create the directory.


dir=/home/dir_name

if [ ! -d $dir ]
then
     mkdir $dir
else
     echo ""Directory exists""
fi

You can directory use mkdir with -p option to create a directory. It will check if the directory is not available it will.

mkdir -p $dir


mkdir -p also allows to create the tree structure of the directory. If you want to create the parent and child directories using same command, can opt mkdir -p

mkdir -p /home/parent_dir /home/parent_dir/child1 /home/parent_dir/child2


    The old tried and true

mkdir /tmp/qq >/dev/null 2>&1


will do what you want with none of the race conditions many of the other solutions have.

Sometimes the simplest (and ugliest) solutions are the best.
    This is a simple function (Bash shell) which lets you create a directory if it doesn't exist.
#------------------------------------------#
# Create a directory if it does not exist. #
#------------------------------------------#
# Note the ""-p"" option in the mkdir        #
# command which creates directories        #
# recursively.                             #
#------------------------------------------#
createDirectory() {
   mkdir -p -- ""$1""
}

You can call the above function as:

createDirectory ""$(mktemp -d dir-example.XXXXX)/fooDir/BarDir""

The above creates fooDir and BarDir if they don't exist. Note the ""-p"" option in the mkdir command which creates directories recursively.
    mkdir does not support -p switch anymore on Windows 8+ systems.

You can use this:

IF NOT EXIST dir_name MKDIR dir_name

    directory_name = ""foo""

if [ -d $directory_name ]
then
    echo ""Directory already exists""
else
    mkdir $directory_name
fi

    Referring to man page man mkdir for option -p

   -p, --parents
          no error if existing, make parent directories as needed


which will create all directories in a given path, if exists throws no error otherwise it creates all directories from left to right in the given path. Try the below command. the directories newdir and anotherdir doesn't exists before issuing this command

Correct Usage

mkdir -p /tmp/newdir/anotherdir

After executing the command you can see newdir and anotherdir created under /tmp. You can issue this command as many times you want, the command always have exit(0). Due to this reason most people use this command in shell scripts before using those actual paths.
    mkdir -p sam



mkdir = Make Directory
-p    = --parents
(no error if existing, make parent directories as needed)

    Or if you want to check for existence first:

if [[ ! -e /path/to/newdir ]]; then
            mkdir /path/to/newdir
fi


-e is the exist test for KornShell.

You can also try googling a KornShell manual.
    if [ !-d $dirName ];then
     if ! mkdir $dirName; then  # Shorter version. Shell will complain if you put braces here though
     echo ""Can't make dir: $dirName""
     fi
fi

    ","[2518, 4158, 223, 107, 30, 84, 11, 49, 17, 27, 10, 16, 14, 10, 3, 10, 2]",1614898,284,2009-04-27T14:47:44,2022-01-21 13:58:02Z,
